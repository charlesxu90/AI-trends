title,year,source,authors,class,keywords,abstract,pdf_link
$\alpha$-VAEs : Optimising variational inference by learning data-dependent divergence skew,2021,ICML,"['Jacob Deasy', 'Tom Andrew McIver', 'Nikola Simidjievski', 'Pietro Lio']",poster,"['variational inference', 'variational autoencoders', 'constrained optimisation', 'lossy compression']"," The {\em skew-geometric Jensen-Shannon divergence} $\left(\textrm{JS}^{\textrm{G}_{\alpha}}\right)$ allows for an intuitive interpolation between forward and reverse Kullback-Leibler (KL) divergence based on the skew parameter $\alpha$. While the benefits of the skew in $\textrm{JS}^{\textrm{G}_{\alpha}}$ are clear---balancing forward/reverse KL in a comprehensible manner---the choice of optimal skew remains opaque and requires an expensive grid search. In this paper we introduce $\alpha$-VAEs, which extend the $\textrm{JS}^{\textrm{G}_{\alpha}}$ variational autoencoder by allowing for learnable, and therefore data-dependent, skew. We motivate the use of a parameterised skew in the dual divergence by analysing trends dependent on data complexity in synthetic examples. We also prove and discuss the dependency of the divergence minimum on the input data and encoder parameters, before empirically demonstrating that this dependency does not reduce to either direction of KL divergence for benchmark datasets. Finally, we demonstrate that optimised skew values consistently converge across a range of initial values and provide improved denoising and reconstruction properties. These render $\alpha$-VAEs an efficient and practical modelling choice across a range of tasks, datasets, and domains.",https://api.openreview.net/pdf/9cca0f9730a6c965eccb505c33d224dfc2ba1f3d.pdf
Diffusion Priors In Variational Autoencoders,2021,ICML,"['Antoine Wehenkel', 'Gilles Louppe']",poster,"['Diffusion models', 'VAE', 'autoencoder', 'variational inference', 'likelihood based models', 'prior modelling.']","Among likelihood-based approaches for deep generative modelling, variational autoencoders (VAEs) offer scalable amortized posterior inference and fast sampling. However, VAEs are also more and more outperformed by competing models such as normalizing flows (NFs), deep-energy models, or the new denoising diffusion probabilistic models (DDPMs). In this preliminary work, we improve VAEs by demonstrating how DDPMs can be used for modelling the prior distribution of the latent variables. The diffusion prior model improves upon Gaussian priors of classical VAEs and is competitive with NF-based priors. Finally, we hypothesize that hierarchical VAEs could similarly benefit from the enhanced capacity of diffusion priors.",https://api.openreview.net/pdf/70738d10fa99d506b0cfbf46d30478dc1126099c.pdf
Challenges for BBVI with Normalizing Flows,2021,ICML,"['Akash Kumar Dhaka', 'Alejandro Catalina', 'Manushi Welandawe', 'Michael Riis Andersen', 'Jonathan H. Huggins', 'Aki Vehtari']",poster,"['BBVI', 'Variational Inference', 'Importance Sampling', 'Normalizing Flows', 'Pre-asymptotics', 'Stochastic Optimisation']","Current black-box variational inference (BBVI) methods require the user to make numerous design choices---such as the selection of variational objective and approximating family---yet there is little principled guidance on how to do so. We develop a conceptual framework and set of experimental tools to understand the effects of these choices, which we leverage to propose best practices for maximizing posterior approximation accuracy. Our approach is based on studying the pre-asymptotic tail behavior of the density ratios between the joint distribution and the variational approximation, then exploiting insights and tools from the importance sampling literature. We focus on normalizing flow models and give recommendations on how to be used(and diagnostics) in BBVI, though we are not limited to them.",https://api.openreview.net/pdf/22189c22daa3670a13b9261eb7f3a58d44313a6a.pdf
Beyond In-Place Corruption: Insertion and Deletion In Denoising Probabilistic Models,2021,ICML,"['Daniel D. Johnson', 'Jacob Austin', 'Rianne van den Berg', 'Daniel Tarlow']",poster,"['Diffusion', 'Masking', 'Sequence', 'Structure', 'Machine Learning']","Denoising diffusion probabilistic models (DDPMs) have shown impressive results on sequence generation by iteratively corrupting each example and then learning to map corrupted versions back to the original. However, previous work has largely focused on in-place corruption, adding noise to each pixel or token individually while keeping their locations the same. In this work, we consider a broader class of corruption processes and denoising models over sequence data that can insert and delete elements, while still being efficient to train and sample from. We demonstrate that these models outperform standard in-place models on an arithmetic sequence task, and that when trained on the text8 dataset they can be used to fix spelling errors without any fine-tuning.",https://api.openreview.net/pdf/650b60e5ea033d47ff50059103d6402294bf5a1d.pdf
RAD-TTS: Parallel Flow-Based TTS with Robust Alignment Learning and Diverse Synthesis,2021,ICML,"['Kevin J. Shih', 'Rafael Valle', 'Rohan Badlani', 'Adrian Lancucki', 'Wei Ping', 'Bryan Catanzaro']",poster,"['text to speech', 'audio synthesis', 'TTS', 'normalizing flows']","This work introduces a predominantly parallel, end-to-end TTS model based on normalizing flows.
 It extends prior parallel approaches by additionally modeling speech rhythm as a separate generative distribution to facilitate variable token duration during inference. We further propose a robust framework for the on-line extraction of speech-text alignments -- a critical yet highly unstable learning problem in end-to-end TTS frameworks. Our experiments demonstrate that our proposed techniques yield improved alignment quality, better output diversity compared to controlled baselines.",https://api.openreview.net/pdf/ad9861f3176e1b1c5a7c7554ef9e2fbf3243a198.pdf
Improving Continuous Normalizing Flows using a Multi-Resolution Framework,2021,ICML,"['Vikram Voleti', 'Chris Finlay', 'Adam M Oberman', 'Christopher Pal']",poster,"['generative models', 'continuous normalizing flows']","Recent work has shown that Continuous Normalizing Flows (CNFs) can serve as generative models of images with exact likelihood calculation and invertible generation/density estimation. In this work we introduce a Multi-Resolution variant of such models (MRCNF). We introduce a transformation between resolutions that allows for no change in the log likelihood. We show that this approach yields comparable likelihood values for various image datasets, with improved performance at higher resolutions, with fewer parameters, using only 1 GPU.",https://api.openreview.net/pdf/bc0b32810379947c649b4690b80d5366756b68ae.pdf
Equivariant Manifold Flows,2021,ICML,"['Isay Katsman', 'Aaron Lou', 'Derek Lim', 'Qingxuan Jiang', 'Ser-Nam Lim', 'Christopher De Sa']",poster,"['manifold', 'normalizing flow', 'equivariant', 'invariant']","Tractably modelling distributions over manifolds has long been an important goal in the natural sciences. Recent work has focused on developing general machine learning models to learn such distributions. However, for many applications these distributions must respect manifold symmetries---a trait which most previous models disregard. In this paper, we lay the theoretical foundations for learning symmetry-invariant distributions on arbitrary manifolds via equivariant manifold flows. We demonstrate the utility of our approach by using it to learn gauge invariant densities over SU(n) in the context of quantum field theory.",https://api.openreview.net/pdf/d9a7b4893d80e0d5bdd26b0032eda2f3cf9a41de.pdf
Agent Forecasting at Flexible Horizons using ODE Flows,2021,ICML,"['Alexander Radovic', 'Jiawei He', 'Janahan Ramanan', 'Marcus A Brubaker', 'Andreas Lehrmann']",poster,"['conditional normalizing flows', 'neural odes', 'time series forecasting', 'agent forecasting']","In this work we describe OMEN, a neural ODE based normalizing flow for the prediction of marginal distributions at flexible evaluation horizons, and apply it to agent position forecasting.
 OMEN's architecture embeds an assumption that marginal distributions of a given agent moving forward in time are related, allowing for an efficient representation of marginal distributions through time and allowing for reliable interpolation between prediction horizons seen in training.
 Experiments on a popular agent forecasting dataset demonstrate significant improvements over most baseline approaches, and comparable performance to the state of the art while providing the new functionality of reliable interpolation of predicted marginal distributions between prediction horizons as demonstrated with synthetic data.",https://api.openreview.net/pdf/25971ce38e24dfe18d04b8472335f90dd9f91d2e.pdf
The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders,2021,ICML,"['Andrej Risteski', 'Divyansh Pareek']",poster,"['invertibility', 'variational autoencoders', 'encoder', 'representational complexity', 'Langevin']","Training and using modern neural-network based latent-variable generative models (like Variational Autoencoders) often require simultaneously training a generative direction along with an inferential (encoding) direction, which approximates the posterior distribution over the latent variables. Thus, the question arises: how complex does the inferential model need to be, in order to be able to accurately model the posterior distribution of a given generative model?  In this paper, we identify an important property of the generative map impacting the required size of the encoder. We show that if the generative map is ``strongly invertible"" (in a sense we suitably formalize), the inferential model need not be much more complex. Conversely, we prove that there exist non-invertible generative maps, for which the encoding direction needs to be exponentially larger (under standard assumptions in computational complexity). Importantly, we do not require the generative model to be layerwise invertible, which a lot of the related literature assumes and isn't satisfied by many architectures used in practice (e.g. convolution and pooling based networks). Thus, we provide theoretical support for the empirical wisdom that learning deep generative models is harder when data lies on a low-dimensional manifold.",https://api.openreview.net/pdf/56adb8cfd7168a71d8c4d318a2a4a3ec18e5dff8.pdf
Discrete Tree Flows via Tree-Structured Permutations,2021,ICML,"['Mai Elkady', 'Jim Lim', 'David I. Inouye']",poster,"['Discrete Normalizing Flows', 'Discrete Data', 'Normalizing flows', 'Decision Tree Algorithms']","While normalizing flows for continuous data have been extensively researched, flows for discrete data have only recently been explored. These prior models, however, suffer from limitations that are distinct from those of continuous flows. Most notably, discrete flow-based models cannot be straightforwardly optimized with conventional deep learning methods because gradients of discrete functions are undefined or zero, and backpropagation can be computationally burdensome compared to alternative discrete algorithms such as decision tree algorithms.  Previous works approximate pseudo-gradients of the discrete functions but do not solve the problem on a fundamental level. Our approach seeks to reduce computational burden and remove the need for pseudo-gradients by developing a discrete flow based on decision trees---building upon the success of efficient tree-based methods for classification and regression for discrete data. We first define a tree-structured permutation (TSP) that compactly encodes a permutation of discrete data where the inverse is easy to compute; thus, we can efficiently compute the density value and sample new data.  We then propose a decision tree algorithm to learn TSPs that estimates the tree structure and simple permutations at each node via a novel criteria. We empirically demonstrate the feasibility of our method on multiple datasets.",https://api.openreview.net/pdf/1884172d8486faa2a649d99369cb988a3a8ed018.pdf
Deep Signature Statistics for Likelihood-free Time-series Models,2021,ICML,"['Joel Dyer', 'Patrick W Cannon', 'Sebastian M Schmon']",poster,[],"Simulation-based inference (SBI) has emerged as a family of methods for performing inference on complex simulation models with intractable likelihood functions. A common bottleneck in SBI is the construction of low-dimensional summary statistics of the data. In this respect, time-series data, often being high-dimensional, multivariate, and complex in structure, present a particular challenge. To address this we introduce deep signature statistics, a principled and automated method for combining summary statistic selection for time-series data with neural SBI methods. Our approach leverages deep signature transforms, trained concurrently with a neural density estimator, to produce informative statistics for multivariate sequential data that encode important geometric properties of the underlying path. We obtain competitive results across benchmark models.",https://api.openreview.net/pdf/710130b7b994b6c4c12e77d53348470bd1aba8e4.pdf
On the expressivity of bi-Lipschitz normalizing flows,2021,ICML,"['Alexandre Verine', 'Yann Chevaleyre', 'Fabrice Rossi', 'benjamin negrevergne']",poster,"['Machine Learning', 'Invertible Networks', 'Normalizing Flow', 'Bilipschitz', 'Bi-Lipschitz', 'Expressivity']","An invertible function is bi-Lipschitz if both the function and its inverse have bounded Lipschitz constants. Nowadays, most Normalizing Flows are bi-Lipschitz by design or by training to limit numerical errors (among other things). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing Flows and identify several target distributions that are difficult to approximate using such models. Then, we characterize the expressivity of bi-Lipschitz Normalizing Flows by giving several lower bounds on the Total Variation distance between these particularly unfavorable distributions and their best possible approximation. Finally, we discuss potential remedies which include using more complex latent distributions.",https://api.openreview.net/pdf/403daf6887f29b69d44982795c7b664c7121f047.pdf
Semantic Perturbations with Normalizing Flows for Improved Generalization,2021,ICML,"['Oğuz Kaan Yüksel', 'Sebastian U Stich', 'Martin Jaggi', 'Tatjana Chavdarova']",poster,"['Machine Learning', 'Latent Space Perturbations', 'Semantic Perturbations', 'Adversarial Perturbations', 'Normalizing Flows', 'ICML']","Several methods from two separate lines of works, namely, data augmentation (DA) and adversarial training techniques, rely on perturbations done in latent space. Often, these methods are either non-interpretable due to their non-invertibility or are notoriously difficult to train due to their numerous hyperparameters. We exploit the exactly reversible encoder-decoder structure of normalizing flows to perform perturbations in the latent space. We demonstrate that these on-manifold perturbations match the performance of advanced DA techniques---reaching $96.6\%$ test accuracy for CIFAR-10 using ResNet-18 and outperform existing methods particularly in low data regimes---yielding $10$--$25\%$ relative improvement of test accuracy from classical training. We find our latent adversarial perturbations, adaptive to the classifier throughout its training, are most effective.",https://api.openreview.net/pdf/cff29865fbaf3213d730ef5b8489c6e5874069b8.pdf
Copula-Based Normalizing Flows,2021,ICML,"['Mike Laszkiewicz', 'Johannes Lederer', 'Asja Fischer']",poster,"['Base Distribution', 'Copula', 'Expressiveness']","Normalizing flows, which learn a distribution by transforming the data to samples from a Gaussian base distribution, have proven powerful density approximations. But their expressive power is limited by this choice of the base distribution. We, therefore, propose to generalize the base distribution to a more elaborate copula distribution to capture the properties of the target distribution more accurately. In a first empirical analysis, we demonstrate that this replacement can dramatically improve the vanilla normalizing flows in terms of flexibility, stability, and effectivity for heavy-tailed data. Our results suggest that the improvements are related to an increased local Lipschitz-stability of the learned flow.",https://api.openreview.net/pdf/c5b36809cea2e0d9212642187d1a4170c262d427.pdf
Generalization of the Change of Variables Formula with Applications to Residual Flows,2021,ICML,"['Niklas Koenen', 'Marvin N. Wright', 'Peter Maass', 'Jens Behrmann']",poster,"['Machine Learning', 'ICML', 'Change of Variables', 'Generalization of Flows', 'Residual Flows', 'Measure Theory']","Normalizing flows leverage the Change of Variables Formula (CVF) to define flexible density models. Yet, the requirement of smooth transformations (diffeomorphisms) in the CVF poses a significant challenge in the construction of these models. To enlarge the design space of flows, we introduce $\mathcal{L}$-diffeomorphisms as generalized transformations which may violate these requirements on zero Lebesgue-measure sets. This relaxation allows e.g. the use of non-smooth activation functions such as ReLU. Finally, we apply the obtained results to planar, radial, and contractive residual flows.",https://api.openreview.net/pdf/ee9f61dff51f8d33fe609a48740b117be95578b0.pdf
A Variational Perspective on Diffusion-Based Generative Models and Score Matching,2021,ICML,"['Chin-Wei Huang', 'Jae Hyun Lim', 'Aaron Courville']",poster,"['SDE', 'score matching', 'diffusion models', 'ODE', 'likelihood estimation', 'variational autoencoder', 'continuous-time flow']","Discrete-time diffusion-based generative models and score matching methods have shown promising results in modeling high-dimensional image data. Recently, Song et al. (2021) show that diffusion processes can be reverted via learning the score function, i.e. the gradient of the log-density of the perturbed data. They propose to plug the learned score function into an inverse formula to define a generative diffusion process. Despite the empirical success, a theoretical underpinning of this procedure is still lacking. In this work, we approach the (continuous-time) generative diffusion directly and derive a variational framework for likelihood estimation, which includes continuous-time normalizing flows as a special case, and can be seen as an infinitely deep variational autoencoder. Under this framework, we show that minimizing the score-matching loss is equivalent to maximizing the ELBO of the plug-in reverse SDE proposed by Song et al. (2021), bridging the theoretical gap.",https://api.openreview.net/pdf/e546417b8c8eeaddb070150227a9dc536c179d40.pdf
Manifold Density Estimation via Generalized Dequantization,2021,ICML,"['James Brofos', 'Marcus A Brubaker', 'Roy R Lederman']",poster,"['density estimation', 'embedded manifolds', 'dequantization']","Density estimation is an important technique for characterizing distributions given observations. Much existing research on density estimation has focused on cases wherein the data lies in a Euclidean space. However, some kinds of data are not well-modeled by supposing that their underlying geometry is Euclidean. Instead, it can be useful to model such data as lying on a {\it manifold} with some known structure. For instance, some kinds of data may be known to lie on the surface of a sphere. We study the problem of estimating densities on manifolds. We propose a method, inspired by the literature on ""dequantization,"" which we interpret through the lens of a coordinate transformation of an ambient Euclidean space and a smooth manifold of interest. Using methods from normalizing flows, we apply this method to the dequantization of smooth manifold structures in order to model densities on the sphere, tori, and the orthogonal group.",https://api.openreview.net/pdf/e9894fc55da04cc035b589d40f93a0eacbcb9a69.pdf
General Invertible Transformations for Flow-based Generative Modeling,2021,ICML,['Jakub Mikolaj Tomczak'],poster,"['Invertible Neural Networks', 'Deep Generative Modeling', 'Normalizing Flows']","In this paper, we present a new class of invertible transformations with an application to flow-based generative models. We indicate that many well-known invertible transformations in reversible logic and reversible neural networks could be derived from our proposition. Next, we propose two new coupling layers that are important building blocks of flow-based generative models. In the experiments on digit data, we present how these new coupling layers could be used in Integer Discrete Flows (IDF), and that they achieve better results than standard coupling layers used in IDF and RealNVP.",https://api.openreview.net/pdf/06ff25a8e2a6aff9897e1e69f95c31d3048d2b5b.pdf
The DEformer: An Order-Agnostic Distribution Estimating Transformer,2021,ICML,"['Michael A. Alcorn', 'Anh Nguyen']",poster,[],"Order-agnostic autoregressive distribution (density) estimation (OADE), i.e., autoregressive distribution estimation where the features can occur in an arbitrary order, is a challenging problem in generative machine learning. Prior work on OADE has encoded feature identity by assigning each feature to a distinct fixed position in an input vector. As a result, architectures built for these inputs must strategically mask either the input or model weights to learn the various conditional distributions necessary for inferring the full joint distribution of the dataset in an order-agnostic way. In this paper, we propose an alternative approach for encoding feature identities, where each feature's identity is included alongside its value in the input. This feature identity encoding strategy allows neural architectures designed for sequential data to be applied to the OADE task without modification. As a proof of concept, we show that a Transformer trained on this input (which we refer to as ""the DEformer"", i.e., the distribution estimating Transformer) can effectively model binarized-MNIST, approaching the performance of fixed-order autoregressive distribution estimating algorithms while still being entirely order-agnostic. Additionally, we find that the DEformer surpasses the performance of recent flow-based architectures when modeling a tabular dataset.",https://api.openreview.net/pdf/7eed5135db809da6575aadfee8eac48d98308b75.pdf
Recurrent Flow Networks: A Recurrent Latent Variable Model for Density Modelling of Urban Mobility,2021,ICML,"['Daniele Gammelli', 'Filipe Rodrigues']",poster,"['Urban Mobility', 'Latent variable models', 'amortized inference', 'normalizing flows']","Mobility-on-demand (MoD) systems represent a rapidly developing mode of transportation wherein travel requests are dynamically handled by a coordinated fleet of vehicles. Crucially, the efficiency of an MoD system highly depends on how well supply and demand distributions are aligned in spatio-temporal space (i.e., to satisfy user demand, cars have to be available in the correct place and at the desired time). When modelling urban mobility as temporal sequences, current approaches typically rely on either (i) a spatial discretization (e.g. ConvLSTMs), or (ii) a Gaussian mixture model to describe the conditional output distribution.
In this paper, we argue that both of these approaches could exhibit structural limitations when faced with highly complex data distributions such as for urban mobility densities. To address this issue, we introduce recurrent flow networks which combine deterministic and stochastic recurrent hidden states with conditional normalizing flows and show how the added flexibility allows our model to generate distributions matching potentially complex urban topologies. ",https://api.openreview.net/pdf/d356ee091b0384b0365659bc9b96c425f8344945.pdf
Representation Learning in Continuous-Time Score-Based Generative Models,2021,ICML,"['Korbinian Abstreiter', 'Stefan Bauer', 'Arash Mehrjou']",poster,"['representation learning', 'score-based', 'diffusion models', 'generative modeling']","Score-based methods represented as stochastic differential equations on a continuous time domain have recently proven successful as a non-adversarial generative model.
Training such models relies on denoising score matching, which can be seen as multi-scale denoising autoencoders.
Here, we augment the denoising score-matching framework to enable representation learning without any supervised signal.
GANs and VAEs learn representations by directly transforming latent codes to data samples.
In contrast, score-based representation learning relies on a new formulation of the denoising score-matching objective and thus encodes information needed for denoising.
We show how this difference allows for manual control of the level of detail encoded in the representation.",https://api.openreview.net/pdf/4358d03c3bae5a468259f1245308a02362d68ac5.pdf
Understanding Event-Generation Networks via Uncertainties,2021,ICML,"['Marco Bellagente', 'Michel Luchmann', 'Manuel Haussmann', 'Tilman Plehn']",poster,"['normalizing flow', 'bayesian neural network', 'LHC']","Generative models and normalizing flow based models have made great progress in recent years both in their theoretical development as well as in a growing number of applications. As such models become applied more and more with it increases the desire for predictive uncertainty to know when to trust the underlying model. In this extended abstract we target the application area of Large Hadron Collider (LHC) simulations and show how to extend normalizing flows with probabilistic Bayesian Neural Network based transformations to model LHC events with uncertainties. 
",https://api.openreview.net/pdf/114b11e5c6361c550d7ef96b98ef4c0057d7e278.pdf
Disentangled Predictive Representation for Meta-Reinforcement Learning,2021,ICML,"['Sephora Madjiheurem', 'Laura Toni']",poster,"['reinforcement learning', 'representation learning', 'unsupervised learning', 'meta-learning']","A major challenge in reinforcement learning is the design of agents that are able to generalize across tasks that share common  dynamics. A viable solution is  meta-reinforcement learning,   which identifies common structures among past tasks to be then generalized to new tasks (meta-test). Prior works learn meta-representation jointly while solving tasks, resulting in representations that not generalize  well across policies, leading to sampling-inefficiency during   meta-test phases. In this work, we introduce state2vec, an efficient and low-complexity unsupervised framework for learning disentangled representations that are more general.  
The state embedding vectors learned with state2vec capture the geometry of the underlying state space, resulting in high-quality basis functions for linear value function approximation. ",https://api.openreview.net/pdf/ada5c3307a6d9cb135038d79f06d906e1624ac4a.pdf
Decoupling Exploration and Exploitation in Reinforcement Learning,2021,ICML,"['Lukas Schäfer', 'Filippos Christianos', 'Josiah Hanna', 'Stefano V Albrecht']",poster,"['Reinforcement Learning', 'Exploration', 'Intrinsic Rewards', 'Decoupling']","Intrinsic rewards are commonly applied to improve exploration in reinforcement learning. However, these approaches suffer from instability caused by non-stationary reward shaping and strong dependency on hyperparameters. In this work, we propose Decoupled RL (DeRL) which trains separate policies for exploration and exploitation. DeRL can be applied with on-policy and off-policy RL algorithms. We evaluate DeRL algorithms in two sparse-reward environments with multiple types of intrinsic rewards. We show that DeRL is more robust to scaling and speed of decay of intrinsic rewards and converges to the same evaluation returns than intrinsically motivated baselines in fewer interactions.",https://api.openreview.net/pdf/46220385a893173c54e1564b867ab04e8ab67ac7.pdf
Density-Based Bonuses on Learned Representations for Reward-Free Exploration in Deep Reinforcement Learning,2021,ICML,"['Omar Darwiche Domingues', 'Corentin Tallec', 'Remi Munos', 'Michal Valko']",poster,"['reinforcement learning', 'exploration', 'reward-free', 'representation learning']","In this paper, we study the problem of representation learning and exploration in reinforcement learning. We propose a framework to compute exploration bonuses based on density estimation, that can be used with any representation learning method, and that allows the agent to explore without extrinsic rewards. In the special case of tabular Markov decision processes (MDPs), this approach mimics the behavior of theoretically sound algorithms. In continuous and partially observable MDPs, the same approach can be applied by learning a latent representation, on which a probability density is estimated.",https://api.openreview.net/pdf/7fc06e37c9f3dd23b6c3cc41e89705f4d0d1dbd4.pdf
Tangent Space Least Adaptive Clustering,2021,ICML,"['James Buenfil', 'Samson J Koelle', 'Marina Meila']",poster,[],"The biasing of dynamical simulations along collective variables uncovered by unsupervised learning has become a standard approach in analysis of molecular systems. However, despite parallels with reinforcement learning (RL), state of the art RL methods have yet to reach the molecular dynamics community. The interaction between unsupervised learning, dynamical simulations, and RL is therefore a promising area of research. We introduce a method for enhanced sampling that uses nonlinear geometry estimated by an unsupervised learning algorithm in a reinforcement-learning enhanced sampler. We give theoretical background justifying this method, and show results on data.",https://api.openreview.net/pdf/d81f964af9488c15c395659b12831483e5b717a5.pdf
Representation Learning for Out-of-distribution Generalization in Reinforcement Learning,2021,ICML,"['Frederik Träuble', 'Andrea Dittadi', 'Manuel Wuthrich', 'Felix Widmaier', 'Peter Vincent Gehler', 'Ole Winther', 'Francesco Locatello', 'Olivier Bachem', 'Bernhard Schölkopf', 'Stefan Bauer']",poster,"['representations', 'robustness', 'out-of-distribution', 'generalization', 'reinforcement learning']","Learning data representations that are useful for various downstream tasks is a cornerstone of artificial intelligence. While existing methods are typically evaluated on downstream tasks such as classification or generative image quality, we propose to assess representations through their usefulness in downstream control tasks, such as reaching or pushing objects. By training over 10,000 reinforcement learning policies, we extensively evaluate to what extent different representation properties affect out-of-distribution (OOD) generalization. Finally, we demonstrate zero-shot transfer of these policies from simulation to the real world, without any domain randomization or fine-tuning. This paper aims to establish the first systematic characterization of the usefulness of learned representations for real-world OOD downstream tasks.",https://api.openreview.net/pdf/126671cd362894052eaf114f9f45161638eb312b.pdf
SparseDice: Imitation Learning for Temporally Sparse Data via Regularization,2021,ICML,"['Alberto Camacho', 'Izzeddin Gur', 'Marcin Lukasz Moczulski', 'Ofir Nachum', 'Aleksandra Faust']",poster,['imitation learning'],"Imitation learning learns how to act by observing the behavior of an expert demonstrator. We are concerned with a setting where the demonstrations comprise only a subset of state-action pairs (as opposed to the whole trajectories). Our setup reflects the limitations of real-world problems when accessing the expert data. For example, user logs may contain incomplete traces of behavior, or in robotics non-technical human demonstrators may describe trajectories using only a subset of all state-action pairs. A recent approach to imitation learning via distribution matching, ValueDice, tends to overfit when demonstrations are temporally sparse. We counter the overfitting by contributing regularization losses. Our empirical evaluation with Mujoco benchmarks shows that we can successfully learn from very sparse and scarce expert data. Moreover, (i) the quality of the learned policies is often comparable to those learned with full expert trajectories, and (ii) the number of training steps required to learn from sparse data is similar to the number of training steps when the agent has access to full expert trajectories.",https://api.openreview.net/pdf/387f3ef0a3195a53b0cadbdad3fbd58e2720e78e.pdf
CoBERL: Contrastive BERT for Reinforcement Learning,2021,ICML,"['Andrea Banino', 'Adria Puigdomenech Badia', 'Jacob C Walker', 'Tim Scholtes', 'Jovana Mitrovic', 'Charles Blundell']",poster,"['Reinforcement Learning', 'Contrastive Learning', 'Representation Learning', 'Transformer', 'Deep Reinforcement Learning']","Many reinforcement learning (RL) agents require a large amount of experience to solve tasks.  We propose Contrastive BERT for RL (CoBERL), an agent that combines a new contrastive loss and a hybrid LSTM-transformer architecture to tackle the challenge of improving data efficiency. CoBERL enables efficient, robust learning from pixels across a wide range of domains. We use bidirectional masked prediction in combination with a generalization of recent contrastive methods to learn better representations for transformers in RL, without the need of hand engineered data augmentations. We find that CoBERL consistently improves performance across the full Atari suite, a set of control tasks and a challenging 3D environment.",https://api.openreview.net/pdf/d64f621a30662642992e481cb47403f562e646e6.pdf
Control-Oriented Model-Based Reinforcement Learning with Implicit Differentiation,2021,ICML,"['Evgenii Nikishin', 'Romina Abachi', 'Rishabh Agarwal', 'Pierre-Luc Bacon']",poster,"['reinforcement learning', 'model-based reinforcement learning', 'bi-level optimization']","The shortcomings of maximum likelihood estimation in the context of model-based reinforcement learning have been highlighted by an increasing number of papers. When the model class is misspecified or has a limited representational capacity, model parameters with high likelihood might not necessarily result in high performance of the agent on a downstream control task. To alleviate this problem, we propose an end-to-end approach for model learning which directly optimizes the expected returns using implicit differentiation. We treat a value function that satisfies the Bellman optimality operator induced by the model as an implicit function of model parameters and show how to differentiate the function. We provide theoretical and empirical evidence highlighting the benefits of our approach in the model misspecification regime compared to likelihood-based methods.",https://api.openreview.net/pdf/8584ab9ae04bd6d59f04e9555a159fcfabe50e54.pdf
Visual Adversarial Imitation Learning using Variational Models,2021,ICML,"['Rafael Rafailov', 'Tianhe Yu', 'Aravind Rajeswaran', 'Chelsea Finn']",poster,"['imitation learning', 'generative models', 'vision', 'POMDP', 'self-supervised reinforcement learning']","Reward function specification, which requires considerable human effort and iteration, remains a major impediment for learning behaviors through deep reinforcement learning. In contrast, providing visual demonstrations of desired behaviors presents an easier and more natural way to teach agents. We consider a setting where an agent is provided a fixed dataset of visual demonstrations illustrating how to perform a task, and must learn to solve the task using the provided demonstrations and unsupervised environment interactions. This setting presents a number of challenges including representation learning for visual observations, sample complexity due to high dimensional spaces, and learning instability due to the lack of a fixed reward or learning signal. Towards addressing these challenges, we develop a variational model-based adversarial imitation learning (V-MAIL) algorithm. The model-based approach provides a strong signal for representation learning, enables sample efficiency, and improves the stability of adversarial training by enabling on-policy learning. Through experiments involving several vision-based locomotion and manipulation tasks, we find that V-MAIL learns successful visuomotor policies in a sample-efficient manner, has better stability compared to prior work, and also achieves higher asymptotic performance. We further find that by transferring the learned models, V-MAIL can learn new tasks from visual demonstrations without any additional environment interactions. All results including videos can be found online at \url{https://sites.google.com/view/variational-mail}.",https://api.openreview.net/pdf/bed1841517fd845fe7598b93dc477f2ce5e126cb.pdf
Decision Transformer: Reinforcement Learning via Sequence Modeling,2021,ICML,"['Lili Chen', 'Kevin Lu', 'Aravind Rajeswaran', 'Kimin Lee', 'Aditya Grover', 'Michael Laskin', 'Pieter Abbeel', 'Aravind Srinivas', 'Igor Mordatch']",poster,"['transformers', 'reinforcement learning', 'deep learning', 'generative modeling']","We present a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",https://api.openreview.net/pdf/c3efdd0b44aca06cc2aa3d74c39ec20ff33240b3.pdf
Episodic Memory for Subjective-Timescale Models,2021,ICML,"['Alexey Zakharov', 'Matthew Crosby', 'Zafeirios Fountas']",poster,"['subjective timescales', 'model-based reinforcement learning', 'episodic memory']","Planning in complex environments requires reasoning over multi-step timescales. However, in model-based learning, an agent’s model is more commonly defined over transitions between consecutive states. This leads to plans using intermediate states that are either unnecessary, or worse, introduce cumulative prediction errors. Inspired by the recent works on human time perception, we devise a novel approach for learning a transition dynamics model based on the sequences of episodic memories that define an agent's subjective timescale – over which it learns world dynamics and over which future planning is performed. We analyse the emergent benefits of the subjective-timescale model (STM) by incorporating it into two disparate model-based methods – Dreamer and deep active inference. Using 3D visual foraging tasks, we demonstrate that STM can systematically vary the temporal extent of its predictions and is more likely to predict future salient events (such as new objects coming into view). In comparison to the agents trained using objective timescales, STM agents also collect more rewards due to their ability to perform flexible planning and a more pronounced exploratory behaviour.",https://api.openreview.net/pdf/65c583c294556c53ba568ce48fdb570534bd5b28.pdf
Reinforcement Learning as One Big Sequence Modeling Problem,2021,ICML,"['Michael Janner', 'Qiyang Li', 'Sergey Levine']",poster,"['reinforcement learning', 'sequence modeling', 'transformers']","Reinforcement learning (RL) is typically concerned with estimating single-step policies or single-step models, leveraging the Markov property to factorize the problem in time. However, we can also view RL as a sequence modeling problem, with the goal being to predict a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether powerful, high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide simple and effective solutions to the RL problem. To this end, we explore how RL can be reframed as ``one big sequence modeling'' problem, using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Addressing RL as a sequence modeling problem significantly simplifies a range of design decisions: we no longer require separate behavior policy constraints, as is common in prior work on offline model-free RL, and we no longer require ensembles or other epistemic uncertainty estimators, as is common in prior work on model-based RL. All of these roles are filled by the same Transformer sequence model. In our experiments, we demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL.",https://api.openreview.net/pdf/20e48ba65f1734cb0f489561568e86df6111d105.pdf
Exploration-Driven Representation Learning in Reinforcement Learning,2021,ICML,"['Akram Erraqabi', 'Mingde Zhao', 'Marlos C. Machado', 'Yoshua Bengio', 'Sainbayar Sukhbaatar', 'Ludovic Denoyer', 'Alessandro Lazaric']",poster,"['Representation learning', 'self-supervised', 'exploration']","Learning reward-agnostic representations is an emerging paradigm in reinforcement learning. These representations can be leveraged for several purposes ranging from reward shaping to skill discovery. Nevertheless, in order to learn such representations, existing methods often rely on assuming uniform access to the state space. With such a privilege, the agent’s coverage of the environment can be limited which hurts the quality of the learned representations. In this work, we introduce a method that explicitly couples representation learning with exploration when the agent is not provided with a uniform prior over the state space. Our method learns representations that constantly drive exploration while the data generated by the agent’s exploratory behavior drives the learning of better representations. We empirically validate our approach in goal-achieving tasks, demonstrating that the learned representation captures the dynamics of the environment, leads to more accurate value estimation, and to faster credit assignment, both when used for control and for reward shaping. Finally, the exploratory policy that emerges from our approach proves to be successful at continuous navigation tasks with sparse rewards.",https://api.openreview.net/pdf/3e91d2541923e3311ffca1fe0cc277df713fe9a9.pdf
Learning to Represent State with Perceptual Schemata,2021,ICML,"['Wilka Torrico Carvalho', 'Murray Shanahan']",poster,"['reinforcement learning', 'representation learning', 'modularity', 'zero-shot generalization']","The real world is large and complex. It is filled with many objects besides those defined by a task and objects can move with their own interesting dynamics. How should an agent learn to represent state to support efficient learning and generalization in such an environment? In this work, we present a novel memory architecture, Perceptual Schemata, for learning and zero-shot generalization in environments that have many, potentially moving objects. Perceptual Schemata represents state using a combination of schema modules that each learn to attend to and maintain stateful representations of different subspaces of a spatio-temporal tensor describing the agent’s observations. We present empirical results that Perceptual Schemata enables a state representation that can maintain multiple objects observed in sequence with independent dynamics while an LSTM cannot. We additionally show that Perceptual Schemata can generalize more gracefully to larger environments with more distractor objects, while an LSTM quickly overfits to the training tasks.",https://api.openreview.net/pdf/a4e1bdc5223ff78ce231e3627649268d8cc8db3b.pdf
Data-Efficient Exploration with Self Play for Atari,2021,ICML,"['Michael Laskin', 'Catherine Cang', 'Ryan Rudes', 'Pieter Abbeel']",poster,"['unsupervised reinforcement learning', 'exploration', 'self play']","Most reinforcement learning (RL) algorithms rely on hand-crafted extrinsic rewards to learn skills. However, crafting a reward function for each skill is not scalable and results in narrow agents that learn reward-specific skills. To alleviate the reliance on reward engineering it is important to develop RL algorithms capable of efficiently acquiring skills with no rewards extrinsic to the agent. While much progress has been made on reward-free exploration in RL, current methods struggle to explore efficiently. Self-play has long been a promising approach for acquiring skills but most successful applications have been in multi-agent zero-sum games with extrinsic reward. In this work, we present SelfPlayer, a data-efficient single-agent self-play exploration algorithm. SelfPlayer samples hard but achievable goals from the agent’s past by maximizing a symmetric KL divergence between the visitation distributions of two copies of the agent, Alice and Bob. We show that SelfPlayer outperforms prior leading self-supervised exploration algorithms such as GoExplore and Curiosity on the data-efficient Atari benchmark. 
",https://api.openreview.net/pdf/eb024d6dc6d246282ee38a157c79f322557383f5.pdf
Intrinsic Control of Variational Beliefs in Dynamic Partially-Observed Visual Environments,2021,ICML,"['Nicholas Rhinehart', 'Jenny Wang', 'Glen Berseth', 'John D Co-Reyes', 'Danijar Hafner', 'Chelsea Finn', 'Sergey Levine']",poster,[],"Humans and animals explore their environment and acquire useful skills even in the absence of clear goals, exhibiting intrinsic motivation. The study of intrinsic motivation in artificial agents is concerned with the following question: what is a good general-purpose objective for an agent? We study this question in dynamic partially-observed environments, and argue that a compact and general learning objective is to minimize the entropy of the agent's state visitation estimated using a latent state-space model. This objective induces an agent to both gather information about its environment, corresponding to reducing uncertainty, and to gain control over its environment, corresponding to reducing the unpredictability of future world states. We instantiate this approach as a deep reinforcement learning agent equipped with a deep variational Bayes filter. We find that our agent learns to discover, represent, and exercise control of dynamic objects in a variety of partially-observed environments sensed with visual observations without extrinsic reward.",https://api.openreview.net/pdf/c26587aa0eedcb7ee36090ca24ff80f4c3fb432c.pdf
Discovering Diverse Nearly Optimal Policies with Successor Features,2021,ICML,"['Tom Zahavy', ""Brendan O'Donoghue"", 'Andre Barreto', 'Sebastian Flennerhag', 'Volodymyr Mnih', 'Satinder Singh']",poster,"['quality diversity', 'reinforcement learning']","Finding different solutions to the same problem is a key aspect of intelligence associated with creativity and adaptation to novel situations. In reinforcement learning, a set of diverse policies can be useful for exploration, transfer, hierarchy, and robustness. We propose Diverse Successive Policies, a method for discovering policies that are diverse in the space of Successor Features, while assuring that they are near optimal. We formalize the problem as a Constrained Markov Decision Process (CMDP) where the goal is to find policies that maximize diversity, characterized by an intrinsic diversity reward, while remaining near-optimal with respect to the extrinsic reward of the MDP. We also analyze how recently proposed robustness and discrimination rewards perform and find that they are sensitive to the initialization of the procedure and may converge to sub-optimal solutions. To alleviate this, we propose new explicit diversity rewards that aim to minimize the correlation between the Successor Features of the policies in the set. We compare the different diversity mechanisms in the DeepMind Control Suite and find that the type of explicit diversity we are proposing is important to discover distinct behavior, like for example different locomotion patterns.",https://api.openreview.net/pdf/d00e028fc1a71f1fb2309c958fba10c6248cdc38.pdf
Reward is enough for convex MDPs,2021,ICML,"['Tom Zahavy', ""Brendan O'Donoghue"", 'Guillaume Desjardins', 'Satinder Singh']",poster,"['Unsupervised RL', 'convex optimisation', 'reinforcement learning', 'diversity', 'apprenticeship learning']","Maximising a cumulative reward function that is Markov and stationary, i.e, defined over state-action pairs and independent of time, is sufficient to capture many kinds of goals in a Markov Decision Process (MDP) based on the Reinforcement Learning (RL) problem formulation. However, not all goals can be captured in this manner. Specifically, it is easy to see that Convex MDPs in which goals are expressed as convex functions of stationary distributions cannot, in general, be formulated in this manner. In this paper, we reformulate the convex MDP problem as a min-max game between the policy and cost (negative reward) players using Fenchel duality and propose a meta-algorithm for solving it. We show that the average of the policies produced by an RL agent that maximizes the non-stationary reward produced by the cost player converges to an optimal solution to the convex MDP. Finally, we show that the meta-algorithm unifies several disparate branches of reinforcement learning algorithms in the literature, such as apprenticeship learning, variational intrinsic control, constrained MDPs, and pure exploration into a single framework.",https://api.openreview.net/pdf/e48e8982315dafaa3fdf1668249f38be401ed32e.pdf
Unsupervised Skill-Discovery and Skill-Learning in Minecraft,2021,ICML,"['Juan José Nieto', 'Roger Creus Castanyer', 'Xavier Giro-i-Nieto']",poster,"['reinforcement learning', 'self-supervised learning', 'unsupervised reinforcement learning', 'skill-discovery', 'skill-learning', 'intrinsic motivation', 'empowerment', 'minecraft']","Pre-training Reinforcement Learning (RL) agents in a task-agnostic manner has shown promising results. However, previous works still struggle in learning and discovering meaningful skills in high-dimensional state-spaces. We approach the problem by leveraging unsupervised skill discovery and self-supervised learning of state representations. In our work, we learn a compact latent representation by making use of variational and contrastive techniques. We demonstrate that both enable RL agents to learn a set of basic navigation skills by maximizing an information theoretic objective. We assess our method in Minecraft 3D maps with different complexities. Our results show that representations and conditioned policies learned from pixels are enough for toy examples, but do not scale to realistic and complex maps. To overcome these limitations, we explore alternative input observations such as the relative position of the agent along with the raw pixels. ",https://api.openreview.net/pdf/6e0c633f6cea56d97b4f5c137ae528f00d4353c8.pdf
Learning Task-Relevant Representations with Selective Contrast for Reinforcement Learning in a Real-World Application,2021,ICML,"['Flemming Brieger', 'Daniel Alexander Braun', 'Sascha Lange']",poster,[],"We use contrastive learning to obtain task-relevant state-representations from images for reinforcement learning in a real-world system. To test the quality of the representations, an agent is trained with reinforcement learning in the Neuro-Slot-Car environment (Kietzmann & Riedmiller, 2009; Lange et al., 2012). In our experiments, we restrict the distribution from which samples are drawn for comparison in the contrastive loss. Our results show, that the choice of sampling distribution for negative samples is essential to allow task-relevant features to be represented in the presence of more prevalent, but irrelevant features. This adds to recent research on feature suppression and feature invariance in contrastive representation learning. With the training of the reinforcement learning agent, we present to our knowledge a first approach of using contrastive learning of state-representations for control in a real-world environment, using only images from one static camera.",https://api.openreview.net/pdf/8bd63d8c719d1c4981e480f4e474700c7e952d7e.pdf
"Exploration via Empowerment Gain: Combining Novelty, Surprise and Learning Progress",2021,ICML,"['Philip Becker-Ehmck', 'Maximilian Karl', 'Jan Peters', 'Patrick van der Smagt']",poster,"['exploration', 'empowerment', 'intrinsic motivation', 'surprise', 'novelty']","Exploration in the absence of a concrete task is a key characteristic of autonomous agents and vital for the emergence of intelligent behaviour. Various intrinsic motivation frameworks have been suggested, such as novelty seeking, surprise maximisation or empowerment. Here we focus on the latter, empowerment, an agent-centric and information-theoretic measure of an agent's perceived influence on the world. By considering improving one's empowerment estimator - we call it empowerment gain (EG) - we derive a novel exploration criterion that focuses directly on the desired goal: exploration in order to help the agent recognise its capability to interact with the world. We propose a new theoretical framework based on improving a parametrised estimation of empowerment and show how it integrates novelty, surprise and learning progress into a single formulation. Empirically, we validate our theoretical findings on some simple but instructive grid world environments. We show that while such an agent is still novelty seeking, i.e. interested in exploring the whole state space, it focuses on exploration where its perceived influence is greater, avoiding areas of greater stochasticity or traps that limit its control.",https://api.openreview.net/pdf/22ee6eabe1973eb57c02e3a8444ced4d1188e14c.pdf
MASAI: Multi-agent Summative Assessment Improvement for Unsupervised Environment Design,2021,ICML,"['Yiping Wang', 'Michael Brandon Haworth']",poster,[],"Reinforcement Learning agents require a distribution of environments for their policy to be trained on. The method or process of defining these environments directly impacts robustness and generalization of the learned agent policies. In single agent reinforcement learning, this problem is often solved by domain randomization, or randomizing the environment and tasks within the scope of the desired operating domain of the agent. The challenge here is to generate both structured and solvable environments that guide the agent's learning process. Most recently, works have sought to produce the environments under the Unsupervised Environment Design (UED) formulation. However, these methods lead to a proliferation of adversarial agents to train one agent for a single agent problem in a discretized task domain. In this work, we aim to automatically generate environments that are solvable and challenging for the continuous multi-agent setting. We base our solution on the Teacher-Student relationship with parameter sharing $\textit{Students}$ where we re-imagine the $\textit{Teacher}$ as an environment generator for UED. Our approach uses one environment generator agent ($\textit{Teacher}$) for any number of learning agents ($\textit{Students}$). We qualitatively and quantitatively demonstrate that, in terms of multi-agent ($\geq$ 8 agents) navigation and steering, $\textit{Students}$ trained by our approach outperform agents using heuristic search, as well as agents trained by domain randomization. Our code is available at https://github.com/GAIDG-Lab/MASAI. ",https://api.openreview.net/pdf/3e3dcf77121913400704ed62d2920c9248207ce9.pdf
Explore and Control with Adversarial Surprise,2021,ICML,"['Arnaud Fickinger', 'Natasha Jaques', 'Samyak Parajuli', 'Michael Chang', 'Nicholas Rhinehart', 'Glen Berseth', 'Stuart Russell', 'Sergey Levine']",poster,[],"Reinforcement learning (RL) provides a framework for learning goal-directed policies given user-specified rewards. However, since designing rewards often requires substantial engineering effort, we are interested in the problem of learning without rewards, where agents must discover useful behaviors in the absence of task-specific incentives. Intrinsic motivation is a family of unsupervised RL techniques which develop general objectives for an RL agent to optimize that lead to better exploration or the discovery of skills. In this paper, we propose a new unsupervised RL technique based on an adversarial game which pits two policies against each other to compete over the amount of surprise an RL agent experiences. The policies each take turns controlling the agent. The Explore policy maximizes entropy, putting the agent into surprising or unfamiliar situations. Then, the Control policy takes over and seeks to recover from those situations by minimizing entropy. The game harnesses the power of multi-agent competition to drive the agent to seek out increasingly surprising parts of the environment while learning to gain mastery over them. We show empirically that our method leads to the emergence of complex skills by exhibiting clear phase transitions. Furthermore, we show both theoretically \---via a latent state space coverage argument\--- and empirically that our method has the potential to be applied to the exploration of stochastic, partially-observed environments. We show that Adversarial Surprise learns more complex behaviors, and explores more effectively than competitive baselines, outperforming intrinsic motivation methods based on active inference, novelty-seeking (Random Network Distillation (RND)), and multi-agent unsupervised RL (Asymmetric Self-Play (ASP)) in MiniGrid, Atari and VizDoom environments.",https://api.openreview.net/pdf/1c3e3e5953f432e8441c8a95cab2cca7d1c6f5f7.pdf
Hierarchical Few-Shot Imitation with Skill Transition Models,2021,ICML,"['Kourosh Hakhamaneshi', 'Ruihan Zhao', 'Albert Zhan', 'Pieter Abbeel', 'Michael Laskin']",poster,"['behavioral priors', 'skill extraction', 'imitation learning', 'few-shot learning']","A desirable property of autonomous agents is the ability to both solve long-horizon problems and generalize to unseen tasks. Recent advances in data-driven skill learning have shown that extracting behavioral priors from offline data can enable agents to solve challenging long-horizon tasks with reinforcement learning. However, generalization to tasks unseen during behavioral prior training remains an outstanding challenge. To this end, we present Few-shot Imitation with Skill Transition Models (FIST), an algorithm that extracts skills from offline data and utilizes them to generalize to unseen tasks given a few downstream demonstrations. FIST learns an inverse skill dynamics model and utilizes a semi-parametric approach for imitation. We show that FIST is capable of generalizing to new tasks and substantially outperforms prior baselines in navigation experiments requiring traversing unseen parts of a large maze and 7-DoF robotic arm experiments requiring manipulating previously unseen objects in a kitchen.",https://api.openreview.net/pdf/17e249608aa40177504c2ddb174d8b5d5ab4f9b6.pdf
Exploration and preference satisfaction trade-off in reward-free learning,2021,ICML,"['Noor Sajid', 'Panagiotis Tigas', 'Alexey Zakharov', 'Zafeirios Fountas', 'Karl Friston']",poster,[],"Biological agents have meaningful interactions with their environment despite the absence of immediate reward signals. In such instances, the agent can learn preferred modes of behaviour that lead to predictable states - necessary for survival. In this paper, we pursue the notion that this learnt behaviour can be a consequence of reward-free preference learning that ensures an appropriate trade-off between exploration and preference satisfaction. For this, we introduce a model-based Bayesian agent equipped with a preference learning mechanism (pepper) using conjugate priors. These conjugate priors are used to augment the expected free energy planner for learning preferences over states (or outcomes) across time. Importantly, our approach enables the agent to learn preferences that encourage adaptive behaviour at test time. We illustrate this in the OpenAI Gym FrozenLake and the 3D mini-world environments -- with and without volatility. Given a constant environment, these agents learn confident (i.e., precise) preferences and act to satisfy them. Conversely, in a volatile setting, perpetual preference uncertainty maintains exploratory behaviour. Our experiments suggest that learnable (reward-free) preferences entail a trade-off between exploration and preference satisfaction. Pepper offers a straightforward framework suitable for designing adaptive agents when reward functions cannot be predefined as in real environments. ",https://api.openreview.net/pdf/3472fef25f645bd2c6a462b66afb2be03b1dfb9d.pdf
Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching,2021,ICML,"['Pierre-Alexandre Kamienny', 'Jean Tarbouriech', 'Alessandro Lazaric', 'Ludovic Denoyer']",poster,[],"Learning meaningful behaviors in the absence of a task-specific reward function is a challenging problem in reinforcement learning. A desirable unsupervised objective is to learn a set of diverse skills that provide a thorough coverage of the state space while being directed, i.e., reliably reaching distinct regions of the environment. At test time, an agent could then leverage these skills to solve sparse reward problems by performing efficient exploration and finding an effective goal-directed policy with little-to-no additional learning. Unfortunately, it is challenging to learn skills with such properties, as diffusing (e.g., stochastic policies performing good coverage) skills are not reliable in targeting specific states, whereas directed (e.g., goal-based policies) skills provide limited coverage. In this paper, inspired by the mutual information framework, we propose a novel algorithm designed to maximize coverage while ensuring a constraint on the directedness of each skill. In particular, we design skills with a decoupled policy structure, with a first part trained to be directed and a second diffusing part that ensures local coverage. Furthermore, we leverage the directedness constraint to adaptively add or remove skills as well as incrementally compose them along a tree that is grown to achieve a thorough coverage of the environment. We illustrate how our learned skills enable to efficiently solve sparse-reward downstream tasks in navigation environments, comparing favorably with existing baselines.",https://api.openreview.net/pdf/d3252c97646a41f435fdd3901c4ed9e0b989e530.pdf
Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning,2021,ICML,"['Víctor Campos', 'Pablo Sprechmann', 'Steven Stenberg Hansen', 'Andre Barreto', 'Steven Kapturowski', 'Alex Vitvitskyi', 'Adria Puigdomenech Badia', 'Charles Blundell']",poster,"['deep reinforcement learning', 'transfer learning', 'unsupervised learning', 'exploration']","Designing agents that acquire knowledge autonomously and use it to solve new tasks efficiently is an important challenge in reinforcement learning. Knowledge acquired during an unsupervised pre-training phase is often transferred by fine-tuning neural network weights once rewards are exposed, as is common practice in supervised domains. Given the nature of the reinforcement learning problem, we argue that standard fine-tuning strategies alone are not enough for efficient transfer in challenging domains. We introduce Behavior Transfer (BT), a technique that leverages pre-trained policies for exploration and that is complementary to transferring neural network weights. Our experiments show that, when combined with large-scale pre-training in the absence of rewards, existing intrinsic motivation objectives can lead to the emergence of complex behaviors. These pre-trained policies can then be leveraged by BT to discover better solutions than without pre-training, and combining BT with standard fine-tuning strategies results in additional benefits. The largest gains are generally observed in domains requiring structured exploration, including settings where the behavior of the pre-trained policies is misaligned with the downstream task. ",https://api.openreview.net/pdf/2ddb1a16f61fdbdcccb3576e5ffc6d39959fc939.pdf
Learning to Explore Multiple Environments without Rewards,2021,ICML,"['Mirco Mutti', 'Mattia Mancassola', 'Marcello Restelli']",poster,"['Reward-free reinforcement learning', 'Maximum state entropy exploration']","Several recent works have been dedicated to the pure exploration of a single reward-free environment. Along this line, we address the problem of learning to explore a class of multiple reward-free environments with a unique general strategy, which aims to provide a universal initialization to subsequent reinforcement learning problems specified over the same class. Notably, the problem is inherently multi-objective as we can trade off the exploration performance between environments in many ways. In this work, we foster an exploration strategy that is sensitive to the most adverse cases within the class. Hence, we cast the exploration problem as the maximization of the mean of a critical percentile of the state visitation entropy induced by the exploration strategy over the class of environments. Then, we present a policy gradient algorithm, MEMENTO, to optimize the introduced objective through mediated interactions with the class. Finally, we empirically demonstrate the ability of the algorithm in learning to explore challenging classes of continuous environments and we show that reinforcement learning greatly benefits from the pre-trained exploration strategy when compared to learning from scratch.",https://api.openreview.net/pdf/801072427dad052b47c81329cb02e63de7a1de13.pdf
Reward-Free Policy Space Compression for Reinforcement Learning,2021,ICML,"['Mirco Mutti', 'Stefano Del Col', 'Marcello Restelli']",poster,"['Markov Decision Processes', 'Reward-Free Pre-Training', 'Policy Space Compression']","In reinforcement learning, we encode the potential behaviors of an agent interacting with an environment into an infinite set of policies, called policy space, typically represented by a family of parametric functions. Dealing with such a policy space is a hefty challenge, which often causes sample and computational inefficiencies. However, we argue that a limited number of policies is actually relevant when we also account for the structure of the environment and of the policy parameterization, as many of them would induce very similar interactions, i.e., state-action distributions. In this paper, we seek for a reward-free compression of the policy space into a finite set of representative policies, such that, given any policy $\pi$, the minimum Rényi divergence between the state-action distributions of the representative policies and the state-action distribution of $\pi$ is bounded. We show that this compression of the policy space can be formulated as a set cover problem, and it is inherently NP-hard. Nonetheless, we propose a game-theoretic reformulation for which a locally optimal solution can be efficiently found by iteratively stretching the compressed space to cover the most challenging policy. Finally, we provide an empirical evaluation to illustrate the compression procedure in simple domains, and its ripple effects in reinforcement learning.",https://api.openreview.net/pdf/f864a3bdddff1b3fd331c8a1ac95e748f32da83c.pdf
The Importance of Non-Markovianity in Maximum State Entropy Exploration,2021,ICML,"['Mirco Mutti', 'Riccardo De Santi', 'Marcello Restelli']",poster,"['Controlled Markov processes', 'Maximum state entropy exploration', 'Non-Markovian exploration']","In the maximum state entropy exploration framework, an agent interacts with a reward-free environment to learn a policy that maximizes the entropy of the expected state visitations it is inducing. Hazan et al. (2019) noted that the class of Markovian stochastic policies is sufficient for the maximum state entropy objective, and exploiting non-Markovianity is generally considered pointless in this setting. In this paper, we argue that non-Markovianity is instead paramount for maximum state entropy exploration in a finite-sample regime. Especially, we recast the objective to target the expected entropy of the induced state visitations in a single trial. Then, we show that the class of non-Markovian deterministic policies is sufficient for the introduced objective, while Markovian policies suffer non-zero regret in general. However, we prove that the problem of finding an optimal non-Markovian policy is NP-hard. Despite this negative result, we discuss avenues to address the problem in a tractable way and how non-Markovian exploration could benefit the sample efficiency of online reinforcement learning in future works.",https://api.openreview.net/pdf/b00e788b6eabae8aae5defe73bd120c08ad5f1dd.pdf
Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation,2021,ICML,"['Nicklas Hansen', 'Hao Su', 'Xiaolong Wang']",poster,"['reinforcement learning', 'generalization', 'data augmentation']","While agents trained by Reinforcement Learning (RL) can solve increasingly challenging tasks directly from visual observations, generalizing learned skills to novel environments remains very challenging. Extensive use of data augmentation is a promising technique for improving generalization in RL, but it is often found to decrease sample efficiency and can even lead to divergence. In this paper, we investigate causes of instability when using data augmentation in common off-policy RL algorithms. We identify two problems, both rooted in high-variance Q-targets. Based on our findings, we propose a simple yet effective technique for stabilizing this class of algorithms under augmentation. We perform extensive empirical evaluation of image-based RL using both ConvNets and Vision Transformers (ViT) on a family of benchmarks based on DeepMind Control Suite, as well as in robotic manipulation tasks. Our method greatly improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image-based RL. We further show that our method scales to RL with ViT-based architectures, and that data augmentation may be especially important in this setting. Code and videos: https://nicklashansen.github.io/SVEA",https://api.openreview.net/pdf/9fd366557f101b176c436842a6bd1dd03bba561d.pdf
Pretrained Encoders are All You Need,2021,ICML,"['Mina Khan', 'Advait Prashant Rane', 'Srivatsa P', 'Shriram Chenniappa', 'Rishabh Anand', 'Sherjil Ozair', 'Patricia Maes']",poster,"['Unsupervised Representation Learning', 'Atari', 'Reinforcement Learning', 'Pretrained Models']","Data-efficiency and generalization are key challenges in deep learning and deep reinforcement learning as many models are trained on large-scale, domain-specific, and expensive-to-label datasets. Self-supervised models trained on large-scale uncurated datasets have shown successful transfer to diverse settings. We investigate using pretrained image representations and spatio-temporal attention for state representation learning in Atari. We also explore fine-tuning pretrained representations with self-supervised techniques, i.e., contrastive predictive coding, spatio-temporal contrastive learning, and augmentations. Our results show that pretrained representations are at par with state-of-the-art self-supervised methods trained on domain-specific data. Pretrained representations, thus, yield data and compute-efficient state representations.",https://api.openreview.net/pdf/f7808ed24eb174542cff98a887b9b3aa45f314ae.pdf
Visualizing MuZero Models,2021,ICML,"['joery de Vries', 'Ken Voskuil', 'Thomas M. Moerland', 'Aske Plaat']",poster,"['Model-based reinforcement learning', 'value equivalent models', 'representation learning', 'latent models']","MuZero, a model-based reinforcement learning algorithm that uses a value equivalent dynamics model, achieved state-of-the-art performance in Chess, Shogi and the game of Go. In contrast to standard forward dynamics models that predict a full next state, value equivalent models are trained to predict a future value, thereby emphasizing value relevant information in the representations. While value equivalent models have shown strong empirical success, there is no research yet that visualizes and  investigates what types of representations these models actually learn. Therefore, in this paper we visualize the latent representation of MuZero agents. We find that action trajectories may diverge between observation embeddings and internal state transition dynamics, which could lead to instability during planning. Based on this insight, we propose two regularization techniques to stabilize MuZero's performance. Additionally, we provide an open-source implementation of MuZero along with an interactive visualizer of learned representations, which may aid further investigation of value equivalent algorithms.",https://api.openreview.net/pdf/5291aeee6cdca5380d0e16d0c999ef187d0a00cf.pdf
Did I do that? Blame as a means to identify controlled effects in reinforcement learning,2021,ICML,"['Oriol Corcoll Andreu', 'Raul Vicente']",poster,"['reinforcement learning', 'causality', 'unsupervised learning']","Modeling controllable aspects of the environment enable better prioritization of interventions and has become a popular exploration strategy in reinforcement learning methods. Despite repeatedly achieving State-of-the-Art results, this approach has only been studied as a proxy to a reward-based task and has not yet been evaluated on its own. We show that solutions relying on action prediction fail to model important events. Humans, on the other hand, assign blame to their actions to decide what they controlled. Here we propose Controlled Effect Network (CEN), an unsupervised method based on counterfactual measures of blame. CEN is evaluated in a wide range of environments showing that it can identify controlled effects better than popular models based on action prediction.",https://api.openreview.net/pdf/68ba75bc11ef9e4b5873e1023ba4b6df417969de.pdf
Diffeomorphic Explanations with Normalizing Flows,2021,ICML,"['Ann-Kathrin Dombrowski', 'Jan E Gerken', 'Pan Kessel']",oral,"['normalizing flows', 'explanations', 'data manifold']","Normalizing flows are diffeomorphisms which are parameterized by neural networks. As a result, they can induce coordinate transformations in the tangent space of the data manifold. In this work, we demonstrate that such transformations can be used to generate interpretable explanations for decisions of neural networks. More specifically, we perform gradient ascent in the base space of the flow to generate counterfactuals which are classified with great confidence as a specified target class. We analyze this generation process theoretically using Riemannian differential geometry and establish a rigorous theoretical connection between gradient ascent on the data manifold and in the base space of the flow.",https://api.openreview.net/pdf/ebad8bb4870ed872df648764094106a44eb45139.pdf
Efficient Bayesian Sampling Using Normalizing Flows to Assist Markov Chain Monte Carlo Methods,2021,ICML,"['Marylou Gabrié', 'Grant M. Rotskoff', 'Eric Vanden-Eijnden']",oral,"['MCMC', 'normalizing flows', 'Bayesian inference']","Normalizing flows can generate complex target distributions and thus show promise in many applications in Bayesian statistics as an alternative or complement to MCMC for sampling posteriors.
Since no data set from the target posterior distribution is available beforehand, the flow is typically trained using the reverse Kullback-Leibler (KL) divergence that only requires samples from a base distribution. This strategy  may perform poorly when the posterior is complicated and hard to sample with an untrained normalizing flow. 
Here we explore a distinct training strategy, using the direct KL divergence as loss, in which samples from  the posterior are generated by (i) assisting a local MCMC algorithm on the posterior with a normalizing flow to accelerate its mixing rate and (ii) using the data generated this way to train the flow. 
The method only requires a limited amount of \textit{a~priori} input about the posterior, and can be used to estimate the evidence required for model validation, as we illustrate on  examples.",https://api.openreview.net/pdf/2a67d9240685dff7e1d9d5e33938ab3a78921503.pdf
Distilling the Knowledge from Conditional Normalizing Flows,2021,ICML,"['Dmitry Baranchuk', 'Vladimir Aliev', 'Artem Babenko']",spotlight,[],"Normalizing flows are a powerful class of generative models demonstrating strong performance in several speech and vision problems. In contrast to other generative models, normalizing flows are latent variable models with tractable likelihoods and allow for stable training. However, they have to be carefully designed to represent invertible functions with efficient Jacobian determinant calculation. In practice, these requirements lead to overparameterized and sophisticated architectures that are inferior to alternative feed-forward models in terms of inference time and memory consumption. In this work, we investigate whether one can distill flow-based models into more efficient alternatives. We provide a positive answer to this question by proposing a simple distillation approach and demonstrating its effectiveness on state-of-the-art conditional flow-based models for image super-resolution and speech synthesis.",https://api.openreview.net/pdf/0e4fead03f8b99829f8c26d242b7beea5a701a92.pdf
Universal Approximation for Log-concave Distributions using Well-conditioned Normalizing Flows,2021,ICML,"['Holden Lee', 'Chirag Pabbaraju', 'Anish Prasad Sevekari', 'Andrej Risteski']",spotlight,"['affine couplings', 'representational power', 'conditioning', 'Langevin', 'dynamical systems']","Affine-coupling models (Dinh et al., 2014; 2016)are a particularly common type of normalizing flows,  for  which  the  Jacobian  of  the  latent-to-observable-variable transformation is triangular, allowing  the  likelihood  to  be  computed  in  linear time. Despite the widespread usage of affinecouplings, the special structure of the architecture makes understanding their representational power  challenging.   The  question  of  universal approximation  was  only  recently  resolved  by three parallel papers (Huang et al., 2020; Zhanget al., 2020; Koehler et al., 2020) – who showed reasonably regular distributions can be approximated arbitrarily well using affine couplings –albeit with networks with a nearly-singular Jacobian.  As ill-conditioned Jacobians are an obstacle  for  likelihood-based  training,  the  funda-mental question remains: which distributions can be approximated using well-conditioned affine coupling flows? In this paper, we show that any log-concave distribution can be approximated using well-conditioned affine-coupling flows.   Interms of proof techniques, we uncover and lever-age deep connections between affine coupling architectures, underdamped Langevin dynamics (a stochastic differential equation often used to sample from Gibbs measures) and H ́enon maps (a structured dynamical system that appears in the study of symplectic diffeomorphisms). In terms of informing practice, we approximate a padded version of the input distribution with iid Gaussians – a strategy which (Koehler et al., 2020) empirically observed to result in better-conditioned flows, but had hitherto no theoretical grounding. Our proof can thus be seen as providing theoretical evidence for the benefits of Gaussian padding when training normalizing flows.",https://api.openreview.net/pdf/639205294154e7c96a849f58771388d9a4dc6afe.pdf
Task-agnostic Continual Learning with Hybrid Probabilistic Models,2021,ICML,"['Polina Kirichenko', 'Mehrdad Farajtabar', 'Dushyant Rao', 'Balaji Lakshminarayanan', 'Nir Levine', 'Ang Li', 'Huiyi Hu', 'Andrew Gordon Wilson', 'Razvan Pascanu']",spotlight,"['continual learning', 'normalizing flows', 'generative models']","Learning new tasks continuously without forgetting on a constantly changing data distribution is essential for real-world problems but extremely challenging for modern deep learning. In this work we propose HCL, a Hybrid generative-discriminative approach to Continual Learning for classification. We model the distribution of each task and each class with a normalizing flow. The flow is used to learn the data distribution, perform classification, identify task changes and avoid forgetting, all leveraging the invertibility and exact likelihood which are uniquely enabled by the normalizing flow model. We use the generative capabilities of the flow to avoid catastrophic forgetting through generative replay and a novel functional regularization technique. For task identification, we use state-of-the-art anomaly detection techniques based on measuring the typicality of model's statistics. We demonstrate the strong performance of HCL on a range of continual learning benchmarks such as split-MNIST, split-CIFAR and SVHN-MNIST.",https://api.openreview.net/pdf/f0a9661ca2c5b450cc0f2f48c8ffc70e20a15f79.pdf
Conformal Embedding Flows: Tractable Density Estimation on Learned Manifolds,2021,ICML,"['Brendan Leigh Ross', 'Jesse C Cresswell']",spotlight,"['generative modelling', 'normalizing flows', 'injective flows', 'image generation', 'manifold learning']","Normalizing flows are generative models that provide tractable density estimation by transforming a simple distribution into a complex one. However, flows cannot directly model data supported on an unknown low-dimensional manifold. We propose Conformal Embedding Flows, which learn low-dimensional manifolds with tractable densities. We argue that composing a standard flow with a trainable conformal embedding is the most natural way to model manifold-supported data. To this end, we present a series of conformal building blocks and demonstrate experimentally that flows can model manifold-supported distributions without sacrificing tractable likelihoods.",https://api.openreview.net/pdf/5263cc8a29c2340c0ff79aa268a329db1d53c82a.pdf
Why be adversarial? Let's cooperate!: Cooperative Dataset Alignment via JSD Upper Bound,2021,ICML,"['Wonwoong Cho', 'Ziyu Gong', 'David I. Inouye']",spotlight,"['Unsupervised dataset alignment', 'Invertible flows']","Unsupervised dataset alignment estimates a transformation that maps two or more source domains to a shared aligned domain given only the domain datasets. This task has many applications including generative modeling, unsupervised domain adaptation, and socially aware learning. Most prior works use adversarial learning (i.e., min-max optimization), which can be challenging to optimize and evaluate. A few recent works explore non-adversarial flow-based (i.e., invertible) approaches, but they lack a unified perspective. Therefore, we propose to unify and generalize previous flow-based approaches under a single non-adversarial framework, which we prove is equivalent to minimizing an upper bound on the Jensen-Shannon Divergence (JSD). Importantly, our problem reduces to a min-min, i.e., cooperative, problem and can provide a natural evaluation metric for unsupervised dataset alignment.",https://api.openreview.net/pdf/e2a8743e99cc15246724d0f695b58bb2d8e7a3be.pdf
Sliced Iterative Normalizing Flows,2021,ICML,"['Biwei Dai', 'Uros Seljak']",spotlight,"['Generative Models', 'Normalizing Flow', 'Optimal Transport', 'Sliced Wasserstein Distance']","We develop an iterative (greedy) deep learning algorithm which is able to transform an arbitrary probability distribution function (PDF) into the target PDF. The model is based on iterative Optimal Transport of a series of 1D slices, matching on each slice the marginal PDF to the target.  As special cases of this algorithm, we introduce two sliced iterative Normalizing Flow (SINF) models, which map from the data to the latent space (GIS) and vice versa (SIG). We show that SIG is able to generate high quality samples of image datasets, which match the GAN benchmarks. GIS obtains competitive results on density estimation tasks compared to the density trained NFs. SINF has very few hyperparameters and is very stable during training. When trained on small training sets it is both faster and achieves higher $p(x)$ than current alternatives. ",https://api.openreview.net/pdf/f8811256872baa97d65a7671002cbfe80ea0c93c.pdf
Representational aspects of depth and conditioning in normalizing flows,2021,ICML,"['Frederic Koehler', 'Viraj Mehta', 'Andrej Risteski']",spotlight,"['normalizing flows', 'representation', 'function approximation', 'depth', 'conditioning']","Normalizing flows are among the most popular paradigms in generative modeling, especially for images, primarily because we can efficiently evaluate the likelihood of a data point. Training normalizing flows can be difficult because models which produce good samples typically need to be extremely deep and can often be poorly conditioned: since they are parametrized as invertible maps from $\mathbb{R}^d \to \mathbb{R}^d$, and typical training data like images intuitively is lower-dimensional, the learned maps often have Jacobians that are close to being singular. In our paper, we tackle representational aspects around depth and conditioning of normalizing flows: both for general invertible architectures, and for a particular common architecture, affine couplings. We prove that $\Theta(1)$ affine coupling layers suffice to exactly represent a permutation or $1 \times 1$ convolution, as used in GLOW, showing that representationally the choice of partition is not a bottleneck for depth. We also show that shallow affine coupling networks are universal approximators in Wasserstein distance if ill-conditioning is allowed, and experimentally investigate related phenomena involving padding. Finally, we show a depth lower bound for general flow architectures with few neurons per layer and bounded Lipschitz constant.",https://api.openreview.net/pdf/5850e46a57ad4db7eba7b774d15141264be85ddd.pdf
Interpreting diffusion score matching using normalizing flow,2021,ICML,"['Wenbo Gong', 'Yingzhen Li']",spotlight,"['normalizing flow', 'score matching', 'Stein discrepancy']","Scoring matching (SM), and its related counterpart, Stein discrepancy (SD) have achieved great success in model training and evaluations. However, recent research shows their limitations when dealing with certain types of distributions. One possible fix is incorporating the original score matching (or Stein discrepancy) with a diffusion matrix, which is called diffusion score matching (DSM) (or diffusion Stein discrepancy (DSD)) . However, the lack of the interpretation of the diffusion limits its usage within simple distributions and manually chosen matrix. In this work, we plan to fill this gap by interpreting the diffusion matrix using normalizing flows. Specifically, we theoretically prove that DSM (or DSD) is equivalent to the original score matching (or score matching) evaluated in the transformed space defined by the normalizing flow, where the diffusion matrix is the inverse of the flow's Jacobian matrix. In addition, we also build its connection to Riemannian manifolds, and further extend it to continuous flows, where the change of DSM is characterized by an ODE. ",https://api.openreview.net/pdf/2d3c030fb097e76bcc4d18044b65e546a9775bed.pdf
Rectangular Flows for Manifold Learning,2021,ICML,"['Anthony L. Caterini', 'Gabriel Loaiza-Ganem', 'Geoff Pleiss', 'John Patrick Cunningham']",spotlight,"['Normalizing Flows', 'Injective Flows', 'Manifold Learning', 'Density Estimation']","Normalizing flows allow for tractable maximum likelihood estimation of their parameters but are incapable of modelling low-dimensional manifold structure in observed data. Flows which injectively map from low- to high-dimensional space provide promise for fixing this issue, but the resulting likelihood-based objective becomes more challenging to evaluate. Current approaches avoid computing the entire objective -- which may induce pathological behaviour -- or assume the manifold structure is known beforehand and thus are not widely applicable. Instead, we propose two methods relying on tricks from automatic differentiation and numerical linear algebra to either evaluate or approximate the full likelihood objective, performing end-to-end manifold learning and density estimation. We study the trade-offs between our methods, demonstrate improved results over previous injective flows, and show promising results on out-of-distribution detection.",https://api.openreview.net/pdf/0edc3b7e0da7cdab9a3a0d70ec8074b851c86485.pdf
Discrete Denoising Flows,2021,ICML,"['Alexandra Lindt', 'Emiel Hoogeboom']",spotlight,"['Discrete Flows', 'Lossless Compression']","Discrete flow-based models are a recently proposed class of generative models that learn invertible transformations for discrete random variables. Since they do not require data dequantization and maximize an exact likelihood objective, they can be used in a straight-forward manner for lossless compression. In this paper, we introduce a new discrete flow-based model for categorical random variables: Discrete Denoising Flows (DDFs). In contrast with other discrete flow-based models, our model can be locally trained without introducing gradient bias. We show that DDFs outperform Discrete Flows on modelling a toy example, binary MNIST and Cityscapes segmentation maps, measured in log-likelihood.

",https://api.openreview.net/pdf/add2b673e0ac3b1e3b43eb346358d1ce1920d14f.pdf
On Fast Sampling of Diffusion Probabilistic Models,2021,ICML,"['Zhifeng Kong', 'Wei Ping']",spotlight,"['Fast sampling', 'diffusion probabilistic models', 'image generation', 'audio synthesis']","In this work, we propose FastDPM, a unified framework for fast sampling in diffusion probabilistic models. FastDPM generalizes previous methods and gives rise to new algorithms with improved sample quality. We systematically investigate the fast sampling methods under this framework across different domains, on different datasets, and with different amount of conditional information provided for generation. We find the performance of a particular method depends on data domains (e.g., image or audio), the trade-off between sampling speed and sample quality, and the amount of conditional information. We further provide insights and recipes on the choice of methods for practitioners.",https://api.openreview.net/pdf/6840c6d98e6bb25c6eda6ce73ceee69519d3a91a.pdf
Universal Approximation of Residual Flows in Maximum Mean Discrepancy,2021,ICML,"['Zhifeng Kong', 'Kamalika Chaudhuri']",spotlight,"['Residual flows', 'normalizing flows', 'Maximum Mean Discrepancy', 'universal approximation', 'expressiveness']","Normalizing flows are a class of flexible deep generative models that offer easy likelihood computation. Despite their empirical success, there is little theoretical understanding of their expressiveness. In this work, we study residual flows, a class of normalizing flows composed of Lipschitz residual blocks. We prove residual flows are universal approximators in maximum mean discrepancy. We provide upper bounds on the number of residual blocks to achieve approximation under different assumptions. ",https://api.openreview.net/pdf/6cc0168080deffc789c17ecac9f1854906338cb3.pdf
Discovering and Achieving Goals with World Models,2021,ICML,"['Russell Mendonca', 'Oleh Rybkin', 'Kostas Daniilidis', 'Danijar Hafner', 'Deepak Pathak']",oral,"['unsupervised goal reaching', 'unsupervised rl', 'goal-conditioned rl', 'exploration', 'model-based rl', 'world models']","How can an artificial agent learn to solve a wide range of tasks in a complex visual environment in the absence of external supervision? We decompose this question into two problems, global exploration of the environment and learning to reliably reach situations found during exploration. We introduce the Explore Achieve Network (ExaNet), a unified solution to these by learning a world model from the high-dimensional images and using it to train an explorer and an achiever policy from imagined trajectories. Unlike prior methods that explore by reaching previously visited states, our explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever. After the unsupervised phase, ExaNet solves tasks specified by goal images without any additional learning. We introduce a challenging benchmark spanning across four standard robotic manipulation and locomotion domains with a total of over 40 test tasks. Our agent substantially outperforms previous approaches to unsupervised goal reaching and achieves goals that require interacting with multiple objects in sequence. Finally, to demonstrate the scalability and generality of our approach, we train a single general agent across four distinct environments. For videos, see https://sites.google.com/view/exanet/home.",https://api.openreview.net/pdf/f72ed04798d1db0963d3667b2caaf400148edc8d.pdf
Learning Task Agnostic Skills with Data-driven Guidance,2021,ICML,"['Even Klemsdal', 'Sverre Herland', 'Abdulmajid Murad']",oral,"['Artificial Intelligence', 'Unsupervised Reinforcement Learning', 'Reinforcement Learning', 'Skill discovery']","To increase autonomy in reinforcement learning, agents need to learn useful behaviours without reliance on manually designed reward functions. To that end, skill discovery methods have been used to learn the intrinsic options available to an agent using task-agnostic objectives. However, without the guidance of task-specific rewards, emergent behaviours are generally useless due to the under-constrained problem of skill discovery in complex and high-dimensional spaces. This paper proposes a framework for guiding the skill discovery towards the subset of expert-visited states using a learned state projection. We apply our method in various reinforcement learning (RL) tasks and show that such a projection results in more useful behaviours.",https://api.openreview.net/pdf/cfb3d77df95f7646dd0260a4d8a374f7497c6036.pdf
Planning from Pixels in Environments with Combinatorially Hard Search Spaces,2021,ICML,"['Marco Bagatella', 'Miroslav Olšák', 'Michal Rolinek', 'Georg Martius']",oral,"['World Model', 'Representation Learning', 'Model-Based Reinforcement Learning']","The ability to form complex plans based on raw visual input is a litmus test for current capabilities of artificial intelligence, as it requires a seamless combination of visual processing and abstract algorithmic execution, two traditionally separate areas of computer science. A recent surge of interest in this field brought advances that yield good performance in tasks ranging from arcade games to continuous control; these methods however do not come without significant issues, such as limited generalization capabilities and difficulties when dealing with combinatorially hard planning instances. Our contribution is two-fold: (i) we present a method that learns to represent its environment as a latent graph and leverages state reidentification to reduce the complexity of finding a good policy from exponential to linear (ii) we introduce a set of lightweight environments with an underlying discrete combinatorial structure in which planning is challenging even for humans. Moreover, we show that our methods achieves strong empirical generalization to variations in the environment, even across highly disadvantaged regimes, such as “one-shot” planning, or in an offline RL paradigm which only provides low-quality trajectories.",https://api.openreview.net/pdf/50ddd9bd8e605fdeacf6cfe1dff541cb702b0afb.pdf
Automated Discovery of Adaptive Attacks on Adversarial Defenses,2021,ICML,"['Chengyuan Yao', 'Pavol Bielik', 'Petar Tsankov', 'Martin Vechev']",oral,[],"Reliable evaluation of adversarial defenses is a challenging task, currently limited to an expert who manually crafts attacks that exploit the defense’s inner workings, or to approaches based on ensemble of fixed attacks, none of which may be effective for the specific defense at hand. Our key observation is that custom attacks are composed from a set of reusable building blocks, such as fine-tuning relevant attack parameters, network transformations, and custom loss functions. Based on this observation, we present an extensible framework that defines a search space over these reusable building blocks and automatically discovers an effective attack on a given model with an unknown defense by searching over suitable combinations of these blocks. We evaluated our framework on 23 adversarial defenses and showed it outperforms AutoAttack, the current state-of-the-art tool for reliable evaluation of adversarial defenses: our discovered attacks are either stronger, producing 3.0%-50.8% additional adversarial examples (10 cases), or are typically 2x faster while enjoying similar adversarial robustness (13 cases).",https://api.openreview.net/pdf/e755a2178bf687543a257452a5f0fe96a072f5ba.pdf
Multimodal AutoML on Structured Tables with Text Fields,2021,ICML,"['Xingjian Shi', 'Jonas Mueller', 'Nick Erickson', 'Mu Li', 'Alex Smola']",oral,"['multimodal', 'automl', 'transformer', 'text']","We  design  automated  supervised  learning  systems  for  data  tables  that  not  only  contain numeric/categorical columns,  but  text  fields  as  well.  Here  we  assemble  15  multimodal data tables that each contain some text fields and stem from a real business application. Over this benchmark, we evaluate numerous multimodal AutoML strategies, including a standard two-stage approach where NLP is used to featurize the text such that AutoML for tabular data can then be applied. We propose various practically superior strategies based on multimodal adaptations of Transformer networks and stack ensembling of these networks with classical tabular models.  Beyond performing the best in our benchmark, our proposed (fully automated) methodology manages to rank 1st place (against human data scientists) when fit to the raw tabular/text data in two MachineHack prediction competitions  and 2nd place (out of 2380 teams) in Kaggle’s Mercari Price Suggestion Challenge.",https://api.openreview.net/pdf/aba28b8aed8429cf3e9e792a64b5e2597ca4075e.pdf
Discovering Weight Initializers with Meta Learning,2021,ICML,"['Dmitry Baranchuk', 'Artem Babenko']",oral,"['meta-learning', 'deep learning']","Deep neural network training largely depends on the choice of initial weight distribution. However, this choice can often be nontrivial.  Existing theoretical results for this problem mostly cover simple architectures, e.g., feedforward networks with ReLU activations.  The architectures  used  for  practical  problems  are  more  complex  and  often  incorporate  many overlapping modules, making them challenging for theoretical analysis.  Therefore, practitioners have to use heuristic initializers with questionable optimality and stability.  In this study, we propose a task-agnostic approach that discovers initializers for specific network architectures and optimizers by learning the initial weight distributions directly through the use of Meta-Learning.  In several supervised and unsupervised learning scenarios, we show the advantage of our initializers in terms of both faster convergence and higher model performance. ",https://api.openreview.net/pdf/a597f1f6c5b44361eab920ddf054a57e116b81d6.pdf
Defending Adversaries Using  Unsupervised Feature Clustering VAE,2021,ICML,"['Cheng Zhang', 'Pan Gao']",poster,[],"We propose a modified VAE (variational autoencoder) as a denoiser to remove adversarial perturbations for image classification. Vanilla VAE's purpose is to make latent variables approximating normal distribution, which reduces the latent inter-class distance of  data points. Our proposed VAE modifies this problem by adding a latent variable cluster. So the VAE can guarantee inter-class distance of latent variables and learn class-wised features. Our Feature Clustering VAE performs better on removing perturbations and reconstructing the image to defend adversarial attacks. ",https://api.openreview.net/pdf/8e7eefbd0d8f6498e9bf48cbf950f2d8ee17ed9f.pdf
Universal Adversarial Head: Practical Protection against Video Data Leakage,2021,ICML,"['Jiawang Bai', 'Bin Chen', 'Dongxian Wu', 'Chaoning Zhang', 'Shu-Tao Xia']",poster,"['privacy protection', 'video retrieval', 'adversarial attack']","While online video sharing becomes more popular, it also causes unconscious leakage of personal information in the video retrieval systems like deep hashing. An adversary can collect users' private information from the video database by querying similar videos. This paper focuses on bypassing the deep video hashing based retrieval to prevent information from being maliciously collected.  We propose $universal \ adversarial \ head$ (UAH), which crafts adversarial query videos by prepending the original videos with a sequence of adversarial frames to perturb the normal hash codes in the Hamming space. This adversarial head can be obtained just using a few videos, and mislead the retrieval system to return irrelevant videos on most natural query videos. Furthermore, to obey the principle of information protection, we expand the proposed method to a data-free paradigm to generate the UAH, without access to users' original videos. Extensive experiments demonstrate the protection effectiveness of our method under various settings.",https://api.openreview.net/pdf/c156b98ad652cd2c8cdbd4efc760ef34d998f1e4.pdf
"Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples",2021,ICML,"['Nelson Manohar-Alers', 'Ryan Feng', 'Sahib Singh', 'Jiguo Song', 'Atul Prakash']",poster,"['adversarial attack detectors', 'adversarial attack classifier', 'anomaly feature vector', 'AFV', 'adversarial machine learning']","We present DeClaW, a system for detecting, classifying,  and warning of adversarial inputs presented to a classification neural network. In contrast to current state-of-the-art methods that, given an input, detect whether an input is clean or adversarial,  we  aim  to  also  identify  the  types  of adversarial attack (e.g., PGD, Carlini-Wagner or clean). To achieve this, we extract statistical profiles, which we term as anomaly feature vectors, from a set of latent features. Preliminary findings suggest that AFVs can help distinguish among several types of adversarial attacks (e.g.,  PGD versus Carlini-Wagner) with close to 93% accuracy on the CIFAR-10 dataset. The results open the door to using AFV-based methods for exploring not only adversarial attack detection but also classification of the attack type and then design of attack-specific mitigation strategies.",https://api.openreview.net/pdf/3b28a2b5176f4b440674ea0ff9e96325efff3626.pdf
Beta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Neural Network Robustness Verification,2021,ICML,"['Shiqi Wang', 'Huan Zhang', 'Kaidi Xu', 'Xue Lin', 'Suman Jana', 'Cho-Jui Hsieh', 'J Zico Kolter']",poster,"['neural network verification', 'branch and bound', 'adversarial robustness', 'robustness certification']","We develop $\beta$-CROWN, a new bound propagation based method that can fully encode neuron split constraints in branch-and-bound (BaB) based complete verification via optimizable parameters $\beta$. When jointly optimized in intermediate layers, $\beta$-CROWN generally produces better bounds than typical LP verifiers with neuron split constraints, while being as efficient and parallelizable as CROWN on GPUs. Applied to complete robustness verification benchmarks, $\beta$-CROWN with BaB is close to three orders of magnitude faster than LP-based BaB methods, and is at least 3 times faster than winners of VNN-COMP 2020 competition while producing lower timeout rates. By terminating BaB early, our method can also be used for efficient incomplete verification. We achieve higher verified accuracy in many settings over powerful incomplete verifiers, including those based on convex barrier breaking techniques. Compared to the typically tightest but very costly semidefinite programming (SDP) based incomplete verifiers, we obtain higher verified accuracy with three orders of magnitudes less verification time, and enable better certification for verification-agnostic (e.g., adversarially trained) networks.",https://api.openreview.net/pdf/6b6a15c90c8b2b5b36962fccdf07d49a9a61e150.pdf
Less is More: Feature Selection for Adversarial Robustness with Compressive Counter-Adversarial Attacks,2021,ICML,"['Emre Ozfatura', 'Muhammad Zaid Hameed', 'Kerem Ozfatura', 'Deniz Gunduz']",poster,"['Adversarial learning', 'compression', 'counter-attack', 'activation supression']","A common observation regarding adversarial attacks is that they mostly give rise to false activation at the penultimate layer to fool the classifier.  Assuming that these activation values correspond to certain features of the input, the objective  becomes  choosing  the  features  that  are most useful for classification. Hence, we propose a novel approach to identify the important features by employing counter-adversarial attacks, which highlights the consistency at the penultimate layer with respect to perturbations on input samples.  First, we empirically show that there exist a subset of features, classification based in which bridge the gap between the clean and robust accuracy. Second, we propose a simple yet efficient mechanism to identify those features by searching the neighborhood of input sample. We then select features by observing the consistency of the activation values at the penultimate layer.",https://api.openreview.net/pdf/9c4767f10a6d647d0357549ca9672ab54945ab2a.pdf
Limited Budget Adversarial Attack Against Online Image Stream,2021,ICML,"['Hossein Mohasel Arjomandi', 'Mohammad Khalooei', 'Maryam Amirmazlaghani']",poster,"['Adversarial Attack', 'Online Image Stream', 'Threat Model']","An adversary wants to attack a limited number of images within a stream of known length to reduce
the exposure risk. Also, the adversary wants to maximize the success rate of the performed attacks. We show that with very minimal changes in images data, majority of attacking attempt would fail, however some attempts still lead to succeed. We detail an algorithm that choose the optimal images which lead to successful attack . We apply our approach on MNIST and prove it’s significant outcome compared to the state of the art.",https://api.openreview.net/pdf/2cdf2a4d75aab8c186d1d2b4e71829c13a76fe1c.pdf
Maximizing the robust margin provably overfits on noiseless data,2021,ICML,"['Konstantin Donhauser', 'Alexandru Tifrea', 'Michael Aerni', 'Reinhard Heckel', 'Fanny Yang']",poster,"['robust overfitting', 'adversarial robustness', 'adversarial training', 'theory']","Numerous recent works show that overparameterization implicitly reduces variance, suggesting vanishing benefits for explicit regularization in high dimensions. However, this narrative has been challenged by empirical observations indicating that adversarially trained deep neural networks suffer from robust overfitting. While existing explanations attribute this phenomenon  to noise or problematic samples in the training data set, we prove that even on entirely noiseless data, achieving a vanishing adversarial logistic training loss is suboptimal compared to regularized counterparts.",https://api.openreview.net/pdf/360b481337fce563121c3ba45a4acc2634954d0d.pdf
Generate More Imperceptible Adversarial Examples for Object Detection,2021,ICML,"['Siyuan Liang', 'Xingxing Wei', 'Xiaochun Cao']",poster,"['adversarial attack', 'object detection', 'transfer attack']","Object detection methods based on deep neural networks are vulnerable to adversarial examples. The existing attack methods have the following problems: 1) the training generator takes a long time and is difﬁcult to extend to a large dataset; 2) the excessive destruction of the image features does not improve the black-box attack effect(the generated adversarial examples have poor transferability) and brings about visible perturbations. In response to these problems, we proposed a more imperceptible attack(MI attack) with a stopping condition of feature destruction and a noise cancellation mechanism. Finally, the generator generates subtle adversarial perturbations, which can not only attack the object detection models that are based on proposal and regression but also boost the training speed by 4-6 times. Experiments show that the MI method has achieved state-of-the-art attack performance in the large datasets PASCAL VOC.",https://api.openreview.net/pdf/8dd38924490919b2e7886a51851ca08293cfc749.pdf
Robust Recovery of Adversarial Examples,2021,ICML,"['Tejas Bana', 'Jatan Loya', 'Siddhant Ravindra Kulkarni']",poster,"['Recovery Adversarial Sample', 'FGSM', 'RFGSM', 'Attention UNet', 'Self Attention', 'GAN']","Adversarial examples are semantically associated with one class, but modern deep learning architectures fail to see the semantics and associate them to another class. As a result, these examples pose a profound risk to almost every deep learning model. Our proposed architecture can recover such examples effectively with more than 4x the magnitude of attacks than the capability of the state-of-the-art model, despite having lesser parameters than the VGG-13 model. It is composed of a U-Net with the characteristics of self-attention & cross-attention, which enhances the semantics of the image. Our work also encompasses the differences in the results between Noise and Image reconstruction methodologies.",https://api.openreview.net/pdf/0ffe52f4c4626034fe2f78700fb4b0042b1ac386.pdf
The Interplay between Distribution Parameters and the Accuracy-Robustness Tradeoff in Classification,2021,ICML,"['Alireza Mousavi Hosseini', 'Amir Mohammad Abouei', 'Mohammad Hossein Rohban']",poster,"['Adversarial Robustness', 'Accuracy-Robustness Tradeoff', 'Natural Accuracy Gap']","Adversarial training tends to result in models that are less accurate on natural (unperturbed) examples compared to standard models. This can be attributed to either an algorithmic shortcoming or a fundamental property of the training data distribution, which admits different solutions for optimal standard and adversarial classifiers. In this work, we focus on the latter case under a binary Gaussian mixture classification problem. Unlike earlier work, we aim to derive the natural accuracy gap between the optimal Bayes and adversarial classifiers, and study the effect of different distributional parameters, namely separation between class centroids, class proportions, and the covariance matrix, on the derived gap. We show that under certain conditions, the natural error of the optimal adversarial classifier, as well as the gap, are locally minimized when classes are balanced, contradicting the performance of the Bayes classifier where perfect balance induces the worst accuracy. Moreover, we show that with an $\ell_\infty$ bounded perturbation and an adversarial budget of $\epsilon$, this gap is $\Theta(\epsilon^2)$ for the worst-case parameters, which for suitably small $\epsilon$ indicates the theoretical possibility of achieving robust classifiers with near-perfect accuracy, which is rarely reflected in practical algorithms.",https://api.openreview.net/pdf/c2ae306c9ad2091fc9d30b818f4fe3fe40aa34f6.pdf
Fast Certified Robust Training with Short Warmup,2021,ICML,"['Zhouxing Shi', 'Yihan Wang', 'Huan Zhang', 'Jinfeng Yi', 'Cho-Jui Hsieh']",poster,"['Adversarial robustness', 'certified robustness', 'certified robust training']","State-of-the-art (SOTA) methods for certified robust training including interval bound propagation (IBP) and CROWN-IBP usually use a long warmup schedule with hundreds or thousands epochs and are thus costly. In this paper, we identify two important issues, namely exploded bounds at initialization, and the imbalance in ReLU activation states, which make certified training difficult and unstable, and thereby long warmup was previously needed. For fast training with short warmup, we propose three improvements, including a weight initialization for IBP training, fully adding Batch Normalization (BN), and regularization during warmup to tighten certified bounds and balance ReLU activation states.  With a short warmup for fast training, we are already able to outperform literature SOTA trained with hundreds or thousands epochs under the same network architecture.",https://api.openreview.net/pdf/cb92978a2caa3ca94cc51111660d5a6a70784ce4.pdf
Combating Adversaries with Anti-Adversaries,2021,ICML,"['Motasem Alfarra', 'Juan Camilo Perez', 'Ali Thabet', 'Adel Bibi', 'Philip Torr', 'Bernard Ghanem']",poster,"['Adversarial Attacks', 'Network Robustness']","Deep neural networks are vulnerable to small input perturbations known as adversarial attacks. Inspired by the fact that these adversaries are constructed by iteratively minimizing the confidence of a network for the true class label, we propose the anti-adversary layer, aimed at countering this effect. In particular, our layer generates an input perturbation in the opposite direction of the adversarial one and feeds the classifier a perturbed version of the input. Our approach is training-free and theoretically supported. We verify the effectiveness of our approach by combining our layer with both nominally and robustly trained models, and conduct large-scale experiments from black-box to adaptive attacks on CIFAR10, CIFAR100 and ImageNet. Our anti-adversary layer significantly enhances model robustness while coming at no cost on clean accuracy. ",https://api.openreview.net/pdf/3b55393c7aba35d1e2cca0d27d225adf2a516c21.pdf
Strategically-timed State-Observation Attacks on Deep Reinforcement Learning Agents,2021,ICML,"['You Qiaoben', 'Xinning Zhou', 'Chengyang Ying', 'Jun Zhu']",poster,[]," Deep reinforcement learning (DRL) policies are vulnerable to the adversarial attack on their observations, which may mislead real-world RL agents to catastrophic failures. Several works have shown the effectiveness of this type of adversarial attacks. But these adversaries are inclined to be detected because these adversaries do not inhibit their attacks activity. Recent works provide heuristic methods by attacking the victim agent at a small subset of time steps, but it aims at lack for theoretical principles. Inspired by the idea that adversarial attacks at each time step have different efforts, we denote a novel strategically-timed attack called Tentative Frame Attack for continuous control environments. We further propose a theoretical framework of finding optimal frame attack. Following this framework, we trained the frame attack strategy online with the victim agents and a fixed adversary. The empirical results show that our adversaries achieve the state-of-the-art performance on DRL agents which outperforms the full-timed attack.",https://api.openreview.net/pdf/970d86aca5ab8dc7b48d673cea74e8b0775f61e7.pdf
Entropy Weighted Adversarial Training,2021,ICML,"['Minseon Kim', 'Jihoon Tack', 'Jinwoo Shin', 'Sung Ju Hwang']",poster,"['Adversarial training', 'Entropy weighting']","Adversarial training methods, which minimizes the loss of adversarially-perturbed training examples, have been extensively studied as a solution to improve the robustness of the deep neural networks. However, most adversarial training methods treat all training examples equally, while each example may have a different impact on the model's robustness during the course of training. Recent works have exploited such unequal importance of adversarial samples to model's robustness, which has been shown to obtain high robustness against untargeted PGD attacks. However, we empirically observe that they make the feature spaces of adversarial samples across different classes overlap, and thus yield more high-entropy samples whose labels could be easily flipped. This makes them more vulnerable to targeted adversarial perturbations. Moreover, to address such limitations, we propose a simple yet effective weighting scheme, Entropy-Weighted Adversarial Training (EWAT), which weighs the loss for each adversarial training example proportionally to the entropy of its predicted distribution, to focus on examples whose labels are more uncertain. We validate our method on multiple benchmark datasets and show that it achieves an impressive increase of robust accuracy.",https://api.openreview.net/pdf/48c0a5b920ae14054a8ccada2e0f60a99e30b2b9.pdf
Adversarial Interaction Attacks: Fooling AI to Misinterpret Human Intentions,2021,ICML,"['Nodens Koren', 'Xingjun Ma', 'Qiuhong Ke', 'Yisen Wang', 'James Bailey']",poster,[],"Understanding the actions of both humans and artificial intelligence (AI) agents is important before modern AI systems can be fully integrated into our daily life. In this paper, we show that, despite their current huge success, deep learning based AI systems can be easily fooled by subtle adversarial noise to misinterpret the intention of an action in interaction scenarios. Based on a case study of skeleton-based human interactions, we propose a novel adversarial attack on interactions, and demonstrate how DNN-based interaction models can be tricked to predict the participants' reactions in unexpected ways. Our study highlights potential risks in the interaction loop with AI and humans, which need to be carefully addressed when deploying AI systems in safety-critical applications.",https://api.openreview.net/pdf/20f4508584f1ce081da14517b645223e966789dc.pdf
Towards Safe Reinforcement Learning via Constraining Conditional Value at Risk,2021,ICML,"['Chengyang Ying', 'Xinning Zhou', 'Dong Yan', 'Jun Zhu']",poster,"['Reinforcement Learning', 'Risk Sensitive', 'Safety', 'Policy Optimization', 'Reward Evaluation']","Though deep reinforcement learning (DRL) has obtained substantial success, it may encounter catastrophic failures due to the intrinsic uncertainty caused by stochastic policies and environment variability. To address this issue, we propose a novel reinforcement learning framework of CVaR-Proximal-Policy-Optimization (CPPO) by rating the conditional value-at-risk (CVaR) as an assessment for risk. We show that performance degradation under observation state disturbance and transition probability disturbance theoretically depends on the range of disturbance as well as the gap of value function between different states. Therefore, constraining the value function among states with CVaR can improve the robustness of the policy.  Experimental results show that CPPO achieves higher cumulative reward and exhibits stronger robustness against observation state disturbance and transition probability disturbance in environment dynamics among a series of continuous control tasks in MuJoCo. ",https://api.openreview.net/pdf/423a37a366fbd06a376c59254370c49ca58c595f.pdf
Adversarial Semantic Contour for Object Detection,2021,ICML,"['Yichi Zhang', 'Zijian Zhu', 'Xiao Yang', 'Jun Zhu']",poster,"['Adversarial Attack', 'Object Detection']","Modern object detectors are vulnerable to adversarial examples, which brings potential risks to numerous applications, e.g., self-driving car. Among attacks regularized by $\ell_p$ norm, $\ell_0$-attack aims to modify as few pixels as possible. Nevertheless, the problem is nontrivial since it generally requires to optimize the shape along with the texture simultaneously, which is an NP-hard problem. To address this issue, we propose a novel method of Adversarial Semantic Contour (ASC) guided by object contour as prior. With this prior, we reduce the searching space to accelerate the $\ell_0$ optimization, and also introduce more semantic information which should affect the detectors more. Based on the contour, we optimize the selection of modified pixels via sampling and their colors with gradient descent alternately. Extensive experiments demonstrate that our proposed ASC outperforms the most commonly manually designed patterns (e.g., square patches and grids) on task of disappearing. By modifying no more than 5\% and 3.5\% of the object area respectively, our proposed ASC can successfully mislead the mainstream object detectors including the SSD512, Yolov4, Mask RCNN, Faster RCNN, etc.",https://api.openreview.net/pdf/b39bd40c1607037496b17568925ced1b0b365263.pdf
Membership Inference Attacks on Lottery Ticket Networks,2021,ICML,"['Aadesh Mahavir Bagmar', 'Shishira Maiya', 'Shruti Bidwalkar', 'Amol Deshpande']",poster,"['Membership Inference Attacks', 'Lottery Tickets', 'Adversarial Attacks', 'Machine Learning', 'Privacy', 'Security', 'Empirical Study']","The vulnerability of the Lottery Ticket Hypothesis has not been studied from the purview of Membership Inference Attacks. Through this work, we are the first to empirically show that the lottery ticketed networks are equally vulnerable to membership inference attacks. A Membership Inference Attack (MIA) is the process of determining whether a data sample belongs to a training set of a trained model or not. Membership Inference Attacks could leak critical information about the training data that can be used for targeted attacks. Recent deep learning models often have very large memory footprints and a high computational cost associated with training and drawing inferences. Lottery Ticket Hypothesis is used to prune the networks to find smaller sub-networks that at least match the performance of the original model in terms of test accuracy in a similar number of iterations. We used  CIFAR-10, CIFAR-100, and ImageNet datasets to perform image classification tasks and observe that the attack accuracies are similar. We also see that the attack accuracy varies directly according to the number of classes in the dataset and the sparsity of the network. We demonstrate that these attacks are transferable across models with high accuracy.",https://api.openreview.net/pdf/ef07e710d4c2929dc238dfa58bcf159c628f7da7.pdf
Adversarially Robust Learning via Entropic Regularization,2021,ICML,"['Gauri Jagatap', 'Ameya Joshi', 'Animesh Basak Chowdhury', 'Siddharth Garg', 'Chinmay Hegde']",poster,"['adversarial learning', 'entropy', 'robustness', 'adversarial training', 'robust optimization']","In this paper we propose a new family of algorithms, ATENT, for training adversarially robust deep neural networks. We formulate a new loss function that is equipped with an entropic regularization. Our loss considers the contribution of adversarial samples that are drawn from a specially designed distribution that assigns high probability to points with high loss and in the immediate neighborhood of training samples. ATENT achieves competitive (or better) performance in terms of robust classification accuracy as compared to several state-of-the-art robust learning approaches on benchmark datasets such as MNIST and CIFAR-10.",https://api.openreview.net/pdf/c20ad0cf1365fb89bb473d329dc4d6bc6d7ffcba.pdf
Non-Robust Feature Mapping in Deep Reinforcement Learning,2021,ICML,['Ezgi Korkmaz'],poster,"['deep reinforcement learning', 'adversarial examples', 'robustness', 'deep learning', 'adversarial training', 'adversarial', 'reinforcement learning', 'security', 'safety']","Adversarial perturbations to state observations can dramatically degrade the performance of deep reinforcement learning policies, and thus raise concerns regarding the robustness of deep reinforcement learning agents. A sizeable body of work has focused on addressing the robustness problem in deep reinforcement learning, and there are several recent proposals for adversarial training methods in the deep reinforcement learning domain. In our work we focus on the robustness of state-of-the-art adversarially trained deep reinforcement learning policies and vanilla trained deep reinforcement learning polices. We propose two novel algorithms to map non-robust features in deep reinforcement learning policies. We conduct several experiments in the Arcade Learning Environment (ALE), and with our proposed feature mapping algorithms we show that while the state-of-the-art adversarial training method eliminates a certain set of non-robust features, a new set of non-robust features more intrinsic to the adversarial training are created. Our results lay out concerns that arise when using existing state-of-the-art adversarial training methods, and we believe our proposed feature mapping algorithm can aid in the process of building more robust deep reinforcement learning policies.",https://api.openreview.net/pdf/4fb7589299a2f6042a93bb4f183b36b6ffa62bcb.pdf
Adversarially Trained Neural Policies in the Fourier Domain,2021,ICML,['Ezgi Korkmaz'],poster,"['reinforcement learning', 'deep learning', 'adversarial', 'deep reinforcement learning', 'robustness', 'adversarial training', 'deep neural networks', 'safety']","Reinforcement learning policies based on deep neural networks are vulnerable to imperceptible adversarial perturbations to their inputs, in much the same way as neural network image classifiers. Recent work has proposed several methods for adversarial training for deep reinforcement learning agents to improve robustness to adversarial perturbations. In this paper, we study the effects of adversarial training on the neural policy learned by the agent. In particular, we compare the Fourier spectrum of minimal perturbations computed for both adversarially trained and vanilla trained neural policies. Via experiments in the OpenAI Atari environments we show that minimal perturbations computed for adversarially trained policies are more focused on lower frequencies in the Fourier domain, indicating a higher sensitivity of these policies to low frequency perturbations. We believe our results can be an initial step towards understanding the relationship between adversarial training and different notions of robustness for neural policies.",https://api.openreview.net/pdf/2a705ae6cc544c5ee6b365ea1688940ab67b39e1.pdf
Detecting AutoAttack Perturbations in the Frequency Domain,2021,ICML,"['Peter Lorenz', 'Paula Harder', 'Dominik Straßel', 'Margret Keuper', 'Janis Keuper']",poster,"['autoattack', 'spectral defense', 'cifar', 'imagenet', 'fourier', 'defense']","Recently, adversarial attacks on image classification networks by the AutoAttack (Croce & Hein, 2020b) framework have drawn a lot of attention. While AutoAttack has shown a very high attack success rate, most defense approaches are focusing on network hardening and robustness enhancements, like adversarial training. This way, the currently best-reported method can withstand ∼ 66% of adversarial examples on CIFAR10. In this paper, we investigate the spatial and frequency domain properties of AutoAttack and propose an alternative defense. Instead of hardening a network, we detect adversarial attacks during inference, rejecting manipulated inputs. Based on a rather simple and fast analysis in the frequency domain, we introduce two different detection algorithms. First, a black box detector which only operates on the input images and achieves a detection accuracy of 100% on the  AutoAttack CIFAR10 benchmark and 99.3% on ImageNet, for eps = 8/255 in both cases. Second, a whitebox detector using an analysis of CNN featuremaps, leading to a detection rate of also 100% and 98.7% on the same benchmarks. ",https://api.openreview.net/pdf/3c77140a5987f9273f6aca9b86e9165bc7703f4a.pdf
Adversarial EXEmples: Functionality-preserving Optimization of Adversarial Windows Malware,2021,ICML,"['Luca Demetrio', 'Battista Biggio', 'Giovanni Lagorio', 'Alessandro Armando', 'Fabio Roli']",poster,"['malware', 'adversarial machine learning']","Windows malware classifiers that rely on static analysis have been proven vulnerable to adversarial EXEmples, i.e., malware samples carefully manipulated to evade detection.
However, such attacks are typically optimized via query-inefficient algorithms that iteratively apply random manipulations on the input malware, and require checking that the malicious functionality is preserved after manipulation through computationally-expensive validations.
To overcome these limitations, we propose RAMEn, a general framework for creating adversarial EXEmples via  functionality-preserving manipulations.
RAMEn optimizes their parameters of such manipulations via gradient-based (white-box) and gradient-free (black-box) attacks, implementing many state-of-the-art attacks for crafting adversarial Windows malware. It also includes a family of black-box attacks, called GAMMA, which optimize the injection of benign content to facilitate evasion. Our experiments show that gradient-based and gradient-free attacks can bypass  malware detectors based on deep learning, non-differentiable models trained on hand-crafted features, and even some renowned commercial products.",https://api.openreview.net/pdf/cdf82d9c8790047801f246670f212de87b4e7b40.pdf
AID-Purifier: A Light Auxiliary Network for Boosting Adversarial Defense,2021,ICML,"['Duhun Hwang', 'Eunjung Lee', 'Wonjong Rhee']",poster,"['Adversarial defense', 'adversarial purification', 'adversarial attack']","We propose an AID-purifier that can boost the robustness of adversarially-trained networks by purifying their inputs. AID-purifier is an auxiliary network that works as an add-on to an already trained main classifier. To keep it computationally light, it is trained as a discriminator with a binary cross-entropy loss. To obtain additionally useful information from the adversarial examples, the architecture design is closely related to information maximization principles where two layers of the main classification network are piped to the auxiliary network. To assist the iterative optimization procedure of purification, the auxiliary network is trained with AVmixup. AID-purifier can be used together with other purifiers such as PixelDefend for an extra enhancement. The overall results indicate that the best performing adversarially-trained networks can be enhanced by the best performing purification networks, where AID-purifier is a competitive candidate that is light and robust.",https://api.openreview.net/pdf/1cdf8ee9484aee89568632b5cb437ff2a78fe895.pdf
Audio Injection Adversarial Example Attack,2021,ICML,"['Xiaolei Liu', 'Xingshu Chen', 'Mingyong Yin', 'Yulong Wang', 'Teng Hu', 'Kangyi Ding']",poster,"['adversarial example', 'audio injection attack']","We study the problem of audio adversarial example attacks with sparse perturbations. Compared with image adversarial example attacks, attacking audio is more challenging because the audio structure is more complex and the perturbation is difficult to conceal. To overcome this challenge, we propose an audio injection adversarial example attack, which provides a new sight light to increase the concealment of attack behavior. Experiments demonstrate that the proposed audio injection adversarial example attack can significantly reduce the perturbation proportion and achieve a better attack effect than traditional attack methods.",https://api.openreview.net/pdf/ba213d51e6bdabb957eb5ab3bdccacb4e30bbc3a.pdf
On Success and Simplicity: A Second Look at Transferable Targeted Attacks,2021,ICML,"['Zhengyu Zhao', 'Zhuoran Liu', 'Martha Larson']",poster,"['Adversarial attacks', 'targeted transferability', 'meaningful evaluation', 'realistic settings']","Achieving transferability of targeted attacks is reputed to be remarkably difficult, and state-of-the-art approaches are resource-intensive due to training target-specific model(s) with additional data. In our work, we find, however, that simple transferable attacks which require neither additional data nor model training can achieve surprisingly high targeted transferability. This insight has been overlooked mainly due to the widespread practice of unreasonably restricting attack optimization to few iterations. In particular, we, for the first time, identify the state-of-the-art performance of a simple logit loss. Our investigation is conducted in a wide range of transfer settings, especially including three new, realistic settings: ensemble transfer with little model similarity, transfer to low-ranked target classes, and transfer to the real-world Google Cloud Vision API. Results in these new settings demonstrate that the commonly adopted, easy settings cannot fully reveal the actual properties of different attacks and may cause misleading comparisons. Overall, the aim of our analysis is to inspire a more meaningful evaluation on targeted transferability.",https://api.openreview.net/pdf/e6dc48929e8fca6d50a5ba06107ddadb7087ae83.pdf
Uncovering Universal Features: How Adversarial Training Improves Adversarial Transferability,2021,ICML,"['Jacob M. Springer', 'Melanie Mitchell', 'Garrett T. Kenyon']",poster,"['adversarial examples', 'transferability', 'universality']","Adversarial examples for neural networks are known to be transferable: examples optimized to be misclassified by a “source” network are often misclassified by other “destination” networks. Here, we show that training the source network to be “slightly robust”---that is, robust to small-magnitude adversarial examples---substantially improves the transferability of targeted attacks, even between architectures as different as convolutional neural networks and transformers. In fact, we show that these adversarial examples can transfer representation (penultimate) layer features substantially better than adversarial examples generated with non-robust networks. We argue that this result supports a non-intuitive hypothesis: slightly robust networks exhibit universal features---ones that tend to overlap with the features learned by all other networks trained on the same dataset. This suggests that the features of a single slightly-robust neural network may be useful to derive insight about the features of every non-robust neural network trained on the same distribution.",https://api.openreview.net/pdf/2cf1100fea5c5601e402457524e0d4e1c3356bf7.pdf
Towards Transferable Adversarial Perturbations with Minimum Norm,2021,ICML,"['Fangcheng Liu', 'Chao Zhang', 'Hongyang Zhang']",poster,"['Adversarial example', 'Transfer attack', 'Minimum-norm perturbation']","Transfer-based adversarial example is one of the most important classes of black-box attacks. Prior work in this direction often requires a fixed but large perturbation radius to reach a good transfer success rate. In this work, we propose a \emph{geometry-aware framework} to generate transferable adversarial perturbation with minimum norm for each input. Analogous to model selection in statistical machine learning, we leverage a validation model to select the optimal perturbation budget for each image. Extensive experiments verify the effectiveness of our framework on improving image quality of the crafted adversarial examples. The methodology is the foundation of our entry to the CVPR'21 Security AI Challenger: Unrestricted Adversarial Attacks on ImageNet, in which we ranked 1st place out of 1,559 teams and surpassed the runner-up submissions by 4.59\% and 23.91\% in terms of final score and average image quality level, respectively.",https://api.openreview.net/pdf/63a267a701e94a6f4379e510bb18a0feba0a34b5.pdf
Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks,2021,ICML,"['Zhengyan Zhang', 'Guangxuan Xiao', 'Yongwei Li', 'Tian Lv', 'Fanchao Qi', 'Zhiyuan Liu', 'Yasheng Wang', 'Xin Jiang', 'Maosong Sun']",poster,"['Pre-trained models', 'Backdoor attacks', 'AI security']","Pre-trained models (PTMs) have been widely used in various downstream tasks. The parameters of PTMs are distributed on the Internet and may suffer backdoor attacks. In this work, we demonstrate the universal vulnerability of PTMs, where fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary downstream tasks. Specifically, attackers can add a simple pre-training task, which restricts the output representations of trigger instances to pre-defined vectors, namely neuron-level backdoor attack (NeuBA). If the backdoor functionality is not eliminated during fine-tuning, the triggers can make the fine-tuned model predict fixed labels by pre-defined vectors. In the experiments of both natural language processing (NLP) and computer vision (CV), we show that NeuBA absolutely controls the predictions for trigger instances without any knowledge of downstream tasks. Finally, we apply several defense methods to NeuBA and find that model pruning is a promising direction to resist NeuBA by excluding backdoored neurons. Our findings sound a red alarm for the wide use of PTMs.",https://api.openreview.net/pdf/1cc11ab778ba03f41a45f941b3a3e42ccb867cc6.pdf
Towards Achieving Adversarial Robustness Beyond Perceptual Limits,2021,ICML,"['Sravanti Addepalli', 'Samyak Jain', 'Gaurang Sriramanan', 'Shivangi Khare', 'Venkatesh Babu Radhakrishnan']",poster,"['Adversarial Robustness', 'Adversarial Defense', 'Adversarial Training', 'Large perturbations']","The vulnerability of Deep Neural Networks to Adversarial Attacks has fuelled research towards building robust models. While most existing Adversarial Training algorithms aim towards defending against imperceptible attacks, real-world adversaries are not limited by such constraints. In this work, we aim to achieve adversarial robustness at larger epsilon bounds. We first discuss the ideal goals of an adversarial defense algorithm beyond perceptual limits, and further highlight the shortcomings of naively extending existing training algorithms to higher perturbation bounds. In order to overcome these shortcomings, we propose a novel defense, Oracle-Aligned Adversarial Training (OA-AT), that attempts to align the predictions of the network with that of an Oracle during adversarial training. The proposed approach achieves state-of-the-art performance at large epsilon bounds ($\ell_\infty$ bound of $16/255$) while outperforming adversarial training algorithms such as AWP, TRADES and PGD-AT at standard perturbation bounds ($\ell_\infty$ bound of $8/255$) as well.",https://api.openreview.net/pdf/df6f35e642e924085b24fa5adea2b278e82067a7.pdf
Improving Visual Quality of Unrestricted Adversarial Examples with Wavelet-VAE,2021,ICML,"['Wenzhao Xiang', 'Chang Liu', 'Shibao Zheng']",poster,"['Unrestricted adversarial attacks', 'Wavelet transform', 'VAE']","Traditional adversarial examples are typically generated by adding perturbation noise to the input image within a small matrix norm. In practice, unrestricted adversarial attack has raised great concern and presented a new threat to the AI safety. In this paper, we propose a wavelet-VAE structure to reconstruct an input image and generate adversarial examples by modifying the latent code. Different from perturbation-based attack, the modifications of the proposed method are not limited but imperceptible to human eyes.  Experiments show that our method can generate high quality adversarial examples on ImageNet dataset.",https://api.openreview.net/pdf/39468e6483120a8db4251bdc5f4edcb6b9c8efeb.pdf
Query-based Adversarial Attacks on Graph with Fake Nodes,2021,ICML,"['Zhengyi Wang', 'Hao Zhongkai', 'Jun Zhu']",poster,[],"While deep neural networks have achieved great success on the graph analysis, recent works have shown that they are also vulnerable to adversarial attacks where fraudulent users can fool the model with a limited number of queries. Compared with adversarial attacks on image classification, performing adversarial attack on graphs is challenging because of the discrete and non-differential nature of a graph. To address these issues, we proposed Cluster Attack, a novel adversarial attack by introducing a set of fake nodes to the original graph which can mislead the classification on certain victim nodes. Moreover, our attack is performed in a practical and unnoticeable manner. Extensive experiments demonstrate the effectiveness of our method in terms of the success rate of attack. ",https://api.openreview.net/pdf/63ac36075be865bd043edc5e3c3da847f4b7145b.pdf
Generalizing Adversarial Training to Composite Semantic Perturbations,2021,ICML,"['Yun-Yun Tsai', 'Lei Hsiung', 'Pin-Yu Chen', 'Tsung-Yi Ho']",poster,"['Composite Attack', 'Order Scheduling', 'Generalized Adversarial Training', 'Semantic Perturbations', 'Machine Learning', 'ICML']","Model robustness against adversarial examples has been widely studied, yet the lack of generalization to more realistic scenarios can be challenging. Specifically, recent works using adversarial training can successfully improve model robustness, but these works primarily consider adversarial threat models limited to $\ell_{p}$-norm bounded perturbations and might overlook semantic perturbations and their composition. In this paper, we firstly propose a novel method for generating composite adversarial examples. By utilizing component-wise PGD update and automatic attack- order scheduling, our method can find the optimal attack composition. We then propose generalized adversarial training (GAT) to extend model robustness from $\ell_{p}$ norm to composite semantic perturbations, such as Hue, Saturation, Brightness, Contrast, and Rotation. The results show that GAT can be robust not only on any single attack but also on combination of multiple attacks. GAT also outperforms baseline adversarial training approaches by a significant margin.",https://api.openreview.net/pdf/16422226bc6ff811dd31cfb3a8b89e49ce30cf72.pdf
On the Effectiveness of Poisoning against Unsupervised Domain Adaptation,2021,ICML,"['Akshay Mehra', 'Bhavya Kailkhura', 'Pin-Yu Chen', 'Jihun Hamm']",poster,"['Data Poisoning', 'Unsupervised domain adaptation']","Data poisoning attacks manipulate victim's training data to compromise their model performance, after training.
Previous works on poisoning have shown the inability of a small amount of poisoned data at significantly reducing the test accuracy of deep neural networks. In this work, we propose an upper bound on the test error induced by additive poisoning, which explains the difficulty of poisoning against deep neural networks. However, the limited effect of poisoning is restricted to the setting where training and test data are from the same distribution.  To demonstrate this, we study the effect of poisoning in an unsupervised domain adaptation (UDA) setting where the source and the target domain distributions are different. We propose novel data poisoning attacks that prevent UDA methods from learning a representation that generalizes well on the target domain. Our poisoning attacks significantly lower the target domain accuracy of state-of-the-art UDA methods on popular benchmark UDA tasks, dropping it to almost 0% in some cases, with the addition of only 10% poisoned data. The effectiveness of our attacks in the UDA setting highlights the seriousness of the threat posed by data poisoning and the importance of data curation in machine learning.",https://api.openreview.net/pdf/cb2205af2a15f57b5962c42a97b52ea50c33d9c3.pdf
Disrupting Model Training with Adversarial Shortcuts,2021,ICML,"['Ivan Evtimov', 'Ian Connick Covert', 'Aditya Kusupati', 'Tadayoshi Kohno']",poster,"['adversarial shortcuts', 'disrupting training']","When data is publicly released for human consumption, it is unclear how to prevent its unauthorized usage for machine learning purposes. Successful model training may be preventable with carefully designed dataset modifications, and we present a proof-of-concept approach for the image classification setting. We propose methods based on the notion of adversarial shortcuts, which encourage models to rely on non-robust signals rather than semantic features, and our experiments demonstrate that these measures successfully prevent deep learning models from achieving high accuracy on real, unmodified data examples",https://api.openreview.net/pdf/dd53ffe7435264117bf259361106e67afcb397ca.pdf
Self-Supervised Iterative Contextual Smoothing for Efficient Adversarial Defense against Gray- and Black-Box Attack,2021,ICML,"['Sungmin Cha', 'Naeun Ko', 'YoungJoon Yoo', 'Taesup Moon']",poster,"['Adversarial defense', 'smoothing', 'self-supervised learning', 'gray-box attack', 'black-box attack']","We propose a novel and effective input transformation based adversarial defense method against gray- and black-box attack, which is computationally efficient and does not require any adversarial training or retraining of a classification model. We first show that a very simple iterative Gaussian smoothing can effectively wash out adversarial noise and achieve substantially high robust accuracy. Based on the observation, we propose Self-Supervised Iterative Contextual Smoothing (SSICS), which aims to  reconstruct the original discriminative features from the Gaussian-smoothed image in context-adaptive manner, while still smoothing out the adversarial noise. From the experiments on ImageNet, we show that our SSICS achieves both high standard accuracy and very competitive robust accuracy for the gray- and black-box attacks; e.g., transfer-based PGD-attack and score-based attack. A noteworthy point to stress is that our defense is free of computationally expensive adversarial training, yet, can approach its robust accuracy via input transformation.",https://api.openreview.net/pdf/1350dc94682745a22a6959ff4c4928291bf04374.pdf
Poisoning the Search Space in Neural Architecture Search,2021,ICML,"['Robert Wu', 'Nayan Saxena', 'Rohan Jain']",poster,"['Poisoning Attacks', 'Neural Architecture Search', 'Adversarial Deep Learning', 'Automated Machine Learning', 'Reinforcement Learning']","Deep learning has proven to be a highly effective problem-solving tool for object detection and image segmentation across various domains such as healthcare and autonomous driving.  At the heart of this performance lies neural architecture design which relies heavily on domain knowledge and prior experience on the researchers' behalf. More recently, this process of finding the most optimal architectures, given an initial search space of possible operations, was automated by Neural Architecture Search (NAS). In this paper, we evaluate the robustness of one such algorithm  known as Efficient NAS (ENAS) against data agnostic poisoning attacks on the original search space with carefully designed ineffective operations.  By evaluating algorithm performance on the CIFAR-10 dataset, we empirically demonstrate how our novel search space poisoning (SSP) approach and multiple-instance poisoning attacks exploit design flaws in the ENAS controller to result in inflated prediction error rates for child networks. Our results provide insights into the challenges to surmount in using NAS for more adversarially robust architecture search.",https://api.openreview.net/pdf/9f1a1154cf837a12d817b264c38b9bf9dce6021f.pdf
Attacking Graph Classification via Bayesian Optimisation,2021,ICML,"['Xingchen Wan', 'Henry Kenlay', 'Binxin Ru', 'Arno Blaas', 'Michael Osborne', 'Xiaowen Dong']",poster,"['adversarial attack', 'Bayesian optimisation', 'graph classification']","Graph neural networks have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method  for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method and analyse patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models. ",https://api.openreview.net/pdf/3d4d1369dfe4fb1659a278d8a0bba531f5989e51.pdf
Adversarial for Good? How the Adversarial ML Community's Values Impede Socially Beneficial Uses of Attacks,2021,ICML,"['Kendra Albert', 'Maggie Delano', 'Bogdan Kulynych', 'Ram Shankar Siva Kumar']",poster,"['Adversarial Machine Learning', 'Ethical Impact']","Attacks from adversarial machine learning (ML) have the potential to be used ``for good'': they can be used to run counter to the existing power structures within ML, creating breathing space for those who would otherwise be the targets of surveillance and control. But most research on adversarial ML has not engaged in developing tools for resistance against ML systems. Why? In this paper, we review the broader impact statements that adversarial ML researchers wrote as part of their NeurIPS 2020 papers and assess the assumptions that authors have about the goals of their work. We also collect information about how authors view their work's impact more generally. We find that most adversarial ML researchers at NeurIPS hold two fundamental assumptions that will make it difficult for them to consider socially beneficial uses of attacks: (1) it is desirable to make systems robust, independent of context, and (2) attackers of systems are normatively bad and defenders of systems are normatively good. That is, despite their expressed and supposed neutrality, most adversarial ML researchers believe that the goal of their work is to secure systems, making it difficult to conceptualize and build tools for disrupting the status quo.",https://api.openreview.net/pdf/d7b5f8e8ca31d1f9fd26ee16bdf1afc66d667c62.pdf
Adversarial Sample Detection via Channel Pruning,2021,ICML,"['Zuohui Chen', 'RenXuan Wang', 'Yao Lu', 'jingyang Xiang', 'Qi Xuan']",poster,"['adversarial sample detection', 'neural networks', 'model pruning']"," Adversarial attacks are the main security issue of deep neural networks. Detecting adversarial samples is an effective mechanism for defending adversarial attacks. Previous works on detecting adversarial samples show superior in accuracy but consume too much memory and computing resources. In this paper, we propose an adversarial sample detection method based on pruned models. We find that pruned neural network models are sensitive to adversarial samples, i.e., the pruned models tend to output labels different from the original model when given adversarial samples. Moreover, the channel pruned model has an extremely small model size and actual computational cost. Experiments on CIFAR10 and SVHN show that the FLOPs and size of our generated model are only 24.46\% and 4.86\% of the original model. It outperforms the SOTA multi-model based detection method (87.47\% and 63.00\%) by 5.29\% and 30.92\% on CIFAR10 and SVHN, respectively, with significantly fewer models used.",https://api.openreview.net/pdf/523829cd1cc31480c39ab47d7a52c3ca458f8114.pdf
SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Adversarial Robustness,2021,ICML,"['Jongheon Jeong', 'Sejun Park', 'Minkyu Kim', 'Heung-Chang Lee', 'Doguk Kim', 'Jinwoo Shin']",poster,"['randomized smoothing', 'mixup', 'adversarial robustness', 'certified defense', 'adversarial defense', 'confidence calibration']","Randomized smoothing is currently a state-of-the-art method to construct a certifiably robust classifier from neural networks against $\ell_2$-adversarial perturbations. Under the paradigm, the robustness of a classifier is aligned with the prediction confidence, i.e., the higher confidence from a smoothed classifier implies the better robustness. This motivates us to rethink the fundamental trade-off between accuracy and robustness in terms of calibrating confidences of smoothed classifier. In this paper, we propose a simple training scheme, coined SmoothMix, to control the robustness of smoothed classifiers via self-mixup: it trains convex combinations of samples along the direction of adversarial perturbation for each input. The proposed procedure effectively identifies over-confident, near off-class samples as a cause of limited robustness in case of smoothed classifiers, and offers an intuitive way to adaptively set a new decision boundary between these samples for better robustness. Our experiments show that the proposed method can significantly improve the certified $\ell_2$-robustness of smoothed classifiers compared to state-of-the-art robust training methods.",https://api.openreview.net/pdf/ba362967915a9bb2249f40cd5b12288e671dff2b.pdf
BadNL: Backdoor Attacks Against NLP Models,2021,ICML,"['Xiaoyi Chen', 'Ahmed Salem', 'Michael Backes', 'Shiqing Ma', 'Yang Zhang']",poster,"['Backdoor attacks', 'NLP models']","Deep Neural Networks (DNNs) have progressed rapidly during the past decade. Meanwhile, DNN models have been shown to be vulnerable to various security and privacy attacks. One such attack that has attracted a great deal of attention recently is the backdoor attack. 
Previous backdoor attacks mainly focus on computer vision tasks. In this paper, we perform the first systematic investigation of the backdoor attack against natural language processing (NLP) models with a focus on sentiment analysis task. Specifically, we propose three methods to construct triggers, including Word-level, Char-level, and Sentence-level triggers. Our attacks achieve an almost perfect attack success rate with a negligible effect on the original model's utility. For instance, using the Word-level triggers, our backdoor attack achieves a 100% attack success rate with only a utility drop of 0.18%, 1.26%, and 0.19% on three benchmark sentiment analysis datasets.",https://api.openreview.net/pdf/cd42e8b674a13e8adb2af88c2edbc79a0a505235.pdf
Hidden Patch Attacks for Optical Flow,2021,ICML,['Benjamin Wortman'],poster,['Adversarial Machine'],"Adversarial patches have been of interest to researchers in recent years due to their easy implementation in real world attacks. In this paper we expand upon previous research by demonstrating a new ""hidden"" patch attack on optical flow. By altering the transparency during training we can generate patches that are invariant to their background meaning they can be inconspicuously applied using a transparent film to any number of objects. This also has the added benefit of reducing training costs when mass producing adversarial objects, since only one trained patch is needed for any application. Although this specific implementation is demonstrated using a white box attack on optical flow, it can be generalized to other scenarios such as object recognition or semantic segmentation.",https://api.openreview.net/pdf/28909a4b29a33d368e5a79d35634b46a666afa97.pdf
Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis,2021,ICML,"['Anisie Uwimana', 'Ransalu Senanayake']",poster,"['Deep learning. neural networks', 'Out of Distribution samples']","Deep learning models have become a popular choice for medical image analysis. However, the poor generalization performance of deep learning models limits them from being deployed in the real world as robustness is critical for medical applications. For instance, the state-of-the-art Convolutional Neural Networks (CNNs) fail to detect samples drawn statistically far away from the
training distribution or adversarially. In this work, we experimentally evaluate the robustness of a Mahalanobis distance-based 
confidence score, a simple yet effective method for detecting abnormal input samples, in classifying malaria parasitized cells and uninfected cells. Results indicated that the Mahalanobis confidence score detector exhibits improved performance and robustness of deep learning models, and achieves state-of-the-art performance on both out-of-distribution and adversarial samples. ",https://api.openreview.net/pdf/1539d277edca1573bf9434b70656e979050f91fe.pdf
Empirical robustification of pre-trained classifiers,2021,ICML,"['Mohammad Sadegh Norouzzadeh', 'Wan-Yi Lin', 'Leonid Boytsov', 'Leslie Rice', 'Huan Zhang', 'Filipe Condessa', 'J Zico Kolter']",poster,[],"Most pre-trained classifiers, though they may work extremely well on the domain they were trained upon, are not trained in a robust fashion, and therefore are sensitive to adversarial attacks.  A recent technique, denoised-smoothing, demonstrated that it was possible to create certifiably robust classifiers from a pre-trained classifier (without any retraining) by pre-pending a denoising network and wrapping the entire pipeline within randomized smoothing.  However, this is a costly procedure, which requires multiple queries due to the randomized smoothing element, and which ultimately is very dependent on the quality of the denoiser.  In this paper, we demonstrate that a more conventional “adversarial training” approach also works when applied to this robustification process.   Specifically, we show that by training an image-to-image translation model, prepended to a pre-trained classifier, with losses that optimize for both the fidelity of the image reconstruction and the adversarial performance of the end-to-end system, we can robustify pre-trained classifiers to a higher empirical degree of accuracy than denoised smoothing.  Further, these robustifers are also transferable to some degree across multiple classifiers and even some architectures, illustrating that in some real sense they are removing the “adversarial manifold” from the input data, a task that has traditionally been very challenging for “conventional” preprocessing methods.",https://api.openreview.net/pdf/8f195d022d7fd7cb13957354211b5b1d4dac15c3.pdf
On Frank-Wolfe Adversarial Training,2021,ICML,"['Theodoros Tsiligkaridis', 'Jay Roberts']",poster,"['adversarial training', 'robustness', 'loss landscape', 'Frank-Wolfe optimization']","We develop a theoretical framework for adversarial training (AT) with FW optimization (FW-AT) that reveals a geometric connection between the loss landscape and the distortion of $\ell_\infty$ FW attacks (the attack's $\ell_2$ norm). Specifically, we show that high distortion of FW attacks is equivalent to low variation along the attack path. It is then experimentally demonstrated on various deep neural network architectures that $\ell_\infty$ attacks against robust models achieve near maximal $\ell_2$ distortion. To demonstrate the utility of our theoretical framework we develop FW-Adapt, a novel adversarial training algorithm which uses simple distortion measure to adapt the number of attack steps during training. FW-Adapt provides strong robustness against white- and black-box attacks at lower training times than PGD-AT.",https://api.openreview.net/pdf/9015d2e975e9914636da19126be4c44d48c4498e.pdf
Enhancing Certified Robustness via Smoothed Weighted Ensembling,2021,ICML,"['Chizhou Liu', 'Yunzhen Feng', 'Ranran Wang', 'Bin Dong']",poster,"['Adversairal Robustness', 'Randomized Smoothing', 'Ensembling']","Randomized smoothing has achieved state-of-the-art certified robustness against $l_2$-norm adversarial attacks. However, it is not wholly resolved on how to find the optimal base classifier for randomized smoothing. In this work, we employ a Smoothed WEighted ENsembling (SWEEN) scheme to improve the performance of randomized smoothed classifiers. We show the ensembling generality that SWEEN can help achieve optimal certified robustness. Furthermore, theoretical analysis proves that the optimal SWEEN model can be obtained from training under mild assumptions. We also develop an adaptive prediction algorithm to reduce the prediction and certification cost of SWEEN models. Extensive experiments show that SWEEN models outperform the upper envelope of their corresponding candidate models by a large margin. Moreover, SWEEN models constructed using a few small models can achieve comparable performance to a single large model with a notable reduction in training time.",https://api.openreview.net/pdf/2fb328ede436d60685d6668b9cc456fb24ee74a4.pdf
Attention-Guided Black-box Adversarial Attacks with Large-Scale Multiobjective Evolutionary Optimization,2021,ICML,"['Jie Wang', 'Zhaoxia Yin', 'Jing Jiang', 'Yang Du']",poster,"['Deep neural network', 'adversarial example', 'black-box attack', 'large-scale multiobjective evolutionary algorithm', 'attention mechanism']","Recent black-box adversarial attacks may struggle to balance their attack ability and visual quality of the generated adversarial examples (AEs) in tackling high-resolution images. In this paper, We propose an attention-guided black-box adversarial attack based on the large-scale multiobjective evolutionary optimization, termed as LMOA. By considering the spatial semantic information of images, we firstly take advantage of the attention map to determine the perturbed pixels. Then, a large-scale multiobjective evolutionary algorithm is employed to traverse the reduced pixels in the salient region. Extensive experimental results have verified the effectiveness of the proposed LMOA on the ImageNet dataset. ",https://api.openreview.net/pdf/7de51fed29b90255d84efe1be39de6460b1d57b6.pdf
A Closer Look at the Adversarial Robustness of Information Bottleneck Models,2021,ICML,"['Iryna Korshunova', 'David Stutz', 'Alexander A Alemi', 'Olivia Wiles', 'Sven Gowal']",poster,"['Information Bottlenecks', 'Adversarial Robustness']","We study the adversarial robustness of information bottleneck models for classification. Previous works showed that the robustness of models trained with information bottlenecks can improve upon adversarial training. Our evaluation under a diverse range of white-box $l_{\infty}$ attacks suggests that information bottlenecks alone are not a strong defense strategy, and that previous results were likely influenced by gradient obfuscation.",https://api.openreview.net/pdf/0b75c86744cf3d8d44c65bfd6ce113cebe90e2d5.pdf
Attacking Few-Shot Classifiers with Adversarial Support Poisoning,2021,ICML,"['Elre Talea Oldewage', 'John F Bronskill', 'Richard E Turner']",poster,"['meta-learning', 'few-shot learning', 'poisoning', 'adversarial attack']","This paper examines the robustness of deployed few-shot meta-learning systems when they are fed an imperceptibly perturbed few-shot dataset, showing that the resulting predictions on test inputs can become worse than chance. This is achieved by developing a novel attack, Adversarial Support Poisoning or ASP, which crafts a poisoned set of examples. When even a small subset of malicious data points is inserted into the support set of a meta-learner, accuracy is significantly reduced. We evaluate the new attack on a variety of few-shot classification algorithms and scenarios, and propose a form of adversarial training that significantly improves robustness against both poisoning and evasion attacks.",https://api.openreview.net/pdf/04cb262180663676cb1668c5a741f67f6c5aaba8.pdf
Demystifying Adversarial Training via A Unified Probabilistic Framework,2021,ICML,"['Yifei Wang', 'Yisen Wang', 'Jiansheng Yang', 'Zhouchen Lin']",poster,[],"Adversarial Training (AT) is known as an effective approach to enhance the robustness of deep neural networks. Recently researchers notice that robust models with AT have good generative ability and can synthesize realistic images, while the reason behind it is yet under-explored. In this paper, we demystify this phenomenon by developing a unified probabilistic framework, called Contrastive Energy-based Models (CEM). On the one hand, we provide the first probabilistic characterization of AT through a unified understanding of robustness and generative ability. On the other hand, our CEM can also naturally generalize AT to the unsupervised scenario and develop principled unsupervised AT methods. Based on these, we propose principled adversarial sampling algorithms in both supervised and unsupervised scenarios. Experiments show our sampling algorithms significantly improve the sampling quality and achieves an Inception score of 9.61 on CIFAR-10, which is superior to previous energy-based models and comparable to state-of-the-art generative models.",https://api.openreview.net/pdf/c6940bdb85cfdbaf7af1752fbd0b76f16c86a0f7.pdf
Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples,2021,ICML,"['Maura Pintor', 'Luca Demetrio', 'Angelo Sotgiu', 'Giovanni Manca', 'Ambra Demontis', 'Nicholas Carlini', 'Battista Biggio', 'Fabio Roli']",poster,"['Debugging', 'machine learning', 'adversarial machine learning']","Evaluating robustness of machine-learning models to adversarial examples is a challenging problem. Many defenses have been shown to provide a false sense of security by causing gradient-based attacks to fail, and they have been broken under more rigorous evaluations.
Although guidelines and best practices have been suggested to improve current adversarial robustness evaluations, the lack of automatic testing and debugging tools makes it difficult to apply these recommendations in a systematic manner.
In this work, we overcome these limitations by (i) defining a set of quantitative indicators which unveil common failures in the optimization of gradient-based attacks, and (ii) proposing specific mitigation strategies within a systematic evaluation protocol. 
Our extensive experimental analysis shows that the proposed indicators of failure can be used to visualize, debug and improve current adversarial robustness evaluations, providing a first concrete step towards automatizing and systematizing current adversarial robustness evaluations.",https://api.openreview.net/pdf/17cc9fb03eba4a81e5398489360ffc67236bfb95.pdf
ROPUST: Improving Robustness through Fine-tuning with Photonic Processors and Synthetic Gradients,2021,ICML,"['Alessandro Cappelli', 'Ruben Ohana', 'Julien Launay', 'Laurent Meunier', 'Iacopo Poli']",poster,"['adversarial attacks', 'adversarial defense', 'alternative training methods', 'direct feedback alignment', 'optical computing', 'fine-tuning']","Robustness to adversarial attacks is typically obtained through expensive adversarial training with Projected Gradient Descent. We introduce ROPUST, a remarkably simple and efficient method to leverage robust pre-trained models and further increase their robustness, at no cost in natural accuracy. Our technique relies on the use of an Optical Processing Unit (OPU), a photonic co-processor, and a fine-tuning step performed with Direct Feedback Alignment, a synthetic gradient training scheme. We test our method on nine different models against four attacks in RobustBench, consistently improving over state-of-the-art performance. We also introduce phase retrieval attacks, specifically designed to target our own defense. We show that even with state-of-the-art phase retrieval techniques, ROPUST is effective.",https://api.openreview.net/pdf/eebcf663372d63cb257f7512a11feae9e1ca522f.pdf
A Primer on Multi-Neuron Relaxation-based Adversarial Robustness Certification,2021,ICML,['Kevin Roth'],poster,"['Adversarial Robustness', 'Robustness Certification', 'Adversarial Examples', 'Multi-Neuron Relaxation', 'Deep Learning']","The existence of adversarial examples poses a real danger when deep neural networks are deployed in the real world. The go-to strategy to quantify this vulnerability is to evaluate the model against specific attack algorithms. This approach is however inherently limited, as it says little about the robustness of the model against more powerful attacks not included in the evaluation. We develop a unified mathematical framework to describe relaxation-based robustness certification methods, which go beyond adversary-specific robustness evaluation and instead provide provable robustness guarantees against attacks by any adversary. We discuss the fundamental limitations posed by single-neuron relaxations and show how the recent ``k-ReLU'' multi-neuron relaxation framework of Singh et al. (2019) obtains tighter correlation-aware activation bounds by leveraging additional relational constraints among groups of neurons. Specifically, we show how additional pre-activation bounds can be mapped to corresponding post-activation bounds and how they can in turn be used to obtain tighter robustness certificates. We also present an intuitive way to visualize different relaxation-based certification methods. By approximating multiple non-linearities jointly instead of separately, the k-ReLU method is able to bypass the convex barrier imposed by single neuron relaxations.",https://api.openreview.net/pdf/1d952a33ff6daae33aa76cc6851d2e7f5eea521c.pdf
Meta Adversarial Training against Universal Patches,2021,ICML,"['Jan Hendrik Metzen', 'Nicole Finnie', 'Robin Hutmacher']",poster,"['robustness', 'adversarial examples', 'adversarial training', 'physical-world adversarial attacks', 'adversarial patch', 'universal perturbation']","Recently demonstrated physical-world adversarial attacks have exposed vulnerabilities in perception systems that pose severe risks for safety-critical applications such as autonomous driving. These attacks place adversarial artifacts in the physical world that indirectly cause the addition of a universal patch to inputs of a model that can fool it in a variety of contexts. Adversarial training is the most effective defense against image-dependent adversarial attacks. However, tailoring adversarial training to universal patches is computationally expensive since the optimal universal patch depends on the model weights which change during training. We propose meta adversarial training (MAT), a novel combination of adversarial training with meta-learning, which overcomes this challenge by meta-learning universal patches along with model training. MAT requires little extra computation while continuously adapting a large set of patches to the current model. MAT considerably increases robustness against universal patch attacks on image classification and traffic-light detection.",https://api.openreview.net/pdf/53ca7198ed70f5c79d0da95daa85859aff252791.pdf
Examining the Human Perceptibility of Black-Box Adversarial Attacks on Face Recognition,2021,ICML,"['Benjamin Spetter-Goldstein', 'Nataniel Ruiz', 'Sarah Adel Bargal']",poster,"['adversarial attack', 'black-box attack', 'face recognition', 'human perceptibility']","The modern open internet contains billions of public images of human faces across the web, especially on social media websites used by half the world's population. In this context, Face Recognition (FR) systems have the potential to match faces to specific names and identities, creating glaring privacy concerns. Adversarial attacks are a promising way to grant users privacy from FR systems by disrupting their capability to recognize faces. Yet, such attacks can be perceptible to human observers, especially under the more challenging black-box threat model. In the literature, the justification for the imperceptibility of such attacks hinges on bounding metrics such as $\ell_p$ norms. However, there is not much research on how these norms match up with human perception. Through examining and measuring both the effectiveness of recent black-box attacks in the face recognition setting and their corresponding human perceptibility through survey data, we demonstrate the trade-offs in perceptibility that occur as attacks become more aggressive. We also show how the $\ell_2$ norm and other metrics do not correlate with human perceptibility in a linear fashion, thus making these norms suboptimal at measuring adversarial attack perceptibility.",https://api.openreview.net/pdf/d19a5cd72405c25d958e7ad6d21dcdd82667b58e.pdf
Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks,2021,ICML,"['Xiao Yang', 'Yinpeng Dong', 'Tianyu Pang']",poster,[],"Transfer-based adversarial attacks can effectively evaluate model robustness in the black-box setting. Though several methods have demonstrated impressive transferability of untargeted adversarial examples, targeted adversarial transferability is still challenging. In this paper, we develop a simple yet practical framework to efficiently craft targeted transfer-based adversarial examples. Specifically, we propose a conditional generative attacking model, which can generate the adversarial examples targeted at different classes by simply altering the class embedding and share a single backbone. Extensive experiments demonstrate that our method improves the success rates of targeted black-box attacks by a significant margin over the existing methods --- it reaches an average success rate of 29.6\% against six diverse models based only on one substitute white-box model in the standard testing of NeurIPS 2017 competition, which outperforms the state-of-the-art gradient-based attack methods (with an average success rate of $<$2\%) by a large margin. Moreover, the proposed method is also more efficient beyond an order of magnitude than gradient-based methods.",https://api.openreview.net/pdf/0ffc47c6fd847383bb8ea77f3755ee9ec756b4c8.pdf
Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks ,2021,ICML,"['Fan Liu', 'Shuyu Zhao', 'Xuelong Dai', 'Bin Xiao']",poster,['Adversarial Training， Adversarial Robustness， Meta Learning'],"Meta-learning model can quickly adapt to new tasks using few-shot labeled data. However, despite achieving good generalization on few-shot classification tasks,  it is still challenging to improve the adversarial robustness of the meta-learning model in few-shot learning. Although adversarial training (AT) methods such as Adversarial Query (AQ) can improve the adversarially robust performance of meta-learning models, AT is still computationally expensive training. On the other hand,  meta-learning models trained with AT will drop significant accuracy on the original clean images.  This paper proposed a meta-learning method on the adversarially robust neural network called Long-term Cross Adversarial Training (LCAT). LCAT will update meta-learning model parameters cross along the natural and adversarial sample distribution direction with long-term to improve both adversarial and clean few-shot classification accuracy. Due to cross-adversarial training,  LCAT only needs half of the adversarial training epoch than AQ, resulting in a low adversarial training computation. Experiment results show that LCAT achieves superior performance both on the clean and adversarial few-shot classification accuracy than SOTA adversarial training methods for meta-learning models.",https://api.openreview.net/pdf/1e46623c3e477de267b9c2f37118a14dfed02672.pdf
Improve Generalization and Robustness of Neural Networks via Weight Scale Shifting Invariant Regularizations,2021,ICML,"['Ziquan Liu', 'Yufei CUI', 'Antoni B. Chan']",poster,"['Generalization', 'Adversarial Robustness', 'Regularization']","Using weight decay to penalize the L2 norms of weights in neural networks has been a standard training practice to regularize the complexity of networks. In this paper, we show that a family of regularizers, including weight decay, is ineffective at penalizing the intrinsic norms of weights for networks with positively homogeneous activation functions, such as linear, ReLU and max-pooling functions. As a result of homogeneity, functions specified by the networks are invariant to the shifting of weight scales between layers. The ineffective regularizers are sensitive to such shifting and thus poorly regularize the model capacity, leading to overfitting. To address this shortcoming, we propose an improved regularizer that is invariant to weight scale shifting and thus effectively constrains the intrinsic norm of a neural network. The derived regularizer is an upper bound for the input gradient of the network so minimizing the improved regularizer also benefits the adversarial robustness. We demonstrate the efficacy of our proposed regularizer on various datasets and neural network architectures at improving generalization and adversarial robustness.",https://api.openreview.net/pdf/7d264ebd0efca74b0473188390d2f22f1d93103e.pdf
Helper-based Adversarial Training: Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off,2021,ICML,"['Rahul Rade', 'Seyed-Mohsen Moosavi-Dezfooli']",oral,"['adversarial training', 'robustness']","While adversarial training has become the de facto approach for training robust classifiers, it leads to a drop in accuracy. This has led to prior works postulating that accuracy is inherently at odds with robustness. Yet, the phenomenon remains inexplicable. In this paper, we closely examine the changes induced in the decision boundary of a deep network during adversarial training. We find that adversarial training leads to unwarranted increase in the margin along certain adversarial directions, thereby hurting accuracy. Motivated by this observation, we present a novel algorithm, called Helper-based Adversarial Training (HAT), to reduce this effect by incorporating additional wrongly labelled examples during training. Our proposed method provides a notable improvement in accuracy without compromising robustness. It achieves a better trade-off between accuracy and robustness in comparison to existing defenses.",https://api.openreview.net/pdf/88e4b0a05f41fc12ba3beec9bdb09911303dbfad.pdf
Detecting Adversarial Examples Is (Nearly) As Hard As Classifying Them,2021,ICML,['Florian Tramer'],oral,"['Adversarial Examples', 'Detection', 'Hardness Reductions']","Making classifiers robust to adversarial examples is hard. 
Thus, many defenses tackle the seemingly easier task of \emph{detecting} perturbed inputs.

We show a barrier towards this goal. We prove a general \emph{hardness reduction} between detection and classification of adversarial examples: given a robust detector for attacks at distance $\epsilon$ (in some metric), we can build a similarly robust (but inefficient) \emph{classifier} for attacks at distance $\epsilon/2$.

Our reduction is computationally inefficient, and thus cannot be used to build practical classifiers. Instead, it is a useful sanity check to test whether empirical detection results imply something much stronger than the authors presumably anticipated. 
%(indeed, building inefficient robust classifiers is also presumed to be very challenging).

To illustrate, we revisit $13$ detector defenses. For $10/13$ cases, we show that the claimed detection results would imply an inefficient classifier with robustness far beyond the state-of-the-art.",https://api.openreview.net/pdf/561a65907132b7e44f5972e55d80f7985d2a7f7f.pdf
Consistency Regularization for Adversarial Robustness,2021,ICML,"['Jihoon Tack', 'Sihyun Yu', 'Jongheon Jeong', 'Minseon Kim', 'Sung Ju Hwang', 'Jinwoo Shin']",oral,"['adversarial example', 'adversarial robustness', 'consistency regularization']","Adversarial training (AT) is currently one of the most successful methods to obtain the adversarial robustness of deep neural networks. However, the phenomenon of robust overfitting, i.e., the robustness starts to decrease significantly during AT, has been problematic, not only making practitioners consider a bag of tricks for a successful training, e.g., early stopping, but also incurring a significant generalization gap in the robustness. In this paper, we propose an effective regularization technique that prevents robust overfitting by optimizing an auxiliary 'consistency' regularization loss during AT. Specifically, it forces the predictive distributions after attacking from two different augmentations of the same instance to be similar with each other. Our experimental results demonstrate that such a simple regularization technique brings significant improvements in the test robust accuracy of a wide range of AT methods. More remarkably, we also show that our method could significantly help the model to generalize its robustness against unseen adversaries, e.g., other types or larger perturbations compared to those used during training.",https://api.openreview.net/pdf/d5056d6a1c086e916a4ce76b016fe63971f7bf61.pdf
Adversarial Robustness of Streaming Algorithms through Importance Sampling,2021,ICML,"['Vladimir Braverman', 'Avinatan Hassidim', 'Yossi Matias', 'Mariano Schain', 'Sandeep Silwal', 'Samson Zhou']",oral,"['adversarial robustness', 'streaming algorithms', 'coresets']","In the adversarial streaming model, an adversary gives an algorithm a sequence of adaptively chosen updates as a data stream and the goal of the algorithm is to compute or approximate some predetermined function for every prefix of the adversarial stream. However, the adversary may generate future updates based on previous outputs of the algorithm and in particular, the adversary may gradually learn the random bits internally used by an algorithm to manipulate dependencies in the input. This is especially problematic as many important problems in the streaming model require randomized algorithms, as they are known to not admit any deterministic algorithms that use sublinear space. In this paper, we introduce adversarially robust streaming algorithms for central machine learning and algorithmic tasks, such as regression and clustering, as well as their more general counterparts, subspace embedding, low-rank approximation, and coreset construction.  Our results are based on a simple, but powerful, observation that many importance sampling-based algorithms give rise to adversarial robustness in contrast to sketching based algorithms, which are very prevalent in the streaming literature but suffer from adversarial attacks. In addition, we show that the well-known merge and reduce paradigm used for corset construction in streaming is adversarially robust. To the best of our knowledge, these are the first adversarially robust results for these problems yet require no new algorithmic implementations. Finally, we empirically confirm the robustness of our algorithms on various adversarial attacks and demonstrate that by contrast, some common existing algorithms are not robust. ",https://api.openreview.net/pdf/ebe06a8506f6ec2abf8e0299e90d67ea3f888938.pdf
Data Poisoning Won’t Save You From Facial Recognition,2021,ICML,"['Evani Radiya-Dixit', 'Florian Tramer']",oral,"['Poisoning attacks', 'facial recognition', 'arms race', 'adaptive defenses']","Data poisoning has been proposed as a compelling defense against facial recognition models trained on Web-scraped pictures. By perturbing the images they post online, users can fool models into misclassifying future (unperturbed) pictures.
		
We demonstrate that this strategy provides a false sense of security, as it ignores an inherent asymmetry between the parties: users' pictures are perturbed once and for all before being published and scraped, and must thereafter fool all future models---including models trained adaptively against the users' past attacks, or models that use technologies discovered after the attack. 
		
We evaluate two poisoning attacks against large-scale facial recognition, Fawkes 500,000+ downloads) and LowKey. We demonstrate how an ``oblivious'' model trainer can simply wait for future developments in computer vision to nullify the protection of pictures collected in the past. We further show that an adversary with black-box access to the attack can train a robust model that resists the perturbations of collected pictures.
		
We caution that facial recognition poisoning will not admit an ''arms race'' between attackers and defenders. Once perturbed pictures are scraped, the attack cannot be changed so any future defense irrevocably undermines users' privacy.",https://api.openreview.net/pdf/cfe448c8e3256b77942916112eec9a4116521a7c.pdf
Certified robustness against adversarial patch attacks via randomized cropping,2021,ICML,"['Wan-Yi Lin', 'Fatemeh Sheikholeslami', 'jinghao shi', 'Leslie Rice', 'J Zico Kolter']",oral,"['adversarial machine learning', 'patch attack', 'certifiable defense', 'randomized crop']","    This paper proposes a certifiable defense against adversarial patch attacks on image classification. Our approach classifies random crops from the original image independently and classifies the original image as the majority vote over predicted classes of the crops.  Leveraging the fact that a patch attack can only influence a certain number of pixels in the image, we derive certified robustness bounds for the classifier. Our method is particularly effective when realistic transformations are applied to the adversarial patch, such as affine transformations. Such transformations occur naturally when an adversarial patch is physically introduced in a scene. Our method improves upon the current state of the art in defending against patch attacks on CIFAR10 and ImageNet, both in terms of certified accuracy and inference time.",https://api.openreview.net/pdf/282715229ed05eb1d1af36a68f316f6b4a75e232.pdf
Is It Time to Redefine the Classification Task for Deep Learning Systems?,2021,ICML,"['Keji Han', 'Yun Li', 'Songcan Chen']",oral,"['adversarial robustness', 'deep learning system']","Many works have demonstrated that deep neural networks (DNNs) are vulnerable to adversarial examples. A deep learning system involves a couple of elements: the learning task, data set, deep model, loss, and optimizer. Each element may cause the vulnerability of the deep learning system, and simply attributing the vulnerability of the deep learning system to the deep model may impede addressing the adversarial attack. So we redefine the robustness of DNNs as the robustness of the deep neural learning system, and we experimentally find that the vulnerability of the deep learning system also roots in the learning task itself. In detail, this paper defines the interval-label classification task for the deep classification system, whose labels are predefined non-overlapping intervals instead of a fixed value (hard label) or probability vector (soft label). The experimental results demonstrate that the interval-label classification task is more robust than the traditional classification task while retaining accuracy.",https://api.openreview.net/pdf/6a548de997f5f8beea6c6bd35ae7d5196f0758ce.pdf
Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints,2021,ICML,"['Maura Pintor', 'Fabio Roli', 'Wieland Brendel', 'Battista Biggio']",oral,"['machine learning', 'adversarial machine learning', 'computer vision']","Evaluating adversarial robustness amounts to finding the minimum perturbation needed to have an input sample misclassified. 
The inherent complexity of the underlying optimization requires current gradient-based attacks to be carefully tuned, initialized, and possibly executed for many computationally-demanding iterations, even if specialized to a given perturbation model.
In this work, we overcome these limitations by proposing a fast minimum-norm (FMN) attack that works with different $\ell_p$-norm perturbation models ($p=0, 1, 2, \infty$), is robust to hyperparameter choices, does not require adversarial starting points, and converges within few lightweight steps. 
It works by iteratively finding the sample misclassified with maximum confidence within an $\ell_p$-norm constraint of size $\epsilon$, while adapting $\epsilon$ to minimize the distance of the current sample to the decision boundary.
Extensive experiments show that FMN significantly outperforms existing attacks in terms of convergence speed and computation time, while reporting comparable or even smaller perturbation sizes.",https://api.openreview.net/pdf/bbd8922fab4afa05506ac8ed9ccda272823c20b9.pdf
Defending against Model Stealing via Verifying Embedded External Features,2021,ICML,"['Linghui Zhu', 'Yiming Li', 'Xiaojun Jia', 'Yong Jiang', 'Shu-Tao Xia', 'Xiaochun Cao']",oral,"['Model Stealing', 'Ownership Verification', 'Model Privacy', 'AI Security']","Well-trained models are valuable intellectual properties for their owners. Recent studies revealed that the adversaries can `steal' deployed models even when they have no training sample and can only query the model. Currently, there were some defense methods to alleviate this threat, mostly by increasing the cost of model stealing. In this paper, we explore the defense problem from another angle by \emph{verifying whether a suspicious model contains the knowledge of defender-specified external features}. We embed the \emph{external features} by \emph{poisoning} a few training samples via style transfer. After that, we train a meta-classifier, based on the gradient of predictions, to determine whether a suspicious model is stolen from the victim. Our method is inspired by the understanding that the stolen models should contain the knowledge of (external) features learned by the victim model. Experimental results demonstrate that our approach is effective in defending against different model stealing attacks simultaneously.",https://api.openreview.net/pdf/a49512c05e6d4f56c7aec0912d5da695abd698be.pdf
On-the-fly learning of adaptive strategies with bandit algorithms,2021,ICML,"['Rashid Bakirov', 'Damien Fay', 'Bogdan Gabrys']",poster,['machine learning'],"Automation of machine learning model development is increasingly becoming an established research area. While automated model selection and automated data pre-processing have been studied in depth, there is, however, a gap concerning automated model adaptation strategies for streaming data with non-stationarities. This has previously been addressed by heuristic generic adaptation strategies in the batch streaming setting. While showing promising performance, these strategies contain some limitations. In this work, we propose using multi-armed bandit algorithms for learning adaptive strategies from incrementally streaming data on-the-fly. Empirical results using established bandit algorithms show a comparable performance to two common stream learning algorithms. ",https://api.openreview.net/pdf/b0d6b061ce857b93a2917256c00e72f1327c8ab5.pdf
Adaptation-Agnostic Meta-Training,2021,ICML,"['Jiaxin Chen', 'Li-Ming Zhan', 'Xiao-Ming Wu', 'Fu-lai Chung']",poster,['meta-learning'],"Many meta-learning algorithms can be formulated into an interleaved process, in the sense that task-specific predictors are learned during inner-task adaptation and meta-parameters are updated during meta-update. The normal meta-training strategy needs to differentiate through the inner-task adaptation procedure to optimize the meta-parameters. This leads to a constraint that the inner-task algorithms should be solved analytically. Under this constraint, only simple algorithms with analytical solutions can be applied as the inner-task algorithms, limiting the model expressiveness. To lift the limitation, we propose an adaptation-agnostic meta-training strategy. Following our proposed strategy, we are capable to apply stronger algorithms (e.g., an ensemble of different types of algorithms) as the inner-task algorithm to achieve superior performance comparing with popular baselines.",https://api.openreview.net/pdf/dc46f3bbfbf08d17d0ff28a9b770da1a4353aff3.pdf
Automated Learning Rate Scheduler for Large-batch Training,2021,ICML,"['Chiheon Kim', 'Saehoon Kim', 'Jongmin Kim', 'Donghoon Lee', 'Sungwoong Kim']",poster,[],"Large-batch training has been essential in leveraging large-scale datasets and models in deep learning. While it is computationally beneficial to use large batch sizes, it often requires a specially designed learning rate (LR) schedule to achieve a comparable level of performance as in smaller batch training. Especially, when the number of training epochs is constrained, the use of a large LR and a warmup strategy is critical in the final performance of large-batch training due to the reduced number of updating steps. In this work, we propose an automated LR scheduling algorithm which is effective for neural network training with a large batch size under the given epoch budget. In specific, the whole schedule consists of two phases: adaptive warmup and predefined decay, where the LR is increased until the training loss no longer decreases and decreased to zero until the end of training. Here, whether the training loss has reached the minimum value is robustly checked with Gaussian process smoothing in an online manner with a low computational burden. Coupled with adaptive stochastic optimizers such as AdamP and LAMB, the proposed scheduler successfully adjusts the LRs without cumbersome hyperparameter tuning and achieves comparable or better performances than tuned baselines on various image classification benchmarks and architectures with a wide range of batch sizes.",https://api.openreview.net/pdf/ed74ed087759fbe6a39d420de1b0f83e6cbee272.pdf
Tabular Data: Deep Learning is Not All You Need,2021,ICML,"['Ravid Shwartz-Ziv', 'Amitai Armon']",poster,"['AutoML', 'Tabular Data', 'Deep Learning', 'Tree Ensemble Models']","A key element of AutoML systems is setting the types of models that will be used for each type of task. For classification and regression problems with tabular data, the use of tree ensemble models (like XGBoost) is usually recommended. However, several deep learning models for tabular data have recently been proposed, claiming to outperform XGBoost for some use-cases. In this paper, we explore whether these deep models should be a recommended option for tabular data, by rigorously comparing the new deep models to XGBoost on a variety of datasets. In addition to systematically comparing their accuracy, we consider the tuning and computation they require. Our study shows that XGBoost outperforms these deep models across the datasets, including datasets used in the papers that proposed the deep models. We also demonstrate that XGBoost requires much less tuning. On the positive side, we show that an ensemble of the deep models and XGBoost performs better on these datasets than XGBoost alone.",https://api.openreview.net/pdf/38da49608da2fca5e8556d1b4d7c1dd97af716d3.pdf
Incorporating domain knowledge into neural-guided search via in situ priors and constraints,2021,ICML,"['Brenden K Petersen', 'Claudio Santiago', 'Mikel Landajuela']",poster,"['AutoML', 'reinforcement learning', 'symbolic regression']","Many AutoML problems involve optimizing discrete objects under a black-box reward. Neural-guided search provides a flexible means of searching these combinatorial spaces using an autoregressive recurrent neural network. A major benefit of this approach is that builds up objects $\textit{sequentially}$—this provides an opportunity to incorporate domain knowledge into the search by directly modifying the logits emitted during sampling. In this work, we formalize a framework for incorporating such $\textit{in situ}$ priors and constraints into neural-guided search, and provide sufficient conditions for enforcing constraints. We integrate several priors and constraints from existing works into this framework, propose several new ones, and demonstrate their efficacy in informing the task of symbolic regression.",https://api.openreview.net/pdf/e311d33c743c78361c64b20e06ffac78d47334a3.pdf
Ranking Architectures by Feature Extraction Capabilities,2021,ICML,"['Debadeepta Dey', 'Shital Shah', 'Sebastien Bubeck']",poster,"['NAS', 'ranking', 'search', 'speedup']","The fundamental problem in Neural Architecture Search (NAS) is to eﬃciently ﬁnd high-
performing ones from a search space of architectures. We propose a simple but powerful
method for ranking architectures FEAR in any search space. FEAR leverages the viewpoint
that neural networks are powerful non-linear feature extractors. By training diﬀerent
architectures in the search space to the same training or validation error and subsequently
comparing the usefulness of the features extracted on the task-dataset of interest by freezing
most of the architecture we obtain quick estimates of the relative performance. We validate
FEAR on Natsbench topology search space on three diﬀerent datasets against competing
baselines and show strong ranking correlation especially compared to recently proposed
zero-cost methods. FEAR especially excels at ranking high-performance architectures in the
search space. When used in the inner loop of discrete search algorithms like random search,
FEAR can cut down the search time by approximately 2.4x without losing accuracy. We additionally
empirically study very recently proposed zero-cost measures for ranking and ﬁnd that they
breakdown in ranking performance as training proceeds and also that data-agnostic ranking
scores which ignore the dataset do not generalize across dissimilar datasets.",https://api.openreview.net/pdf/8900cdc8d627479a708e637403c5c0bff5591487.pdf
Neural Fixed-Point Acceleration for Convex Optimization,2021,ICML,"['Shobha Venkataraman', 'Brandon Amos']",poster,"['convex optimization', 'meta-learning', 'acceleration']","Fixed-point iterations are at the heart of numerical computing and are often a computational bottleneck in real-time applications, which typically instead need a fast solution of moderate accuracy. Classical acceleration methods for fixed-point problems focus on designing algorithms with theoretical guarantees that apply to any fixed-point problem. We present neural fixed-point acceleration, a framework to automatically learn to accelerate convex fixed-point problems that are drawn from a distribution, using ideas from meta-learning and classical acceleration algorithms.  We apply our framework to SCS, the state-of-the-art solver for convex cone programming, and design models and loss functions to overcome the challenges of learning over unrolled optimization and acceleration instabilities. Our work brings neural acceleration into any optimization problem expressible with CVXPY.  This is relevant to AutoML as we (meta-)learn improvements to a convex optimization solver that replaces an acceleration component that is traditionally hand-crafted.

Upon acceptance, we will openly release the source code containing our batched and differentiable PyTorch implementation of SCS with neural acceleration and all of the supplementary files necessary to fully reproduce our results.",https://api.openreview.net/pdf/8fd7d8fc1775915e6b342a52c2761558a26f8a82.pdf
Replacing the Ex-Def Baseline in AutoML by Naive AutoML,2021,ICML,"['Felix Mohr', 'Marcel Wever']",poster,"['Automated Machine Learning', 'Independency Assumption', 'Baselines']","Automated Machine Learning (AutoML) is the problem of automatically finding the pipeline with the best generalization performance on some given dataset. AutoML has received enormous attention in the last decade and has been addressed with sophisticated black-box optimization techniques like Bayesian Optimization, Genetic Algorithms, or Tree Search. These approaches are almost never compared to simple baselines to see how much they improve over simple but easy to implement approaches. We present Naive AutoML, a very simple baseline for AutoML that exploits meta-knowledge about machine learning problems and makes simplifying, yet, effective assumptions to quickly come to high-quality solutions. In 1h experiments, state of the art approaches can hardly improve over Naive AutoML which in turn comes along with advantages such as interpretability and flexibility.",https://api.openreview.net/pdf/680b4e707ba6719dcc44450dc09796bf27097843.pdf
PonderNet: Learning to Ponder,2021,ICML,"['Andrea Banino', 'Jan Balaguer', 'Charles Blundell']",poster,"['adaptive computation', 'probabilistic', 'recurrent network', 'transformer', 'ponder']","In standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.",https://api.openreview.net/pdf/14a922903f062e193a925b7feae15fdb1b54ca46.pdf
LRTuner: A Learning Rate Tuner for Deep Neural Networks,2021,ICML,"['Nikhil Iyer', 'Thejas Venkatesh', 'Nipun Kwatra', 'Ramachandran Ramjee', 'Muthian Sivathanu']",poster,"['Learning rate', 'Deep Learning', 'Stochastic Optimization']","One very important hyper-parameter for training deep neural networks is the learning rate schedule of the optimizer. The choice of learning rate schedule determines the computational cost of getting close to a minima, how close you actually get to the minima, and most importantly the kind of local minima (wide/narrow) attained. The kind of minima attained has a significant impact on the generalization accuracy of the network. Current systems employ hand tuned learning rate schedules, which are painstakingly tuned for each network and dataset. Given that the state space of schedules is huge, finding a satisfactory learning rate schedule can be very time consuming. In this paper, we present LRTuner, a method for tuning the learning rate as training proceeds. Our method works with any optimizer, and we demonstrate results on SGD with Momentum, and Adam optimizers.
    
We extensively evaluate LRTuner on multiple datasets, models, and across optimizers. We compare favorably against standard learning rate schedules for the given dataset and models, including ImageNet on Resnet-50, Cifar-10 on Resnet-18, and SQuAD fine-tuning on BERT. For example on ImageNet with Resnet-50, LRTuner shows up to 0.2% absolute gains in test accuracy compared to the hand-tuned baseline schedule. Moreover, LRTuner can achieve the same accuracy as the baseline schedule in 29% less optimization steps. ",https://api.openreview.net/pdf/2e3c6e642e09e1fb02b70c4cd1b3e5a4af49cda8.pdf
Sequential Automated Machine Learning: Bandits-driven Exploration using a Collaborative Filtering Representation,2021,ICML,"['Maxime Heuillet', 'Benoit Debaque', 'Audrey Durand']",poster,"['automl', 'collaborative filtering', 'bandits', 'sequential automl']","The goal of Automated Machine Learning (AutoML) is to make Machine Learning (ML) tools more accessible. Collaborative Filtering (CF) methods have shown great success in automating the creation of machine learning pipelines. In this work, we frame the AutoML problem under a sequential setting where datasets arrive one at a time. On each dataset, an agent can try a small number of pipelines (exploration) before recommending a pipeline for this dataset (recommendation). The goal is to maximize the performance of the recommended pipelines over the sequence of datasets. More specifically, we focus on the exploration policy used for selecting the pipelines to explore before making the recommendation. We propose an approach based on the LinUCB bandit algorithm that leverages the latent representations extracted from matrix factorization (MF). We show that the exploration policy impacts the recommendation performance and that MF-based latent representations are more useful for exploration than for recommendation.",https://api.openreview.net/pdf/b69cd3feb4c4da27ebc106ccf634c061632e6600.pdf
Meta Learning the Step Size in Policy Gradient Methods,2021,ICML,"['Luca Sabbioni', 'Francesco Corda', 'Marcello Restelli']",poster,"['Meta Reinforcement Learning', 'Hyperparameter tuning', 'Policy Gradient']","Policy-based algorithms are among the most widely adopted techniques in model-free RL, thanks to their strong theoretical groundings and good properties in continuous action spaces. Unfortunately, these methods require precise and problem-specific hyperparameter tuning to achieve good performance and, as a consequence, they tend to struggle when asked to accomplish a series of heterogeneous tasks. In particular, the selection of the step size has a crucial impact on the ability to learn a highly performing policy, affecting the speed and the stability of the training process, and often being the main culprit for poor results. In this paper, we tackle these issues with a Meta Reinforcement Learning approach, by introducing a new formulation, known as meta-MDP, that can be used to solve any hyperparameter selection problem in RL with contextual processes. After providing a theoretical Lipschitz bound to the performance in different tasks, we adopt the proposed framework to train a batch RL algorithm to dynamically recommend the most adequate step size for different policies and tasks. In conclusion, we present an experimental campaign to show the advantages of selecting an adaptive learning rate in heterogeneous environments.",https://api.openreview.net/pdf/6b1d7f8ae8da3c141bb51dfafcac68de45298fd3.pdf
Mutation is all you need,2021,ICML,"['Lennart Schneider', 'Florian Pfisterer', 'Martin Binder', 'Bernd Bischl']",poster,"['Bayesian Optimization', 'Neural Architecture Search', 'BANANAS', 'Benchmark']","Neural architecture search (NAS) promises to make deep learning accessible to non-experts by automating architecture engineering of deep neural networks.
BANANAS is one state-of-the-art NAS method that is embedded within the Bayesian optimization framework.
Recent experimental findings have demonstrated the strong performance of BANANAS on the NAS-Bench-101 benchmark being determined by its path encoding and not its choice of surrogate model.
We present experimental results suggesting that the performance of BANANAS on the NAS-Bench-301 benchmark is determined by its acquisition function optimizer, which minimally mutates the incumbent.",https://api.openreview.net/pdf/f1172eb68af5ffa2bbcac7344bbae558bd714a86.pdf
Towards Explaining Hyperparameter Optimization via Partial Dependence Plots,2021,ICML,"['Julia Moosbauer', 'Julia Herbinger', 'Giuseppe Casalicchio', 'Marius Lindauer', 'Bernd Bischl']",poster,"['Automated machine learning', 'interpretable machine learning', 'partial dependence plots']","Automated hyperparameter optimization (HPO) can support practitioners to obtain peak performance in machine learning models.
However, there is often a lack of valuable insights into the effects of different hyperparameters on the final model performance.
This lack of comprehensibility and transparency makes it difficult to trust and understand the automated HPO process and its results.
We suggest using interpretable machine learning (IML) to gain insights from the experimental data obtained during HPO and especially discuss the popular case of Bayesian optimization (BO).
BO tends to focus on promising regions with potential high-performance configurations and thus induces a sampling bias.
Hence, many IML techniques, like Partial Dependence Plots (PDP), carry the risk of generating biased interpretations.
By leveraging the posterior uncertainty of the BO surrogate model, we introduce a variant of the PDP with estimated confidence bands.
In addition, we propose to partition the hyperparameter space to obtain more confident and reliable PDPs in relevant sub-regions.
In an experimental study, we provide quantitative evidence for the increased quality of the PDPs within sub-regions.",https://api.openreview.net/pdf/aeed90cee79372b7c5b62565e277003c06facf41.pdf
Bag of Baselines for Multi-objective Joint Neural Architecture Search and Hyperparameter Optimization,2021,ICML,"['Sergio Izquierdo', 'Julia Guerrero-Viu', 'Sven Hauns', 'Guilherme Miotto', 'Simon Schrodi', 'André Biedenkapp', 'Thomas Elsken', 'Difan Deng', 'Marius Lindauer', 'Frank Hutter']",poster,"['AutoML', 'Hyperparameter Optimization', 'Neural Architecture Search', 'Multi-Objective Optimization']","While both neural architecture search (NAS) and hyperparameter optimization (HPO) have been studied extensively in recent years, NAS methods typically assume fixed hyperparameters and vice versa. 
Furthermore, NAS has recently often been framed as a multi-objective optimization problem, in order to take, e.g., resource requirements into account.
In this paper, we propose a set of methods that extend current approaches to jointly optimize neural architectures and hyperparameters with respect to multiple objectives. We hope that these methods will serve as simple baselines for future research on multi-objective joint NAS + HPO.",https://api.openreview.net/pdf/d339b96e2dab29893fdf24775dc5b45d315e2d95.pdf
Leveraging Theoretical Tradeoffs in Hyperparameter Selection for Improved Empirical Performance,2021,ICML,"['Parikshit Ram', 'Alexander G. Gray', 'Horst Samulowitz']",poster,"['HPO', 'approximate ERM']","The tradeoffs in the excess risk incurred from data-driven learning of a single model has been studied by decomposing the excess risk into approximation, estimation and optimization errors. In this paper, we focus on the excess risk incurred in data-driven hyperparameter optimization (HPO) and its interaction with approximate empirical risk minimization (ERM) necessitated by large data. We present novel bounds for the excess risk in various common scenarios in HPO. Based on these results, we propose practical heuristics that allow us to improve performance or reduce computational overhead of data-driven HPO, demonstrating over $2 \times$ speedup with no loss in predictive performance in our preliminary results.",https://api.openreview.net/pdf/1301346c794e8701ac4339433fc7fd0b26b06cb4.pdf
AutoML Adoption in ML Software,2021,ICML,"['Koen Van der Blom', 'Alex Serban', 'Holger Hoos', 'Joost Visser']",poster,"['AutoML', 'software engineering', 'engineering practices', 'survey', 'interviews']","Machine learning (ML) has become essential to a vast range of applications, while ML experts are in short supply. To alleviate this problem, AutoML aims to make ML easier and more efficient to use. Even so, it is not clear to which extent AutoML techniques are actually adopted in an engineering context, nor what facilitates or inhibits adoption. To study this, we define AutoML engineering practices, measure their adoption through surveys, and distil first insights into factors influencing adoption from two initial interviews. Depending on the practice, results show around 20 to 30% of the respondents have not adopted it at all and many more only partially, leaving substantial room for increases in adoption. The interviews indicate adoption may in part be inhibited by usability issues with AutoML frameworks and the increased computational resources needed for adoption.",https://api.openreview.net/pdf/7b850c491e0dbe43e7ede9178b4dc667eda1cf5a.pdf
Dynamic Pruning of a Neural Network via Gradient Signal-to-Noise Ratio,2021,ICML,"['Julien Niklas Siems', 'Aaron Klein', 'Cedric Archambeau', 'Maren Mahsereci']",poster,"['Pruning', 'Neural Network', 'Gradient statistics']","While training highly overparameterized neural networks is common practice in deep learning, research into post-hoc weight-pruning suggests that more than 90% of parameters can be removed without loss in predictive performance.  To save resources, zero-shot and one-shot pruning attempt to find such a sparse representation at initialization or at an early stage of training. Though efficient, there is no justification, why the sparsity structure should not change during training. Dynamic sparsity pruning undoes this limitation and allows to adapt the structure of the sparse neural network during training. Recent approaches rely on weight magnitude pruning, which has been shown to be sub-optimal when applied at earlier training stages. In this work we propose to use the gradient noise to make pruning decisions. The procedure enables us to automatically adjust the sparsity during training without imposing a hand-designed sparsity schedule, while at the same time being able to recover from previous pruning decisions by unpruning connections as necessary. We evaluate our new method on image and tabular datasets and demonstrate that we reach similar performance as the dense model from which extract the sparse network, while exposing less hyperparameters than other dynamic sparsity methods.",https://api.openreview.net/pdf/06f0e32c816ad09a591e9677e68a085643765dc7.pdf
Towards Model Selection using Learning Curve Cross-Validation,2021,ICML,"['Felix Mohr', 'Jan N. van Rijn']",poster,"['model selection', 'algorithm selection', 'learning curves']","Cross-validation (CV) methods such as leave-one-out cross-validation, k-fold cross-validation, and Monte-Carlo cross-validation estimate the predictive performance of a learner by repeatedly training it on a large portion of the given data and testing on the remaining data. These techniques have two drawbacks. First, they can be unnecessarily slow on large datasets. Second, providing only point estimates, they give almost no insights into the learning process of the validated algorithm. In this paper, we propose a new approach for validation based on learning curves (LCCV). Instead of creating train-test splits with a large portion of training data, LCCV iteratively increases the number of training examples used for training. In the context of model selection, it eliminates models that can be safely dismissed from the candidate pool. We run a large scale experiment on the 67 datasets from the AutoML benchmark, and empirically show that LCCV in over 90\% of the cases leads to similar performance (at most 0.5\% difference) as 10-fold CV, but provides additional insights on the behaviour of a given model. On top of this, LCCV results in runtime reductions between 20% and over 50% on half of the 67 datasets from the AutoML benchmark. This can be incorporated in various AutoML frameworks, to speed up the internal evaluation of candidate models. As such, these results can be used orthogonal to other advances in the field of AutoML.",https://api.openreview.net/pdf/2ff19d41452b9b3a2c5ddbbaf150a30bd399260f.pdf
Bandit Limited Discrepancy Search and Application to Machine Learning Pipeline Optimization,2021,ICML,"['Akihiro Kishimoto', 'Djallel Bouneffouf', 'Radu Marinescu', 'Parikshit Ram', 'Ambrish Rawat', 'Martin Wistuba', 'Paulito Pedregosa Palmes', 'Adi Botea']",poster,"['automated machine learning', 'machine pipeline optimization', 'limited discrepancy search', 'multi-armed bandit']","Optimizing a machine learning (ML) pipeline has been an important topic of AI and ML.  Despite recent progress, this topic remains a challenging problem, due to potentially many combinations to consider as well as slow training and validation.  We present the BLDS algorithm for optimized algorithm selection (ML operations) in a fixed ML pipeline structure.  BLDS performs multi-fidelity optimization for selecting ML algorithms trained with smaller computational overhead, while controlling its pipeline search based on multi-armed bandit and limited discrepancy search.  Our experiments on well-known benchmarks show that BLDS is superior to competing algorithms.
",https://api.openreview.net/pdf/f463ffc2690727b7e4b43e97ade3442d0d9b0f04.pdf
GPy-ABCD: A Configurable Automatic Bayesian Covariance Discovery Implementation,2021,ICML,"['Thomas Fletcher', 'Alan Bundy', 'Kwabena Nuamah']",poster,"['Model selection', 'hyper-parameter optimization', 'model search', 'automatic feature extraction', 'ABCD', 'Gaussian Process Regression']","Gaussian Processes (GPs) are a very flexible class of nonparametric models frequently used in supervised learning tasks because of their ability to fit data with very few assumptions, namely just the type of correlation (kernel) the data is expected to display. Automatic Bayesian Covariance Discovery (ABCD) is an iterative GP regression framework aimed at removing the requirement for even this initial correlation form assumption. An original ABCD implementation exists and is a complex stand-alone system designed to produce long-form text analyses of provided data. This paper presents a lighter, more functional and configurable implementation of the ABCD idea, outputting only fit models and short descriptions: the Python package GPy-ABCD, which was developed as part of an adaptive modelling component for the FRANK query-answering system. It uses a revised model-space search algorithm and removes a search bias which was required in order to retain model explainability in the original system.",https://api.openreview.net/pdf/7d42651b755b6da644336a209526a4b597f13e46.pdf
Latency-Aware Neural Architecture Search with Multi-Objective Bayesian Optimization,2021,ICML,"['David Eriksson', 'Pierce I-Jen Chuang', 'Samuel Daulton', 'Peng Xia', 'Akshat Shrivastava', 'Arun Babu', 'Shicong Zhao', 'Ahmed A Aly', 'Ganesh Venkatesh', 'Maximilian Balandat']",poster,"['Bayesian Optimization', 'Gaussian Process', 'AutoML', 'Natural Language Understanding']","When tuning the architecture and hyperparameters of large machine learning models for on-device deployment, it is desirable to understand the optimal trade-offs between on-device latency and model accuracy.  In this work, we leverage recent methodological advances in Bayesian optimization over high-dimensional search spaces and multi-objective Bayesian optimization to efficiently explore these trade-offs for a production-scale on-device natural language understanding model at Facebook.",https://api.openreview.net/pdf/2a14c9fc0a31bcb7cea4f0344363796910e159f9.pdf
A resource-efficient method for repeated HPO and NAS problems,2021,ICML,"['Giovanni Zappella', 'David Salinas', 'Cedric Archambeau']",poster,"['HPO', 'NAS', 'multi-armed bandits', 'resource-efficiency', 'transfer learning', 'online learning']","In this work we consider the problem of repeated hyperparameter and neural architecture search (HNAS).We propose an extension of Successive Halving that is able to leverage information gained in previous HNAS problems with the goal of saving computational resources. We empirically demonstrate that our solution is able to drastically decrease costs while maintaining accuracy and being robust to negative transfer. Our method is significantly simpler than competing transfer learning approaches, setting a new baseline for transfer learning in HNAS.
",https://api.openreview.net/pdf/e9902883a74bc0e53decf354f819bc0b361c1dc5.pdf
