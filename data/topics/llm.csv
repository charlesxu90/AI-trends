title,year,source,authors,class,keywords,abstract,pdf_link,topic,google_scholar_link
Coarse-Fine Networks for Temporal Activity Detection in Videos,2021,CVPR,Kumara Kahatapitiya;Michael S. Ryoo,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Kahatapitiya_Coarse-Fine_Networks_for_Temporal_Activity_Detection_in_Videos_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=Coarse-Fine+Networks+for+Temporal+Activity+Detection+in+Videos
Globally Optimal Relative Pose Estimation With Gravity Prior,2021,CVPR,Yaqing Ding;Daniel Barath;Jian Yang;Hui Kong;Zuzana Kukelova,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Ding_Globally_Optimal_Relative_Pose_Estimation_With_Gravity_Prior_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=Globally+Optimal+Relative+Pose+Estimation+With+Gravity+Prior
Repetitive Activity Counting by Sight and Sound,2021,CVPR,Yunhua Zhang;Ling Shao;Cees G. M. Snoek,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Repetitive_Activity_Counting_by_Sight_and_Sound_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=Repetitive+Activity+Counting+by+Sight+and+Sound
VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization,2021,CVPR,Seunghwan Choi;Sunghyun Park;Minsoo Lee;Jaegul Choo,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Choi_VITON-HD_High-Resolution_Virtual_Try-On_via_Misalignment-Aware_Normalization_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=VITON-HD:+High-Resolution+Virtual+Try-On+via+Misalignment-Aware+Normalization
Anchor-Constrained Viterbi for Set-Supervised Action Segmentation,2021,CVPR,Jun Li;Sinisa Todorovic,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Anchor-Constrained_Viterbi_for_Set-Supervised_Action_Segmentation_CVPR_2021_paper.pdf,optimization;segmentation;llm,https://scholar.google.com/scholar?q=Anchor-Constrained+Viterbi+for+Set-Supervised+Action+Segmentation
Multi-Label Activity Recognition Using Activity-Specific Features and Activity Correlations,2021,CVPR,Yanyi Zhang;Xinyu Li;Ivan Marsic,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Multi-Label_Activity_Recognition_Using_Activity-Specific_Features_and_Activity_Correlations_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=Multi-Label+Activity+Recognition+Using+Activity-Specific+Features+and+Activity+Correlations
Joint Deep Model-Based MR Image and Coil Sensitivity Reconstruction Network (Joint-ICNet) for Fast MRI,2021,CVPR,Yohan Jun;Hyungseob Shin;Taejoon Eo;Dosik Hwang,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Jun_Joint_Deep_Model-Based_MR_Image_and_Coil_Sensitivity_Reconstruction_Network_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=Joint+Deep+Model-Based+MR+Image+and+Coil+Sensitivity+Reconstruction+Network+(Joint-ICNet)+for+Fast+MRI
Bractivate: Dendritic Branching in Medical Image Segmentation Neural Architecture Search,2021,ICLR,['Leila Abdelrahman'],poster,"['Neural Architecture Search', 'Segmentation', 'Computer Vision']","Researchers manually compose most neural networks through painstaking experimentation.
This process is taxing and explores only a limited subset of possible
architecture. Researchers design architectures to address objectives ranging from low
space complexity to high accuracy through hours of experimentation. Neural architecture
search (NAS) is a thriving field for automatically discovering architectures
achieving these same objectives. Addressing these ever-increasing challenges in
computing, we take inspiration from the brain because it has the most efficient
neuronal wiring of any complex structure; its physiology inspires us to propose
Bractivate, a NAS algorithm inspired by neural dendritic branching. An evolutionary algorithm that adds new skip connection combinations to the most active blocks in the network, propagating salient
information through the network. We apply our methods to lung x-ray, cell nuclei
microscopy, and electron microscopy segmentation tasks to highlight Bractivate's robustness.
Moreover, our ablation studies emphasize dendritic branching's necessity: ablating
these connections leads to significantly lower model performance. We finally
compare our discovered architecture with other state-of-the-art UNet models,
highlighting how efficient skip connections allow Bractivate to achieve comparable
results with substantially lower space and time complexity, proving how
Bractivate balances efficiency with performance. We invite you to work with our
code here: \href{https://tinyurl.com/bractivate}{\textcolor{violet}{https://tinyurl.com/bractivate}}.",/pdf/d3affccd05fea7e285903b2a1ea7e70481f732c2.pdf,graph;zero_few-shot;active learning;segmentation;llm,https://scholar.google.com/scholar?q=Bractivate:+Dendritic+Branching+in+Medical+Image+Segmentation+Neural+Architecture+Search
Probing BERT in Hyperbolic Spaces,2021,ICLR,"['Boli Chen', 'Yao Fu', 'Guangwei Xu', 'Pengjun Xie', 'Chuanqi Tan', 'Mosha Chen', 'Liping Jing']",poster,"['Hyperbolic', 'BERT', 'Probe', 'Syntax', 'Sentiment']","Recently, a variety of probing tasks are proposed to discover linguistic properties learned in contextualized word embeddings. Many of these works implicitly assume these embeddings lay in certain metric spaces, typically the Euclidean space. This work considers a family of geometrically special spaces, the hyperbolic spaces, that exhibit better inductive biases for hierarchical structures and may better reveal linguistic hierarchies encoded in contextualized representations. We introduce a $\textit{Poincaré probe}$, a structural probe projecting these embeddings into a Poincaré subspace with explicitly defined hierarchies. We focus on two probing objectives: (a) dependency trees where the hierarchy is defined as head-dependent structures; (b) lexical sentiments where the hierarchy is defined as the polarity of words (positivity and negativity). We argue that a key desideratum of a probe is its sensitivity to the existence of linguistic structures. We apply our probes on BERT, a typical contextualized embedding model. In a syntactic subspace, our probe better recovers tree structures than Euclidean probes, revealing the possibility that the geometry of BERT syntax may not necessarily be Euclidean. In a sentiment subspace, we reveal two possible meta-embeddings for positive and negative sentiments and show how lexically-controlled contextualization would change the geometric localization of embeddings. We demonstrate the findings with our Poincaré probe via extensive experiments and visualization. Our results can be reproduced at https://github.com/FranxYao/PoincareProbe",/pdf/5787fd974583617cec6dae3f0a6f5eea632dad93.pdf,transformer;representation;metric;meta-learning;llm,https://scholar.google.com/scholar?q=Probing+BERT+in+Hyperbolic+Spaces
Graph Information Bottleneck for Subgraph Recognition,2021,ICLR,"['Junchi Yu', 'Tingyang Xu', 'Yu Rong', 'Yatao Bian', 'Junzhou Huang', 'Ran He']",poster,[],"Given the input graph and its label/property, several key problems  of graph learning, such as finding interpretable subgraphs, graph denoising and graph compression,  can be  attributed to the fundamental problem of recognizing a subgraph of the original one.  This subgraph shall be as informative as possible, yet contains less redundant and noisy structure. This problem setting is closely related to the well-known information bottleneck (IB) principle, which, however, has less been studied for the irregular graph data and graph neural networks (GNNs). In this paper, we propose a framework of Graph Information Bottleneck (GIB) for the subgraph recognition problem in deep graph learning. Under this framework, one can recognize the maximally informative yet compressive subgraph, named IB-subgraph.  However, the GIB objective is notoriously hard to optimize, mostly due to the intractability of the mutual information of irregular graph data and the unstable optimization process. In order to tackle these challenges, we propose:  i) a GIB objective based-on a mutual information estimator for the irregular graph data; ii) a bi-level optimization scheme to maximize the GIB objective; iii) a connectivity loss to stabilize the optimization process. We evaluate the properties of the IB-subgraph in three application scenarios: improvement of graph classification, graph interpretation and graph denoising. Extensive experiments demonstrate that the information-theoretic  IB-subgraph  enjoys superior graph properties. ",/pdf/45a07fde0c34644e0b294e4bb7bb3c045bc3429a.pdf,graph;optimization;llm,https://scholar.google.com/scholar?q=Graph+Information+Bottleneck+for+Subgraph+Recognition
Undistillable: Making A Nasty Teacher That CANNOT teach students,2021,ICLR,"['Haoyu Ma', 'Tianlong Chen', 'Ting-Kuei Hu', 'Chenyu You', 'Xiaohui Xie', 'Zhangyang Wang']",poster,"['knowledge distillation', 'avoid knowledge leaking']","Knowledge Distillation (KD) is a widely used technique to transfer knowledge from pre-trained teacher models to  (usually more lightweight) student models. However, in certain situations, this technique is more of a curse than a blessing. For instance, KD poses a potential risk of exposing intellectual properties (IPs): even if a trained machine learning model is released in ``black boxes'' (e.g., as executable software or APIs without open-sourcing code), it can still be replicated by KD through imitating input-output behaviors. To prevent this unwanted effect of KD, this paper introduces and investigates a concept called $\textit{Nasty Teacher}$: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. We propose a simple yet effective algorithm to build the nasty teacher, called $\textit{self-undermining knowledge distillation}$. Specifically, we aim to maximize the difference between the output of the nasty teacher and a normal pre-trained network. Extensive experiments on several datasets demonstrate that our method is effective on both standard KD and data-free KD, providing the desirable KD-immunity to model owners for the first time. We hope our preliminary study can draw more awareness and interest in this new practical problem of both social and legal importance. Our codes and pre-trained models can be found at: $\url{https://github.com/VITA-Group/Nasty-Teacher}$.",/pdf/42f6ff4cc0e85c1f3a226c56205d2f78953cdc7c.pdf,transfer learning;distillation;llm,https://scholar.google.com/scholar?q=Undistillable:+Making+A+Nasty+Teacher+That+CANNOT+teach+students
How much progress have we made in neural network training? A New Evaluation Protocol for Benchmarking Optimizers,2021,ICLR,"['Yuanhao Xiong', 'Xuanqing Liu', 'Li-Cheng Lan', 'Yang You', 'Si Si', 'Cho-Jui Hsieh']",poster,"['deep learning', 'optimization', 'benchmarking']","Many optimizers have been proposed for training deep neural networks, and they often have multiple hyperparameters, which make it tricky to benchmark their performance. In this work, we propose a new benchmarking protocol to evaluate both end-to-end efficiency (training a model from scratch without knowing the best hyperparameter) and data-addition training efficiency (the previously selected hyperparameters are used for periodically re-training the model with newly collected data). For end-to-end efficiency, unlike previous work that assumes random hyperparameter tuning, which over-emphasizes the tuning time, we propose to evaluate with a bandit hyperparameter tuning strategy. A human study is conducted to show our evaluation protocol matches human tuning behavior better than the random search. For data-addition training, we propose a new protocol for assessing the hyperparameter sensitivity to data shift. We then apply the proposed benchmarking framework to 7 optimizers and various tasks, including computer vision, natural language processing, reinforcement learning, and graph mining. Our results show that there is no clear winner across all the tasks. 
",/pdf/41949bb057785b4b6c93879f2d53f7aa946f79f7.pdf,reinforcement learning;graph;llm,https://scholar.google.com/scholar?q=How+much+progress+have+we+made+in+neural+network+training?+A+New+Evaluation+Protocol+for+Benchmarking+Optimizers
Dataset Curation Beyond Accuracy,2021,ICLR,"['Johan Bjorck', 'Carla P Gomes']",poster,"['crowd-sourcing', 'calibration', 'dataset', 'uncertainty']","Neural networks are known to be data-hungry, and collecting large labeled datasets is often a crucial step in deep learning deployment. Researchers have studied dataset aspects such as distributional shift and labeling cost, primarily using downstream prediction accuracy for evaluation. In sensitive real-world applications such as medicine and self-driving cars, not only is the accuracy important, but also the calibration -- the extent that model uncertainty reflects the actual correctness likelihood. It has recently been shown that modern neural networks are ill-calibrated. In this work, we take a complementary approach -- studying how dataset properties, rather than architecture, affects calibration. For the common issue of dataset imbalance, we show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. We also study the effects of label quality, showing how label noise dramatically increases calibration error. Furthermore, poor calibration can come from small dataset sizes, which we motive via results on network expressivity. Our experiments demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation.",/pdf/19547af72952e4c1e55f808961f95c872390e5fb.pdf,llm,https://scholar.google.com/scholar?q=Dataset+Curation+Beyond+Accuracy
Linking average- and worst-case perturbation robustness via class selectivity and dimensionality,2021,ICLR,"['Matthew L Leavitt', 'Ari S. Morcos']",poster,"['robustness', 'adversarial robustness', 'corruptions', 'class selectivity', 'deep learning']","Representational sparsity is known to affect robustness to input perturbations in deep neural networks (DNNs), but less is known about how the semantic content of representations affects robustness. Class selectivity—the variability of a unit’s responses across data classes or dimensions—is one way of quantifying the sparsity of semantic representations. Given recent evidence that class selectivity may not be necessary for, and in some cases can impair generalization, we sought to investigate whether it also confers robustness (or vulnerability) to perturbations of input data. We found that class selectivity leads to increased vulnerability to average-case (naturalistic) perturbations in ResNet18, ResNet50, and ResNet20, as measured using Tiny ImageNetC (ResNet18 and ResNet50) and CIFAR10C (ResNet20). Networks regularized to have lower levels of class selectivity are more robust to average-case perturbations, while networks with higher class selectivity are more vulnerable. In contrast, we found that class selectivity increases robustness to multiple types of worst-case (i.e. white box adversarial) perturbations, suggesting that while decreasing class selectivity is helpful for average-case perturbations, it is harmful for worst-case perturbations. To explain this difference, we studied the dimensionality of the networks' representations: we found that the dimensionality of early-layer representations is inversely proportional to a network's class selectivity, and that adversarial samples cause a larger increase in early-layer dimensionality than corrupted samples. We also found that the input-unit gradient was more variable across samples and units in high-selectivity networks compared to low-selectivity networks. These results lead to the conclusion that units participate more consistently in low-selectivity regimes compared to high-selectivity regimes, effectively creating a larger attack surface and hence vulnerability to worst-case perturbations.",/pdf/848eefeaa1a854e84a24e48036cc39f7746e3532.pdf,zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Linking+average-+and+worst-case+perturbation+robustness+via+class+selectivity+and+dimensionality
Offline Adaptive Policy Leaning in Real-World Sequential Recommendation Systems,2021,ICLR,"['Xiong-Hui Chen', 'Yang Yu', 'Qingyang Li', 'Zhiwei Tony Qin', 'Wenjie Shang', 'Yiping Meng', 'Jieping Ye']",poster,"['Reinforcement learning', 'Recommendation system']","The training process of RL requires many trial-and-errors that are costly in real-world applications. To avoid the cost, a promising solution is to learn the policy from an offline dataset, e.g., to learn a simulator from the dataset, and train optimal policies in the simulator. By this approach, the quality of policies highly relies on the fidelity of the simulator. Unfortunately, due to the stochasticity and unsteadiness of the real-world and the unavailability of online sampling, the distortion of the simulator is inevitable. In this paper, based on the model learning technique, we propose a new paradigm to learn an RL policy from offline data in the real-world sequential recommendation system (SRS). Instead of increasing the fidelity of models for policy learning, we handle the distortion issue via learning to adapt to diverse simulators generated by the offline dataset. The adaptive policy is suitable to real-world environments where dynamics are changing and have stochasticity in the offline setting. Experiments are conducted in synthetic environments and a real-world ride-hailing platform. The results show that the method overcomes the distortion problem and produces robust recommendations in the unseen real-world.",/pdf/3c97baa0fffb5a8f23694b3ebf9014a63be5ff77.pdf,offline reinforcement learning;graph;online learning;adaptive;llm,https://scholar.google.com/scholar?q=Offline+Adaptive+Policy+Leaning+in+Real-World+Sequential+Recommendation+Systems
The Unbalanced Gromov Wasserstein Distance: Conic Formulation and Relaxation,2021,ICLR,"['Thibault Sejourne', 'François-Xavier Vialard', 'Gabriel Peyré']",poster,"['Gromov-Wasserstein', 'Non-convex optimization', 'Optimal Transport', 'Partial matching']","Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. This includes for instance predicting properties of molecules in quantum chemistry or generating graphs with varying connectivity. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. This distance has been successfully applied to supervised learning and generative modeling, for applications as diverse as quantum chemistry or natural language processing. The GW distance is however limited to the comparison of metric measure spaces endowed with a probability distribution. This strong limitation is problematic for many applications in ML where there is no a priori natural normalization on the total mass of the data. Furthermore, imposing an exact conservation of mass across spaces is not robust to outliers and often leads to irregular matching. To alleviate these issues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence.This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efficiently tackled using a highly parallelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical simulations to highlight the salient features of the unbalanced divergence and its potential applications in ML.",/pdf/85a7cbe50f7781f24692916b1e62cc214ff5e7d7.pdf,graph;optimization;zero_few-shot;generative model;metric;llm,https://scholar.google.com/scholar?q=The+Unbalanced+Gromov+Wasserstein+Distance:+Conic+Formulation+and+Relaxation
Support-set bottlenecks for video-text representation learning,2021,ICLR,"['Mandela Patrick', 'Po-Yao Huang', 'Yuki Asano', 'Florian Metze', 'Alexander G Hauptmann', 'Joao F. Henriques', 'Andrea Vedaldi']",poster,"['video representation learning', 'multi-modal learning', 'video-text learning', 'contrastive learning']","The dominant paradigm for learning video-text representations – noise contrastive learning – increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semantically-related – for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample’s caption must be reconstructed as a weighted combination of a support set of visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX, ActivityNet, and MSVD for video-to-text and text-to-video retrieval.",/pdf/a650da3e5bc4bc919f69887e2a9264dc61a58c94.pdf,graph;representation;generative model;contrastive learning;llm,https://scholar.google.com/scholar?q=Support-set+bottlenecks+for+video-text+representation+learning
Conformation-Guided Molecular Representation with Hamiltonian Neural Networks,2021,ICLR,"['Ziyao Li', 'Shuwen Yang', 'Guojie Song', 'Lingsheng Cai']",poster,"['Molecular Representation', 'Neural Physics Engines', 'Molecular Dynamics', 'Graph Neural Networks']","Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- & rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.",/pdf/7a5a25fdbe36c7b0286d17dafd233f47bf7dd30c.pdf,graph;zero_few-shot;representation;3d;llm,https://scholar.google.com/scholar?q=Conformation-Guided+Molecular+Representation+with+Hamiltonian+Neural+Networks
Balancing Robustness and Sensitivity using Feature Contrastive Learning,2021,ICLR,"['Seungyeon Kim', 'Daniel Glasner', 'Srikumar Ramalingam', 'Cho-Jui Hsieh', 'Kishore Papineni', 'Sanjiv Kumar']",poster,"['deep learning', 'non-adversarial robustness', 'sensitivity', 'input perturbation', 'contextual feature utility', 'contextual feature sensitivity.']","It is generally believed that robust training of extremely large networks is critical to their success in real-world applications. However, when taken to the extreme, methods that promote robustness can hurt the model’s sensitivity to rare or underrepresented patterns. In this paper, we discuss this trade-off between robustness and sensitivity by introducing two notions: contextual feature utility and contextual feature sensitivity. We propose Feature Contrastive Learning (FCL) that encourages the model to be more sensitive to the features that have higher contextual utility. Empirical results demonstrate that models trained with FCL achieve a better balance of robustness and sensitivity, leading to improved generalization in the presence of noise.",/pdf/1c3f624e56375d32298d8085dcac9fbc6e890b1c.pdf,contrastive learning;llm,https://scholar.google.com/scholar?q=Balancing+Robustness+and+Sensitivity+using+Feature+Contrastive+Learning
Error Controlled Actor-Critic Method to Reinforcement Learning,2021,ICLR,"['Xingen Gao', 'Fei Chao', 'Changle Zhou', 'Zhen Ge', 'Chih-Min Lin', 'Longzhi Yang', 'Xiang Chang', 'Changjing Shang']",poster,"['reinforcement learning', 'actor-critic', 'function approximation', 'approximation error', 'KL divergence']","In the reinforcement learning (RL) algorithms which incorporate function approximation methods, the approximation error of value function inevitably cause overestimation phenomenon and have a negative impact on the convergence of the algorithms. To mitigate the negative effects of approximation error, we propose a new actor-critic algorithm called Error Controlled Actor-critic which ensures confining the approximation error in value function. In this paper, we firstly present an analysis of how the approximation error can hinder the optimization process of actor-critic methods. Then, we *derive an upper boundary of the approximation error of Q function approximator, and found that the error can be lowered by placing restrictions on the KL-divergence between every two consecutive policies during the training phase of the policy.* The results of experiments on a range of continuous control tasks from OpenAI gym suite demonstrate that the proposed actor-critic algorithm apparently reduces the approximation error and significantly outperforms other model-free RL algorithms.",/pdf/c4be5250d095d9adc06e881bd14a009f3964d378.pdf,reinforcement learning;optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Error+Controlled+Actor-Critic+Method+to+Reinforcement+Learning
On Size Generalization in Graph Neural Networks,2021,ICLR,"['Gilad Yehudai', 'Ethan Fetaya', 'Eli Meirom', 'Gal Chechik', 'Haggai Maron']",poster,"['graph neural networks', 'gnn', 'generalization', 'Weisfeiler-Lehman']","Graph neural networks (GNNs) can process graphs of different sizes but their capacity to generalize across sizes is still not well understood. Size generalization is key to numerous GNN applications, from solving combinatorial optimization problems to learning in molecular biology. In such problems, obtaining labels and training on large graphs can be prohibitively expensive, but training on smaller graphs is possible. 

This paper puts forward the size-generalization question and characterizes important aspects of that problem theoretically and empirically.
We prove that even for very simple tasks, such as counting the number of nodes or edges in a graph, GNNs do not naturally generalize to graphs of larger size. Instead, their generalization performance is closely related to the distribution of local patterns of connectivity and features and how that distribution changes from small to large graphs. Specifically, we prove that for many tasks, there are weight assignments for GNNs that can perfectly solve the task on small graphs but fail on large graphs, if there is a discrepancy between their local patterns. We further demonstrate on several tasks, that training GNNs on small graphs results in solutions which do not generalize to larger graphs. We then formalize size generalization as a domain-adaption problem and describe two learning setups where size generalization can be improved. First, as a self-supervised learning problem (SSL) over the target domain of large graphs. Second as a semi-supervised learning problem when few samples are available in the target domain. We demonstrate the efficacy of these solutions on a diverse set of benchmark graph datasets. ",/pdf/e81854f653c0c15fc70b0201236bf9e86fa3b2b3.pdf,graph;optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=On+Size+Generalization+in+Graph+Neural+Networks
TopoTER: Unsupervised Learning of Topology Transformation Equivariant Representations,2021,ICLR,"['Xiang Gao', 'Wei Hu', 'Guo-Jun Qi']",poster,"['Unsupervised learning', 'node representations', 'mutual information']","We present the Topology Transformation Equivariant Representation (TopoTER) learning, a general paradigm of unsupervised learning of node representations of graph data for the wide applicability to Graph Convolutional Neural Networks (GCNNs). We formalize the TopoTER from an information-theoretic perspective, by maximizing the mutual information between topology transformations and node representations before and after the transformations. We derive that maximizing such mutual information can be relaxed to minimizing the cross entropy between the applied topology transformation and its estimation from node representations. In particular, we seek to sample a subset of node pairs from the original graph and flip the edge connectivity between each pair to transform the graph topology. Then, we self-train a representation encoder to learn node representations by reconstructing the topology transformations from the feature representations of the original and transformed graphs. In experiments, we apply the TopoTER to the downstream node and graph classification tasks, and results show that the TopoTER outperforms the state-of-the-art unsupervised approaches.",/pdf/05fb59f941fd0c1eb202bb1206eb2663bc957145.pdf,graph;representation;llm,https://scholar.google.com/scholar?q=TopoTER:+Unsupervised+Learning+of+Topology+Transformation+Equivariant+Representations
Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win,2021,ICLR,"['Utku Evci', 'Yani Ioannou', 'Cem Keskin', 'Yann Dauphin']",poster,"['sparse training', 'sparsity', 'pruning', 'lottery ticket hypothesis', 'lottery tickets', 'sparse initialization', 'initialization', 'deep learning', 'gradient flow']","Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and also have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exception of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). In this work, we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; and (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and propose a modified initialization for unstructured connectivity. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from — however, this comes at the cost of learning novel solutions.",/pdf/f0a5e4a0c915f9cf7fe3c7d25d1932011b580efd.pdf,zero_few-shot;inference;sparse;flow;llm,https://scholar.google.com/scholar?q=Gradient+Flow+in+Sparse+Neural+Networks+and+How+Lottery+Tickets+Win
TAM: Temporal Adaptive Module for Video Recognition,2021,ICLR,"['Zhaoyang Liu', 'Limin Wang', 'Wayne Wu', 'Chen Qian', 'Tong Lu']",poster,"['Action Recognition', 'Temporal Adaptive Module', 'Temporal Adaptive Network']","Temporal modeling is crucial for capturing spatiotemporal structure in videos for action recognition. Video data is with extremely complex dynamics along its temporal dimension due to various factors such as camera motion, speed variation, and different activities. To effectively capture this diverse motion pattern, this paper presents a new temporal adaptive module ({\bf TAM}) to generate video-specific kernels based on its own feature maps. TAM proposes a unique two-level adaptive modeling scheme by decoupling dynamic kernels into a location sensitive importance map and a location invariant aggregation weight. The importance map is learned in a local temporal window to capture short term information, while the aggregation weight is generated from a global view with a focus on long-term structure. TAM is a principled module and could be integrated into 2D CNNs to yield a powerful video architecture (TANet) with a very small extra computational cost. The extensive experiments on Kinetics-400 and Something-Something datasets, demonstrate that the TAM outperforms other temporal modeling methods consistently owing to its temporal adaptive modeling strategy.",/pdf/41f7405b8750a9cc2034f2da4cb42cf2c1dcc0f3.pdf,adaptive;llm,https://scholar.google.com/scholar?q=TAM:+Temporal+Adaptive+Module+for+Video+Recognition
Creative Sketch Generation,2021,ICLR,"['Songwei Ge', 'Vedanuj Goswami', 'Larry Zitnick', 'Devi Parikh']",poster,"['creativity', 'sketches', 'part-based', 'GAN', 'dataset', 'generative art']","Sketching or doodling is a popular creative activity that people engage in. However, most existing work in automatic sketch understanding or generation has focused on sketches that are quite mundane. In this work, we introduce two datasets of creative sketches -- Creative Birds and Creative Creatures -- containing 10k sketches each along with part annotations. We propose DoodlerGAN -- a part-based Generative Adversarial Network (GAN) -- to generate unseen compositions of novel part appearances. Quantitative evaluations as well as human studies demonstrate that sketches generated by our approach are more creative and of higher quality than existing approaches. In fact, in Creative Birds, subjects prefer sketches generated by DoodlerGAN over those drawn by humans!",/pdf/373ac60cbcad73b611ba4a2ec2c15fd2747b84ea.pdf,generative model;llm,https://scholar.google.com/scholar?q=Creative+Sketch+Generation
Self-supervised Representation Learning with Relative Predictive Coding,2021,ICLR,"['Yao-Hung Hubert Tsai', 'Martin Q. Ma', 'Muqiao Yang', 'Han Zhao', 'Louis-Philippe Morency', 'Ruslan Salakhutdinov']",poster,"['self-supervised learning', 'contrastive learning', 'dependency based method']","This paper introduces Relative Predictive Coding (RPC), a new contrastive representation learning objective that maintains a good balance among training stability, minibatch size sensitivity, and downstream task performance. The key to the success of RPC is two-fold. First, RPC introduces the relative parameters to regularize the objective for boundedness and low variance. Second, RPC contains no logarithm and exponential score functions, which are the main cause of training instability in prior contrastive objectives. We empirically verify the effectiveness of RPC on benchmark vision and speech self-supervised learning tasks. Lastly, we relate RPC with mutual information (MI) estimation, showing RPC can be used to estimate MI with low variance.",/pdf/282575516141216df413ed34796271b6b81c1ac1.pdf,zero_few-shot;representation;contrastive learning;llm,https://scholar.google.com/scholar?q=Self-supervised+Representation+Learning+with+Relative+Predictive+Coding
"With False Friends Like These, Who Can Have Self-Knowledge?",2021,ICLR,"['Lue Tao', 'Songcan Chen']",poster,"['Robustness', 'Adversarial Risk', 'Neural Networks', 'Machine Learning Security']","Adversarial examples arise from excessive sensitivity of a model. Commonly studied adversarial examples are malicious inputs, crafted by an adversary from correctly classified examples, to induce misclassification. This paper studies an intriguing, yet far overlooked consequence of the excessive sensitivity, that is, a misclassified example can be easily perturbed to help the model to produce correct output. Such perturbed examples look harmless, but actually can be maliciously utilized by a false friend to make the model self-satisfied. Thus we name them hypocritical examples. With false friends like these, a poorly performed model could behave like a state-of-the-art one. Once a deployer trusts the hypocritical performance and uses the ""well-performed"" model in real-world applications, potential security concerns appear even in benign environments. In this paper, we formalize the hypocritical risk for the first time and propose a defense method specialized for hypocritical examples by minimizing the tradeoff between natural risk and an upper bound of hypocritical risk. Moreover, our theoretical analysis reveals connections between adversarial risk and hypocritical risk. Extensive experiments verify the theoretical results and the effectiveness of our proposed methods.",/pdf/a5829dd51dc29c9441bd1979f9fb1a8dc004a99a.pdf,llm,"https://scholar.google.com/scholar?q=With+False+Friends+Like+These,+Who+Can+Have+Self-Knowledge?"
"Laplacian Eigenspaces, Horocycles and Neuron Models on Hyperbolic Spaces",2021,ICLR,['Ming-Xi Wang'],poster,"['hyperbolic learning', 'hyperbolic neural network', 'Poincare embedding']","We use hyperbolic Poisson kernel to construct the horocycle neuron model on hyperbolic spaces, which is a spectral generalization of the classical neuron model. We prove a universal approximation theorem for horocycle neurons. As a corollary, this theorem leads to a state-of-the-art result on the expressivity of neurons of the hyperbolic MLR. Our experiments get state-of-the-art results on the Poincare-embedding tree classification task and the two-dimensional visualization of images.",/pdf/6980ca2ff050b0760638c7b77236e9e254bbf027.pdf,llm,"https://scholar.google.com/scholar?q=Laplacian+Eigenspaces,+Horocycles+and+Neuron+Models+on+Hyperbolic+Spaces"
The Benefit of Distraction: Denoising Remote Vitals Measurements Using Inverse Attention,2021,ICLR,"['Ewa Magdalena Nowara', 'Daniel McDuff', 'Ashok Veeraraghavan']",poster,"['convolutional attention networks', 'denoising', 'computer vision', 'camera-based physiology']","Attention is a powerful concept in computer vision. End-to-end networks that learn to focus selectively on regions of an image or video often perform strongly. However, other image regions, while not necessarily containing the signal of interest, may contain useful context. We present an approach that exploits the idea that statistics of noise may be shared between the regions that contain the signal of interest and those that do not. Our technique uses the inverse of an attention mask to generate a noise estimate that is then used to denoise temporal observations. We apply this to the task of camera-based physiological measurement. A convolutional attention network is used to learn which regions of a video contain the physiological signal and generate a preliminary estimate. A noise estimate is obtained by using the pixel intensities in the inverse regions of the learned attention mask, this in turn is used to refine the estimate of the physiological signal. We perform experiments on two large benchmark datasets and show that this approach produces state-of-the-art results, increasing the signal-to-noise ratio by up to 5.8 dB, reducing heart rate and breathing rate estimation error by as much as 30%, recovering subtle pulse waveform dynamics, and generalizing from RGB to NIR videos without retraining.",/pdf/bb599f647c91118c45608c632697d0064d0e4cc6.pdf,transformer;llm,https://scholar.google.com/scholar?q=The+Benefit+of+Distraction:+Denoising+Remote+Vitals+Measurements+Using+Inverse+Attention
Continual Memory: Can We Reason After Long-Term Memorization?,2021,ICLR,"['Zhu Zhang', 'Chang Zhou', 'Zhou Zhao', 'Zhijie Lin', 'Jingren Zhou', 'Hongxia Yang']",poster,"['Memory Augmented Neural Networks', 'Continual Memory', 'Reasoning After Long-Term Memorization']","Existing reasoning tasks often follow the setting of ‘’end-to-end reasoning'', which has an important assumption that the input contents can be always accessed while reasoning. However, human beings frequently adopt another reasoning setting in daily life, referred to ‘’reasoning after memorizing''. Concretely, human beings have the ability to unconsciously memorize their experiences within limited memory capacity, from which they can recall and respond to subsequent tasks. In this setting, the input contents are no longer available during reasoning, thus we need to compress and memorize the input stream in one pass, trying to answer general queries that are unseen before. Memory augmented neural networks introduce a write-read memory to perform such human-like memorization and reasoning, but they continually update the memory from current information and inevitably forget the early contents, failing to answer the queries relevant to early information.  In this paper, we propose the Continual Memory (CM) to explore this ability of reasoning after long-term memorization.  To alleviate the gradual forgetting of early information, we develop self-supervised memorization training with item-level and sequence-level objectives. We demonstrate several interesting characteristics of our continual memory via synthetic data, and evaluate its performance by several downstream tasks, including long-term text QA, long-term video QA and recommendation with long sequences.",/pdf/c9787079ff483ffd91f2f8d64521996a09f54357.pdf,zero_few-shot;augmentation;llm,https://scholar.google.com/scholar?q=Continual+Memory:+Can+We+Reason+After+Long-Term+Memorization?
Out-of-Distribution Generalization via Risk Extrapolation (REx),2021,ICLR,"['David Krueger', 'Ethan Caballero', 'Joern-Henrik Jacobsen', 'Amy Zhang', 'Jonathan Binas', 'Rémi LE PRIOL', 'Dinghuai Zhang', 'Aaron Courville']",poster,"['out of distribution', 'domain generalization', 'invariant risk minimization', 'robust optimization', 'invariant causal prediction', 'spurious features', 'generalization']","Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model’s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing some robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.",/pdf/4bd62302b6d405f74efd5c5fd0f3475106d03480.pdf,graph;optimization;transfer learning;llm,https://scholar.google.com/scholar?q=Out-of-Distribution+Generalization+via+Risk+Extrapolation+(REx)
Perceptual Adversarial Robustness: Defense Against Unseen Threat Models,2021,ICLR,"['Cassidy Laidlaw', 'Sahil Singla', 'Soheil Feizi']",poster,[],"A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the definition of adversarial attacks that are imperceptible to human eyes. Most current attacks and defenses try to get around this issue by considering restrictive adversarial threat models such as those bounded by $L_2$ or $L_\infty$ distance, spatial perturbations, etc. However, models that are robust against any of these restrictive threat models are still fragile against other threat models, i.e. they have poor generalization to unforeseen attacks. Moreover, even if a model is robust against the union of several restrictive threat models, it is still susceptible to other imperceptible adversarial examples that are not contained in any of the constituent threat models. To resolve these issues, we propose adversarial training against the set of all imperceptible adversarial examples. Since this set is intractable to compute without a human in the loop, we approximate it using deep neural networks. We call this threat model the neural perceptual threat model (NPTM); it includes adversarial examples with a bounded neural perceptual distance (a neural network-based approximation of the true perceptual distance) to natural images. Through an extensive perceptual study, we show that the neural perceptual distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model.

Under the NPTM, we develop novel perceptual adversarial attacks and defenses. Because the NPTM is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five diverse adversarial attacks: $L_2$, $L_\infty$, spatial, recoloring, and JPEG. We find that PAT achieves state-of-the-art robustness against the union of these five attacks—more than doubling the accuracy over the next best model—without training against any of them. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial training defense with this property.

Code and data are available at https://github.com/cassidylaidlaw/perceptual-advex",/pdf/eb768a2d394baa94aebe2efba341cc5a5244428d.pdf,llm,https://scholar.google.com/scholar?q=Perceptual+Adversarial+Robustness:+Defense+Against+Unseen+Threat+Models
Synthesising Realistic Calcium Traces of Neuronal Populations Using GAN,2021,ICLR,"['Bryan M. Li', 'Theoklitos Amvrosiadis', 'Nathalie Rochefort', 'Arno Onken']",poster,"['calcium imaging', 'calcium traces', 'generative adversarial networks', 'spike train analysis']","Calcium imaging has become a powerful and popular technique to monitor the activity of large populations of neurons in vivo. However, for ethical considerations and despite recent technical developments, recordings are still constrained to a limited number of trials and animals. This limits the amount of data available from individual experiments and hinders the development of analysis techniques and models for more realistic sizes of neuronal populations. The ability to artificially synthesize realistic neuronal calcium signals could greatly alleviate this problem by scaling up the number of trials. Here, we propose a Generative Adversarial Network (GAN) model to generate realistic calcium signals as seen in neuronal somata with calcium imaging. To this end, we propose CalciumGAN, a model based on the WaveGAN architecture and train it on calcium fluorescent signals with the Wasserstein distance. We test the model on artificial data with known ground-truth and show that the distribution of the generated signals closely resembles the underlying data distribution. Then, we train the model on real calcium traces recorded from the primary visual cortex of behaving mice and confirm that the deconvolved spike trains match the statistics of the recorded data. Together, these results demonstrate that our model can successfully generate realistic calcium traces, thereby providing the means to augment existing datasets of neuronal activity for enhanced data exploration and modelling.",/pdf/4ce35dba37e4f0554a9a4bda596c3418f0313281.pdf,graph;optimization;transformer;generative model;llm,https://scholar.google.com/scholar?q=Synthesising+Realistic+Calcium+Traces+of+Neuronal+Populations+Using+GAN
Efficient Inference of Flexible Interaction in Spiking-neuron Networks,2021,ICLR,"['Feng Zhou', 'Yixuan Zhang', 'Jun Zhu']",poster,"['neural spike train', 'nonlinear Hawkes process', 'auxiliary latent variable', 'conjugacy']","Hawkes process provides an effective statistical framework for analyzing the time-dependent interaction of neuronal spiking activities. Although utilized in many real applications, the classic Hawkes process is incapable of modelling inhibitory interactions among neurons. Instead, the nonlinear Hawkes process allows for a more flexible influence pattern with excitatory or inhibitory interactions. In this paper, three sets of auxiliary latent variables (Polya-Gamma variables, latent marked Poisson processes and sparsity variables) are augmented to make functional connection weights in a Gaussian form, which allows for a simple iterative algorithm with analytical updates. As a result, an efficient expectation-maximization (EM) algorithm is derived to obtain the maximum a posteriori (MAP) estimate. We demonstrate the accuracy and efficiency performance of our algorithm on synthetic and real data. For real neural recordings, we show our algorithm can estimate the temporal dynamics of interaction and reveal the interpretable functional connectivity underlying neural spike trains. ",/pdf/e38ae418aa78989951e0c37ca6c629aa4c6e96b7.pdf,zero_few-shot;online learning;inference;augmentation;llm,https://scholar.google.com/scholar?q=Efficient+Inference+of+Flexible+Interaction+in+Spiking-neuron+Networks
Interactive Weak Supervision: Learning Useful Heuristics for Data Labeling,2021,ICLR,"['Benedikt Boecking', 'Willie Neiswanger', 'Eric Xing', 'Artur Dubrawski']",poster,"['weak supervision', 'data programming', 'data labeling', 'active learning']","Obtaining large annotated datasets is critical for training successful machine learning models and it is often a bottleneck in practice. Weak supervision offers a promising alternative for producing labeled datasets without ground truth annotations by generating probabilistic labels using multiple noisy heuristics. This process can scale to large datasets and has demonstrated state of the art performance in diverse domains such as healthcare and e-commerce. One practical issue with learning from user-generated heuristics is that their creation requires creativity, foresight, and domain expertise from those who hand-craft them, a process which can be tedious and subjective. We develop the first framework for interactive weak supervision in which a method proposes heuristics and learns from user feedback given on each proposed heuristic. Our experiments demonstrate that only a small number of feedback iterations are needed to train models that achieve highly competitive test set performance without access to ground truth training labels. We conduct user studies, which show that users are able to effectively provide feedback on heuristics and that test set results track the performance of simulated oracles.",/pdf/7b8b0c300266ed08c27bfea5eaaa513189d0caab.pdf,zero_few-shot;active learning;llm,https://scholar.google.com/scholar?q=Interactive+Weak+Supervision:+Learning+Useful+Heuristics+for+Data+Labeling
Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks,2021,ICLR,"['He Wang', 'Zetian Jiang', 'Li Yi', 'Kaichun Mo', 'Hao Su', 'Leonidas Guibas']",poster,"['3D point cloud', 'GAN', 'sampling pattern', 'evaluation metrics', 'discriminator']","In this paper, we examine the long-neglected yet important effects of point sam- pling patterns in point cloud GANs. Through extensive experiments, we show that sampling-insensitive discriminators (e.g. PointNet-Max) produce shape point clouds with point clustering artifacts while sampling-oversensitive discriminators (e.g. PointNet++, DGCNN, PointConv, KPConv) fail to guide valid shape generation. We propose the concept of sampling spectrum to depict the different sampling sensitivities of discriminators. We further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several perceptual metrics forming a sampling spectrum of metrics. Guided by the proposed sampling spectrum, we discover a middle-point sampling-aware baseline discriminator, PointNet-Mix, which improves all existing point cloud generators by a large margin on sampling-related metrics. We point out that, given that recent research has been focused on the generator design, the discriminator design needs more attention. Our work provides both suggestions and tools for building future discriminators. We will release the code to facilitate future research.",/pdf/919eeec38e1d80ac1249371565b44bd45f43fc23.pdf,graph;transformer;generative model;metric;3d;llm,https://scholar.google.com/scholar?q=Rethinking+Sampling+in+3D+Point+Cloud+Generative+Adversarial+Networks
Learning to Generate 3D Shapes with Generative Cellular Automata,2021,ICLR,"['Dongsu Zhang', 'Changwoon Choi', 'Jeonghwan Kim', 'Young Min Kim']",poster,"['3D generation', 'generative models']","In this work, we present a probabilistic 3D generative model, named Generative Cellular Automata, which is able to produce diverse and high quality shapes. We formulate the shape generation process as sampling from the transition kernel of a Markov chain, where the sampling chain eventually evolves to the full shape of the learned distribution. The transition kernel employs the local update rules of cellular automata, effectively reducing the search space in a high-resolution 3D grid space by exploiting the connectivity and sparsity of 3D shapes. Our progressive generation only focuses on the sparse set of occupied voxels and their neighborhood, thus enables the utilization of an expressive sparse convolutional network. We propose an effective training scheme to obtain the local homogeneous rule of generative cellular automata with sequences that are slightly different from the sampling chain but converge to the full shapes in the training data. Extensive experiments on probabilistic shape completion and shape generation demonstrate that our method achieves competitive performance against recent methods.",/pdf/859e40f96ad083b6715504dbb3c3340a92590b79.pdf,generative model;sparse;3d;llm,https://scholar.google.com/scholar?q=Learning+to+Generate+3D+Shapes+with+Generative+Cellular+Automata
Ranking Cost: One-Stage Circuit Routing by Directly Optimizing Global Objective Function,2021,ICLR,"['Shiyu Huang', 'Bin Wang', 'Dong Li', 'Jianye Hao', 'Jun Zhu', 'Ting Chen']",poster,"['Evolution Strategy', 'Circuit Routing', 'A*', 'PCB']","Circuit routing has been a historically challenging problem in designing electronic systems such as very large-scale integration (VLSI) and printed circuit boards (PCBs). The main challenge is that connecting a large number of electronic components under specific design rules and constraints involves a very large search space, which is proved to be NP-complete. 
Early solutions are typically designed with hard-coded heuristics, which suffer from problems of non-optimum solutions and lack of flexibility for new design needs. Although a few learning-based methods have been proposed recently, their methods are cumbersome and hard to extend to large-scale applications. In this work, we propose a new algorithm for circuit routing, named as Ranking Cost (RC),  which innovatively combines search-based methods (i.e., A* algorithm) and learning-based methods (i.e., Evolution Strategies) to form an efficient and trainable router under a proper parameterization. Different from two-stage routing methods ( i.e., first global routing and then detailed routing), our method involves a one-stage procedure that directly optimizes the global objective function, thus it can be easy to adapt to new routing rules and constraints. In our method, we introduce a new set of variables called cost maps, which can help the A* router to find out proper paths to achieve the global object. We also train a ranking parameter, which can produce the ranking order and further improve the performance of our method. Our algorithm is trained in an end-to-end manner and does not use any artificial data or human demonstration. In the experiments, we compare our method with the sequential A* algorithm and a canonical reinforcement learning approach, and results show that our method outperforms these baselines with higher connectivity rates and better scalability. Our ablation study shows that our trained cost maps can capture the global information and guide the routing result to approach global optimum.",/pdf/ec8e118c51f87fa20391fc131e45c55b21167147.pdf,reinforcement learning;graph;optimization;llm,https://scholar.google.com/scholar?q=Ranking+Cost:+One-Stage+Circuit+Routing+by+Directly+Optimizing+Global+Objective+Function
Multilayer Dense Connections for Hierarchical Concept Classification,2021,ICLR,"['Toufiq Parag', 'Hongcheng Wang']",poster,"['Deep Learning: Applications', 'Methodology', 'and Theory', 'Recognition: Detection', 'Categorization', 'Retrieval and Matching', 'Scene Understanding', 'Visual Reasoning']","Classification is a pivotal function for many computer vision tasks such as image recognition, object  detection, scene segmentation. Multinomial logistic regression with a single final layer of dense connections has become the ubiquitous technique for CNN-based classification. While these classifiers project a mapping between the input and a set of output category classes, they do not typically yield  a comprehensive description of the category. In particular, when a CNN based image classifier correctly identifies the image of a Chimpanzee, its output does not clarify that Chimpanzee is a member of Primate, Mammal, Chordate families and a living thing. We propose a multilayer dense connectivity for a CNN to simultaneously predict the category \emph{and} its conceptual superclasses in hierarchical order. We experimentally demonstrate that our proposed dense connections, in conjunction with popular convolutional feature layers,  can learn to predict the  conceptual classes with minimal increase in network size while maintaining the categorical classification accuracy.",/pdf/cdac261532bbd0af551f99660fcc9bcccc659255.pdf,segmentation;llm,https://scholar.google.com/scholar?q=Multilayer+Dense+Connections+for+Hierarchical+Concept+Classification
Dynamic Graph: Learning Instance-aware Connectivity for Neural Networks,2021,ICLR,"['Kun Yuan', 'Quanquan Li', 'Dapeng Chen', 'Aojun Zhou', 'Junjie Yan']",poster,"['dynamic network', 'data-dependent', 'complete graph']","One practice of employing deep neural networks is to apply the same architecture to all the input instances. However, a fixed architecture may not be representative enough for data with high diversity. To promote the model capacity, existing approaches usually employ larger convolutional kernels or deeper network structure, which may increase the computational cost. In this paper, we address this issue by raising the Dynamic Graph Network (DG-Net). The network learns the instance-aware connectivity, which creates different forward paths for different instances. Specifically, the network is initialized as a complete directed acyclic graph, where the nodes represent convolutional blocks and the edges represent the connection paths. We generate edge weights by a learnable module \textit{router} and select the edges whose weights are larger than a threshold, to adjust the connectivity of the neural network structure. Instead of using the same path of the network, DG-Net aggregates features dynamically in each node, which allows the network to have more representation ability. To facilitate the training, we represent the network connectivity of each sample in an adjacency matrix. The matrix is updated to aggregate features in the forward pass, cached in the memory, and used for gradient computing in the backward pass.  We verify the effectiveness of our method with several static architectures, including MobileNetV2, ResNet, ResNeXt, and RegNet. Extensive experiments are performed on ImageNet classification and COCO object detection, which shows the effectiveness and generalization ability of our approach.",/pdf/51cf6bf6a8fb30f24c6838c226f2e3853d9c3076.pdf,graph;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Dynamic+Graph:+Learning+Instance-aware+Connectivity+for+Neural+Networks
BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction,2021,ICLR,"['Yuhang Li', 'Ruihao Gong', 'Xu Tan', 'Yang Yang', 'Peng Hu', 'Qi Zhang', 'Fengwei Yu', 'Wei Wang', 'Shi Gu']",poster,"['Post Training Quantization', 'Mixed Precision', 'Second-order analysis']","We study the challenging task of neural network quantization without end-to-end retraining, called Post-training Quantization (PTQ). PTQ usually requires a small subset of training data but produces less powerful quantized models than Quantization-Aware Training (QAT). In this work, we propose a novel PTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. BRECQ leverages the basic building blocks in neural networks and reconstructs them one-by-one. In a comprehensive theoretical study of the second-order error, we show that BRECQ achieves a good balance between cross-layer dependency and generalization error. To further employ the power of quantization, the mixed precision technique is incorporated in our framework by approximating the inter-layer and intra-layer sensitivity. Extensive experiments on various handcrafted and searched neural architectures are conducted for both image classification and object detection tasks. And for the first time we prove that, without bells and whistles, PTQ can attain 4-bit ResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster production of quantized models.  Codes are available at https://github.com/yhhhli/BRECQ.",/pdf/1d36f62d5b1b84de637026ab1c69e37fe5b20613.pdf,graph;llm,https://scholar.google.com/scholar?q=BRECQ:+Pushing+the+Limit+of+Post-Training+Quantization+by+Block+Reconstruction
Later Span Adaptation for Language Understanding,2021,ICLR,"['Rongzhou Bao', 'Zhuosheng Zhang', 'hai zhao']",poster,[],"Pre-trained contextualized language models (PrLMs) broadly use fine-grained tokens (words or sub-words) as minimal linguistic unit in pre-training phase. Introducing span-level information in pre-training has shown capable of further enhancing PrLMs. However, such methods require enormous resources and are lack of adaptivity due to huge computational requirement from pre-training. Instead of too early fixing the linguistic unit  input as nearly all previous work did, we propose a novel method that combines span-level information into the representations generated by PrLMs during fine-tuning phase for better flexibility. In this way, the modeling procedure of span-level texts can be more adaptive to different downstream tasks. In detail, we divide the sentence into several spans according to the segmentation generated by a pre-sampled dictionary. Based on the sub-token-level representation provided by PrLMs, we enhance the connection between the tokens in each span and gain a representation with enhanced span-level information. Experiments are conducted on GLUE benchmark and prove that our approach could remarkably enhance the performance of PrLMs in various natural language understanding tasks.",/pdf/f89bde3d3801f0f4a50f42589350c1e72c940710.pdf,representation;adaptive;segmentation;llm,https://scholar.google.com/scholar?q=Later+Span+Adaptation+for+Language+Understanding
Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting,2021,ICLR,"['Giorgos Bouritsas', 'Fabrizio Frasca', 'Stefanos Zafeiriou', 'Michael M. Bronstein']",poster,"['graph neural networks', 'graph representation learning', 'network analysis', 'network motifs', 'subgraph isomoprhism']","While Graph Neural Networks (GNNs) have achieved remarkable results in a variety of applications, recent studies exposed important shortcomings in their ability to capture the structure of the underlying graph. It has been shown that the expressive power of standard GNNs is bounded by the Weisfeiler-Lehman (WL) graph isomorphism test, from which they inherit proven limitations such as the inability to detect and count graph substructures. On the other hand, there is significant empirical evidence, e.g. in network science and bioinformatics, that substructures are often informative for downstream tasks, suggesting that it is desirable to design GNNs capable of leveraging this important source of information. To this end, we propose a novel topologically-aware message passing scheme based on substructure encoding. We show that our architecture allows incorporating domain-specific inductive biases and that it is strictly more expressive than the WL test. Importantly, in contrast to recent works on the expressivity of GNNs, we do not attempt to adhere to the WL hierarchy; this allows us to retain multiple attractive properties of standard GNNs such as locality and linear network complexity, while being able to disambiguate even hard instances of graph isomorphism. We extensively evaluate our method on graph classification and regression tasks and show state-of-the-art results on multiple datasets including molecular graphs and social networks.",/pdf/fb78801fd3f4d7624899fc211252a2b877b3315e.pdf,graph;zero_few-shot;active learning;llm,https://scholar.google.com/scholar?q=Improving+Graph+Neural+Network+Expressivity+via+Subgraph+Isomorphism+Counting
Topology-Aware Segmentation Using Discrete Morse Theory,2021,ICLR,"['Xiaoling Hu', 'Yusu Wang', 'Li Fuxin', 'Dimitris Samaras', 'Chao Chen']",poster,"['Topology', 'Morse theory', 'Image segmentation']","In the segmentation of fine-scale structures from natural and biomedical images, per-pixel accuracy is not the only metric of concern. Topological correctness, such as vessel connectivity and membrane closure, is crucial for downstream analysis tasks. In this paper, we propose a new approach to train deep image segmentation networks for better topological accuracy. In particular, leveraging the power of discrete Morse theory (DMT), we identify global structures, including 1D skeletons and 2D patches, which are important for topological accuracy. Trained with a novel loss based on these global structures, the network performance is significantly improved especially near topologically challenging locations (such as weak spots of connections and membranes). On diverse datasets, our method achieves superior performance on both the DICE score and topological metrics.",/pdf/e328a81d734d6f9eb2808b309985c8aeffb1b27b.pdf,graph;zero_few-shot;metric;segmentation;llm,https://scholar.google.com/scholar?q=Topology-Aware+Segmentation+Using+Discrete+Morse+Theory
Neural representation and generation for RNA secondary structures,2021,ICLR,"['Zichao Yan', 'William L. Hamilton', 'Mathieu Blanchette']",poster,"['Graph neural network', 'Deep generative modeling', 'Machine learning', 'Drug discovery', 'RNA structure', 'RNA structure embedding', 'RNA-protein interaction prediction']","Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules.",/pdf/bc9ac721f355bf79861be2019a3d66689b2b8a33.pdf,graph;optimization;representation;generative model;llm,https://scholar.google.com/scholar?q=Neural+representation+and+generation+for+RNA+secondary+structures
Temporal and Object Quantification Nets,2021,ICLR,"['Jiayuan Mao', 'Zhezheng Luo', 'Chuang Gan', 'Joshua B. Tenenbaum', 'Jiajun Wu', 'Leslie Pack Kaelbling', 'Tomer Ullman']",poster,"['Temporal Modeling', 'Object-Centric Representations']","We aim to learn generalizable representations for complex activities by quantifying over both entities and time, as in “the kicker is behind all the other players,” or “the player controls the ball until it moves toward the goal.”  Such a structural inductive bias of object relations, object quantification, and temporal orders will enable the learned representation to generalize to situations with varying numbers of agents, objects, and time courses. In this paper, we present Temporal and Object Quantification Nets (TOQ-Nets), which provide such structural inductive bias for learning composable action concepts from time sequences that describe the properties and relations of multiple entities. We evaluate TOQ-Nets on two benchmarks: trajectory-based soccer event detection, and 6D pose-based manipulation concept learning. We demonstrate that TOQ-Nets can generalize from small amounts of data to scenarios where there are more agents and objects than were present during training. The learned concepts are also robust with respect to temporally warped sequences and easily transfer to other prediction tasks in a similar domain.",/pdf/bad5b357e2f031c30768a32d2993467eb096a779.pdf,reinforcement learning;representation;transfer learning;llm,https://scholar.google.com/scholar?q=Temporal+and+Object+Quantification+Nets
Long Live the Lottery: The Existence of Winning Tickets in Lifelong Learning,2021,ICLR,"['Tianlong Chen', 'Zhenyu Zhang', 'Sijia Liu', 'Shiyu Chang', 'Zhangyang Wang']",poster,"['lottery tickets', 'winning tickets', 'lifelong learning']","The lottery ticket hypothesis states that a highly sparsified sub-network can be trained in isolation, given the appropriate weight initialization. This paper extends that hypothesis from one-shot task learning, and demonstrates for the first time that such extremely compact and independently trainable sub-networks can be also identified in the lifelong learning scenario, which we call lifelong tickets. We show that the resulting lifelong ticket can further be leveraged to improve the performance of learning over continual tasks. However, it is highly non-trivial to conduct network pruning in the lifelong setting. Two critical roadblocks arise: i) As many tasks now arrive sequentially, finding tickets in a greedy weight pruning fashion will inevitably suffer from the intrinsic bias, that the earlier emerging tasks impact more; ii) As lifelong learning is consistently challenged by catastrophic forgetting, the compact network capacity of tickets might amplify the risk of forgetting. In view of those, we introduce two pruning options, e.g., top-down and bottom-up, for finding lifelong tickets. Compared to the top-down pruning that extends vanilla (iterative) pruning over sequential tasks, we show that the bottom-up one, which can dynamically shrink and (re-)expand model capacity, effectively avoids the undesirable excessive pruning in the early stage. We additionally introduce lottery teaching that further overcomes forgetting via knowledge distillation aided by external unlabeled data. Unifying those ingredients, we demonstrate the existence of very competitive lifelong tickets, e.g., achieving 3-8% of the dense model size with even higher accuracy, compared to strong class-incremental learning baselines on CIFAR-10/CIFAR-100/Tiny-ImageNet datasets. Codes available at https://github.com/VITA-Group/Lifelong-Learning-LTH.",/pdf/b8a21c081c59f4808478ea25d46d2743c4d51298.pdf,graph;distillation;llm,https://scholar.google.com/scholar?q=Long+Live+the+Lottery:+The+Existence+of+Winning+Tickets+in+Lifelong+Learning
Learning A Minimax Optimizer: A Pilot Study,2021,ICLR,"['Jiayi Shen', 'Xiaohan Chen', 'Howard Heaton', 'Tianlong Chen', 'Jialin Liu', 'Wotao Yin', 'Zhangyang Wang']",poster,"['Learning to Optimize', 'Minimax Optimization']","Solving continuous minimax optimization is of extensive practical interest, yet notoriously unstable and difficult. This paper introduces the learning to optimize(L2O) methodology to the minimax problems for the first time and addresses its accompanying unique challenges. We first present Twin-L2O, the first dedicated minimax L2O method consisting of two LSTMs for updating min and max variables separately. The decoupled design is found to facilitate learning, particularly when the min and max variables are highly asymmetric. Empirical experiments on a variety of minimax problems corroborate the effectiveness of Twin-L2O. We then discuss a crucial concern of Twin-L2O, i.e., its inevitably limited generalizability to unseen optimizees. To address this issue, we present two complementary strategies. Our first solution, Enhanced Twin-L2O, is empirically applicable for general minimax problems, by improving L2O training via leveraging curriculum learning. Our second alternative, called Safeguarded Twin-L2O, is a preliminary theoretical exploration stating that under some strong assumptions, it is possible to theoretically establish the convergence of Twin-L2O. We benchmark our algorithms on several testbed problems and compare against state-of-the-art minimax solvers. The code is available at:  https://github.com/VITA-Group/L2O-Minimax.",/pdf/d13003c8f7a704dcb012e4a100a51df5332b5286.pdf,graph;optimization;metric;curriculum learning;llm,https://scholar.google.com/scholar?q=Learning+A+Minimax+Optimizer:+A+Pilot+Study
"Poisoned classifiers are not only backdoored, they are fundamentally broken",2021,ICLR,"['Mingjie Sun', 'Siddhant Agarwal', 'J Zico Kolter']",poster,"['Backdoor Attacks', 'Denoised Smoothing', 'Perceptually-Aligned Gradients']","Under a commonly-studied “backdoor” poisoning attack against classification models, an attacker adds a small “trigger” to a subset of the training data, such that the presence of this trigger at test time causes the classifier to always predict some target class. It is often implicitly assumed that the poisoned classifier is vulnerable exclusively to the adversary who possesses the trigger. In this paper, we show empirically that this view of backdoored classifiers is fundamentally incorrect. We demonstrate that anyone with access to the classifier, even without access to any original training data or trigger, can construct several alternative triggers that are as effective or more so at eliciting the target class at test time. We construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a recent process called Denoised Smoothing, and then extracting colors or cropped portions of adversarial images. We demonstrate the effectiveness of our attack through extensive experiments on ImageNet and TrojAI datasets, including a user study which demonstrates that our method allows users to easily determine the existence of such backdoors in existing poisoned classifiers. Furthermore, we demonstrate that our alternative triggers can in fact look entirely different from the original trigger, highlighting that the backdoor actually learned by the classifier differs substantially from the trigger image itself. Thus, we argue that there is no such thing as a “secret” backdoor in poisoned classifiers: poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to the classifier.",/pdf/4959459ccc8a6c2d401fe6ca978ce4b82b4f3ff0.pdf,graph;zero_few-shot;llm,"https://scholar.google.com/scholar?q=Poisoned+classifiers+are+not+only+backdoored,+they+are+fundamentally+broken"
Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification,2021,ICLR,"['Yingxue Zhou', 'Steven Wu', 'Arindam Banerjee']",poster,[],"Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension $p$, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where $p \gg n$, the number of training samples. Existing lower bounds on private ERM show that such dependence on $p$ is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in $p$. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\epsilon$).

",/pdf/72b062d9241693b3927766cc1cea70d6cd216bd2.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Bypassing+the+Ambient+Dimension:+Private+SGD+with+Gradient+Subspace+Identification
Revisiting the Stability of Stochastic Gradient Descent: A Tightness Analysis,2021,ICLR,"['Yikai Zhang', 'Samuel Bald', 'wenjia Zhang', 'Vamsi Pritham Pingali', 'Chao Chen', 'Mayank Goswami']",poster,"['SGD', 'Stability', 'Generalization', 'Deep Learning']","The technique of algorithmic stability has been used to capture the generalization power of several learning models, especially those trained with stochastic gradient descent (SGD). This paper investigates the tightness of the algorithmic stability bounds for SGD given by~\cite{hardt2016train}. We show that the analysis of~\cite{hardt2016train} is tight for convex objective functions, but loose for non-convex objective functions. In the non-convex case we provide a tighter upper bound on the stability (and hence generalization error), and provide evidence that it is asymptotically tight up to a constant factor. 
However, deep neural networks trained with SGD exhibit much better stability and generalization in practice than what is suggested by these (tight) bounds,
namely, linear or exponential degradation with time for SGD with constant step size. We aim towards characterizing deep learning loss functions with good generalization guarantees, despite being trained using SGD with constant step size.
 We propose the Hessian Contractive (HC) condition, which specifies the contractivity of regions containing local minima in the neural network loss landscape. We provide empirical evidence that this condition holds for several loss functions, and provide theoretical evidence that the known tight SGD stability bounds for convex and non-convex loss functions can be circumvented by HC loss functions, thus partially explaining the generalization of deep neural networks. ",/pdf/ad5b575024f693627446124f7fec75daa743b18d.pdf,active learning;llm,https://scholar.google.com/scholar?q=Revisiting+the+Stability+of+Stochastic+Gradient+Descent:+A+Tightness+Analysis
UMEC: Unified model and embedding compression for efficient recommendation systems,2021,ICLR,"['Jiayi Shen', 'Haotao Wang', 'Shupeng Gui', 'Jianchao Tan', 'Zhangyang Wang', 'Ji Liu']",poster,"['recommendation system', 'model compression', 'ADMM', 'resource constrained']","The recommendation system (RS) plays an important role in the content recommendation and retrieval scenarios. The core part of the system is the Ranking neural network, which is usually a bottleneck of whole system performance during online inference.  In this work, we propose a unified model and embedding compression (UMEC) framework to hammer an efficient neural network-based recommendation system.  Our framework jointly learns input feature selection and neural network compression together, and solve them as an end-to-end resource-constrained optimization problem using ADMM.  Our method outperforms other baselines in terms of neural network Flops, sparse embedding feature size and the number of sparse embedding features.  We evaluate our method on the public benchmark of DLRM, trained over the Kaggle Criteo dataset. The codes can be found at https://github.com/VITA-Group/UMEC.",/pdf/4b861c09a3f34663e9ee26c8e3b407a0d5f009f4.pdf,optimization;online learning;inference;sparse;llm,https://scholar.google.com/scholar?q=UMEC:+Unified+model+and+embedding+compression+for+efficient+recommendation+systems
Evaluations and Methods for Explanation through Robustness Analysis,2021,ICLR,"['Cheng-Yu Hsieh', 'Chih-Kuan Yeh', 'Xuanqing Liu', 'Pradeep Kumar Ravikumar', 'Seungyeon Kim', 'Sanjiv Kumar', 'Cho-Jui Hsieh']",poster,"['Interpretability', 'Explanations', 'Adversarial Robustness']","Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel set of evaluation criteria for such feature based explanations by robustness analysis. In contrast to existing evaluations which require us to specify some way to ""remove"" features that could inevitably introduces biases and artifacts, we make use of the subtler notion of smaller adversarial perturbations. By optimizing towards our proposed evaluation criteria, we obtain new explanations that are loosely necessary and sufficient for a prediction. We further extend the explanation to extract the set of features that would move the current prediction to a target class by adopting targeted adversarial attack for the robustness analysis. Through experiments across multiple domains and a user study, we validate the usefulness of our evaluation criteria and our derived explanations.",/pdf/d6f38258eb28ef886c3aee4efb08046abbe76e5a.pdf,llm,https://scholar.google.com/scholar?q=Evaluations+and+Methods+for+Explanation+through+Robustness+Analysis
Go with the flow: Adaptive control for Neural ODEs,2021,ICLR,"['Mathieu Chalvidal', 'Matthew Ricci', 'Rufin VanRullen', 'Thomas Serre']",poster,"['Neural ODEs', 'Optimal Control Theory', 'Hypernetworks', 'Normalizing flows']","Despite their elegant formulation and lightweight memory cost, neural ordinary differential equations (NODEs) suffer from known representational limitations. In particular, the single flow learned by NODEs cannot express all homeomorphisms from a given data space to itself, and their static weight parameterization restricts the type of functions they can learn compared to discrete architectures with layer-dependent weights. Here, we describe a new module called neurally-controlled ODE (N-CODE) designed to improve the expressivity of NODEs. The parameters of N-CODE modules are dynamic variables governed by a trainable map from initial or current activation state, resulting in forms of open-loop and closed-loop control, respectively. A single module is sufficient for learning a distribution on non-autonomous flows that adaptively drive neural representations. We provide theoretical and empirical evidence that N-CODE circumvents limitations of previous NODEs models and show how increased model expressivity manifests in several supervised and unsupervised learning problems. These favorable empirical results indicate the potential of using data- and activity-dependent plasticity in neural networks across numerous domains.",/pdf/8dc540aca5360588117ac82153292b6a2d2f4897.pdf,zero_few-shot;representation;adaptive;flow;llm,https://scholar.google.com/scholar?q=Go+with+the+flow:+Adaptive+control+for+Neural+ODEs
An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,2021,ICLR,"['Alexey Dosovitskiy', 'Lucas Beyer', 'Alexander Kolesnikov', 'Dirk Weissenborn', 'Xiaohua Zhai', 'Thomas Unterthiner', 'Mostafa Dehghani', 'Matthias Minderer', 'Georg Heigold', 'Sylvain Gelly', 'Jakob Uszkoreit', 'Neil Houlsby']",poster,"['computer vision', 'image recognition', 'self-attention', 'transformer', 'large-scale training']","While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",/pdf/a4aa24aed16fcb6f23d1067f1a5ecf47d7115f63.pdf,transformer;transfer learning;llm,https://scholar.google.com/scholar?q=An+Image+is+Worth+16x16+Words:+Transformers+for+Image+Recognition+at+Scale
BREEDS: Benchmarks for Subpopulation Shift,2021,ICLR,"['Shibani Santurkar', 'Dimitris Tsipras', 'Aleksander Madry']",poster,"['benchmarks', 'distribution shift', 'hierarchy', 'robustness']","We develop a methodology for assessing the robustness of models to subpopulation shift---specifically, their ability to generalize to novel data subpopulations that were not observed during training. Our approach leverages the class structure underlying existing datasets to control the data subpopulations that comprise the training and test distributions. This enables us to synthesize realistic distribution shifts whose sources can be precisely controlled and characterized, within existing
large-scale datasets. Applying this methodology to the ImageNet dataset, we create a suite of subpopulation shift benchmarks of varying granularity. We then validate that the corresponding shifts are tractable by obtaining human baselines. Finally, we utilize these benchmarks to measure the sensitivity of standard model architectures as well as the effectiveness of existing train-time robustness interventions. ",/pdf/267e1b0387f6edaaa3b1145def1009b6803d55b6.pdf,llm,https://scholar.google.com/scholar?q=BREEDS:+Benchmarks+for+Subpopulation+Shift
Quantifying Task Complexity Through Generalized Information Measures,2021,ICLR,"['Aditya Chattopadhyay', 'Benjamin David Haeffele', 'Donald Geman', 'Rene Vidal']",poster,"['Task Complexity', 'Information Pursuit', 'Deep Generative Models', 'Information Theory', 'Variational Autoencoders', 'Normalizing Flows']","How can we measure the “complexity” of a learning task so that we can compare one task to another? From classical information theory, we know that entropy is a useful measure of the complexity of a random variable and provides a lower bound on the minimum expected number of bits needed for transmitting its state. In this paper, we propose to measure the complexity of a learning task by the minimum expected number of questions that need to be answered to solve the task. For example, the minimum expected number of patches that need to be observed to classify FashionMNIST images. We prove several properties of the proposed complexity measure, including connections with classical entropy and sub-additivity for multiple tasks. As the computation of the minimum expected number of questions is generally intractable, we propose a greedy procedure called “information pursuit” (IP), which selects one question at a time depending on previous questions and their answers. This requires learning a probabilistic generative model relating data and questions to the task, for which we employ variational autoencoders and normalizing flows. We illustrate the usefulness of the proposed measure on various binary image classification tasks using image patches as the query set. Our results indicate that the complexity of a classification task increases as signal-to-noise ratio decreases, and that classification of the KMNIST dataset is more complex than classification of the FashionMNIST dataset. As a byproduct of choosing patches as queries, our approach also provides a principled way of determining which pixels in an image are most informative for a task.",/pdf/6ee161ae6fb4d28aa181f4fbeb4c5b2037962480.pdf,zero_few-shot;vae;generative model;flow;llm,https://scholar.google.com/scholar?q=Quantifying+Task+Complexity+Through+Generalized+Information+Measures
On the Impossibility of Global Convergence in Multi-Loss Optimization,2021,ICLR,['Alistair Letcher'],poster,"['impossibility', 'global', 'convergence', 'optimization', 'multi-loss', 'multi-player', 'multi-agent', 'gradient', 'descent']","Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.",/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf,reinforcement learning;optimization;multi-agent;llm,https://scholar.google.com/scholar?q=On+the+Impossibility+of+Global+Convergence+in+Multi-Loss+Optimization
Activation Relaxation: A Local Dynamical Approximation to Backpropagation in the Brain,2021,ICLR,"['Beren Millidge', 'Alexander Tschantz', 'Anil K Seth', 'Christopher Buckley']",poster,"['Neural Networks', 'Biological Plausibility', 'Backprop']","The backpropagation of error algorithm (backprop) has been instrumental in the recent success of deep learning. However, a key question remains as to whether backprop can be formulated in a manner suitable for implementation in neural circuitry. The primary challenge is to ensure that any candidate formulation uses only local information, rather than relying on global signals as in standard backprop. Recently several algorithms for approximating backprop using only local signals have been proposed. However, these algorithms typically impose other requirements which challenge biological plausibility: for example, requiring complex and precise connectivity schemes, or multiple sequential backwards phases with information being stored across phases. Here, we propose a novel algorithm, Activation Relaxation (AR), which is motivated by constructing the backpropagation gradient as the equilibrium point of a dynamical system. Our algorithm converges rapidly and robustly to the correct backpropagation gradients, requires only a single type of computational unit, utilises only a single parallel backwards relaxation phase, and can operate on arbitrary computation graphs. We illustrate these properties by training deep neural networks on visual classification tasks, and describe simplifications to the algorithm which remove further obstacles to neurobiological implementation (for example, the weight-transport problem, and the use of nonlinear derivatives), while preserving performance.",/pdf/51f4c78ee76485a103b587738dd8fcd70394d2ad.pdf,graph;online learning;llm,https://scholar.google.com/scholar?q=Activation+Relaxation:+A+Local+Dynamical+Approximation+to+Backpropagation+in+the+Brain
Adversarial Feature Desensitization,2021,ICLR,"['Pouya Bashivan', 'Mojtaba Faramarzi', 'Touraj Laleh', 'Blake Aaron Richards', 'Irina Rish']",poster,"['adversarial robustness', 'adversarial learning', 'convolutional neural networks']","Deep neural networks can now perform many tasks that were once thought to be only feasible for humans. While reaching impressive performance under standard settings, such networks are known to be  susceptible to  adversarial attacks -- slight but carefully constructed perturbations of the inputs which drastically decrease the network performance. Here we propose a new way to improve the network robustness against adversarial attacks by   focusing on robust representation learning based on  adversarial training procedure, called here Adversarial Feature Desensitization (AFD). AFD desensitizes the representation via an adversarial game between the embedding network and an adversarial discriminator introduced on top of  the standard predictive model, which is trained to distinguish between the clean and perturbed inputs from their high-level representations. Our method substantially improves the state-of-the-art in robust classification on MNIST, CIFAR10, and CIFAR100 datasets. More importantly, we demonstrate that AFD has better generalization ability than previous methods, as the learned features maintain their robustness across a wide range of perturbations, including perturbations not seen during training. These results indicate that reducing feature sensitivity is a promising approach for ameliorating the problem of adversarial attacks in deep neural networks. ",/pdf/68f5bdf6d6094cbd758caf917e5496ce3091dfa9.pdf,representation;llm,https://scholar.google.com/scholar?q=Adversarial+Feature+Desensitization
CDT: Cascading Decision Trees for Explainable Reinforcement Learning,2021,ICLR,"['Zihan Ding', 'Pablo Hernandez-Leal', 'Gavin Weiguang Ding', 'Changjian Li', 'Ruitong Huang']",poster,"['Explainable Reinforcement Learning', 'Decision Tree', 'Matrix Factorization']","Deep Reinforcement Learning (DRL) has recently achieved significant advances in various domains. However, explaining the policy of RL agents still remains an open problem due to several factors, one being the complexity of explaining neural networks decisions. Recently, a group of works have used decision-tree-based models to learn explainable policies. Soft decision trees (SDTs) and discretized differentiable decision trees (DDTs) have been demonstrated to achieve both good performance and share the benefit of having explainable policies. In this work, we further improve the results for tree-based explainable RL in both performance and explainability. Our proposal, Cascading Decision Trees (CDTs) apply representation learning on the decision path to allow richer expressivity. Empirical results show that in both situations, where CDTs are used as policy function approximators or as imitation learners to explain black-box policies, CDTs can achieve better performances with more succinct and explainable models than SDTs. As a second contribution our study reveals limitations of explaining black-box policies via imitation learning with tree-based explainable models, due to its inherent instability.",/pdf/f1c897e98d2a45395cacf2fef8cef6e221822bd2.pdf,reinforcement learning;zero_few-shot;representation;imitation learning;decision trees;llm,https://scholar.google.com/scholar?q=CDT:+Cascading+Decision+Trees+for+Explainable+Reinforcement+Learning
Progressive Skeletonization: Trimming more fat from a network at initialization,2021,ICLR,"['Pau de Jorge', 'Amartya Sanyal', 'Harkirat Behl', 'Philip Torr', 'Grégory Rogez', 'Puneet K. Dokania']",poster,"['Pruning', 'Pruning at initialization', 'Sparsity']","Recent studies have shown that skeletonization (pruning parameters) of networks at initialization provides all the practical benefits of sparsity both at inference and training time, while only marginally degrading their performance. However, we observe that beyond a certain level of sparsity (approx 95%), these approaches fail to preserve the network performance, and to our surprise, in many cases perform even worse than trivial random pruning. To this end, we propose an objective to find a skeletonized network with maximum foresight connection sensitivity (FORCE) whereby the trainability, in terms of connection sensitivity, of a pruned network is taken into consideration. We then propose two approximate procedures to maximize our objective (1) Iterative SNIP: allows parameters that were unimportant at earlier stages of skeletonization to become important at later stages; and (2) FORCE: iterative process that allows exploration by allowing already pruned parameters to resurrect at later stages of skeletonization. Empirical analysis on a large suite of experiments show that our approach, while providing at least as good performance as other recent approaches on moderate pruning levels, provide remarkably improved performance on high pruning levels (could remove up to 99.5% parameters while keeping the networks trainable).",/pdf/c774cc8cddefe7ca854f3b051602f706c05b7239.pdf,graph;zero_few-shot;inference;llm,https://scholar.google.com/scholar?q=Progressive+Skeletonization:+Trimming+more+fat+from+a+network+at+initialization
A Simple Unified Information Regularization Framework for Multi-Source Domain Adaptation,2021,ICLR,"['Geon Yeong Park', 'Sang wan Lee']",poster,"['Multi-source Domain Adaptation', 'Transfer learning', 'Adversarial learning', 'Information theory']","Adversarial learning strategy has demonstrated remarkable performance in dealing with single-source unsupervised Domain Adaptation (DA) problems, and it has recently been applied to multi-source DA problems. While most of the existing DA methods use multiple domain discriminators, the effect of using multiple discriminators on the quality of latent space representations has been poorly understood. Here we provide theoretical insights into potential pitfalls of using multiple domain discriminators: First, domain-discriminative information is inevitably distributed across multiple discriminators. Second, it is not scalable in terms of computational resources. Third, the variance of stochastic gradients from multiple discriminators may increase, which significantly undermines training stability. To fully address these issues, we situate adversarial DA in the context of information regularization. First, we present a unified information regularization framework for multi-source DA. It provides a theoretical justification for using a single and unified domain discriminator to encourage the synergistic integration of the information gleaned from each domain. Second, this motivates us to implement a novel neural architecture called a Multi-source Information-regularized Adaptation Networks (MIAN). The proposed model significantly reduces the variance of stochastic gradients and increases computational-efficiency. Large-scale simulations on various multi-source DA scenarios demonstrate that MIAN, despite its structural simplicity, reliably outperforms other state-of-the-art methods by a large margin especially for difficult target domains.",/pdf/a47b09841c573507304b83c4ab92655ae8a6d64f.pdf,graph;representation;llm,https://scholar.google.com/scholar?q=A+Simple+Unified+Information+Regularization+Framework+for+Multi-Source+Domain+Adaptation
Learnable Embedding sizes for Recommender Systems,2021,ICLR,"['Siyi Liu', 'Chen Gao', 'Yihong Chen', 'Depeng Jin', 'Yong Li']",poster,"['Recommender Systems', 'Deep Learning', 'Embedding Size']","The embedding-based representation learning is commonly used in deep learning recommendation models to map the raw sparse features to dense vectors. The traditional embedding manner that assigns a uniform size to all features has two issues. First, the numerous features inevitably lead to a gigantic embedding table that causes a high memory usage cost. Second, it is likely to cause the over-fitting problem for those features that do not require too large representation capacity. Existing works that try to address the problem always cause a significant drop in recommendation performance or suffers from the limitation of unaffordable training time cost. In this paper, we proposed a novel approach, named PEP (short for Plug-in Embedding Pruning), to reduce the size of the embedding table while avoiding the drop of recommendation accuracy. PEP prunes embedding parameter where the pruning threshold(s) can be adaptively learned from data. Therefore we can automatically obtain a mixed-dimension embedding-scheme by pruning redundant parameters for each feature. PEP is a general framework that can plug in various base recommendation models. Extensive experiments demonstrate it can efficiently cut down embedding parameters and boost the base model's performance. Specifically, it achieves strong recommendation performance while reducing 97-99% parameters. As for the computation cost, PEP only brings an additional 20-30% time cost compare with base models. ",/pdf/57ca06dcb49d0ba8a6c705fcd2a34d58a7cf8c3a.pdf,representation;adaptive;sparse;llm,https://scholar.google.com/scholar?q=Learnable+Embedding+sizes+for+Recommender+Systems
Globally Injective ReLU networks,2021,ICLR,"['Michael Puthawala', 'Konik Kothari', 'Matti Lassas', 'Ivan Dokmanić', 'Maarten V. de Hoop']",poster,"['Generative models', 'injectivity of neural networks', 'universal approximation', 'inference', 'compressed sensing with generative priors', 'well-posedness', 'random projections']","Injectivity plays an important role in generative models where it enables inference; in inverse problems and compressed sensing with generative priors it is a precursor to well posedness. We establish sharp characterizations of injectivity of fully-connected and convolutional ReLU layers and networks. First, through a layerwise analysis, we show that an expansivity factor of two is necessary and sufficient for injectivity by constructing appropriate weight matrices. We show that global injectivity with iid Gaussian matrices, a commonly used tractable model, requires larger expansivity between 3.4 and 10.5. We also characterize the stability of inverting an injective network via worst-case Lipschitz constants of the inverse. We then use arguments from differential topology to study injectivity of deep networks and prove that any Lipschitz map can be approximated by an injective ReLU network. Finally, using an argument based on random projections, we show that an end-to-end---rather than layerwise---doubling of the dimension suffices for injectivity. Our results establish a theoretical basis for the study of nonlinear inverse and inference problems using neural networks.",/pdf/a6621d2ce49556d9b4cbed5b11293fb7dba85929.pdf,generative model;online learning;inference;llm,https://scholar.google.com/scholar?q=Globally+Injective+ReLU+networks
Causal Screening to Interpret Graph Neural Networks,2021,ICLR,"['Xiang Wang', 'Yingxin Wu', 'An Zhang', 'Xiangnan He', 'Tat-seng Chua']",poster,"['Feature Attribution', 'Graph Neural Networks', 'Explainable Methods', 'Causal Effect']","With the growing success of graph neural networks (GNNs), the explainability of GNN is attracting considerable attention. However, current works on feature attribution, which frame explanation generation as attributing a prediction to the graph features, mostly focus on the statistical interpretability. They may struggle to distinguish causal and noncausal effects of features, and quantify redundancy among features, thus resulting in unsatisfactory explanations. In this work, we focus on the causal interpretability in GNNs and propose a method, Causal Screening, from the perspective of cause-effect. It incrementally selects a graph feature (i.e., edge) with large causal attribution, which is formulated as the individual causal effect on the model outcome. As a model-agnostic tool, Causal Screening can be used to generate faithful and concise explanations for any GNN model. Further, by conducting extensive experiments on three graph classiﬁcation datasets, we observe that Causal Screening achieves signiﬁcant improvements over state-of-the-art approaches w.r.t. two quantitative metrics: predictive accuracy, contrastivity, and safely passes sanity checks.",/pdf/6cbc051b2a41c91c6991ee72c22b3b26b40304ba.pdf,graph;transformer;generative model;metric;llm,https://scholar.google.com/scholar?q=Causal+Screening+to+Interpret+Graph+Neural+Networks
All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks,2021,ICLR,"['Juinn-Dar Huang', 'Cheng-Wei Huang', 'Tim-Wei Chen']",poster,"['8-bit floating-point format', 'accuracy loss minimization', 'numerics', 'memory-efficient inference', 'deep learning']","Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors – the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit – are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation – both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\%\sim 0.3\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.",/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf,zero_few-shot;inference;llm,https://scholar.google.com/scholar?q=All-You-Can-Fit+8-Bit+Flexible+Floating-Point+Format+for+Accurate+and+Memory-Efficient+Inference+of+Deep+Neural+Networks
Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning,2021,ICLR,"['Sai Praneeth Karimireddy', 'Martin Jaggi', 'Satyen Kale', 'Mehryar Mohri', 'Sashank J. Reddi', 'Sebastian U Stich', 'Ananda Theertha Suresh']",poster,"['Federated learning', 'Federated optimization', 'Adaptive optimization', 'Adam', 'Variance Reduction', 'Distributed optimization', 'Decentralized optimization']","Federated learning is a challenging optimization problem due to the heterogeneity of the data across different clients. Such heterogeneity has been observed to induce \emph{client drift} and significantly degrade the performance of algorithms designed for this setting.
In contrast, centralized learning with centrally collected data does not experience such drift, and has seen great empirical and theoretical progress with innovations such as momentum, adaptivity, etc.
In this work, we propose a general framework {\sc Mime} which mitigates client-drift and adapts arbitrary centralized optimization algorithms (e.g. SGD, Adam, etc.) to federated learning.
{\sc Mime} uses a combination of \emph{control-variates} and \emph{server-level statistics} (e.g. momentum) at every client-update step to ensure that each local update mimics that of the centralized method. Our thorough theoretical and empirical analyses strongly establish \mime's superiority over other baselines.",/pdf/a29051e277c7f3fcf9c8bd6ed1f643dbf6bc5632.pdf,graph;optimization;federated learning;llm,https://scholar.google.com/scholar?q=Mime:+Mimicking+Centralized+Stochastic+Algorithms+in+Federated+Learning
Adapting to Reward Progressivity via Spectral Reinforcement Learning,2021,ICLR,"['Michael Dann', 'John Thangarajah']",poster,"['Reinforcement Learning', 'Deep Reinforcement Learning']","In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.",/pdf/a3879d188d633661f6593af48f3c561aefbfb6ff.pdf,reinforcement learning;zero_few-shot;llm,https://scholar.google.com/scholar?q=Adapting+to+Reward+Progressivity+via+Spectral+Reinforcement+Learning
Learned ISTA with Error-based Thresholding for Adaptive Sparse Coding,2021,ICLR,"['Li Ziang', 'Wu Kailun', 'Yiwen Guo', 'Changshui Zhang']",poster,"['Sparse coding', 'Learned ISTA', 'Convergence Analysis']","The learned iterative shrinkage thresholding algorithm (LISTA) introduces deep unfolding models with learnable thresholds in the shrinkage function for sparse coding. Drawing on some theoretical insights, we advocate an error-based thresholding (EBT) mechanism for LISTA, which leverages a function of the layer-wise reconstruction error to suggest an appropriate threshold value for each observation on each layer. We show that the EBT mechanism well-disentangles the learnable parameters in the shrinkage functions from the reconstruction errors, making them more adaptive to the various observations. With rigorous theoretical analyses, we show that the proposed EBT can lead to faster convergence on the basis of LISTA and its variants, in addition to its higher adaptivity. Extensive experimental results confirm our theoretical analyses and verify the effectiveness of our methods.",/pdf/af0eb62f571d0d053e5876db6e8c2a4e485d7b41.pdf,adaptive;sparse;llm,https://scholar.google.com/scholar?q=Learned+ISTA+with+Error-based+Thresholding+for+Adaptive+Sparse+Coding
Natural World Distribution via Adaptive Confusion Energy Regularization,2021,ICLR,"['Yen-Chi Hsu', 'Cheng-Yao Hong', 'Wan-Cyuan Fan', 'Ding-Jie Chen', 'Ming-Sui Lee', 'davi geiger', 'Tyng-Luh Liu']",poster,"['Fine-Grained Visual Classification', 'long-tailed distribution', 'confusion energy']","We introduce a novel and adaptive batch-wise regularization based on the proposed Batch Confusion Norm (BCN) to flexibly address the natural world distribution which usually involves fine-grained and long-tailed properties at the same time. The Fine-Grained Visual Classification (FGVC) problem is notably characterized by two intriguing properties, significant inter-class similarity and intra-class variations, which cause learning an effective FGVC classifier a challenging task. Existing techniques attempt to capture the discriminative parts by their modified attention mechanism. The long-tailed distribution of visual classification poses a great challenge for handling the class imbalance problem. Most of existing solutions usually focus on the class-balancing strategies, classifier normalization, or alleviating the negative gradient of tailed categories. Depart from the conventional approaches, we propose to tackle both problems simultaneously with the adaptive confusion concept. When inter-class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details. On the other hand, when inter-class similarity is not an issue, the class predictions from different samples would unavoidably yield a substantial BCN loss, and prompt the network learning to further reduce the cross-entropy loss. More importantly, extending the existing confusion energy-based framework to account for long-tailed scenario, BCN can learn to exert proper distribution of confusion strength over tailed and head categories to improve classification performance. While the resulting FGVC model by the BCN technique is effective, the performance can be consistently boosted by incorporating extra attention mechanism. In our experiments, we have obtained state-of-the-art results on several benchmark FGVC datasets, and also demonstrated that our approach is competitive on the popular natural world distribution dataset, iNaturalist2018. ",/pdf/584bf002b19b20eb8ae94f1f724a508a910c02d9.pdf,graph;transformer;adaptive;llm,https://scholar.google.com/scholar?q=Natural+World+Distribution+via+Adaptive+Confusion+Energy+Regularization
Pareto Adversarial Robustness: Balancing Spatial Robustness and Sensitivity-based Robustness,2021,ICLR,"['Ke Sun', 'Mingjie Li', 'Zhouchen Lin']",poster,[],"Adversarial robustness, mainly including sensitivity-based robustness and spatial robustness, plays an integral part in the robust generalization. In this paper, we endeavor to design strategies to achieve comprehensive adversarial robustness. To hit this target, firstly we investigate the less-studied spatial robustness and then integrate existing spatial robustness methods by incorporating both local and global spatial vulnerability into one spatial attack design. Based on this exploration, we further present a comprehensive relationship between natural accuracy, sensitivity-based and different spatial robustness, supported by the strong evidence from the perspective of representation. More importantly, in order to balance these mutual impact within different robustness into one unified framework, we incorporate the Pareto criterion into the adversarial robustness analysis, yielding a novel strategy towards comprehensive robustness called \textit{Pareto Adversarial Training}. The resulting Pareto front, the set of optimal solutions, provides the set of optimal balance among natural accuracy and different adversarial robustness, shedding light on solutions towards comprehensive robustness in the future. To the best of our knowledge, we are the first to consider comprehensive robustness via the multi-objective optimization.",/pdf/19dad14b16e7d53143022400d01cf129ba4f9c25.pdf,optimization;representation;llm,https://scholar.google.com/scholar?q=Pareto+Adversarial+Robustness:+Balancing+Spatial+Robustness+and+Sensitivity-based+Robustness
A Design Space Study for LISTA and Beyond,2021,ICLR,"['Tianjian Meng', 'Xiaohan Chen', 'Yifan Jiang', 'Zhangyang Wang']",poster,[],"In recent years, great success has been witnessed in building problem-specific deep networks from unrolling iterative algorithms, for solving inverse problems and beyond. Unrolling is believed to incorporate the model-based prior with the learning capacity of deep learning. This paper revisits \textit{the role of unrolling as a design approach for deep networks}: to what extent its resulting special architecture is superior, and can we find better? Using LISTA for sparse recovery as a representative example, we conduct the first thorough \textit{design space study} for the unrolled models.  Among all possible variations, we focus on extensively varying the connectivity patterns and neuron types, leading to a gigantic design space arising from LISTA. To efficiently explore this space and identify top performers, we leverage the emerging tool of neural architecture search (NAS). We carefully examine the searched top architectures in a number of settings, and are able to discover networks that consistently better than LISTA. We further present more visualization and analysis to ``open the black box"", and find that the searched top architectures demonstrate highly consistent and potentially transferable patterns. We hope our study to spark more reflections and explorations on how to better mingle model-based optimization prior and data-driven learning.",/pdf/f6321fc76124be68a636149a64dea811815c73a0.pdf,graph;optimization;sparse;transfer learning;llm,https://scholar.google.com/scholar?q=A+Design+Space+Study+for+LISTA+and+Beyond
Invertible Manifold Learning for Dimension Reduction,2021,ICLR,"['Siyuan Li', 'Haitao Lin', 'Zelin Zang', 'Lirong Wu', 'Jun Xia', 'Stan Z. Li']",poster,"['Manifold Learning', 'Inverse Model', 'Representation Learing']","It is widely believed that a dimension reduction (DR) process drops information inevitably in most practical scenarios. Thus, most methods try to preserve some essential information of data after DR, as well as manifold based DR methods. However, they usually fail to yield satisfying results, especially in high-dimensional cases. In the context of manifold learning, we think that a good low-dimensional representation should preserve the topological and geometric properties of data manifolds, which involve exactly the entire information of the data manifolds. In this paper, we define the problem of information-lossless NLDR with the manifold assumption and propose a novel two-stage NLDR method, called invertible manifold learning ($\textit{inv-ML}$), to tackle this problem. A $\textit{local isometry}$ constraint of preserving local geometry is applied under this assumption in $\textit{inv-ML}$. Firstly, a homeomorphic $\textit{sparse coordinate transformation}$ is learned to find the low-dimensional representation without losing topological information. Secondly, a $\textit{linear compression}$ is performed on the learned sparse coding, with the trade-off between the target dimension and the incurred information loss. Experiments are conducted on seven datasets with a neural network implementation of $\textit{inv-ML}$, called $\textit{i-ML-Enc}$, which demonstrate that the proposed $\textit{inv-ML}$ not only achieves invertible NLDR in comparison with typical existing methods but also reveals the characteristics of the learned manifolds through linear interpolation in latent space. Moreover, we find that the reliability of tangent space approximated by the local neighborhood on real-world datasets is key to the success of manifold based DR algorithms. The code will be made available soon.",/pdf/6648b37e4eed77b354b71e28bff8208b3205f813.pdf,optimization;zero_few-shot;representation;metric;sparse;llm,https://scholar.google.com/scholar?q=Invertible+Manifold+Learning+for+Dimension+Reduction
Entropic Risk-Sensitive Reinforcement Learning: A Meta Regret Framework with Function Approximation,2021,ICLR,"['Yingjie Fei', 'Zhuoran Yang', 'Zhaoran Wang']",poster,[],"We study risk-sensitive reinforcement learning with the entropic risk measure and function approximation. We consider the finite-horizon episodic MDP setting, and propose a meta algorithm based on value iteration. We then derive two algorithms for linear and general function approximation, namely RSVI.L and RSVI.G, respectively, as special instances of the meta algorithm. We illustrate that the success of RSVI.L depends crucially on carefully designed feature mapping and regularization that adapt to risk sensitivity. In addition, both RSVI.L and RSVI.G maintain risk-sensitive optimism that facilitates efficient exploration. On the analytic side, we provide regret analysis for the algorithms by developing a meta analytic framework, at the core of which is a risk-sensitive optimism condition. We show that any instance of the meta algorithm that satisfies the condition yields a meta regret bound. We further verify the condition for RSVI.L and RSVI.G under respective function approximation settings to obtain concrete regret bounds that scale sublinearly in the number of episodes. 
",/pdf/82089929f026609e0ef417358fa618e145567947.pdf,reinforcement learning;meta-learning;llm,https://scholar.google.com/scholar?q=Entropic+Risk-Sensitive+Reinforcement+Learning:+A+Meta+Regret+Framework+with+Function+Approximation
X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback,2021,ICLR,"['Jensen Gao', 'Siddharth Reddy', 'Glen Berseth', 'Nicholas Hardy', 'Nikhilesh Natraj', 'Karunesh Ganguly', 'Anca Dragan', 'Sergey Levine']",poster,"['reinforcement learning', 'human-computer interaction']","We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze or neural activity measured by a brain implant. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, a large-scale observational study on handwriting samples from 60 users, and a pilot study with one participant using an electrocorticography-based brain-computer interface. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.",/pdf/b299e92f993e7b50461323c326ad974dc5b67e09.pdf,offline reinforcement learning;graph;online learning;adaptive;llm,https://scholar.google.com/scholar?q=X2T:+Training+an+X-to-Text+Typing+Interface+with+Online+Learning+from+User+Feedback
"Deep Learning is Singular, and That's Good",2021,ICLR,"['Daniel Murfet', 'Susan Wei', 'Mingming Gong', 'Hui Li', 'Jesse Gell-Redman', 'Thomas Quella']",poster,"['deep learning theory', 'effective degrees of freedom', 'generalisation', 'posterior predictive distribution', 'real log canonical threshold', 'singular learning theory']","In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus ``dividing"" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice. ",/pdf/055b587f821423ecb63143818cafdc91a9a974af.pdf,inference;llm,"https://scholar.google.com/scholar?q=Deep+Learning+is+Singular,+and+That's+Good"
Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis,2021,ICLR,"['Rafael Valle', 'Kevin J. Shih', 'Ryan Prenger', 'Bryan Catanzaro']",poster,"['Text to speech synthesis', 'normalizing flows', 'deep learning']","In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.",/pdf/f022be71a045af955f0b061b9fb50186c102f407.pdf,zero_few-shot;generative model;transfer learning;flow;llm,https://scholar.google.com/scholar?q=Flowtron:+an+Autoregressive+Flow-based+Generative+Network+for+Text-to-Speech+Synthesis
Neural Delay Differential Equations,2021,ICLR,"['Qunxi Zhu', 'Yao Guo', 'Wei Lin']",poster,"['Delay differential equations', 'neural networks']","    Neural Ordinary Differential Equations (NODEs), a framework of continuous-depth neural networks, have been widely applied, showing exceptional efficacy in coping with some representative datasets.  Recently, an augmented framework has been successfully developed for conquering some limitations emergent in application of the original framework.  Here we propose a new class of continuous-depth neural networks with delay, named as Neural Delay Differential Equations (NDDEs),  and, for computing the corresponding gradients, we use the adjoint sensitivity method to obtain the delayed dynamics of the adjoint. Since the differential equations with delays are usually seen as dynamical systems of infinite dimension possessing more fruitful dynamics, the NDDEs, compared to the NODEs, own a stronger capacity of nonlinear representations.  Indeed, we analytically validate that the NDDEs are of universal approximators, and further articulate an extension of the NDDEs, where the initial function of the NDDEs is supposed to satisfy ODEs.   More importantly, we use several illustrative examples to demonstrate the outstanding capacities of the NDDEs and the NDDEs with ODEs' initial value.  More precisely, (1) we successfully model the delayed dynamics where the trajectories in the lower-dimensional phase space could be mutually intersected, while the traditional NODEs without any argumentation are not directly applicable for such modeling, and (2) we achieve lower loss and higher accuracy not only for the  data produced synthetically by complex models but also for the real-world image datasets, i.e., CIFAR10, MNIST and SVHN.   Our results on the NDDEs reveal that appropriately articulating the elements of dynamical systems into the network design is truly beneficial to promoting the network performance.",/pdf/39dbf04903dd5453991dacd8dcb2333de44b3a6e.pdf,graph;zero_few-shot;representation;online learning;augmentation;llm,https://scholar.google.com/scholar?q=Neural+Delay+Differential+Equations
Deep Reinforcement Learning with Causality-based Intrinsic Reward,2021,ICLR,"['Peng Zhang', 'Furui Liu', 'Zhitang Chen', 'Jianye HAO', 'Jun Wang']",poster,"['Reinforcement Learning', 'Causal Relation']","Reinforcement Learning (RL) has shown great potential to deal with sequential decision-making problems. However, most RL algorithms do not explicitly consider the relations between entities in the environment. This makes the policy learning suffer from the problems of efficiency, effectivity and interpretability.  In this paper, we propose a novel deep reinforcement learning algorithm, which firstly learns the causal structure of the environment and then leverages the learned causal information to assist policy learning. The proposed algorithm learns a graph to encode the environmental structure by calculating Average Causal Effect (ACE) between different categories of entities, and an intrinsic reward is given to encourage the agent to interact more with entities belonging to top-ranked categories, which significantly boosts policy learning.  Several experiments are conducted on a number of simulation environments to demonstrate the effectiveness and better interpretability of our proposed method.",/pdf/fe2c7c2225b3a9daae903ce01d7360f350c1e292.pdf,reinforcement learning;graph;llm,https://scholar.google.com/scholar?q=Deep+Reinforcement+Learning+with+Causality-based+Intrinsic+Reward
Multi-Task Learning by a Top-Down Control Network,2021,ICLR,"['Hila Levi', 'Shimon Ullman']",poster,"['multi task learning', 'computer vision']","As the range of tasks performed by a general vision system expands, executing multiple tasks accurately and efﬁciently in  a single network has become an important and still open problem. Recent computer vision approaches address this problem by branching networks, or by a channel-wise modulation of the network feature-maps with task speciﬁc vectors. We present a novel architecture that uses a dedicated top-down control network to modify the activation of all the units in the main recognition network in a manner that depends on the selected task, image content, and spatial location. We show the effectiveness of our scheme by achieving signiﬁcantly better results than alternative state-of-the-art approaches on four datasets. We further demonstrate our advantages in terms of task selectivity, scaling the number of tasks and interpretability. 
Code is supplied in the supplementary materials and will be publicly available.",/pdf/a409b57624394464249851730cfadd6b4b5a0b17.pdf,transformer;multi-task;llm,https://scholar.google.com/scholar?q=Multi-Task+Learning+by+a+Top-Down+Control+Network
Mixture Representation Learning with Coupled Autoencoding Agents,2021,ICLR,"['Yeganeh Marghi', 'Rohan Gala', 'Uygar Sümbül']",poster,"['Multi-agent network', 'representation learning', 'collective decision making', 'type-preserving data augmentation']","Jointly identifying a mixture of discrete and continuous factors of variability can help unravel complex phenomena. We study this problem by proposing an unsupervised framework called coupled mixture VAE (cpl-mixVAE), which utilizes multiple interacting autoencoding agents. The individual agents operate on augmented copies of training samples to learn mixture representations, while being encouraged to reach consensus on the categorical assignments. We provide theoretical justification to motivate the use of a multi-agent framework, and formulate it as a variational inference problem. We benchmark our approach on MNIST and dSprites, achieving state-of-the-art categorical assignments while preserving interpretability of the continuous factors. We then demonstrate the utility of this approach in jointly identifying cell types and type-specific, activity-regulated genes for a single-cell gene expression dataset profiling over 100 cortical neuron types.",/pdf/7b0dfa40d15377228524b0b5a908c6a0302899b8.pdf,reinforcement learning;representation;vae;inference;augmentation;multi-agent;llm,https://scholar.google.com/scholar?q=Mixture+Representation+Learning+with+Coupled+Autoencoding+Agents
DDPNOpt: Differential Dynamic Programming Neural Optimizer,2021,ICLR,"['Guan-Horng Liu', 'Tianrong Chen', 'Evangelos Theodorou']",poster,"['deep learning training', 'optimal control', 'trajectory optimization', 'differential dynamica programming']","Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.",/pdf/73e56442dda6b1e73bab62c8ca8c2dac7d319003.pdf,optimization;transformer;online learning;llm,https://scholar.google.com/scholar?q=DDPNOpt:+Differential+Dynamic+Programming+Neural+Optimizer
RRL: A Scalable Classifier for Interpretable Rule-Based Representation Learning,2021,ICLR,"['Zhuo Wang', 'Wei Zhang', 'Ning Liu', 'Jianyong Wang']",poster,"['interpretable representation learning', 'rule-based model', 'scalability']","Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to tackle these issues, but they sacrifice the model interpretability. In this paper, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on 9 small and 4 large data sets show that RRL outperforms the competitive approaches, has low complexity close to the simple decision trees, and is rational for its main technical contributions.",/pdf/771d0844e59b2aac975e3ac11976aa1eefc79220.pdf,zero_few-shot;representation;decision trees;llm,https://scholar.google.com/scholar?q=RRL:+A+Scalable+Classifier+for+Interpretable+Rule-Based+Representation+Learning
Learning Hyperbolic Representations of Topological Features,2021,ICLR,"['Panagiotis Kyriakis', 'Iordanis Fostiropoulos', 'Paul Bogdan']",poster,"['representation learning', 'hyperbolic deep learning', 'persistent homology', 'persistence diagrams']","Learning task-specific representations of persistence diagrams is an important problem in topological data analysis and machine learning. However, current state of the art methods are restricted in terms of their expressivity as they are focused on Euclidean representations. Persistence diagrams often contain features of infinite persistence (i.e., essential features) and Euclidean spaces shrink their importance relative to non-essential features because they cannot assign infinite distance to finite points. To deal with this issue, we propose a method to learn representations of persistence diagrams on hyperbolic spaces, more specifically on the Poincare ball. By representing features of infinite persistence infinitesimally close to the boundary of the ball, their distance to non-essential features approaches infinity, thereby their relative importance is preserved. This is achieved without utilizing extremely high values for the learnable parameters, thus the representation can be fed into downstream optimization methods and trained efficiently in an end-to-end fashion. We present experimental results on graph and image classification tasks and show that the performance of our method is on par with or exceeds the performance of other state of the art methods.
",/pdf/90d9d8bd76c09f1561cba895e6574b5a137974f8.pdf,graph;optimization;representation;llm,https://scholar.google.com/scholar?q=Learning+Hyperbolic+Representations+of+Topological+Features
Selectivity considered harmful: evaluating the causal impact of class selectivity in DNNs,2021,ICLR,"['Matthew L Leavitt', 'Ari S. Morcos']",poster,"['interpretability', 'explainability', 'empirical analysis', 'deep learning', 'selectivity']","The properties of individual neurons are often analyzed in order to understand the biological and artificial neural networks in which they're embedded. Class selectivity—typically defined as how different a neuron's responses are across different classes of stimuli or data samples—is commonly used for this purpose. However, it remains an open question whether it is necessary and/or sufficient for deep neural networks (DNNs) to learn class selectivity in individual units. We investigated the causal impact of class selectivity on network function by directly regularizing for or against class selectivity. Using this regularizer to reduce class selectivity across units in convolutional neural networks increased test accuracy by over 2% in ResNet18 and 1% in ResNet50 trained on Tiny ImageNet. For ResNet20 trained on CIFAR10 we could reduce class selectivity by a factor of 2.5 with no impact on test accuracy, and reduce it nearly to zero with only a small (~2%) drop in test accuracy. In contrast, regularizing to increase class selectivity significantly decreased test accuracy across all models and datasets. These results indicate that class selectivity in individual units is neither sufficient nor strictly necessary, and can even impair DNN performance. They also encourage caution when focusing on the properties of single units as representative of the mechanisms by which DNNs function.",/pdf/17afa91e20d55de3a216e07f59e5ab74d27027a6.pdf,llm,https://scholar.google.com/scholar?q=Selectivity+considered+harmful:+evaluating+the+causal+impact+of+class+selectivity+in+DNNs
Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning,2021,ICLR,"['Aviral Kumar', 'Rishabh Agarwal', 'Dibya Ghosh', 'Sergey Levine']",poster,"['deep Q-learning', 'data-efficient RL', 'rank-collapse', 'offline RL']","We identify an implicit under-parameterization phenomenon in value-based deep RL methods that use bootstrapping: when value functions, approximated using deep neural networks, are trained with gradient descent using iterated regression onto target values generated by previous instances of the value network, more gradient updates decrease the expressivity of the current value network. We char- acterize this loss of expressivity via a drop in the rank of the learned value net- work features, and show that this typically corresponds to a performance drop. We demonstrate this phenomenon on Atari and Gym benchmarks, in both offline and online RL settings. We formally analyze this phenomenon and show that it results from a pathological interaction between bootstrapping and gradient-based optimization. We further show that mitigating implicit under-parameterization by controlling rank collapse can improve performance.",/pdf/766d703650bcb86c35c53681b89ad0a2aba30504.pdf,reinforcement learning;offline reinforcement learning;optimization;online learning;llm,https://scholar.google.com/scholar?q=Implicit+Under-Parameterization+Inhibits+Data-Efficient+Deep+Reinforcement+Learning
Formalizing Generalization and Robustness of Neural Networks to Weight Perturbations,2021,ICLR,"['Yu-Lin Tsai', 'Chia-Yi Hsu', 'Chia-Mu Yu', 'Pin-Yu Chen']",poster,"['Generalization', 'Robustness', 'Neural Network', 'Weight Perturbation', 'Rademacher complexity']","Studying the sensitivity of weight perturbation in neural networks and its impacts on model performance, including generalization and robustness, is an active research topic due to its implications on a wide range of machine learning tasks such as model compression, generalization gap assessment, and adversarial attacks. In this paper, we provide the first formal analysis for feed-forward neural networks with non-negative monotone activation functions against norm-bounded weight perturbations, in terms of the robustness in pairwise class margin functions and the Rademacher complexity for generalization. We further design a new theory-driven loss function for training generalizable and robust neural networks against weight perturbations. Empirical experiments are conducted to validate our theoretical analysis. Our results offer fundamental insights for characterizing the generalization and robustness of neural networks against weight perturbations.",/pdf/88af10d0795c4bb091e7e4db77ffa941945e4fd6.pdf,graph;active learning;llm,https://scholar.google.com/scholar?q=Formalizing+Generalization+and+Robustness+of+Neural+Networks+to+Weight+Perturbations
Hyperrealistic neural decoding: Reconstruction of face stimuli from fMRI measurements via the GAN latent space,2021,ICLR,"['Thirza Dado', 'Yağmur Güçlütürk', 'Luca Ambrogioni', 'Gabrielle Ras', 'Sander E. Bosch', 'Marcel van Gerven', 'Umut Güçlü']",poster,"['Deep learning', 'Face perception', 'fMRI', 'Generative Adversarial Networks', 'Neural decoding']","We introduce a new framework for hyperrealistic reconstruction of perceived naturalistic stimuli from brain recordings. To this end, we embrace the use of generative adversarial networks (GANs) at the earliest step of our neural decoding pipeline by acquiring functional magnetic resonance imaging data as subjects perceived face images created by the generator network of a GAN. Subsequently, we used a decoding approach to predict the latent state of the GAN from brain data. Hence, latent representations for stimulus (re-)generation are obtained, leading to state-of-the-art image reconstructions. Altogether, we have developed a highly promising approach for decoding sensory perception from brain activity and systematically analyzing neural information processing in the human brain.",/pdf/7fa24ae389dfdde6122273c85165d42bd18c7768.pdf,graph;representation;generative model;llm,https://scholar.google.com/scholar?q=Hyperrealistic+neural+decoding:+Reconstruction+of+face+stimuli+from+fMRI+measurements+via+the+GAN+latent+space
Optimism in Reinforcement Learning with Generalized Linear Function Approximation,2021,ICLR,"['Yining Wang', 'Ruosong Wang', 'Simon Shaolei Du', 'Akshay Krishnamurthy']",poster,"['reinforcement learning', 'optimism', 'exploration', 'function approximation', 'theory', 'regret analysis', 'provable sample efficiency']","We design a new provably efficient algorithm for episodic reinforcement learning with generalized linear function approximation. We analyze the algorithm under a new expressivity assumption that we call ``optimistic closure,'' which is strictly weaker than assumptions from prior analyses for the linear setting. With optimistic closure, we prove that our algorithm enjoys a regret bound of $\widetilde{O}\left(H\sqrt{d^3 T}\right)$ where $H$ is the horizon, $d$ is the dimensionality of the state-action features and $T$ is the number of episodes. This is the first statistically and computationally efficient algorithm for reinforcement learning with generalized linear functions.",/pdf/a233880f2576e4e435aefcdb4dce873e39557072.pdf,reinforcement learning;zero_few-shot;llm,https://scholar.google.com/scholar?q=Optimism+in+Reinforcement+Learning+with+Generalized+Linear+Function+Approximation
A Real-time Contribution Measurement Method for Participants in Federated Learning,2021,ICLR,"['Bingjie Yan', 'Yize Zhou', 'Boyi Liu', 'Jun Wang', 'Yuhan Zhang', 'Li Liu', 'Xiaolan Nie', 'Zhiwei Fan', 'Zhixuan Liang']",poster,"['Federated Learning', 'Contribution Evaluation', 'Multi-party Participation']","Federated learning is a framework for protecting distributed data privacy and has participated in commercial activities. However, there is a lack of a sufficiently reasonable contribution measurement mechanism to distribute the reward for each agent. In the commercial union, if there is no mechanism like this, every agent will get the same reward. This is unfair to agents that provide better data, so such a mechanism is needed. To address this issue, this work proposes a real-time contribution measurement method. Firstly, the method defines the impact of each agent. Furthermore, we comprehensively consider the current round and the previous round to obtain the contribution rate of each agent. To verify effectiveness of the proposed method, the work conducts pseudo-distributed training and an experiment on the Penn Treebank dataset. Comparing the Shapley Value in game theory, the comparative experiment result shows that the proposed method is more sensitive to both data quantity and data quality under the premise of maintaining real-time.",/pdf/0dd54c028297fcd3fcaf918ecc43a8740fb1227b.pdf,reinforcement learning;federated learning;llm,https://scholar.google.com/scholar?q=A+Real-time+Contribution+Measurement+Method+for+Participants+in+Federated+Learning
"High-Likelihood Area Matters --- Rewarding Correct,Rare Predictions Under Imbalanced Distributions",2021,ICLR,"['guangxiang zhao', 'Lei Li', 'Xuancheng Ren', 'Xu Sun', 'Bin He']",poster,"['classification', 'imbalance', 'long-tailed', 'likelihood', 'focal loss']","Learning from natural datasets poses significant challenges for traditional classification methods based on the cross-entropy objective due to imbalanced class distributions. It is intuitive to assume that the examples from rare classes are harder to learn so that the classifier is uncertain of the prediction, which establishes the low-likelihood area. Based on this, existing approaches drive the classifier actively to correctly predict those incorrect, rare examples. However, this assumption is one-sided and could be misleading. We find in practice that the high-likelihood area contains correct predictions for rare class examples and it plays a vital role in learning imbalanced class distributions. In light of this finding, we propose the Eureka Loss, which rewards the classifier when examples belong to rare classes in the high-likelihood area are correctly predicted. Experiments on the large-scale long-tailed iNaturalist 2018 classification dataset and the ImageNet-LT benchmark both validate the proposed approach. We further analyze the influence of the Eureka Loss in detail on diverse data distributions.",/pdf/b693bec8d6f56956104b74c1b8be0c975dca39b7.pdf,zero_few-shot;active learning;llm,"https://scholar.google.com/scholar?q=High-Likelihood+Area+Matters+---+Rewarding+Correct,Rare+Predictions+Under+Imbalanced+Distributions"
Active Tuning,2021,ICLR,"['Sebastian Otte', 'Matthias Karlbauer', 'Martin V. Butz']",poster,"['Signal Filtering', 'Recurrent Neural Network', 'Time Series', 'Denoising', 'Temporal Gradients']","We introduce Active Tuning, a novel paradigm for optimizing the internal dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the conventional sequence-to-sequence mapping scheme, Active Tuning decouples the RNN's recurrent neural activities from the input stream, using the unfolding temporal gradient signal to tune the internal dynamics into the data stream. As a consequence, the model output depends only on its internal hidden dynamics and the closed-loop feedback of its own predictions; its hidden state is continuously adapted by means of the temporal gradient resulting from backpropagating the discrepancy between the signal observations and the model outputs through time. In this way, Active Tuning infers the signal actively but indirectly based on the originally learned temporal patterns, fitting the most plausible hidden state sequence into the observations. We demonstrate the effectiveness of Active Tuning on several time series prediction benchmarks, including multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics. Active Tuning consistently improves the robustness, accuracy, and generalization abilities of all evaluated models. Moreover, networks trained for signal prediction and denoising can be successfully applied to a much larger range of noise conditions with the help of Active Tuning. Thus, given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training.",/pdf/2647cc6db0ec69658af5cf03d2128e980c93516e.pdf,graph;online learning;active learning;llm,https://scholar.google.com/scholar?q=Active+Tuning
Structure and randomness in planning and reinforcement learning,2021,ICLR,"['Piotr Kozakowski', 'Piotr Januszewski', 'Konrad Czechowski', 'Łukasz Kuciński', 'Piotr Miłoś']",poster,"['reinforcement learning', 'uncertainty', 'model-based', 'MCTS']","Planning in large state spaces inevitably needs to balance depth and breadth of the search. It has a crucial impact on planners performance and most manage this interplay implicitly. We present a novel method $\textit{Shoot Tree Search (STS)}$, which makes it possible to control this trade-off more explicitly. Our algorithm can be understood as an interpolation between two celebrated search mechanisms: MCTS and random shooting. It also lets the user control the bias-variance trade-off, akin to $TD(n)$, but in the tree search context.

In experiments on challenging domains, we show that STS can get the best of both worlds consistently achieving higher scores. ",/pdf/1a03d78089937a7fb65edfc52e1af495c8bcbee7.pdf,reinforcement learning;graph;llm,https://scholar.google.com/scholar?q=Structure+and+randomness+in+planning+and+reinforcement+learning
Hippocampal representations emerge when training recurrent neural networks on a memory dependent maze navigation task,2021,ICLR,"['Justin Jude', 'Matthias Hennig']",poster,"['recurrent neural network', 'place cell', 'hippocampus', 'neural dynamics']","Can neural networks learn goal-directed behaviour using similar strategies to the brain, by combining the relationships between the current state of the organism and the consequences of future actions? Recent work has shown that recurrent neural networks trained on goal based tasks can develop representations resembling those found in the brain, entorhinal cortex grid cells, for instance. Here we explore the evolution of the dynamics of their internal representations and compare this with experimental data. We observe that once a recurrent network is trained to learn the structure of its environment solely based on sensory prediction, an attractor based landscape forms in the network's representation, which parallels hippocampal place cells in structure and function. Next, we extend the predictive objective to include Q-learning for a reward task, where rewarding actions are dependent on delayed cue modulation. Mirroring experimental findings in hippocampus recordings in rodents performing the same task, this training paradigm causes nonlocal neural activity to sweep forward in space at decision points, anticipating the future path to a rewarded location. Moreover, prevalent choice and cue-selective neurons form in this network, again recapitulating experimental findings. Together, these results indicate that combining predictive, unsupervised learning of the structure of an environment with reinforcement learning can help understand the formation of hippocampus-like representations containing both spatial and task-relevant information.",/pdf/c0b1f19618baa14026ed1976b87d36ba9b80e383.pdf,reinforcement learning;representation;llm,https://scholar.google.com/scholar?q=Hippocampal+representations+emerge+when+training+recurrent+neural+networks+on+a+memory+dependent+maze+navigation+task
Syntactic representations in the human brain: beyond effort-based metrics,2021,ICLR,"['Aniketh Janardhan Reddy', 'Leila Wehbe']",poster,"['neuroscience', 'fMRI', 'syntactic representations', 'graph embeddings']","We are far from having a complete mechanistic understanding of the brain computations involved in language processing and of the role that syntax plays in those computations.  Most language studies do not computationally model syntactic structure, and most studies that do model syntactic processing use effort-based metrics. These metrics capture the effort needed to process the syntactic information given by every word (Brennan et al., 2012; Hale et al., 2018; Brennan et al.,2016). They can reveal where in the brain syntactic processing occurs, but not what features of syntax are processed by different brain regions. Here, we move beyond effort-based metrics and propose explicit features capturing the syntactic structure that is incrementally built while a sentence is being read.  Using these features and functional Magnetic Resonance Imaging (fMRI) recordings of participants reading a natural text, we study the brain representation of syntax. We find that our syntactic structure-based features are better than effort-based metrics at predicting brain activity in various parts of the language system. We show evidence of the brain representation of complex syntactic information such as phrase and clause structures. We see that regions well-predicted by syntactic features are distributed in the language system and are not distinguishable from those processing semantics. Our results call for a shift in the approach used for studying syntactic processing.",/pdf/908a27ebfbaab2cf60d39c5cd36bd6b771b627ae.pdf,graph;representation;metric;llm,https://scholar.google.com/scholar?q=Syntactic+representations+in+the+human+brain:+beyond+effort-based+metrics
Hybrid and Non-Uniform DNN quantization methods using Retro Synthesis data for efficient inference,2021,ICLR,"['TEJPRATAP GVSL', 'Raja Kumar', 'Pradeep NS']",poster,"['quantization', 'dnn inference', 'data free quantization', 'synthetic data', 'model compression']","Existing post-training quantization methods attempt to compensate for the quantization loss by determining the quantized weights and activation ranges with the help of training data. Quantization aware training methods, on the other hand, achieve accuracy near to FP32 models by training the quantized model which consume more time. Both these methods are not effective for privacy constraint applications as they are tightly coupled with training data. In contrast, this paper proposes a data-independent post-training quantization scheme that eliminates the need for training data. This is achieved by generating a faux dataset hereafter called as $\textit{‘Retro-Synthesis Data’}$ from the FP32 model layer statistics and further using it for quantization. This approach outperformed state-of-the-art methods including, but not limited to, ZeroQ and DFQ on models with and without batch-normalization layers for 8, 6 and 4 bit precisions. We also introduced two futuristic variants of post-training quantization methods namely $\textit{‘Hybrid-Quantization’}$ and $\textit{‘Non-Uniform Quantization’}$. The Hybrid-Quantization scheme determines the sensitivity of each layer for per-tensor and per-channel quantization, and thereby generates hybrid quantized models that are $10 - 20\%$ efficient in inference time while achieving same or better accuracy as compared to per-channel quantization. Also this method outperformed FP32 accuracy when applied for models such as ResNet-18, and ResNet-50 onImageNet dataset. In the proposed Non-Uniform quantization scheme, the weights are grouped into different clusters and these clusters are assigned with a varied number of quantization steps depending on the number of weights and their ranges in respective cluster. This method resulted in an accuracy improvement of $1\%$ against state-of-the-art quantization methods on ImageNet dataset.",/pdf/6b5daf187ad95d204a97e8e5a0041efd622ee821.pdf,optimization;inference;llm,https://scholar.google.com/scholar?q=Hybrid+and+Non-Uniform+DNN+quantization+methods+using+Retro+Synthesis+data+for+efficient+inference
Joint Learning of Full-structure Noise in Hierarchical Bayesian Regression Models,2021,ICLR,"['Ali Hashemi', 'Chang Cai', 'Klaus Robert Muller', 'Srikantan Nagarajan', 'Stefan Haufe']",poster,"['Full-structure Noise', 'Hierarchical Bayesian Regression Models', 'Sparse Bayesian Learning', 'Unsupervised Learning', 'Brain Source Imaging', 'Covariance Estimation.']","We consider hierarchical Bayesian (type-II maximum likelihood) models for observations with latent variables for source and noise, where both hyperparameters need to be estimated jointly from data. This problem has application in many domains in imaging including biomagnetic inverse problems. Crucial factors influencing accuracy of source estimation are not only the noise level but also its correlation structure, but existing approaches have not addressed estimation of noise covariance matrices with full structure. Here, we consider the reconstruction of brain activity from electroencephalography (EEG). This inverse problem can be formulated as a linear regression with independent Gaussian scale mixture priors for both the source and noise components. As a departure from  classical sparse Bayesan learning (SBL) models where across-sensor observations are assumed to be independent and identically distributed, we consider Gaussian noise with full covariance structure. Using  Riemannian geometry, we derive an efficient algorithm for updating both source and noise covariance along the manifold of positive definite matrices. Using the majorization-maximization framework, we demonstrate that our algorithm has guaranteed and fast convergence. We validate the algorithm both in simulations and with real data. Our results demonstrate that the novel framework significantly improves upon state-of-the-art techniques in the real-world scenario where the noise is indeed non-diagonal and fully-structured.",/pdf/7abc1c8dc7de22c156b79e63138154095a2d930a.pdf,graph;bayesian;sparse;llm,https://scholar.google.com/scholar?q=Joint+Learning+of+Full-structure+Noise+in+Hierarchical+Bayesian+Regression+Models
Mapping the Timescale Organization of Neural Language Models,2021,ICLR,"['Hsiang-Yun Sherry Chien', 'Jinhan Zhang', 'Christopher Honey']",poster,"['natural language processing', 'LSTM', 'timescale', 'hierarchy', 'temporal context']","In the human brain, sequences of language input are processed within a distributed and hierarchical architecture, in which higher stages of processing encode contextual information over longer timescales. In contrast, in recurrent neural networks which perform natural language processing, we know little about how the multiple timescales of contextual information are functionally organized. Therefore, we applied tools developed in neuroscience to map the “processing timescales” of individual units within a word-level LSTM language model. This timescale-mapping method assigned long timescales to units previously found to track long-range syntactic dependencies. Additionally, the mapping revealed a small subset of the network (less than 15% of units) with long timescales and whose function had not previously been explored. We next probed the functional organization of the network by examining the relationship between the processing timescale of units and their network connectivity. We identified two classes of long-timescale units: “controller” units composed a densely interconnected subnetwork and strongly projected to the rest of the network, while “integrator” units showed the longest timescales in the network, and expressed projection profiles closer to the mean projection profile. Ablating integrator and controller units affected model performance at different positions within a sentence, suggesting distinctive functions of these two sets of units. Finally, we tested the generalization of these results to a character-level LSTM model and models with different architectures. In summary, we demonstrated a model-free technique for mapping the timescale organization in recurrent neural networks, and we applied this method to reveal the timescale and functional organization of neural language models",/pdf/13cd0af93335a87135e32f629677ce93f146f5bb.pdf,llm,https://scholar.google.com/scholar?q=Mapping+the+Timescale+Organization+of+Neural+Language+Models
Disentangling Representations of Text by Masking Transformers,2021,ICLR,"['Xiongyi Zhang', 'Jan-Willem van de Meent', 'Byron C Wallace']",poster,"['disentanglement', 'model pruning', 'representation learning', 'transformers']","Representations in large language models such as BERT encode a range of features into a single vector, which are predictive in the context of a multitude of downstream tasks. In this paper, we explore whether it is possible to learn disentangled representations  by identifying subnetworks in pre-trained models that encode distinct, complementary aspects of the representation. Concretely, we learn binary masks over transformer weights or hidden units to uncover the subset of features that correlate with a specific factor of variation. This sidesteps the need to train a disentangled model from scratch within a particular domain. We evaluate the ability of this method to disentangle representations of syntax and semantics, and sentiment from genre in the context of movie reviews. By combining this method with magnitude pruning we find that we can identify quite sparse subnetworks. Moreover, we find that this disentanglement-via-masking approach performs as well as or better than previously proposed methods based on variational autoencoders and adversarial training. ",/pdf/f8a8cf9435778ae4038e813fe7d1fa3e1a22826b.pdf,transformer;representation;vae;sparse;llm,https://scholar.google.com/scholar?q=Disentangling+Representations+of+Text+by+Masking+Transformers
Cortico-cerebellar networks as decoupled neural interfaces,2021,ICLR,"['Joseph Pemberton', 'Ellen Boven', 'Richard Apps', 'Rui Ponte Costa']",poster,"['systems neuroscience', 'cerebellum', 'neocortex', 'decoupled neural interfaces', 'deep learning', 'decorrelation', 'inverse models', 'forward models']","The brain solves the credit assignment problem remarkably well. For credit to be correctly assigned across multiple cortical areas a given area should, in principle, wait for others to finish their computation. How the brain deals with this locking problem has remained unclear. Deep learning methods suffer from similar locking constraints both on the forward and backward phase. Recently, decoupled neural interfaces (DNI) were introduced as a solution to the forward and backward locking problems.
Here we propose that a specialised brain region, the cerebellum, helps the cerebral cortex solve the locking problem closely matching the computations and architecture of DNI. In particular, we propose that classical cerebellar forward and inverse models are equivalent to solving the backward and forward locking problems, respectively. To demonstrate the potential of this framework we focus on modelling a given brain area as a recurrent neural network in which the cerebellum approximates temporal feedback signals as provided by BPTT. We tested the cortico-cerebellar-DNI (CC-DNI) model in a range of sensorimotor and cognitive tasks that have been shown to be cerebellar-dependent. First, we show that the CC-DNI unlocking mechanisms can facilitate learning in a simple target reaching task. Next, by building on the sequential MNIST task we demonstrate that these results generalise to more complex sensorimotor tasks. Our cortico-cerebellar model readily applies to a wider range of modalities, to demonstrate this we tested the model in a cognitive task, caption generation. Models without the cerebellar-DNI component exhibit deficits similar to those observed in cerebellar patients in both motor and cognitive tasks. Moreover, we used CC-DNI to generate a set of specific neuroscience predictions. Finally, we introduce a CC-DNI model with highly sparse connectivity as observed in the cerebellum, which substantially reduces the number of parameters while improving learning through decorrelation.
Overall, our work offers a novel perspective on the cerebellum as a brain-wide decoupling machine for efficient credit assignment and opens a new avenue of research between deep learning and neuroscience.",/pdf/9a9bb9c545f92910a0e6a756fce89db5509f430e.pdf,optimization;generative model;sparse;llm,https://scholar.google.com/scholar?q=Cortico-cerebellar+networks+as+decoupled+neural+interfaces
On the Importance of Looking at the Manifold,2021,ICLR,"['Nil Adell Mill', 'Jannis Born', 'Nathaniel Park', 'James Hedrick', 'María Rodríguez Martínez', 'Matteo Manica']",poster,"['Topological Learning', 'GNN', 'VAE']","Data rarely lies on uniquely Euclidean spaces. Even data typically represented in regular domains, such as images, can have a higher level of relational information, either between data samples or even relations within samples, e.g., how the objects in an image are linked. With this perspective our data points can be enriched by explicitly accounting for this connectivity and analyzing them as a graph. Herein, we analyze various approaches for unsupervised representation learning and investigate the importance of considering topological information and its impact when learning representations. We explore a spectrum of models, ranging from uniquely learning representations based on the isolated features of the nodes (focusing on Variational Autoencoders), to uniquely learning representations based on the topology (using node2vec) passing through models that integrate both node features and topological information in a hybrid fashion. For the latter we use Graph Neural Networks, precisely Deep Graph Infomax (DGI), and an extension of the typical formulation of the VAE where the topological structure is accounted for via an explicit regularization of the loss (Graph-Regularized VAEs, introduced in this work). To extensively investigate these methodologies, we consider a wide variety of data types: synthetic data point clouds, MNIST, citation networks, and chemical reactions. We show that each of the representations learned by these models may have critical importance for further downstream tasks, and that accounting for the topological features can greatly improve the modeling capabilities for certain problems. We further provide a framework to analyze these, and future models under different scenarios and types of data.",/pdf/853de54c6bb4f30bbec31d0002795f927a1ab5f5.pdf,graph;representation;vae;llm,https://scholar.google.com/scholar?q=On+the+Importance+of+Looking+at+the+Manifold
Dual-Tree Wavelet Packet CNNs for Image Classification,2021,ICLR,"['Hubert Leterme', 'Kévin Polisano', 'Valérie Perrier', 'Karteek Alahari']",poster,"['convolutional neural networks', 'wavelet packet transform', 'dual-tree wavelet packet transform', 'image classification', 'deep learning', 'image processing']","In this paper, we target an important issue of deep convolutional neural networks (CNNs) — the lack of a mathematical understanding of their properties. We present an explicit formalism that is motivated by the similarities between trained CNN kernels and oriented Gabor filters for addressing this problem. The core idea is to constrain the behavior of convolutional layers by splitting them into a succession of wavelet packet decompositions, which are modulated by freely-trained mixture weights. We evaluate our approach with three variants of wavelet decompositions with the AlexNet architecture for image classification as an example. The first variant relies on the separable wavelet packet transform while the other two implement the 2D dual-tree real and complex wavelet packet transforms, taking advantage of their feature extraction properties such as directional selectivity and shift invariance. Our experiments show that we achieve the accuracy rate of standard AlexNet, but with a significantly lower number of parameters, and an interpretation of the network that is grounded in mathematical theory.",/pdf/8797c50437804b0b3888915a5ab2faa982f59ca1.pdf,optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Dual-Tree+Wavelet+Packet+CNNs+for+Image+Classification
Learning to live with Dale's principle: ANNs with separate excitatory and inhibitory units,2021,ICLR,"['Jonathan Cornford', 'Damjan Kalajdzievski', 'Marco Leite', 'Amélie Lamarquette', 'Dimitri Michael Kullmann', 'Blake Aaron Richards']",poster,[]," The units in artificial neural networks (ANNs) can be thought of as abstractions of biological neurons, and ANNs are increasingly used in neuroscience research. However, there are many important differences between ANN units and real neurons. One of the most notable is the absence of Dale's principle, which ensures that biological neurons are either exclusively excitatory or inhibitory. Dale's principle is typically left out of ANNs because its inclusion impairs learning. This is problematic, because one of the great advantages of ANNs for neuroscience research is their ability to learn complicated, realistic tasks. Here, by taking inspiration from feedforward inhibitory interneurons in the brain we show that we can develop ANNs with separate populations of excitatory and inhibitory units that learn just as well as standard ANNs. We call these networks Dale's ANNs (DANNs). We present two insights that enable DANNs to learn well: (1) DANNs are related to normalization schemes, and can be initialized such that the inhibition centres and standardizes the excitatory activity, (2) updates to inhibitory neuron parameters should be scaled using corrections based on the Fisher Information matrix. These results demonstrate how ANNs that respect Dale's principle can be built without sacrificing learning performance, which is important for future work using ANNs as models of the brain. The results may also have interesting implications for how inhibitory plasticity in the real brain operates.",/pdf/4ec70d1b966600fcb426c0ea0982e93dd870f226.pdf,llm,https://scholar.google.com/scholar?q=Learning+to+live+with+Dale's+principle:+ANNs+with+separate+excitatory+and+inhibitory+units
Neuron Activation Analysis for Multi-Joint Robot Reinforcement Learning,2021,ICLR,"['Benedikt Feldotto', 'Heiko Lengenfelder', 'Alois Knoll']",poster,"['Reinforcement Learning', 'Machine Learning', 'Robot Motion Learning', 'DQN', 'Robot Manipulator', 'Target Reaching', 'Network Pruning']","Recent experiments indicate that pre-training of end-to-end Reinforcement Learning neural networks on general tasks can speed up the training process for specific robotic applications. However, it remains open if these networks form general feature extractors and a hierarchical organization that are reused as apparent e.g. in Convolutional Neural Networks. In this paper we analyze the intrinsic neuron activation in networks trained for target reaching of robot manipulators with increasing joint number in a vertical plane. We analyze the individual neuron activity distribution in the network, introduce a pruning algorithm to reduce network size keeping the performance, and with these dense network representations we spot correlations of neuron activity patterns among networks trained for robot manipulators with different joint number. We show that the input and output network layers have more distinct neuron activation in contrast to inner layers. Our pruning algorithm reduces the network size significantly, increases the distance of neuron activation while keeping a high performance in training and evaluation. Our results demonstrate that neuron activity can be mapped among networks trained for robots with different complexity. Hereby, robots with small joint difference show higher layer-wise projection accuracy whereas more different robots mostly show projections to the first layer.",/pdf/dfa95930949d84e2d82cd610b6ad04688cd32bd6.pdf,reinforcement learning;representation;llm,https://scholar.google.com/scholar?q=Neuron+Activation+Analysis+for+Multi-Joint+Robot+Reinforcement+Learning
Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream,2021,ICLR,"['Franziska Geiger', 'Martin Schrimpf', 'Tiago Marques', 'James J. DiCarlo']",poster,"['computational neuroscience', 'primate ventral stream', 'convolutional neural networks', 'biologically plausible learning']","After training on large datasets, certain deep neural networks are surprisingly good models of the neural mechanisms of adult primate visual object recognition. Nevertheless, these models are poor models of the development of the visual system because they posit millions of sequential, precisely coordinated synaptic updates, each based on a labeled image. While ongoing research is pursuing the use of unsupervised proxies for labels, we here explore a complementary strategy of reducing the required number of supervised synaptic updates to produce an adult-like ventral visual stream (as judged by the match to V1, V2, V4, IT, and behavior). Such models might require less precise machinery and energy expenditure to coordinate these updates and would thus move us closer to viable neuroscientific hypotheses about how the visual system wires itself up. Relative to the current leading model of the adult ventral stream, we here demonstrate that the total number of supervised weight updates can be substantially reduced using three complementary strategies: First, we find that only 2% of supervised updates (epochs and images) are needed to achieve ~80% of the match to adult ventral stream. Second, by improving the random distribution of synaptic connectivity, we find that 54% of the brain match can already be achieved “at birth"" (i.e. no training at all). Third, we find that, by training only ~5% of model synapses, we can still achieve nearly 80% of the match to the ventral stream. When these three strategies are applied in combination, we find that these new models achieve ~80% of a fully trained model's match to the brain, while using two orders of magnitude fewer supervised synaptic updates. These results reflect first steps in modeling not just primate adult visual processing during inference, but also how the ventral visual stream might be ""wired up"" by evolution (a model's ""birth"" state) and by developmental learning (a model's updates based on visual experience).",/pdf/bb2236ee64a1874ee01269d95ffaf7eb8cec2a94.pdf,inference;llm,https://scholar.google.com/scholar?q=Wiring+Up+Vision:+Minimizing+Supervised+Synaptic+Updates+Needed+to+Produce+a+Primate+Ventral+Stream
Learning Contextual Perturbation Budgets for Training Robust Neural Networks,2021,ICLR,"['Jing Xu', 'Zhouxing Shi', 'Huan Zhang', 'Jinfeng Yi', 'Cho-Jui Hsieh', 'Liwei Wang']",poster,"['adversarial robustness', 'certified robustness', 'certfied robust training']","Existing methods for training robust neural networks generally aim to make models uniformly robust on all input dimensions. However, different input dimensions are not uniformly important to the prediction. In this paper, we propose a novel framework to train certifiably robust models and learn non-uniform perturbation budgets on different input dimensions, in contrast to using the popular $\ell_\infty$ threat model. We incorporate a perturbation budget generator into the existing certified defense framework, and perform certified training with generated perturbation budgets. In comparison to the radius of $\ell_\infty$ ball in previous works, the robustness intensity is measured by robustness volume which is the multiplication of perturbation budgets on all input dimensions. We evaluate our method on MNIST and CIFAR-10 datasets and show that we can achieve lower clean and certified errors on relatively larger robustness volumes, compared to methods using uniform perturbation budgets. Further with two synthetic datasets constructed from MNIST and CIFAR-10, we also demonstrate that the perturbation budget generator can produce semantically-meaningful budgets, which implies that the generator can capture contextual information and the sensitivity of different features in input images.",/pdf/7f4ca05f208b3cfb4f6e6be664ca35beb49f32e0.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Learning+Contextual+Perturbation+Budgets+for+Training+Robust+Neural+Networks
ARELU: ATTENTION-BASED RECTIFIED LINEAR UNIT,2021,ICLR,"['Chen Dengsheng', 'Jun Li', 'Kai Xu']",poster,"['activation function', 'attention mechanism', 'rectified linear unit']","Element-wise activation functions play a critical role in deep neural networks via affecting the expressivity power and the learning dynamics. Learning-based activation functions have recently gained increasing attention and success. We propose a new perspective of learnable activation function through formulating them with element-wise attention mechanism. In each network layer, we devise an attention module which learns an element-wise, sign-based attention map for the pre-activation feature map. The attention map scales an element based on its sign. Adding the attention module with a rectified linear unit (ReLU) results in an amplification of positive elements and a suppression of negative ones, both with learned, data-adaptive parameters. We coin the resulting activation function Attention-based Rectified Linear Unit (AReLU). The attention module essentially learns an element-wise residue of the activated part of the input, as ReLU can be viewed as an identity transformation. This makes the network training more resis- tant to gradient vanishing. The learned attentive activation leads to well-focused activation of relevant regions of a feature map. Through extensive evaluations, we show that AReLU significantly boosts the performance of most mainstream network architectures with only two extra learnable parameters per layer introduced. Notably, AReLU facilitates fast network training under small learning rates, which makes it especially suited in the case of transfer learning and meta learning.",/pdf/60ce60a67f5efc2de6c92566d1494060661cd117.pdf,transformer;adaptive;meta-learning;transfer learning;llm,https://scholar.google.com/scholar?q=ARELU:+ATTENTION-BASED+RECTIFIED+LINEAR+UNIT
Variational Information Bottleneck for Effective Low-Resource Fine-Tuning,2021,ICLR,"['Rabeeh Karimi mahabadi', 'Yonatan Belinkov', 'James Henderson']",poster,"['Transfer learning', 'NLP', 'large-scale pre-trained language models', 'over-fitting', 'robust', 'biases', 'variational information bottleneck']","While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.",/pdf/62f3ae7c05e30f870e3a6435b704afbd5c5290ba.pdf,zero_few-shot;transformer;representation;inference;transfer learning;llm,https://scholar.google.com/scholar?q=Variational+Information+Bottleneck+for+Effective+Low-Resource+Fine-Tuning
Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections,2021,ICLR,"['Csaba Toth', 'Patric Bonnier', 'Harald Oberhauser']",poster,"['time series', 'sequential data', 'representation learning', 'low-rank tensors', 'classification', 'generative modelling']","Sequential data such as time series, video, or text can be challenging to analyse as the ordered structure gives rise to complex dependencies. At the heart of this is non-commutativity, in the sense that reordering the elements of a sequence can completely change its meaning. We use a classical mathematical object -- the free algebra -- to capture this non-commutativity. To address the innate computational complexity of this algebra, we use compositions of low-rank tensor projections. This yields modular and scalable building blocks that give state-of-the-art performance on standard benchmarks such as multivariate time series classification, mortality prediction and generative models for video.",/pdf/bc313164adf3017b7e94a07aecbd830b43e5c49a.pdf,graph;zero_few-shot;representation;generative model;low-rank;llm,https://scholar.google.com/scholar?q=Seq2Tens:+An+Efficient+Representation+of+Sequences+by+Low-Rank+Tensor+Projections
Is Attention Better Than Matrix Decomposition?,2021,ICLR,"['Zhengyang Geng', 'Meng-Hao Guo', 'Hongxu Chen', 'Xia Li', 'Ke Wei', 'Zhouchen Lin']",poster,"['attention models', 'matrix decomposition', 'computer vision']","As an essential ingredient of modern deep learning, attention mechanism, especially self-attention, plays a vital role in the global correlation discovery. However, is hand-crafted attention irreplaceable when modeling the global context? Our intriguing finding is that self-attention is not better than the matrix decomposition~(MD) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies. We model the global context issue as a low-rank completion problem and show that its optimization algorithms can help design global information blocks. This paper then proposes a series of Hamburgers, in which we employ the optimization algorithms for solving MDs to factorize the input representations into sub-matrices and reconstruct a low-rank embedding. Hamburgers with different MDs can perform favorably against the popular global context module self-attention when carefully coping with gradients back-propagated through MDs. Comprehensive experiments are conducted in the vision tasks where it is crucial to learn the global context, including semantic segmentation and image generation, demonstrating significant improvements over self-attention and its variants. Code is available at https://github.com/Gsunshine/Enjoy-Hamburger.",/pdf/1cb5acc6fe475a215dd1192beec6158b8a4da5dc.pdf,optimization;zero_few-shot;transformer;representation;generative model;segmentation;low-rank;llm,https://scholar.google.com/scholar?q=Is+Attention+Better+Than+Matrix+Decomposition?
Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective,2021,ICLR,"['Muhammet Balcilar', 'Guillaume Renton', 'Pierre Héroux', 'Benoit Gaüzère', 'Sébastien Adam', 'Paul Honeine']",poster,"['Graph Neural Networks', 'Spectral Graph Filter', 'Spectral Analysis']","In the recent literature of Graph Neural Networks (GNN), the expressive power of models has been studied through their capability to distinguish if two given graphs are isomorphic or not. Since the graph isomorphism problem is NP-intermediate, and Weisfeiler-Lehman (WL) test can give sufficient but not enough evidence in polynomial time, the theoretical power of GNNs is usually evaluated by the equivalence of WL-test order, followed by an empirical analysis of the models on some reference inductive and transductive datasets. However, such analysis does not account the signal processing pipeline, whose capability is generally evaluated in the spectral domain. In this paper, we argue that a spectral analysis of GNNs behavior can provide a complementary point of view to go one step further in the understanding of GNNs. By bridging the gap between the spectral and spatial design of graph convolutions, we theoretically demonstrate some equivalence of the graph convolution process regardless it is designed in the spatial or the spectral domain. Using this connection, we managed to re-formulate most of the state-of-the-art graph neural networks into one common framework. This general framework allows to lead a spectral analysis of the most popular GNNs, explaining their performance and showing their limits according to spectral point of view. Our theoretical spectral analysis is confirmed by experiments on various graph databases. Furthermore, we demonstrate the necessity of high and/or band-pass filters on a graph dataset, while the majority of GNN is limited to only low-pass and inevitably it fails.",/pdf/859c9ee357c81e0b9a1cb989b1e23b8b42d741f1.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Analyzing+the+Expressive+Power+of+Graph+Neural+Networks+in+a+Spectral+Perspective
Machine Learning Algorithms for Data Labeling: An Empirical Evaluation,2021,ICLR,"['Teodor Anders Fredriksson', 'David Issa Mattos', 'Jan Bosch', 'Helena Holmström Olsson']",poster,"['Data Labeling', 'Empirical Evaluation', 'Active Machine Learning', 'Semi-Supervised Learning']","The lack of labeled data is a major problem in both research and industrial settings since obtaining labels is often an expensive and time-consuming activity. In the past years, several machine learning algorithms were developed to assist and perform automated labeling in partially labeled datasets. While many of these algorithms are available in open-source packages, there is no research that investigates how these algorithms compare to each other in different types of datasets and with different percentages of available labels. To address this problem, this paper empirically evaluates and compares seven algorithms for automated labeling in terms of accuracy. We investigate how these algorithms perform in six different and well-known datasets with three different types of data, images, texts, and numerical values. We evaluate these algorithms under two different experimental conditions, with 10\% and 50\% labels of available labels in the dataset. Each algorithm, in each dataset for each experimental condition, is evaluated independently ten times with different random seeds. The results are analyzed and the algorithms are compared utilizing a Bayesian Bradley-Terry model. The results indicate that while the algorithms label spreading with K-nearest neighbors perform better in the aggregated results, the active learning algorithms query by instance QBC and query instance uncertainty sample perform better when there is only 10\% of labels available. These results can help machine learning practitioners in choosing optimal machine learning algorithms to label their data.",/pdf/d266ae12758df4ddd0f30d71e6236ea9cd097cc8.pdf,bayesian;active learning;llm,https://scholar.google.com/scholar?q=Machine+Learning+Algorithms+for+Data+Labeling:+An+Empirical+Evaluation
On the Geometry of Deep Bayesian Active Learning,2021,ICLR,"['Xiaofeng Cao', 'Ivor Tsang']",poster,"['Bayesian active learning', 'geometric interpretation', 'core-set construction', 'model uncertainty', 'ellipsoid.']","We present  geometric Bayesian active learning by disagreements (GBALD), a framework that performs BALD on its geometric interpretation interacting with a deep learning model. There are two main components in GBALD: initial acquisitions based on core-set construction and model uncertainty estimation with those initial acquisitions. Our key innovation is to construct the core-set on an ellipsoid, not typical sphere, preventing its updates towards the boundary regions of the distributions. Main improvements over BALD are twofold: relieving sensitivity to uninformative prior and reducing redundant information of model uncertainty. To guarantee the improvements, our generalization analysis proves that, compared to typical Bayesian  spherical interpretation, geodesic search with ellipsoid can derive a tighter lower error bound and achieve higher probability to obtain  a nearly zero error. Experiments on acquisitions with several scenarios demonstrate that, yielding slight perturbations to noisy and repeated samples,  GBALD further achieves significant accuracy improvements  than BALD, BatchBALD and other baselines.",/pdf/21b9750a6e5f0a437e44a3f5d7426ae0dc6450b0.pdf,zero_few-shot;bayesian;metric;active learning;llm,https://scholar.google.com/scholar?q=On+the+Geometry+of+Deep+Bayesian+Active+Learning
Gauge Equivariant Mesh CNNs: Anisotropic convolutions on geometric graphs,2021,ICLR,"['Pim De Haan', 'Maurice Weiler', 'Taco Cohen', 'Max Welling']",poster,"['symmetry', 'equivariance', 'mesh', 'geometric', 'convolution']","A common approach to define convolutions on meshes is to interpret them as a graph and apply graph convolutional networks (GCNs).  Such GCNs utilize isotropic kernels and are therefore insensitive to the relative orientation of vertices and thus to the geometry of the mesh as a whole. We propose Gauge Equivariant Mesh CNNs which generalize GCNs to apply anisotropic gauge equivariant kernels. Since the resulting features carry orientation information, we introduce a geometric message passing scheme defined by parallel transporting features over mesh edges. Our experiments validate the significantly improved expressivity of the proposed model over conventional GCNs and other methods.",/pdf/8892729ba223ed386ef20a476c7f878d8329e7b0.pdf,graph;metric;llm,https://scholar.google.com/scholar?q=Gauge+Equivariant+Mesh+CNNs:+Anisotropic+convolutions+on+geometric+graphs
Learning Mesh-Based Simulation with Graph Networks,2021,ICLR,"['Tobias Pfaff', 'Meire Fortunato', 'Alvaro Sanchez-Gonzalez', 'Peter Battaglia']",poster,"['graph networks', 'simulation', 'mesh', 'physics']","Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied.
Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks.",/pdf/25e22a812f559c7389d64412f32a87195fb7acbb.pdf,graph;representation;llm,https://scholar.google.com/scholar?q=Learning+Mesh-Based+Simulation+with+Graph+Networks
Adversarial Deep Metric Learning,2021,ICLR,"['Thomas Kobber Panum', 'Zi Wang', 'Pengyu Kan', 'Earlence Fernandes', 'Somesh Jha']",poster,"['Deep metric learning', 'adversarial robustness', 'adversarial examples', 'adversarial perturbations', 'adversarial training']","Learning a distance metric between pairs of examples is widely important for various tasks. Deep Metric Learning (DML) utilizes deep neural network architectures to learn semantic feature embeddings where the distance between similar examples is close and dissimilar examples are far. While the underlying neural networks produce good accuracy on naturally occurring samples, they are vulnerable to adversarially-perturbed samples that can reduce their accuracy. To create robust versions of DML models, we introduce a robust training approach. A key challenge is that metric losses are not independent --- they depend on all samples in a mini-batch.  This sensitivity to samples, if not accounted for, can lead to incorrect robust training.  To the best of our knowledge, we are the first to systematically analyze this dependence effect and propose a principled approach for robust training of deep metric learning networks that accounts for the nuances of metric losses. Using experiments on three popular datasets in metric learning, we demonstrate the DML models trained using our techniques display robustness against strong iterative attacks while their performance on unperturbed (natural) samples remains largely unaffected. ",/pdf/ba6fe00a78f894e68afe32cfea7e90389d263ad0.pdf,metric;llm,https://scholar.google.com/scholar?q=Adversarial+Deep+Metric+Learning
Neural Architecture Search without Training,2021,ICLR,"['Joseph Mellor', 'Jack Turner', 'Amos Storkey', 'Elliot J. Crowley']",poster,"['NAS', 'efficiency', 'search', 'fast', 'cheap', 'convnets']","The time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be remedied if we could infer a network's trained accuracy from its initial state. In this work, we examine the correlation of linear maps induced by augmented versions of a single image in untrained networks and motivate how this can be used to give a measure which is highly indicative of a network’s trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU, and verify its effectiveness on NAS-Bench-101 and NAS-Bench-201. Finally, we show that our approach can be readily combined with more expensive search methods for added value: we modify regularised evolutionary search to produce a novel algorithm that outperforms its predecessor.",/pdf/f2d547de8deea43a314207b20c471587da80ba2f.pdf,zero_few-shot;augmentation;llm,https://scholar.google.com/scholar?q=Neural+Architecture+Search+without+Training
Genetic Soft Updates for Policy Evolution in Deep Reinforcement Learning,2021,ICLR,"['Enrico Marchesini', 'Davide Corsi', 'Alessandro Farinelli']",poster,"['Deep Reinforcement Learning', 'Evolutionary Algorithms', 'Formal Verification', 'Machine Learning for Robotics']","The combination of Evolutionary Algorithms (EAs) and Deep Reinforcement Learning (DRL) has been recently proposed to merge the benefits of both solutions. Existing mixed approaches, however, have been successfully applied only to actor-critic methods and present significant overhead. We address these issues by introducing a novel mixed framework that exploits a periodical genetic evaluation to soft update the weights of a DRL agent. The resulting approach is applicable with any DRL method and, in a worst-case scenario, it does not exhibit detrimental behaviours. Experiments in robotic applications and continuous control benchmarks demonstrate the versatility of our approach that significantly outperforms prior DRL, EAs, and mixed approaches. Finally, we employ formal verification to confirm the policy improvement, mitigating the inefficient exploration and hyper-parameter sensitivity of DRL.ment, mitigating the inefficient exploration and hyper-parameter sensitivity of DRL.",/pdf/2a012533ff0b6880941f619b1e03b63abd1414c6.pdf,reinforcement learning;llm,https://scholar.google.com/scholar?q=Genetic+Soft+Updates+for+Policy+Evolution+in+Deep+Reinforcement+Learning
CaLFADS: latent factor analysis of dynamical systems in calcium imaging data,2021,ICLR,"['Luke Yuri Prince', 'Shahab Bakhtiari', 'Colleen J Gillon', 'Blake Aaron Richards']",poster,"['latent variable modelling', 'lfads', 'neuroscience', 'variational autoencoders', 'dynamical systems', 'calcium imaging', 'neural data analysis']","Dynamic latent variable modelling has been a hugely powerful tool in understanding how spiking activity in populations of neurons can perform computations necessary for adaptive behaviour. The success of such approaches has been enabled by the ability to construct models derived with the characterization of spiking activity as point-processes since spiking dynamics occur on a much faster time-scale than the computational dynamics being inferred. Other experimental techniques, such as calcium imaging, pose a problem for latent variable modelling of computational dynamics, since the time-scales of calcium dynamics and computational dynamics overlap. As such, the success of dynamic latent variable modelling in calcium imaging data rests on being able to disentangle the contribution of these two sources of variation. Here we extend recent advances using variational autoencoders to analyze neural data, by incorporating a ladder architecture that can infer a hierarchy of dynamical systems. Using built-in inductive biases for calcium dynamics, we can capture calcium flux as well as underlying dynamics of neural computation. First, we demonstrate with synthetic calcium data that we can correctly infer an underlying Lorenz attractor at the same time as calcium dynamics. Next, we show that we can infer appropriate rotational dynamics in spiking data from macaque motor cortex after it has been converted into calcium fluorescence data via a calcium dynamics model. Finally, we show that our method applied to real calcium imaging data from primary visual cortex in mice allows us to infer latent factors that carry salient sensory information about unexpected stimuli. These results demonstrate that variational ladder autoencoders are a promising approach for inferring hierarchical dynamics in experimental settings where the measured variable has its own slow dynamics, such as calcium imaging data, thereby providing the neuroscience community with a new analysis tool for a wider array of data modalities.",/pdf/54ffcc0face8eeb4852913bc6fb91057211f146d.pdf,graph;zero_few-shot;vae;adaptive;llm,https://scholar.google.com/scholar?q=CaLFADS:+latent+factor+analysis+of+dynamical+systems+in+calcium+imaging+data
Linear Mode Connectivity in Multitask and Continual Learning,2021,ICLR,"['Seyed Iman Mirzadeh', 'Mehrdad Farajtabar', 'Dilan Gorur', 'Razvan Pascanu', 'Hassan Ghasemzadeh']",poster,"['continual learning', 'catastrophic forgetting', 'mode connectivity', 'multitask learning']","Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. 
    However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? 
    Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process.
    Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution.  We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks.",/pdf/258e0f0ad7124932b50cc607ded20cd020bfccf8.pdf,optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Linear+Mode+Connectivity+in+Multitask+and+Continual+Learning
Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity,2021,ICLR,"['JangHyun Kim', 'Wonho Choo', 'Hosan Jeong', 'Hyun Oh Song']",poster,"['Data Augmentation', 'Deep Learning', 'Supervised Learning', 'Discrete Optimization']","While deep neural networks show great performance on fitting to the training distribution, improving the networks' generalization performance to the test distribution and robustness to the sensitivity to input perturbations still remain as a challenge. Although a number of mixup based augmentation strategies have been proposed to partially address them, it remains unclear as to how to best utilize the supervisory signal within each input data for mixup from the optimization perspective. We propose a new perspective on batch mixup and formulate the optimal construction of a batch of mixup data maximizing the data saliency measure of each individual mixup data and encouraging the supermodular diversity among the constructed mixup data. This leads to a novel discrete optimization problem minimizing the difference between submodular functions. We also propose an efficient modular approximation based iterative submodular minimization algorithm for efficient mixup computation per each minibatch suitable for minibatch based neural network training. Our experiments show the proposed method achieves the state of the art generalization, calibration, and weakly supervised localization results compared to other mixup methods. The source code is available at https://github.com/snu-mllab/Co-Mixup.",/pdf/199727fa0bf10af61eddaed5e0c98ee9bbcfdea3.pdf,graph;optimization;zero_few-shot;augmentation;llm,https://scholar.google.com/scholar?q=Co-Mixup:+Saliency+Guided+Joint+Mixup+with+Supermodular+Diversity
Sequence Metric Learning as Synchronization of Recurrent Neural Networks,2021,ICLR,"['Paul Compagnon', 'Grégoire Lefebvre', 'Stefan Duffner', 'Christophe Garcia']",poster,"['Metric learning', 'sequence processing', 'siamese recurrent neural network', 'dynamical systems']","Sequence metric learning is becoming a widely adopted approach for various applications dealing with sequential multi-variate data such as activity recognition or natural language processing and is most of the time tackled with sequence alignment approaches or representation learning. 
In this paper, we propose to study this subject from the point of view of dynamical system theory by drawing the analogy between synchronized trajectories produced by dynamical systems and the distance between similar sequences processed by a siamese recurrent neural network. 
Indeed, a siamese recurrent network comprises two identical sub-networks, two identical dynamical systems which can theoretically achieve complete synchronization if a coupling is introduced between them. 
We therefore propose a new neural network model that implements this coupling with a new gate integrated into the classical Gated Recurrent Unit architecture. This model is thus able to simultaneously learn a similarity metric and the synchronization of unaligned multi-variate sequences in a weakly supervised way.
Our experiments show that introducing such a coupling improves the performance of the siamese Gated Recurrent Unit architecture on an activity recognition dataset. ",/pdf/8bf2d1729e4cad904fdd23b2e92ceeea2ef8f903.pdf,zero_few-shot;representation;metric;multimodal;llm,https://scholar.google.com/scholar?q=Sequence+Metric+Learning+as+Synchronization+of+Recurrent+Neural+Networks
MARS: Markov Molecular Sampling for Multi-objective Drug Discovery,2021,ICLR,"['Yutong Xie', 'Chence Shi', 'Hao Zhou', 'Yuwei Yang', 'Weinan Zhang', 'Yong Yu', 'Lei Li']",poster,"['drug discovery', 'molecular graph generation', 'MCMC sampling']","Searching for novel molecules with desired chemical properties is crucial in drug discovery. Existing work focuses on developing neural models to generate either molecular sequences or chemical graphs. However, it remains a big challenge to find novel and diverse compounds satisfying several properties. In this paper, we propose MARS, a method for multi-objective drug molecule discovery. MARS is based on the idea of generating the chemical candidates by iteratively editing fragments of molecular graphs. To search for high-quality candidates, it employs Markov chain Monte Carlo sampling (MCMC) on molecules with an annealing scheme and an adaptive proposal. To further improve sample efficiency, MARS uses a graph neural network (GNN) to represent and select candidate edits, where the GNN is trained on-the-fly with samples from MCMC. Experiments show that MARS achieves state-of-the-art performance in various multi-objective settings where molecular bio-activity, drug-likeness, and synthesizability are considered. Remarkably, in the most challenging setting where all four objectives are simultaneously optimized, our approach outperforms previous methods significantly in comprehensive evaluations. The code is available at https://github.com/yutxie/mars.",/pdf/4f5a198cf9191eebd5788dea1fd15fcb151d8ef9.pdf,graph;adaptive;llm,https://scholar.google.com/scholar?q=MARS:+Markov+Molecular+Sampling+for+Multi-objective+Drug+Discovery
The Emergence of Individuality in Multi-Agent Reinforcement Learning,2021,ICLR,"['Jiechuan Jiang', 'Zongqing Lu']",poster,[],"Individuality is essential in human society, which induces the division of labor and thus improves the efficiency and productivity. Similarly, it should also be a key to multi-agent cooperation. Inspired by that individuality is of being an individual separate from others, we propose a simple yet efficient method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). EOI learns a probabilistic classifier that predicts a probability distribution over agents given their observation and gives each agent an intrinsic reward of being correctly predicted by the classifier. The intrinsic reward encourages the agents to visit their own familiar observations, and learning the classifier by such observations makes the intrinsic reward signals stronger and in turn makes the agents more identifiable. To further enhance the intrinsic reward and promote the emergence of individuality, two regularizers are proposed to increase the discriminability of the classifier. We implement EOI on top of popular MARL algorithms. Empirically, we show that EOI outperforms existing methods in a variety of multi-agent cooperative scenarios.",/pdf/41daabb792b66e7f5890b043219257882e5dd716.pdf,reinforcement learning;multi-agent;llm,https://scholar.google.com/scholar?q=The+Emergence+of+Individuality+in+Multi-Agent+Reinforcement+Learning
SkipW: Resource Adaptable RNN with Strict Upper Computational Limit,2021,ICLR,"['Tsiry Mayet', 'Anne Lambert', 'Pascal Leguyadec', 'Francoise Le Bolzer', 'François Schnitzler']",poster,"['Recurrent neural networks', 'Flexibility', 'Computational resources']","We introduce Skip-Window, a method to allow recurrent neural networks (RNNs) to trade off accuracy for computational cost during the analysis of a sequence. Similarly to existing approaches, Skip-Window extends existing RNN cells by adding a mechanism to encourage the model to process fewer inputs. Unlike existing approaches, Skip-Window is able to respect a strict computational budget, making this model more suitable for limited hardware. We evaluate this approach on two datasets: a human activity recognition task and adding task. Our results show that Skip-Window is able to exceed the accuracy of existing approaches for a lower computational cost while strictly limiting said cost.",/pdf/6c45c14eaa50cfd7a61ea01da21211148f40eccf.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=SkipW:+Resource+Adaptable+RNN+with+Strict+Upper+Computational+Limit
Selective Classification Can Magnify Disparities Across Groups,2021,ICLR,"['Erik Jones', 'Shiori Sagawa', 'Pang Wei Koh', 'Ananya Kumar', 'Percy Liang']",poster,"['selective classification', 'group disparities', 'log-concavity', 'robustness']","Selective classification, in which models can abstain on uncertain predictions, is a natural approach to improving accuracy in settings where errors are costly but abstentions are manageable. In this paper, we find that while selective classification can improve average accuracies, it can simultaneously magnify existing accuracy disparities between various groups within a population, especially in the presence of spurious correlations. We observe this behavior consistently across five vision and NLP datasets. Surprisingly, increasing abstentions can even decrease accuracies on some groups. To better understand this phenomenon, we study the margin distribution, which captures the model’s confidences over all predictions. For symmetric margin distributions, we prove that whether selective classification monotonically improves or worsens accuracy is fully determined by the accuracy at full coverage (i.e., without any abstentions) and whether the distribution satisfies a property we call left-log-concavity. Our analysis also shows that selective classification tends to magnify full-coverage accuracy disparities. Motivated by our analysis, we train distributionally-robust models that achieve similar full-coverage accuracies across groups and show that selective classification uniformly improves each group on these models. Altogether, our results suggest that selective classification should be used with care and underscore the importance of training models to perform equally well across groups at full coverage.",/pdf/b9ac6534faf7141a9138e3cfcfed7dbada0a6f36.pdf,graph;metric;llm,https://scholar.google.com/scholar?q=Selective+Classification+Can+Magnify+Disparities+Across+Groups
Sample-Efficient Automated Deep Reinforcement Learning,2021,ICLR,"['Jörg K.H. Franke', 'Gregor Koehler', 'André Biedenkapp', 'Frank Hutter']",poster,"['AutoRL', 'Deep Reinforcement Learning', 'Hyperparameter Optimization', 'Neuroevolution']","Despite significant progress in challenging problems across various domains, applying state-of-the-art deep reinforcement learning (RL) algorithms remains challenging due to their sensitivity to the choice of hyperparameters. This sensitivity can partly be attributed to the non-stationarity of the RL problem, potentially requiring different hyperparameter settings at various stages of the learning process. Additionally, in the RL setting, hyperparameter optimization (HPO) requires a large number of environment interactions, hindering the transfer of the successes in RL to real-world applications. In this work, we tackle the issues of sample-efficient and dynamic HPO in RL. We propose a population-based automated RL (AutoRL) framework to meta-optimize arbitrary off-policy RL algorithms. In this framework, we optimize the hyperparameters and also the neural architecture while simultaneously training the agent. By sharing the collected experience across the population, we substantially increase the sample efficiency of the meta-optimization. We demonstrate the capabilities of our sample-efficient AutoRL approach in a case study with the popular TD3 algorithm in the MuJoCo benchmark suite, where we reduce the number of environment interactions needed for meta-optimization by up to an order of magnitude compared to population-based training.",/pdf/50e735ee784190b4976fe22036a75b2ac2feee2b.pdf,reinforcement learning;graph;optimization;meta-learning;transfer learning;llm,https://scholar.google.com/scholar?q=Sample-Efficient+Automated+Deep+Reinforcement+Learning
H-divergence: A Decision-Theoretic Probability Discrepancy Measure ,2021,ICLR,"['Shengjia Zhao', 'Abhishek Sinha', 'Yutong He', 'Aidan Perreault', 'Jiaming Song', 'Stefano Ermon']",poster,"['probability divergence', 'two sample test', 'maximum mean discrepancy']","Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. Based on ideas from decision theory, we investigate a new class of discrepancies that are based on the optimal decision loss. Two probability distributions are different if the optimal decision loss is higher on the mixture distribution than on each individual distribution. We show that this generalizes popular notions of discrepancy measurements such as the Jensen Shannon divergence and the maximum mean discrepancy. We apply our approach to two-sample tests, which evaluates whether two sets of samples come from the same distribution. On various benchmark and real datasets, we demonstrate that tests based on our generalized notion of discrepancy is able to achieve superior test power. We also apply our approach to sample quality evaluation as an alternative to the FID score, and to understanding the effects of climate change on different social and economic activities.",/pdf/29ba5a99028c5547427fa70d85282376463a9d0c.pdf,llm,https://scholar.google.com/scholar?q=H-divergence:+A+Decision-Theoretic+Probability+Discrepancy+Measure+
Dynamic Feature Selection for Efficient and Interpretable Human Activity Recognition,2021,ICLR,"['Randy Ardywibowo', 'Shahin Boluki', 'Zhangyang Wang', 'Bobak J Mortazavi', 'Shuai Huang', 'Xiaoning Qian']",poster,"['dynamic feature selection', 'human activity recognition', 'sparse monitoring']","In many machine learning tasks, input features with varying degrees of predictive capability are usually acquired at some cost. For example, in human activity recognition (HAR) and mobile health (mHealth) applications, monitoring performance should be achieved with a low cost to gather different sensory features, as maintaining sensors incur monetary, computation, and energy cost. We propose an adaptive feature selection method that dynamically selects features for prediction at any given time point. We formulate this problem as an $\ell_0$ minimization problem across time, and cast the combinatorial optimization problem into a stochastic optimization formulation. We then utilize a differentiable relaxation to make the problem amenable to gradient-based optimization. Our evaluations on four activity recognition datasets show that our method achieves a favorable trade-off between performance and the number of features used. Moreover, the dynamically selected features of our approach are shown to be interpretable and associated with the actual activity types.",/pdf/90ae614f77bddbf5e9f886151a7fe2c54ddf1cdb.pdf,optimization;zero_few-shot;adaptive;llm,https://scholar.google.com/scholar?q=Dynamic+Feature+Selection+for+Efficient+and+Interpretable+Human+Activity+Recognition
Deep Positive Unlabeled Learning with a Sequential Bias,2021,ICLR,"['Walter Gerych', 'Thomas Hartvigsen', 'Luke Buquicchio', 'Kavin Chandrasekaran', 'Hamid Mansoor', 'Abdulaziz alajaji']",poster,[],"For many domains, from video stream analytics to human activity recognition, only weakly-labeled datasets are available.
Worse yet, the given labels are often assigned sequentially, resulting in sequential bias. Current Positive Unlabeled (PU) classifiers, a state-of-the-art family of robust semi-supervised methods, are ineffective under sequential bias. In this work, we  propose DeepSPU, the first method to address this sequential bias problem. DeepSPU tackles the two interdependent subproblems of learning both the latent labeling process and the true class likelihoods within one architecture. We achieve this by developing a novel iterative learning strategy aided by theoretically-justified cost terms to avoid collapsing into a naive classifier. Our experimental studies demonstrate that DeepSPU outperforms state-of-the-art methods by over 10% on diverse real-world datasets.",/pdf/2cee04205a5b4a971d091ef558472a52dac5ac41.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Deep+Positive+Unlabeled+Learning+with+a+Sequential+Bias
Uniform-Precision Neural Network Quantization via Neural Channel Expansion,2021,ICLR,"['Seongmin Park', 'Beomseok Kwon', 'Kyuyoung Sim', 'Jieun Lim', 'Tae-Ho Kim', 'Jungwook Choi']",poster,"['deep neural network', 'quantization', 'neural architecture search', 'image classification', 'reduced precision', 'inference']","Uniform-precision neural network quantization has gained popularity thanks to its simple arithmetic unit densely packed for high computing capability. However, it ignores heterogeneous sensitivity to the impact of quantization across the layers, resulting in sub-optimal inference accuracy. This work proposes a novel approach to adjust the network structure to alleviate the impact of uniform-precision quantization. The proposed neural architecture search selectively expands channels for the quantization sensitive layers while satisfying hardware constraints (e.g., FLOPs). We provide substantial insights and empirical evidence that the proposed search method called neural channel expansion can adapt several popular networks' channels to achieve superior 2-bit quantization accuracy on CIFAR10 and ImageNet. In particular, we demonstrate the best-to-date Top-1/Top-5 accuracy for 2-bit ResNet50 with smaller FLOPs and the parameter size.",/pdf/8e122d62bcbf69f1d3f574a691a55fefa5a193fd.pdf,optimization;inference;llm,https://scholar.google.com/scholar?q=Uniform-Precision+Neural+Network+Quantization+via+Neural+Channel+Expansion
Driving through the Lens: Improving Generalization of Learning-based Steering using Simulated Adversarial Examples,2021,ICLR,"['Yu Shen', 'Laura Yu Zheng', 'Manli Shu', 'Weizi Li', 'Tom Goldstein', 'Ming Lin']",poster,"['model robustness', 'data augmentation', 'adversarial training', 'image quality', 'autonomous driving', 'benchmark']","To ensure the wide adoption and safety of autonomous driving, the vehicles need to be able to drive under various lighting, weather, and visibility conditions in different environments. These external and environmental factors, along with internal factors associated with sensors, can pose significant challenges to perceptual data processing, hence affecting the decision-making of the vehicle. In this work, we address this critical issue by analyzing the sensitivity of the learning algorithm with respect to varying quality in the image input for autonomous driving.  Using the results of sensitivity analysis, we further propose an algorithm to improve the overall performance of the task of ``learning to steer''. The results show that our approach is able to enhance the learning outcomes up to 48%. A comparative study drawn between our approach and other related techniques, such as data augmentation and adversarial training, confirms the effectiveness of our algorithm as a way to improve the robustness and generalization of neural network training for self-driving cars.    ",/pdf/9a403699c14be05cc7f687b534bca4f1e8503111.pdf,augmentation;llm,https://scholar.google.com/scholar?q=Driving+through+the+Lens:+Improving+Generalization+of+Learning-based+Steering+using+Simulated+Adversarial+Examples
Robust Overfitting may be mitigated by properly learned smoothening,2021,ICLR,"['Tianlong Chen', 'Zhenyu Zhang', 'Sijia Liu', 'Shiyu Chang', 'Zhangyang Wang']",poster,"['Robust Overfitting', 'Adversarial Training', 'Adversarial Robustness']","A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by $3.72\%\sim6.68\%$ and robust accuracy by $0.22\%\sim2 .03\%$, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ($\ell_{\infty}$ and $\ell_2$), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.",/pdf/099a32f12e88483a4451fe099750daeb8a1a0128.pdf,graph;transfer learning;distillation;llm,https://scholar.google.com/scholar?q=Robust+Overfitting+may+be+mitigated+by+properly+learned+smoothening
Predicting Infectiousness for Proactive Contact Tracing,2021,ICLR,"['Yoshua Bengio', 'Prateek Gupta', 'Tegan Maharaj', 'Nasim Rahaman', 'Martin Weiss', 'Tristan Deleu', 'Eilif Benjamin Muller', 'Meng Qu', 'victor schmidt', 'Pierre-Luc St-Charles', 'hannah alsdurf', 'Olexa Bilaniuk', 'david buckeridge', 'gaetan caron', 'pierre luc carrier', 'Joumana Ghosn', 'satya ortiz gagne', 'Christopher Pal', 'Irina Rish', 'Bernhard Schölkopf', 'abhinav sharma', 'Jian Tang', 'andrew williams']",poster,"['covid-19', 'contact tracing', 'distributed inference', 'set transformer', 'deepset', 'epidemiology', 'applications', 'domain randomization', 'retraining', 'simulation']","The COVID-19 pandemic has spread rapidly worldwide, overwhelming manual contact tracing in many countries and resulting in widespread lockdowns for emergency containment. Large-scale digital contact tracing (DCT) has emerged as a potential solution to resume economic and social activity while minimizing spread of the virus. Various DCT methods have been proposed, each making trade-offs be-tween privacy, mobility restrictions, and public health. The most common approach, binary contact tracing (BCT), models infection as a binary event, informed only by an individual’s test results, with corresponding binary recommendations that either all or none of the individual’s contacts quarantine. BCT ignores the inherent uncertainty in contacts and the infection process, which could be used to tailor messaging to high-risk individuals, and prompt proactive testing or earlier warnings. It also does not make use of observations such as symptoms or pre-existing medical conditions, which could be used to make more accurate infectiousness predictions. In this paper, we use a recently-proposed COVID-19 epidemiological simulator to develop and test methods that can be deployed to a smartphone to locally and proactively predict an individual’s infectiousness (risk of infecting others) based on their contact history and other information, while respecting strong privacy constraints. Predictions are used to provide personalized recommendations to the individual via an app, as well as to send anonymized messages to the individual’s contacts, who use this information to better predict their own infectiousness, an approach we call proactive contact tracing (PCT). Similarly to other works, we find that compared to no tracing, all DCT methods tested are able to reduce spread of the disease and thus save lives, even at low adoption rates, strongly supporting a role for DCT methods in managing the pandemic. Further, we find a deep-learning based PCT method which improves over BCT for equivalent average mobility, suggesting PCT could help in safe re-opening and second-wave prevention.",/pdf/77dc30fa1394d3f1fb24031ef8b391fa4d51c35d.pdf,graph;optimization;zero_few-shot;active learning;llm,https://scholar.google.com/scholar?q=Predicting+Infectiousness+for+Proactive+Contact+Tracing
Emergent Properties of Foveated Perceptual Systems,2021,ICLR,"['Arturo Deza', 'Talia Konkle']",poster,"['Hybrid Perceptual Systems', 'Foveation', 'Visual Crowding', 'Texture', 'Two-stage models']","We introduce foveated perceptual systems -- a hybrid architecture inspired by human vision, to explore the role of a \textit{texture-based} foveation stage on the nature and robustness of subsequently learned visual representation in machines. Specifically, these two-stage perceptual systems first foveate an image, inducing a texture-like encoding of peripheral information -- mimicking the effects of \textit{visual crowding} --  which is then relayed through a convolutional neural network (CNN) trained to perform scene categorization. We find that these foveated perceptual systems learn a visual representation that is \textit{distinct} from their non-foveated counterpart through experiments that probe: 1) i.i.d and o.o.d generalization; 2) robustness to occlusion; 3) a center image bias; and 4) high spatial frequency sensitivity. In addition, we examined the impact of this foveation transform with respect to two additional models derived with a rate-distortion optimization procedure to compute matched-resource systems: a lower resolution non-foveated system, and a foveated system with adaptive Gaussian blurring. The properties of greater i.i.d generalization, high spatial frequency sensitivity, and robustness to occlusion emerged exclusively in our foveated texture-based models, independent of network architecture and learning dynamics. Altogether, these results demonstrate that foveation -- via peripheral texture-based computations -- yields a distinct and robust representational format of scene information relative to standard machine vision approaches, and also provides symbiotic computational support that texture-based peripheral encoding has important representational consequences for processing in the human visual system.
",/pdf/84553251ad9dc326331c61668f0d2e189bed0eee.pdf,optimization;zero_few-shot;representation;adaptive;llm,https://scholar.google.com/scholar?q=Emergent+Properties+of+Foveated+Perceptual+Systems
Adaptive Automotive Radar data Acquisition,2021,ICLR,"['Madhumitha Sakthi', 'Ahmed Tewfik']",poster,"['Compressed Sensing', 'Adaptive acquisition', 'object detection']","In an autonomous driving scenario, it is vital to acquire and efficiently process data from various sensors to obtain a complete and robust perspective of the surroundings. Many studies have shown the importance of having radar data in addition to images since radar improves object detection performance. We develop a novel algorithm motivated by the hypothesis that with a limited sampling budget, allocating more sampling budget to areas with the object as opposed to a uniform sampling budget ultimately improves relevant object detection and classification. In order to identify the areas with objects, we develop an algorithm to process the object detection results from the Faster R-CNN object detection algorithm and the previous radar frame and use these as prior information to adaptively allocate more bits to areas in the scene that may contain relevant objects. We use previous radar frame information to mitigate the potential information loss of an object missed by the image or the object detection network. Also, in our algorithm, the error of missing relevant information in the current frame due to the limited budget sampling of the previous radar frame did not propagate across frames. We also develop an end-to-end transformer-based 2D object detection network using the NuScenes radar and image data. Finally, we compare the performance of our algorithm against that of standard CS and adaptive CS using radar on the Oxford Radar RobotCar dataset.",/pdf/5c2e49d2bb0d47b81396f0627b1e6f5454c5355e.pdf,transformer;adaptive;llm,https://scholar.google.com/scholar?q=Adaptive+Automotive+Radar+data+Acquisition
GANs Can Play Lottery Tickets Too,2021,ICLR,"['Xuxi Chen', 'Zhenyu Zhang', 'Yongduo Sui', 'Tianlong Chen']",poster,"['lottery tickets', 'GAN compression', 'generative adversarial networks']","Deep generative adversarial networks (GANs) have gained growing popularity in numerous scenarios, while usually suffer from high parameter complexities for resource-constrained real-world applications. However, the compression of GANs has less been explored. A few works show that heuristically applying compression techniques normally leads to unsatisfactory results, due to the notorious training instability of GANs. In parallel, the lottery ticket hypothesis shows prevailing success on discriminative models, in locating sparse matching subnetworks capable of training in isolation to full model performance. In this work, we for the first time study the existence of such trainable matching subnetworks in deep GANs. For a range of GANs, we certainly find matching subnetworks at $67\%$-$74\%$ sparsity. We observe that with or without pruning discriminator has a minor effect on the existence and quality of matching subnetworks, while the initialization weights used in the discriminator plays a significant role. We then show the powerful transferability of these subnetworks to unseen tasks. Furthermore, extensive experimental results demonstrate that our found subnetworks substantially outperform previous state-of-the-art GAN compression approaches in both image generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN). Codes available at https://github.com/VITA-Group/GAN-LTH.",/pdf/f9f13cd41ac8fcc30b5177eac267e3a61229f0e4.pdf,optimization;generative model;sparse;transfer learning;llm,https://scholar.google.com/scholar?q=GANs+Can+Play+Lottery+Tickets+Too
Representational correlates of hierarchical phrase structure in deep language models,2021,ICLR,"['Matteo Alleman', 'Jonathan Mamou', 'Miguel A Del Rio', 'Hanlin Tang', 'Yoon Kim', 'SueYeon Chung']",poster,"['bertology', 'interpretability', 'computational neuroscience', 'population coding']","While contextual representations from Transformer-based architectures have set a new standard for many NLP tasks, there is not yet a complete accounting of their inner workings. In particular, it is not entirely clear what aspects of sentence-level syntax are captured by these representations, nor how (if at all) they are built along the stacked layers of the network. In this paper, we aim to address such questions with a general class of input perturbation-based analyses of representations from Transformer networks pretrained on self-supervised objectives. Importing from computational and cognitive neuroscience the notion of representational invariance, we perform a series of probes designed to test the sensitivity of Transformer representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. We experiment with three different perturbations: (1) random permutations of n-grams of varying width, to test the scale at which a representation is sensitive to word position; (2) swapping of two spans which do or do not form a syntactic phrase, to test sensitivity to global phrase structure; and (3) swapping of two adjacent words which do or do not break apart a syntactic phrase, to test sensitivity to local phrase structure. We also connect our probe results to the Transformer architecture by relating the attention mechanism to syntactic distance between two words. Results from the three probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. In particular, sensitivity to local phrase structure increases along deeper layers. Based on our analysis of attention, we show that this is at least partly explained by generally larger attention weights between syntactically distant words.",/pdf/8c924ae31a130e7245a197be4f5a3d54b5919adf.pdf,graph;zero_few-shot;transformer;representation;llm,https://scholar.google.com/scholar?q=Representational+correlates+of+hierarchical+phrase+structure+in+deep+language+models
Energy-based View of Retrosynthesis,2021,ICLR,"['Ruoxi Sun', 'Hanjun Dai', 'Li Li', 'Steven Kearnes', 'Bo Dai']",poster,"['Applications', 'Retrosynthesis', 'Energy-based Model']","Retrosynthesis—the process of identifying a set of reactants to synthesize a target molecule—is of vital importance to material design and drug discovery. Existing machine learning approaches based on language models and graph neural networks have achieved encouraging results. However, the inner connections of these models are rarely discussed, and rigorous evaluations of these models are largely in need. In this paper, we propose a framework that unifies sequence- and graph-based methods as energy-based models (EBMs) with different energy functions. This unified point of view establishes connections between different models and identifies the differences between them, thereby promoting the understanding of model design. We also provide a comprehensive assessment of performance to the community. Moreover, we present a novel “dual” variant within the framework that performs consistent training over Bayesian forward- and backward-prediction by constraining the agreement between the two directions. This model improves the state of the art for template-free approaches where the reaction type is unknown and known.
",/pdf/0977d7a1ee2b42b187578a7615184965951ffa0d.pdf,graph;optimization;bayesian;llm,https://scholar.google.com/scholar?q=Energy-based+View+of+Retrosynthesis
Feature Integration and Group Transformers for Action Proposal Generation,2021,ICLR,"['He-Yen Hsieh', 'Ding-Jie Chen', 'Tung-Ying Lee', 'Tyng-Luh Liu']",poster,"['temporal action proposal', 'transformer', 'video analysis']","The task of temporal action proposal generation (TAPG) aims to provide high-quality video segments, i.e., proposals that potentially contain action events. The performance of tackling the TAPG task heavily depends on two key issues, feature representation and scoring mechanism. To simultaneously take account of both aspects, we introduce an attention-based model, termed as FITS, to address the issues for retrieving high-quality proposals. We first propose a novel Feature-Integration (FI) module to seamlessly fuse two-stream features concerning their interaction to yield a robust video segment representation. We then design a group of Transformer-driven Scorers (TS) to gain the temporal contextual supports over the representations for estimating the starting or ending boundary of an action event. Unlike most previous work to estimate action boundaries without considering the long-range temporal neighborhood, the proposed action-boundary co-estimation mechanism in TS leverages the bi-directional contextual supports for such boundary estimation, which shows the advantage of removing several false-positive boundary predictions. We conduct experiments on two challenging datasets, ActivityNet-1.3 and THUMOS-14. The experimental results demonstrate that the proposed FITS model consistently outperforms state-of-the-art TAPG methods.",/pdf/26381e2fee8c5d2538331711398445fc682248bd.pdf,graph;transformer;representation;generative model;llm,https://scholar.google.com/scholar?q=Feature+Integration+and+Group+Transformers+for+Action+Proposal+Generation
GeDi: Generative Discriminator Guided Sequence Generation,2021,ICLR,"['Ben Krause', 'Akhilesh Deepak Gotmare', 'Bryan McCann', 'Nitish Shirish Keskar', 'Shafiq Joty', 'richard socher', 'Nazneen Rajani']",poster,"['Language modeling', 'controllable generation', 'decoding schemes', 'auto-regressive models', 'language modeling safety']","While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives controllability on par with or better than the state of the art method in a variety of settings, while also achieving generation speeds more than $30$ times faster. Additionally, training GeDi on only three topics allows us to controllably generate new topics zero-shot from just a keyword. Lastly, we show that GeDi can make GPT-2 and GPT-3 significantly less toxic without sacrificing on linguistic fluency, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.",/pdf/0c4f880e8e3b21cbb56ac4339f3811187cd5633d.pdf,zero_few-shot;generative model;bayesian;llm,https://scholar.google.com/scholar?q=GeDi:+Generative+Discriminator+Guided+Sequence+Generation
Layer-adaptive Sparsity for the Magnitude-based Pruning,2021,ICLR,"['Jaeho Lee', 'Sejun Park', 'Sangwoo Mo', 'Sungsoo Ahn', 'Jinwoo Shin']",poster,"['network pruning', 'layerwise sparsity', 'magnitude-based pruning']","Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.
Under various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.
Furthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity",/pdf/6c6e88f6354b6fb0bc2955ecb9e518ca2f65432f.pdf,adaptive;llm,https://scholar.google.com/scholar?q=Layer-adaptive+Sparsity+for+the+Magnitude-based+Pruning
A Truly Constant-time Distribution-aware Negative Sampling,2021,ICLR,"['Shabnam Daghaghi', 'Tharun Medini', 'Beidi Chen', 'Mengnan Zhao', 'Anshumali Shrivastava']",poster,[],"Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full-softmax is very expensive from the computational and energy perspective. There have been a variety of sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there does not exist any sampling scheme that is truly adaptive and also samples the negative classes in constant time every iteration. Therefore, alternative heuristics like random sampling,  static frequency-based sampling, or learning-based biased sampling; which primarily trade either the sampling cost or the adaptivity of samples per iteration, are adopted. In this paper, we show a class of distribution where the sampling scheme is truly adaptive and provably generates negative samples in constant time. We demonstrate a negative sampling implementation that is significantly faster, in terms of wall clock time, compared to the most optimized TensorFlow implementations of standard softmax or other sampling approaches on the best available GPUs (V100s).",/pdf/8ffda2ff56edd3ed2f80c0b18c71097a29636dc3.pdf,zero_few-shot;adaptive;flow;llm,https://scholar.google.com/scholar?q=A+Truly+Constant-time+Distribution-aware+Negative+Sampling
Deep Graph Neural Networks with Shallow Subgraph Samplers,2021,ICLR,"['Hanqing Zeng', 'Muhan Zhang', 'Yinglong Xia', 'Ajitesh Srivastava', 'Rajgopal Kannan', 'Viktor Prasanna', 'Long Jin', 'Andrey Malevich', 'Ren Chen']",poster,"['Graph Neural Networks', 'Graph Sampling', 'Network Embedding']","While Graph Neural Networks (GNNs) are powerful models for learning representations on graphs, most state-of-the-art models do not have significant accuracy gain beyond two to three layers. Deep GNNs fundamentally need to address: 1). expressivity challenge due to oversmoothing, and 2). computation challenge due to neighborhood explosion. We propose a simple ""deep GNN, shallow sampler"" design principle to improve both the GNN accuracy and efficiency --- to generate representation of a target node, we use a deep GNN to pass messages only within a shallow, localized subgraph. A properly sampled subgraph may exclude irrelevant or even noisy nodes, and still preserve the critical neighbor features and graph structures. The deep GNN then smooths the informative local signals to enhance feature learning, rather than oversmoothing the global graph signals into just ""white noise"". We theoretically justify why the combination of deep GNNs with shallow samplers yields the best learning performance. We then propose various sampling algorithms and neural architecture extensions to achieve good empirical results. Experiments on five large graphs show that our models achieve significantly higher accuracy and efficiency, compared with state-of-the-art. ",/pdf/c1fae404c251a6de535b46edbe7090759cabee71.pdf,graph;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Deep+Graph+Neural+Networks+with+Shallow+Subgraph+Samplers
Blind Pareto Fairness and Subgroup Robustness,2021,ICLR,"['Natalia Martinez', 'Martin Bertran', 'Afroditi Papadaki', 'Miguel R. D. Rodrigues', 'Guillermo Sapiro']",poster,"['fairness', 'fairness in machine learning', 'fairness without demographics', 'robustness', 'subgroup robustness', 'blind fairness', 'pareto fairness']","With the wide adoption of machine learning algorithms across various application domains, there is a growing interest in the fairness properties of such algorithms. The vast majority of the activity in the field of group fairness addresses disparities between predeﬁned groups based on protected features such as gender, age, and race, which need to be available at train, and often also at test, time. These approaches are static and retrospective, since algorithms designed to protect groups identified  a priori cannot anticipate and protect the needs of different at-risk groups in the future. In this work we analyze the space of solutions for worst-case fairness beyond demographics, and propose Blind Pareto Fairness (BPF), a method that leverages no-regret dynamics to recover a fair minimax classiﬁer that reduces worst-case risk of  any potential subgroup of sufﬁcient size, and guarantees that the remaining population receives the best possible level of service. BPF addresses  fairness beyond demographics, that is, it  does not rely on predeﬁned notions of at-risk groups, neither at train nor at test time. Our experimental results show that the proposed framework improves worst-case risk in multiple standard datasets, while simultaneously providing better levels of service for the remaining population, in comparison to competing methods.",/pdf/6fe40433443114bc2c892683832d52416321de54.pdf,graph;llm,https://scholar.google.com/scholar?q=Blind+Pareto+Fairness+and+Subgroup+Robustness
Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective,2021,ICLR,"['Wuyang Chen', 'Xinyu Gong', 'Zhangyang Wang']",poster,"['Neural Architecture Search', 'neural tangent kernel', 'number of linear regions']","Neural Architecture Search (NAS) has been explosively studied to automate the discovery of top-performer neural networks. Current works require heavy training of supernet or intensive architecture evaluations, thus suffering from heavy resource consumption and often incurring search bias due to truncated training or approximations. Can we select the best neural architectures without involving any training and eliminate a drastic portion of the search cost? 

We provide an affirmative answer, by proposing a novel framework called \textit{training-free neural architecture search} ($\textbf{TE-NAS}$). TE-NAS ranks architectures by analyzing the spectrum of the neural tangent kernel (NTK), and the number of linear regions in the input space. Both are motivated by recent theory advances in deep networks, and can be computed without any training. We show that: (1) these two measurements imply the $\textit{trainability}$ and $\textit{expressivity}$ of a neural network; and (2) they strongly correlate with the network's actual test accuracy. Further on, we design a pruning-based NAS mechanism to achieve a more flexible and superior trade-off between the trainability and expressivity during the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes high-quality search but only costs $\textbf{0.5}$ and $\textbf{4}$ GPU hours with one 1080Ti on CIFAR-10 and ImageNet, respectively. We hope our work to inspire more attempts in bridging between the theoretic findings of deep networks and practical impacts in real NAS applications.",/pdf/097fe3785855414961469f27465c798144ea4b9e.pdf,graph;llm,https://scholar.google.com/scholar?q=Neural+Architecture+Search+on+ImageNet+in+Four+GPU+Hours:+A+Theoretically+Inspired+Perspective
Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning,2021,ICLR,"['Beliz Gunel', 'Jingfei Du', 'Alexis Conneau', 'Veselin Stoyanov']",poster,"['pre-trained language model fine-tuning', 'supervised contrastive learning', 'natural language understanding', 'few-shot learning', 'robustness', 'generalization']","State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.",/pdf/02dcbc0bf1ebd53ed5b69a2ca9aa27b3d3c53893.pdf,zero_few-shot;transformer;contrastive learning;augmentation;llm,https://scholar.google.com/scholar?q=Supervised+Contrastive+Learning+for+Pre-trained+Language+Model+Fine-tuning
Cross-Attentional Audio-Visual Fusion for Weakly-Supervised Action Localization,2021,ICLR,"['Jun-Tae Lee', 'Mihir Jain', 'Hyoungwoo Park', 'Sungrack Yun']",poster,"['Audio-Visual', 'Multimodal Attention', 'Action localization', 'Event localization', 'Weak-supervision']","Temporally localizing actions in videos is one of the key components for video understanding. Learning from weakly-labeled data is seen as a potential solution towards avoiding expensive frame-level annotations. Different from other works which only depend on visual-modality, we propose to learn richer audiovisual representation for weakly-supervised action localization. First, we propose a multi-stage cross-attention mechanism to collaboratively fuse audio and visual features, which preserves the intra-modal characteristics. Second, to model both foreground and background frames, we construct an open-max classifier that treats the background class as an open-set. Third, for precise action localization, we design consistency losses to enforce temporal continuity for the action class prediction, and also help with foreground-prediction reliability. Extensive experiments on two publicly available video-datasets (AVE and ActivityNet1.2) show that the proposed method effectively fuses audio and visual modalities, and achieves the state-of-the-art results for weakly-supervised action localization.",/pdf/2d9210844c74d2a119c3878f1e6c2475a0d3af86.pdf,zero_few-shot;transformer;representation;llm,https://scholar.google.com/scholar?q=Cross-Attentional+Audio-Visual+Fusion+for+Weakly-Supervised+Action+Localization
Understanding Self-supervised Learning with Dual Deep Networks,2021,ICLR,"['Yuandong Tian', 'Lantao Yu', 'Xinlei Chen', 'Surya Ganguli']",poster,"['self-supervised learning', 'teacher-student setting', 'theoretical analysis', 'hierarchical models', 'representation learning']","We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. We show this leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that in BYOL, the combination of BatchNorm and a predictor network creates an implicit contrastive term, acting as an approximate covariance operator. Additionally, for linear architectures we derive exact solutions for BYOL that provide conceptual insights into how BYOL can learn useful non-collapsed representations without any contrastive terms that separate negative pairs.  Extensive ablation studies justify our theoretical findings. ",/pdf/8d1f5ab6ed92112edc1df40c8243863c18426bea.pdf,zero_few-shot;representation;contrastive learning;augmentation;llm,https://scholar.google.com/scholar?q=Understanding+Self-supervised+Learning+with+Dual+Deep+Networks
CTRLsum: Towards Generic Controllable Text Summarization,2021,ICLR,"['Junxian He', 'Wojciech Maciej Kryscinski', 'Bryan McCann', 'Nazneen Rajani', 'Caiming Xiong']",poster,['controllable text summarization'],"Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset.",/pdf/35ef9528b1e138d28d8091c04007052cdc70a5eb.pdf,inference;llm,https://scholar.google.com/scholar?q=CTRLsum:+Towards+Generic+Controllable+Text+Summarization
Beyond GNNs: A Sample Efficient Architecture for Graph Problems,2021,ICLR,"['Pranjal Awasthi', 'Abhimanyu Das', 'Sreenivas Gollapudi']",poster,"['Graph Neural Networks', 'Deep Learning Theory', 'Graph Connectivity', 'Minimum Spanning Trees']","Despite their popularity in learning problems over graph structured data, existing Graph Neural Networks (GNNs) have inherent limitations for fundamental graph problems such as shortest paths, $k$-connectivity, minimum spanning tree and minimum cuts. In all these instances, it is known that one needs GNNs of high depth, scaling at a polynomial rate with the number of nodes $n$, to provably encode the solution space. This in turn affects their statistical efficiency thus requiring a significant amount of training data in order to obtain networks with good performance. In this work we propose a new hybrid architecture to overcome this limitation. Our proposed architecture that we call as GNNplus networks involve a combination of multiple parallel low depth GNNs along with simple pooling layers involving low depth fully connected networks. We provably demonstrate that for many graph problems, the solution space can be encoded by GNNplus networks using depth that scales only poly-logarithmically in the number of nodes. This significantly improves the amount of training data needed that we establish via improved generalization bounds. Finally, we empirically demonstrate the effectiveness of our proposed architecture for a variety of graph problems.
",/pdf/5b1b77ad997fdde546669c8af0e05570b6e03bdb.pdf,graph;zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Beyond+GNNs:+A+Sample+Efficient+Architecture+for+Graph+Problems
On the expressivity of bi-Lipschitz normalizing flows,2021,ICML,"['Alexandre Verine', 'Yann Chevaleyre', 'Fabrice Rossi', 'benjamin negrevergne']",poster,"['Machine Learning', 'Invertible Networks', 'Normalizing Flow', 'Bilipschitz', 'Bi-Lipschitz', 'Expressivity']","An invertible function is bi-Lipschitz if both the function and its inverse have bounded Lipschitz constants. Nowadays, most Normalizing Flows are bi-Lipschitz by design or by training to limit numerical errors (among other things). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing Flows and identify several target distributions that are difficult to approximate using such models. Then, we characterize the expressivity of bi-Lipschitz Normalizing Flows by giving several lower bounds on the Total Variation distance between these particularly unfavorable distributions and their best possible approximation. Finally, we discuss potential remedies which include using more complex latent distributions.",https://api.openreview.net/pdf/403daf6887f29b69d44982795c7b664c7121f047.pdf,zero_few-shot;flow;llm,https://scholar.google.com/scholar?q=On+the+expressivity+of+bi-Lipschitz+normalizing+flows
Copula-Based Normalizing Flows,2021,ICML,"['Mike Laszkiewicz', 'Johannes Lederer', 'Asja Fischer']",poster,"['Base Distribution', 'Copula', 'Expressiveness']","Normalizing flows, which learn a distribution by transforming the data to samples from a Gaussian base distribution, have proven powerful density approximations. But their expressive power is limited by this choice of the base distribution. We, therefore, propose to generalize the base distribution to a more elaborate copula distribution to capture the properties of the target distribution more accurately. In a first empirical analysis, we demonstrate that this replacement can dramatically improve the vanilla normalizing flows in terms of flexibility, stability, and effectivity for heavy-tailed data. Our results suggest that the improvements are related to an increased local Lipschitz-stability of the learned flow.",https://api.openreview.net/pdf/c5b36809cea2e0d9212642187d1a4170c262d427.pdf,zero_few-shot;flow;llm,https://scholar.google.com/scholar?q=Copula-Based+Normalizing+Flows
Discovering Diverse Nearly Optimal Policies with Successor Features,2021,ICML,"['Tom Zahavy', ""Brendan O'Donoghue"", 'Andre Barreto', 'Sebastian Flennerhag', 'Volodymyr Mnih', 'Satinder Singh']",poster,"['quality diversity', 'reinforcement learning']","Finding different solutions to the same problem is a key aspect of intelligence associated with creativity and adaptation to novel situations. In reinforcement learning, a set of diverse policies can be useful for exploration, transfer, hierarchy, and robustness. We propose Diverse Successive Policies, a method for discovering policies that are diverse in the space of Successor Features, while assuring that they are near optimal. We formalize the problem as a Constrained Markov Decision Process (CMDP) where the goal is to find policies that maximize diversity, characterized by an intrinsic diversity reward, while remaining near-optimal with respect to the extrinsic reward of the MDP. We also analyze how recently proposed robustness and discrimination rewards perform and find that they are sensitive to the initialization of the procedure and may converge to sub-optimal solutions. To alleviate this, we propose new explicit diversity rewards that aim to minimize the correlation between the Successor Features of the policies in the set. We compare the different diversity mechanisms in the DeepMind Control Suite and find that the type of explicit diversity we are proposing is important to discover distinct behavior, like for example different locomotion patterns.",https://api.openreview.net/pdf/d00e028fc1a71f1fb2309c958fba10c6248cdc38.pdf,reinforcement learning;optimization;transfer learning;llm,https://scholar.google.com/scholar?q=Discovering+Diverse+Nearly+Optimal+Policies+with+Successor+Features
"Exploration via Empowerment Gain: Combining Novelty, Surprise and Learning Progress",2021,ICML,"['Philip Becker-Ehmck', 'Maximilian Karl', 'Jan Peters', 'Patrick van der Smagt']",poster,"['exploration', 'empowerment', 'intrinsic motivation', 'surprise', 'novelty']","Exploration in the absence of a concrete task is a key characteristic of autonomous agents and vital for the emergence of intelligent behaviour. Various intrinsic motivation frameworks have been suggested, such as novelty seeking, surprise maximisation or empowerment. Here we focus on the latter, empowerment, an agent-centric and information-theoretic measure of an agent's perceived influence on the world. By considering improving one's empowerment estimator - we call it empowerment gain (EG) - we derive a novel exploration criterion that focuses directly on the desired goal: exploration in order to help the agent recognise its capability to interact with the world. We propose a new theoretical framework based on improving a parametrised estimation of empowerment and show how it integrates novelty, surprise and learning progress into a single formulation. Empirically, we validate our theoretical findings on some simple but instructive grid world environments. We show that while such an agent is still novelty seeking, i.e. interested in exploring the whole state space, it focuses on exploration where its perceived influence is greater, avoiding areas of greater stochasticity or traps that limit its control.",https://api.openreview.net/pdf/22ee6eabe1973eb57c02e3a8444ced4d1188e14c.pdf,reinforcement learning;llm,"https://scholar.google.com/scholar?q=Exploration+via+Empowerment+Gain:+Combining+Novelty,+Surprise+and+Learning+Progress"
Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation,2021,ICML,"['Nicklas Hansen', 'Hao Su', 'Xiaolong Wang']",poster,"['reinforcement learning', 'generalization', 'data augmentation']","While agents trained by Reinforcement Learning (RL) can solve increasingly challenging tasks directly from visual observations, generalizing learned skills to novel environments remains very challenging. Extensive use of data augmentation is a promising technique for improving generalization in RL, but it is often found to decrease sample efficiency and can even lead to divergence. In this paper, we investigate causes of instability when using data augmentation in common off-policy RL algorithms. We identify two problems, both rooted in high-variance Q-targets. Based on our findings, we propose a simple yet effective technique for stabilizing this class of algorithms under augmentation. We perform extensive empirical evaluation of image-based RL using both ConvNets and Vision Transformers (ViT) on a family of benchmarks based on DeepMind Control Suite, as well as in robotic manipulation tasks. Our method greatly improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image-based RL. We further show that our method scales to RL with ViT-based architectures, and that data augmentation may be especially important in this setting. Code and videos: https://nicklashansen.github.io/SVEA",https://api.openreview.net/pdf/9fd366557f101b176c436842a6bd1dd03bba561d.pdf,reinforcement learning;graph;transformer;augmentation;llm,https://scholar.google.com/scholar?q=Stabilizing+Deep+Q-Learning+with+ConvNets+and+Vision+Transformers+under+Data+Augmentation
Strategically-timed State-Observation Attacks on Deep Reinforcement Learning Agents,2021,ICML,"['You Qiaoben', 'Xinning Zhou', 'Chengyang Ying', 'Jun Zhu']",poster,[]," Deep reinforcement learning (DRL) policies are vulnerable to the adversarial attack on their observations, which may mislead real-world RL agents to catastrophic failures. Several works have shown the effectiveness of this type of adversarial attacks. But these adversaries are inclined to be detected because these adversaries do not inhibit their attacks activity. Recent works provide heuristic methods by attacking the victim agent at a small subset of time steps, but it aims at lack for theoretical principles. Inspired by the idea that adversarial attacks at each time step have different efforts, we denote a novel strategically-timed attack called Tentative Frame Attack for continuous control environments. We further propose a theoretical framework of finding optimal frame attack. Following this framework, we trained the frame attack strategy online with the victim agents and a fixed adversary. The empirical results show that our adversaries achieve the state-of-the-art performance on DRL agents which outperforms the full-timed attack.",https://api.openreview.net/pdf/970d86aca5ab8dc7b48d673cea74e8b0775f61e7.pdf,reinforcement learning;zero_few-shot;online learning;llm,https://scholar.google.com/scholar?q=Strategically-timed+State-Observation+Attacks+on+Deep+Reinforcement+Learning+Agents
Adversarially Trained Neural Policies in the Fourier Domain,2021,ICML,['Ezgi Korkmaz'],poster,"['reinforcement learning', 'deep learning', 'adversarial', 'deep reinforcement learning', 'robustness', 'adversarial training', 'deep neural networks', 'safety']","Reinforcement learning policies based on deep neural networks are vulnerable to imperceptible adversarial perturbations to their inputs, in much the same way as neural network image classifiers. Recent work has proposed several methods for adversarial training for deep reinforcement learning agents to improve robustness to adversarial perturbations. In this paper, we study the effects of adversarial training on the neural policy learned by the agent. In particular, we compare the Fourier spectrum of minimal perturbations computed for both adversarially trained and vanilla trained neural policies. Via experiments in the OpenAI Atari environments we show that minimal perturbations computed for adversarially trained policies are more focused on lower frequencies in the Fourier domain, indicating a higher sensitivity of these policies to low frequency perturbations. We believe our results can be an initial step towards understanding the relationship between adversarial training and different notions of robustness for neural policies.",https://api.openreview.net/pdf/2a705ae6cc544c5ee6b365ea1688940ab67b39e1.pdf,reinforcement learning;zero_few-shot;llm,https://scholar.google.com/scholar?q=Adversarially+Trained+Neural+Policies+in+the+Fourier+Domain
Deep inference of latent dynamics with spatio-temporal super-resolution using selective backpropagation through time,2021,NIPS,"['Feng Zhu', 'Andrew R Sedler', 'Harrison A Grier', 'Nauman Ahad', 'Mark A. Davenport', 'Matthew Kaufman', 'Andrea Giovannucci', 'Chethan Pandarinath']",poster,"['Computational neuroscience', 'Systems neuroscience', 'Neural population dynamics', 'intermittent sampling', 'Electrophysiology', 'Calcium imaging', 'Brain-computer interfaces', 'Neuroscience', 'Neuroprosthetics', 'Neural coding', 'Motor control', 'Sequential autoencoders']","Modern neural interfaces allow access to the activity of up to a million neurons within brain circuits. However, bandwidth limits often create a trade-off between greater spatial sampling (more channels or pixels) and the temporal frequency of sampling. Here we demonstrate that it is possible to obtain spatio-temporal super-resolution in neuronal time series by exploiting relationships among neurons, embedded in latent low-dimensional population dynamics. Our novel neural network training strategy, selective backpropagation through time (SBTT), enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The resulting models are able to infer activity for missing samples by combining observations with learned latent dynamics. We test SBTT applied to sequential autoencoders and demonstrate more efficient and higher-fidelity characterization of neural population dynamics in electrophysiological and calcium imaging data. In electrophysiology, SBTT enables accurate inference of neuronal population dynamics with lower interface bandwidths, providing an avenue to significant power savings for implanted neuroelectronic interfaces. In applications to two-photon calcium imaging, SBTT accurately uncovers high-frequency temporal structure underlying neural population activity, substantially outperforming the current state-of-the-art. Finally, we demonstrate that performance could be further improved by using limited, high-bandwidth sampling to pretrain dynamics models, and then using SBTT to adapt these models for sparsely-sampled data.",https://api.openreview.net/pdf/e9c5ec68d5797aeeeed97d96ed3d548b15b73ef6.pdf,graph;zero_few-shot;generative model;inference;sparse;llm,https://scholar.google.com/scholar?q=Deep+inference+of+latent+dynamics+with+spatio-temporal+super-resolution+using+selective+backpropagation+through+time
Skipping the Frame-Level: Event-Based Piano Transcription With Neural Semi-CRFs,2021,NIPS,"['Yujia Yan', 'Frank Cwitkowitz', 'Zhiyao Duan']",poster,"['Music', 'Audio', 'Piano Transcrition', 'Music Transcription', 'Semi-Markov', 'CRFs', 'Sound Event Detection', 'Music Information Retrieval']","Piano transcription systems are typically optimized to estimate pitch activity at each frame of audio. They are often followed by carefully designed heuristics and post-processing algorithms to estimate note events from the frame-level predictions. Recent methods have also framed piano transcription as a multi-task learning problem, where the activation of different stages of a note event are estimated independently. These practices are not well aligned with the desired outcome of the task, which is the specification of note intervals as holistic events, rather than the aggregation of disjoint observations. In this work, we propose a novel formulation of piano transcription, which is optimized to directly predict note events. Our method is based on Semi-Markov Conditional Random Fields (semi-CRF), which produce scores for intervals rather than individual frames. When formulating piano transcription in this way, we eliminate the need to rely on disjoint frame-level estimates for different stages of a note event. We conduct experiments on the MAESTRO dataset and demonstrate that the proposed model surpasses the current state-of-the-art for piano transcription. Our results suggest that the semi-CRF output layer, while still quadratic in complexity, is a simple, fast and well-performing solution for event-based prediction, and may lead to similar success in other areas which currently rely on frame-level estimates.",https://api.openreview.net/pdf/240883eb11d2e89ef2d879ce55c8a2e92919fc47.pdf,zero_few-shot;multi-task;multimodal;llm,https://scholar.google.com/scholar?q=Skipping+the+Frame-Level:+Event-Based+Piano+Transcription+With+Neural+Semi-CRFs
Charting and Navigating the Space of Solutions for Recurrent Neural Networks,2021,NIPS,"['Elia Turner', 'Kabir Vinay Dabholkar', 'Omri Barak']",poster,"['RNN', 'underspecification', 'variability', 'space of solutions', 'neuroscience', 'reverse engineering', 'task optimized networks']","In recent years Recurrent Neural Networks (RNNs) were successfully used to model the way neural activity drives task-related behavior in animals, operating under the implicit assumption that the obtained solutions are universal. Observations in both neuroscience and machine learning challenge this assumption. Animals can approach a given task with a variety of strategies, and training machine learning algorithms introduces the phenomenon of underspecification. These observations imply that every task is associated with a space of solutions. To date, the structure of this space is not understood, limiting the approach of comparing RNNs with neural data.
Here, we characterize the space of solutions associated with various tasks. We first study a simple two-neuron network on a task that leads to multiple solutions. We trace the nature of the final solution back to the network’s initial connectivity and identify discrete dynamical regimes that underlie this diversity. We then examine three neuroscience-inspired tasks: Delayed discrimination, Interval discrimination, and Time reproduction. For each task, we find a rich set of solutions. One layer of variability can be found directly in the neural activity of the networks. An additional layer is uncovered by testing the trained networks' ability to extrapolate, as a perturbation to a system often reveals hidden structure. Furthermore, we relate extrapolation patterns to specific dynamical objects and effective algorithms found by the networks. We introduce a tool to derive the reduced dynamics of networks by generating a compact directed graph describing the essence of the dynamics with regards to behavioral inputs and outputs. Using this representation, we can partition the solutions to each task into a handful of types and show that neural features can partially predict them.
Taken together, our results shed light on the concept of the space of solutions and its uses both in Machine learning and in Neuroscience.",https://api.openreview.net/pdf/2df756efa7a46d4dfa656b8aeab5bd50b787a0a4.pdf,graph;representation;llm,https://scholar.google.com/scholar?q=Charting+and+Navigating+the+Space+of+Solutions+for+Recurrent+Neural+Networks
Gradient Starvation: A Learning Proclivity in Neural Networks,2021,NIPS,"['Mohammad Pezeshki', 'Sékou-Oumar Kaba', 'Yoshua Bengio', 'Aaron Courville', 'Doina Precup', 'Guillaume Lajoie']",poster,"['generalization', 'neural networks', 'dynamics', 'OOD']","We identify and formalize a fundamental gradient descent phenomenon resulting in a learning proclivity in over-parameterized neural networks. Gradient Starvation arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. This work provides a theoretical explanation for the emergence of such feature imbalance in neural networks. Using tools from Dynamical Systems theory, we identify simple properties of learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure in training data. Based on our proposed formalism, we develop guarantees for a novel regularization method aimed at decoupling feature learning dynamics, improving accuracy and robustness in cases hindered by gradient starvation. We illustrate our findings with simple and real-world out-of-distribution (OOD) generalization experiments.",https://api.openreview.net/pdf/21c50d2f86dc6f460965f26c29beb5681b6e510d.pdf,llm,https://scholar.google.com/scholar?q=Gradient+Starvation:+A+Learning+Proclivity+in+Neural+Networks
Periodic Activation Functions Induce Stationarity,2021,NIPS,"['Lassi Meronen', 'Martin Trapp', 'Arno Solin']",poster,"['Bayesian deep learning', 'Gaussian process', 'uncertainty quantification']","Neural network models are known to reinforce hidden data biases, making them unreliable and difficult to interpret. We seek to build models that `know what they do not know' by introducing inductive biases in the function space. We show that periodic activation functions in Bayesian neural networks establish a connection between the prior on the network weights and translation-invariant, stationary Gaussian process priors. Furthermore, we show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. In a series of experiments, we show that periodic activation functions obtain comparable performance for in-domain data and capture sensitivity to perturbed inputs in deep neural networks for out-of-domain detection.",https://api.openreview.net/pdf/e59b24241ac27bee46bade7dc88a061e44434751.pdf,bayesian;llm,https://scholar.google.com/scholar?q=Periodic+Activation+Functions+Induce+Stationarity
Leveraging Recursive Gumbel-Max Trick for Approximate Inference in Combinatorial Spaces,2021,NIPS,"['Kirill Struminsky', 'Artyom Gadetsky', 'Denis Rakitin', 'Danil Karpushkin', 'Dmitry P. Vetrov']",poster,"['structured discrete latent variable', 'gumbel-max trick', 'perturb-and-map', 'gradient estimation', 'score function']","Structured latent variables allow incorporating meaningful prior knowledge into deep learning models. However, learning with such variables remains challenging because of their discrete nature. Nowadays, the standard learning approach is to define a latent variable as a perturbed algorithm output and to use a differentiable surrogate for training. In general, the surrogate puts additional constraints on the model and inevitably leads to biased gradients. To alleviate these shortcomings, we extend the Gumbel-Max trick to define distributions over structured domains. We avoid the differentiable surrogates by leveraging the score function estimators for optimization. In particular, we highlight a family of recursive algorithms with a common feature we call stochastic invariant. The feature allows us to construct reliable gradient estimates and control variates without additional constraints on the model. In our experiments, we consider various structured latent variable models and achieve results competitive with relaxation-based counterparts.",https://api.openreview.net/pdf/881fa6d6d186edcc82e797d60ff554f3c3c399fd.pdf,graph;optimization;zero_few-shot;inference;llm,https://scholar.google.com/scholar?q=Leveraging+Recursive+Gumbel-Max+Trick+for+Approximate+Inference+in+Combinatorial+Spaces
Low-Rank Constraints for Fast Inference in Structured Models,2021,NIPS,"['Justin T Chiu', 'Yuntian Deng', 'Alexander M Rush']",poster,"['structured prediction', 'generative models', 'graphical models', 'inference']","Structured distributions, i.e. distributions over combinatorial spaces, are commonly used to learn latent probabilistic representations from observed data. However, scaling these models is bottlenecked by the high computational and memory complexity with respect to the size of the latent representations. Common models such as Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs) require time and space quadratic and cubic in the number of hidden states respectively. This work demonstrates a simple approach to reduce the computational and memory complexity of a large class of structured models. We show that by viewing the central inference step as a matrix-vector product and using a low-rank constraint, we can trade off model expressivity and speed via the rank.  Experiments with neural parameterized structured models for language modeling, polyphonic music modeling, unsupervised grammar induction, and video modeling show that our approach matches the accuracy of standard models at large state spaces while providing practical speedups.",https://api.openreview.net/pdf/dbb1a4e76fa9b5c76eb18af43529cd235549ab40.pdf,optimization;zero_few-shot;transformer;representation;inference;low-rank;llm,https://scholar.google.com/scholar?q=Low-Rank+Constraints+for+Fast+Inference+in+Structured+Models
Decoupling the Depth and Scope of Graph Neural Networks,2021,NIPS,"['Hanqing Zeng', 'Muhan Zhang', 'Yinglong Xia', 'Ajitesh Srivastava', 'Andrey Malevich', 'Rajgopal Kannan', 'Viktor Prasanna', 'Long Jin', 'Ren Chen']",poster,"['Graph Representation Learning', 'Subgraph Mining', 'Expressive Power', 'Scalability']","State-of-the-art Graph Neural Networks (GNNs) have limited scalability with respect to the graph and model sizes. On large graphs, increasing the model depth often means exponential expansion of the scope (i.e., receptive field). Beyond just a few layers, two fundamental challenges emerge:  1. degraded expressivity due to oversmoothing, and 2. expensive computation due to neighborhood explosion. We propose a design principle to decouple the depth and scope of GNNs – to generate representation of a target entity (i.e., a node or an edge), we first extract a localized subgraph as the bounded-size scope, and then apply a GNN of arbitrary depth on top of the subgraph. A properly extracted subgraph consists of a small number of critical neighbors, while excluding irrelevant ones. The GNN, no matter how deep it is, smooths the local neighborhood into informative representation rather than oversmoothing the global graph into “white noise”. Theoretically, decoupling improves the GNN expressive power from the perspectives of graph signal processing (GCN), function approximation (GraphSAGE) and topological learning (GIN). Empirically, on seven graphs (with up to 110M nodes) and six backbone GNN architectures, our design achieves significant accuracy improvement with orders of magnitude reduction in computation and hardware cost.",https://api.openreview.net/pdf/83666b11b7dac383bb74bdab0e6faf417e0cdc2b.pdf,graph;representation;llm,https://scholar.google.com/scholar?q=Decoupling+the+Depth+and+Scope+of+Graph+Neural+Networks
DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification,2021,NIPS,"['Yongming Rao', 'Wenliang Zhao', 'Benlin Liu', 'Jiwen Lu', 'Jie Zhou', 'Cho-Jui Hsieh']",poster,"['vision transformer', 'deep model acceleration']","Attention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31% $\sim$ 37%  FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision transformers. Equipped with the dynamic token sparsification framework,  DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/raoyongming/DynamicViT",https://api.openreview.net/pdf/de58494859f4d3fbde1ab2a3f90d1099fef4efc6.pdf,transformer;sparse;llm,https://scholar.google.com/scholar?q=DynamicViT:+Efficient+Vision+Transformers+with+Dynamic+Token+Sparsification
CoAtNet: Marrying Convolution and Attention for All Data Sizes,2021,NIPS,"['Zihang Dai', 'Hanxiao Liu', 'Quoc V Le', 'Mingxing Tan']",poster,"['Hybrid', 'Transformer', 'Image Recognition']","Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced ""coat"" nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.",https://api.openreview.net/pdf/b64bb2ad5463c6158408ff3c2f29a9104834c9c0.pdf,optimization;transformer;llm,https://scholar.google.com/scholar?q=CoAtNet:+Marrying+Convolution+and+Attention+for+All+Data+Sizes
Increasing Liquid State Machine Performance with Edge-of-Chaos Dynamics Organized by Astrocyte-modulated Plasticity,2021,NIPS,"['Vladimir A Ivanov', 'Konstantinos P. Michmizos']",poster,"['liquid state machine', 'astrocytes', 'spiking neural networks', 'neuromorphic computing', 'neuron-astrocyte networks', 'STDP', 'self organized criticality']","The liquid state machine (LSM) combines low training complexity and biological plausibility, which has made it an attractive machine learning framework for edge and neuromorphic computing paradigms. Originally proposed as a model of brain computation, the LSM tunes its internal weights without backpropagation of gradients, which results in lower performance compared to multi-layer neural networks. Recent findings in neuroscience suggest that astrocytes, a long-neglected non-neuronal brain cell, modulate synaptic plasticity and brain dynamics, tuning brain networks to the vicinity of the computationally optimal critical phase transition between order and chaos. Inspired by this disruptive understanding of how brain networks self-tune, we propose the neuron-astrocyte liquid state machine (NALSM) that addresses under-performance through self-organized near-critical dynamics. Similar to its biological counterpart, the astrocyte model integrates neuronal activity and provides global feedback to spike-timing-dependent plasticity (STDP), which self-organizes NALSM dynamics around a critical branching factor that is associated with the edge-of-chaos. We demonstrate that NALSM achieves state-of-the-art accuracy versus comparable LSM methods, without the need for data-specific hand-tuning. With a top accuracy of $97.61\%$ on MNIST, $97.51\%$ on N-MNIST, and $85.84\%$ on Fashion-MNIST, NALSM achieved comparable performance to current fully-connected multi-layer spiking neural networks trained via backpropagation. Our findings suggest that the further development of brain-inspired machine learning methods has the potential to reach the performance of deep learning, with the added benefits of supporting robust and energy-efficient neuromorphic computing on the edge.",https://api.openreview.net/pdf/2d472405f88e9dee6f975b1c1e92c60da64e41ac.pdf,graph;zero_few-shot;active learning;llm,https://scholar.google.com/scholar?q=Increasing+Liquid+State+Machine+Performance+with+Edge-of-Chaos+Dynamics+Organized+by+Astrocyte-modulated+Plasticity
Learning from Inside: Self-driven Siamese Sampling and Reasoning for Video Question Answering,2021,NIPS,"['Weijiang Yu', 'Haoteng Zheng', 'Mengfei Li', 'Lei Ji', 'Lijun Wu', 'Nong Xiao', 'Nan Duan']",poster,"['Video Question Answering', 'Multi-view Learning', 'Data-effeciency Reasoning']","Recent advances in the video question answering (i.e., VideoQA) task have achieved strong success by following the paradigm of fine-tuning each clip-text pair independently on the pretrained transformer-based model via supervised learning. Intuitively, multiple samples (i.e., clips) should be interdependent to capture similar visual and key semantic information in the same video. To consider the interdependent knowledge between contextual clips into the network inference, we propose a Siamese Sampling and Reasoning (SiaSamRea) approach, which consists of a siamese sampling mechanism to generate sparse and similar clips (i.e., siamese clips) from the same video, and a novel reasoning strategy for integrating the interdependent knowledge between contextual clips into the network. The reasoning strategy contains two modules: (1) siamese knowledge generation to learn the inter-relationship among clips; (2) siamese knowledge reasoning to produce the refined soft label by propagating the weights of inter-relationship to the predicted candidates of all clips. Finally, our SiaSamRea can endow the current multimodal reasoning paradigm with the ability of learning from inside via the guidance of soft labels. Extensive experiments demonstrate our SiaSamRea achieves state-of-the-art performance on five VideoQA benchmarks, e.g., a significant +2.1% gain on MSRVTT-QA, +2.9% on MSVD-QA, +1.0% on ActivityNet-QA, +1.8% on How2QA and +4.3% (action) on TGIF-QA.",https://api.openreview.net/pdf/025bb6f0c72492bc1348ca9f4b0f0bba7f54509a.pdf,zero_few-shot;transformer;generative model;inference;sparse;multimodal;llm,https://scholar.google.com/scholar?q=Learning+from+Inside:+Self-driven+Siamese+Sampling+and+Reasoning+for+Video+Question+Answering
Minibatch and Momentum Model-based Methods for Stochastic Weakly Convex Optimization,2021,NIPS,"['Qi Deng', 'Wenzhi Gao']",poster,"['model-based optimization', 'weakly convex optimization', 'stochastic optimization', 'algorithm stability', 'momentum methods']","Stochastic model-based methods have received increasing attention lately due to their appealing robustness to the stepsize selection and provable efficiency guarantee. We make two important extensions for improving model-based methods on stochastic weakly convex optimization. First, we propose new minibatch model- based methods by involving a set of samples to approximate the model function in each iteration. For the first time, we show that stochastic algorithms achieve linear speedup over the batch size even for non-smooth and non-convex (particularly, weakly convex) problems. To this end, we develop a novel sensitivity analysis of the proximal mapping involved in each algorithm iteration. Our analysis appears to be of independent interests in more general settings. Second, motivated by the success of momentum stochastic gradient descent, we propose a new stochastic extrapolated model-based method, greatly extending the classic Polyak momentum technique to a wider class of stochastic algorithms for weakly convex optimization. The rate of convergence to some natural stationarity condition is established over a fairly flexible range of extrapolation terms.

While mainly focusing on weakly convex optimization, we also extend our work to convex optimization. We apply the minibatch and extrapolated model-based methods to stochastic convex optimization, for which we provide a new complexity bound and promising linear speedup in batch size. Moreover, an accelerated model-based method based on Nesterov’s momentum is presented, for which we establish an optimal complexity bound for reaching optimality.
",https://api.openreview.net/pdf/88b0bb5d386d0d22ba0446703c5e2959b01956d2.pdf,optimization;zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Minibatch+and+Momentum+Model-based+Methods+for+Stochastic+Weakly+Convex+Optimization
How does a Neural Network's Architecture Impact its Robustness to Noisy Labels?,2021,NIPS,"['Jingling Li', 'Mozhi Zhang', 'Keyulu Xu', 'John P Dickerson', 'Jimmy Ba']",poster,"['noisy labels', 'architectural inductive bias', 'algorithmic alignment', 'graph neural networks']","Noisy labels are inevitable in large real-world datasets. In this work, we explore an area understudied by previous works --- how the network's architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network's robustness via the predictive power in its representations --- the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also find that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels.",https://api.openreview.net/pdf/ad3cf56af1a92f68bdb99333ab2058d0bf47b6f0.pdf,representation;multimodal;llm,https://scholar.google.com/scholar?q=How+does+a+Neural+Network's+Architecture+Impact+its+Robustness+to+Noisy+Labels?
Do Vision Transformers See Like Convolutional Neural Networks?,2021,NIPS,"['Maithra Raghu', 'Thomas Unterthiner', 'Simon Kornblith', 'Chiyuan Zhang', 'Alexey Dosovitskiy']",poster,"['Vision Transformers', 'Representation Analysis', 'Representation Learning', 'Representation Similarity', 'Computer Vision', 'Convolutional Neural Networks']","Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer. ",https://api.openreview.net/pdf/5f1c51b24e372225fdba97e588d23197c187faed.pdf,zero_few-shot;transformer;representation;transfer learning;llm,https://scholar.google.com/scholar?q=Do+Vision+Transformers+See+Like+Convolutional+Neural+Networks?
Dealing With Misspecification In Fixed-Confidence Linear Top-m Identification,2021,NIPS,"['Clémence Réda', 'Andrea Tirinzoni', 'Rémy Degenne']",poster,"['misspecification', 'linear bandits', 'fixed-confidence top-m identification', 'pure exploration', 'recommendation systems', 'multi-armed bandits']","We study the problem of the identification of m arms with largest means under a fixed error rate $\delta$ (fixed-confidence Top-m identification), for misspecified linear bandit models. This problem is motivated by practical applications, especially in medicine and recommendation systems, where linear models are popular due to their simplicity and the existence of efficient algorithms, but in which data inevitably deviates from linearity. In this work, we first derive a tractable lower bound on the sample complexity of any $\delta$-correct algorithm for the general Top-m identification problem. We show that knowing the scale of the deviation from linearity is necessary to exploit the structure of the problem. We then describe the first algorithm for this setting, which is both practical and adapts to the amount of misspecification. We derive an upper bound to its sample complexity which confirms this adaptivity and that matches the lower bound when $\delta \rightarrow 0$. Finally, we evaluate our algorithm on both synthetic and real-world data, showing competitive performance with respect to existing baselines.",https://api.openreview.net/pdf/9dc482b68fa4e5ab9f883b0c5bd85be0c8d27e40.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Dealing+With+Misspecification+In+Fixed-Confidence+Linear+Top-m+Identification
Provably Efficient Reinforcement Learning with Linear Function Approximation under Adaptivity Constraints,2021,NIPS,"['Tianhao Wang', 'Dongruo Zhou', 'Quanquan Gu']",poster,"['reinforcement learning', 'linear function approximation', 'adaptivity constraint']","We study reinforcement learning (RL) with linear function approximation under the adaptivity constraint. We consider two popular limited adaptivity models: the batch learning model and the rare policy switch model, and propose two efficient online RL algorithms for episodic linear Markov decision processes, where the transition probability and the reward function can be represented as a linear function of some known feature mapping. In specific, for the batch learning model, our proposed LSVI-UCB-Batch algorithm achieves an $\tilde O(\sqrt{d^3H^3T} + dHT/B)$ regret, where $d$ is the dimension of the feature mapping, $H$ is the episode length, $T$ is the number of interactions and $B$ is the number of batches. Our result suggests that it suffices to use only $\sqrt{T/dH}$ batches to obtain $\tilde O(\sqrt{d^3H^3T})$ regret. For the rare policy switch model, our proposed LSVI-UCB-RareSwitch algorithm enjoys an $\tilde O(\sqrt{d^3H^3T[1+T/(dH)]^{dH/B}})$ regret, which implies that $dH\log T$ policy switches suffice to obtain the $\tilde O(\sqrt{d^3H^3T})$ regret. Our algorithms achieve the same regret as the LSVI-UCB algorithm \citep{jin2020provably}, yet with a substantially smaller amount of adaptivity. We also establish a lower bound for the batch learning model, which suggests that the dependency on $B$ in our regret bound is tight.",https://api.openreview.net/pdf/f59eed58a153ab2cd100bf413bc9d3ba02ed0df8.pdf,reinforcement learning;optimization;zero_few-shot;online learning;llm,https://scholar.google.com/scholar?q=Provably+Efficient+Reinforcement+Learning+with+Linear+Function+Approximation+under+Adaptivity+Constraints
When Expressivity Meets Trainability: Fewer than $n$ Neurons Can Work,2021,NIPS,"['Jiawei Zhang', 'Yushun Zhang', 'Mingyi Hong', 'Ruoyu Sun', 'Zhi-Quan Luo']",poster,"['deep learning theory', 'narrow', 'small neural networks', 'optimization', 'memorization', 'deep learning', 'neural networks']","Modern neural networks are often quite wide, causing large memory and computation costs. It is thus of great interest to train a narrower network. However, training narrow neural nets remains a challenging task. We ask two theoretical questions: Can narrow networks have as strong expressivity as wide ones? If so, does the loss function exhibit a  benign optimization landscape? In this work, we provide partially affirmative answers to both questions for 1-hidden-layer networks with fewer than $n$ (sample size) neurons when the activation is smooth.  First, we prove that as long as the width $m \geq 2n/d$ (where $d$ is the input dimension), its expressivity is strong, i.e., there exists at least one global minimizer with zero training loss. Second, we identify a nice local region with no local-min or saddle points. Nevertheless, it is not clear whether gradient descent can stay in this nice region. Third, we consider a constrained optimization formulation where the feasible region is the nice local region, and prove that every KKT point is a nearly global minimizer. It is expected that projected gradient methods converge to KKT points under mild technical conditions, but we leave the rigorous convergence analysis to future work. Thorough numerical results show that projected gradient methods on this constrained formulation significantly outperform SGD for training narrow neural nets. ",https://api.openreview.net/pdf/fd4cbecbe6aeec23aee5bff242ed38ddfef25cdd.pdf,graph;optimization;llm,https://scholar.google.com/scholar?q=When+Expressivity+Meets+Trainability:+Fewer+than+$n$+Neurons+Can+Work
BCORLE($\lambda$): An Offline Reinforcement Learning and Evaluation Framework for Coupons Allocation in E-commerce Market,2021,NIPS,"['Yang Zhang', 'Bo Tang', 'Qingyu Yang', 'Dou An', 'Hongyin Tang', 'Chenyang Xi', 'Xueying LI', 'Feiyu Xiong']",poster,"['Application of E-commerce Market；Coupons Allocation', 'Constrained Markov Decision Process', 'Offline Reinforcement Learning', 'Off-policy Evaluation']","Coupons allocation is an important tool for enterprises to increase the activity and loyalty of users on the e-commerce market. One fundamental problem related is how to allocate coupons within a fixed budget while maximizing users' retention on the e-commerce platform. The online e-commerce environment is complicated and ever changing, so it requires the coupons allocation policy learning can quickly adapt to the changes of the company's business strategy. Unfortunately, existing studies with a huge computation overhead can hardly satisfy the requirements of real-time and fast-response in the real world. Specifically, the problem of coupons allocation within a fixed budget is usually formulated as a Lagrangian problem. Existing solutions need to re-learn the policy once the value of Lagrangian multiplier variable $\lambda$ is updated, causing a great computation overhead. Besides, a mature e-commerce market often faces tens of millions of users and dozens of types of coupons which construct the huge policy space, further increasing the difficulty of solving the problem. To tackle with above problems, we propose a budget constrained offline reinforcement learning and evaluation with $\lambda$-generalization (BCORLE($\lambda$)) framework. The proposed method can help enterprises develop a coupons allocation policy which greatly improves users' retention rate on the platform while ensuring the cost does not exceed the budget. Specifically, $\lambda$-generalization method is proposed to lead the policy learning process can be executed according to different $\lambda$ values adaptively, avoiding re-learning new polices from scratch. Thus the computation overhead is greatly reduced. Further, a novel offline reinforcement learning method and an off-policy evaluation algorithm are proposed for policy learning and policy evaluation, respectively. Finally, experiments on the simulation platform and real-world e-commerce market validate the effectiveness of our approach.",https://api.openreview.net/pdf/898c90e91f397bf8492cb139e73396dfba8cf025.pdf,reinforcement learning;offline reinforcement learning;graph;optimization;online learning;adaptive;llm,https://scholar.google.com/scholar?q=BCORLE($\lambda$):+An+Offline+Reinforcement+Learning+and+Evaluation+Framework+for+Coupons+Allocation+in+E-commerce+Market
VigDet: Knowledge Informed Neural Temporal Point Process for Coordination Detection on Social Media,2021,NIPS,"['Yizhou Zhang', 'Karishma Sharma', 'Yan Liu']",poster,"['Coordinated Influence Campaigns', 'Disinformation', 'Social Media', 'Fake News', 'Temporal Point Process', 'Variational Inference']","Recent years have witnessed an increasing use of coordinated accounts on social media, operated by misinformation campaigns to influence public opinion and manipulate social outcomes. Consequently, there is an urgent need to develop an effective methodology for coordinated group detection to combat the misinformation on social media. However, existing works suffer from various drawbacks, such as, either limited performance due to extreme reliance on predefined signatures of coordination, or instead an inability to address the natural sparsity of account activities on social media with useful prior domain knowledge. Therefore, in this paper, we propose a coordination detection framework incorporating neural temporal point process with prior knowledge such as temporal logic or pre-defined filtering functions. Specifically, when modeling the observed data from social media with neural temporal point process, we jointly learn a Gibbs-like distribution of group assignment based on how consistent an assignment is to (1) the account embedding space and (2) the prior knowledge. To address the challenge that the distribution is hard to be efficiently computed and sampled from, we design a theoretically guaranteed variational inference approach to learn a mean-field approximation for it. Experimental results on a real-world dataset show the effectiveness of our proposed method compared to the SOTA model in both unsupervised and semi-supervised settings. We further apply our model on a COVID-19 Vaccine Tweets dataset. The detection result suggests the presence of suspicious coordinated efforts on spreading misinformation about COVID-19 vaccines.",https://api.openreview.net/pdf/ec4dea37f0a434769e5631e9dc282eb6caea8433.pdf,inference;llm,https://scholar.google.com/scholar?q=VigDet:+Knowledge+Informed+Neural+Temporal+Point+Process+for+Coordination+Detection+on+Social+Media
A Non-commutative Extension of  Lee-Seung's Algorithm for Positive Semidefinite Factorizations,2021,NIPS,"['Yong Sheng Soh', 'Antonios Varvitsiotis']",poster,"['PSD factorizations', 'majorization minimization', ""Lee-Seung's algorithm"", 'Semidefinite optimization', ""Lieb's concavity theorem"", 'PSD tensor factorization']","Given a data matrix $X\in \mathbb{R}_+^{m\times n}$ with non-negative entries, a Positive Semidefinite (PSD) factorization of $X$ is a collection of $r \times r$-dimensional PSD matrices $\{A_i\}$ and $\{B_j\}$ satisfying the condition $X_{ij}= \mathrm{tr}(A_i B_j)$ for all $\ i\in [m],\ j\in [n]$.  PSD factorizations are fundamentally linked to understanding the expressiveness of semidefinite programs as well as the power and limitations of quantum resources in information theory.  The PSD factorization task generalizes the Non-negative Matrix Factorization (NMF) problem in which we seek a collection of $r$-dimensional non-negative vectors $\{a_i\}$ and $\{b_j\}$ satisfying $X_{ij}= a_i^T b_j$,  for all $i\in [m],\ j\in [n]$ -- one can recover the latter problem by choosing matrices in the PSD factorization to be diagonal.  The most widely used algorithm for computing NMFs of a matrix is the Multiplicative Update algorithm developed by Lee and Seung, in which non-negativity of the updates is preserved by scaling with positive diagonal matrices.  In this paper, we describe a non-commutative extension of Lee-Seung's algorithm, which we call the Matrix Multiplicative Update (MMU) algorithm, for computing PSD factorizations.  The MMU algorithm ensures that updates remain PSD by congruence scaling with the matrix geometric mean of appropriate PSD matrices, and it retains the simplicity of implementation that the multiplicative update algorithm for NMF enjoys.  Building on the Majorization-Minimization framework, we show that under our update scheme the squared loss objective is non-increasing and fixed points correspond to critical points.  The analysis relies on a Lieb's Concavity Theorem.  Beyond PSD factorizations, we show that the MMU algorithm can be also used as a primitive to calculate block-diagonal PSD factorizations and tensor PSD factorizations.  We demonstrate the utility of our method with experiments on real and synthetic data. ",https://api.openreview.net/pdf/00bdd750fa028ed3e7078a159af19f1909ea0af1.pdf,transformer;metric;llm,https://scholar.google.com/scholar?q=A+Non-commutative+Extension+of++Lee-Seung's+Algorithm+for+Positive+Semidefinite+Factorizations
Nearly-Tight and Oblivious Algorithms for Explainable Clustering,2021,NIPS,"['Buddhima Gamlath', 'Xinrui Jia', 'Adam Polak', 'Ola Svensson']",poster,"['algorithms', 'clustering', 'combinatorial optimization', 'explainability']","We study the problem of explainable clustering in the setting first formalized by Dasgupta, Frost, Moshkovitz, and Rashtchian (ICML 2020). A $k$-clustering is said to be explainable if it is given by a decision tree where each internal node splits data points with a threshold cut in a single dimension (feature), and each of the $k$ leaves corresponds to a cluster. We give an algorithm that outputs an explainable clustering that loses at most a factor of $O(\log^2 k)$ compared to an optimal (not necessarily explainable) clustering for the $k$-medians objective, and a factor of $O(k \log^2 k)$ for the $k$-means objective. This improves over the previous best upper bounds of $O(k)$ and $O(k^2)$, respectively, and nearly matches the previous $\Omega(\log k)$ lower bound for $k$-medians and our new $\Omega(k)$ lower bound for $k$-means. The algorithm is remarkably simple. In particular, given an initial not necessarily explainable clustering in $\mathbb{R}^d$, it is oblivious to the data points and runs in time $O(dk \log^2 k)$, independent of the number of data points $n$. Our upper and lower bounds also generalize to objectives given by higher $\ell_p$-norms.",https://api.openreview.net/pdf/7c651379fa471ae3b52a48fe34113c46d6bcc85f.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Nearly-Tight+and+Oblivious+Algorithms+for+Explainable+Clustering
"Scalars are universal: Equivariant machine learning, structured like classical physics",2021,NIPS,"['Soledad Villar', 'David W Hogg', 'Kate Storey-Fisher', 'Weichi Yao', 'Ben Blum-Smith']",poster,"['invariant theory', 'group theory', 'physics', 'regression', 'machine learning', 'symmetry', 'scalars']","There has been enormous progress in the last few years in designing  neural networks that respect the fundamental symmetries and coordinate freedoms of physical law. Some of these frameworks make use of irreducible representations, some make use of high-order tensor objects, and some apply symmetry-enforcing constraints. Different physical laws obey different combinations of fundamental symmetries, but a large fraction (possibly all) of classical physics is equivariant to translation, rotation, reflection (parity), boost (relativity), and permutations. Here we show that it is simple to parameterize universally approximating polynomial functions that are equivariant under these symmetries, or under the Euclidean, Lorentz, and Poincaré groups, at any dimensionality $d$. The key observation is that nonlinear O($d$)-equivariant (and related-group-equivariant) functions can be universally expressed in terms of a lightweight collection of scalars---scalar products and scalar contractions of the scalar, vector, and tensor inputs. We complement our theory with numerical examples that show that the scalar-based method is simple, efficient, and scalable. ",https://api.openreview.net/pdf/6a07b04f1bd31e145dd6d3bac6cf25557b1ad466.pdf,optimization;representation;online learning;llm,"https://scholar.google.com/scholar?q=Scalars+are+universal:+Equivariant+machine+learning,+structured+like+classical+physics"
Stronger NAS with Weaker Predictors,2021,NIPS,"['Junru Wu', 'Xiyang Dai', 'Dongdong Chen', 'Yinpeng Chen', 'Mengchen Liu', 'Ye Yu', 'Zhangyang Wang', 'Zicheng Liu', 'Mei Chen', 'Lu Yuan']",poster,"['Neural Architecture Search', 'Predictor', 'Bayesian Optimization']","Neural Architecture Search (NAS) often trains and evaluates a large number of architectures. Recent predictor-based NAS approaches attempt to alleviate such heavy computation costs with two key steps: sampling some architecture-performance pairs and fitting a proxy accuracy predictor. Given limited samples, these predictors, however, are far from accurate to locate top architectures due to the difficulty of fitting the huge search space. This paper reflects on a simple yet crucial question: if our final goal is to find the best architecture, do we really need to model the whole space well?. We propose a paradigm shift from fitting the whole architecture space using one strong predictor, to progressively fitting a search path towards the high-performance sub-space through a set of weaker predictors. As a key property of the weak predictors, their probabilities of sampling better architectures keep increasing. Hence we only sample a few well-performed architectures guided by the previously learned predictor and estimate a new better weak predictor. This embarrassingly easy framework, dubbed WeakNAS, produces coarse-to-fine iteration to gradually refine the ranking of sampling space. Extensive experiments demonstrate that WeakNAS costs fewer samples to find top-performance architectures on NAS-Bench-101 and NAS-Bench-201. Compared to state-of-the-art (SOTA) predictor-based NAS methods, WeakNAS outperforms all with notable margins, e.g., requiring at least 7.5x less samples to find global optimal on NAS-Bench-101. WeakNAS can also absorb their ideas to boost performance more. Further, WeakNAS strikes the new SOTA result of 81.3% in the ImageNet MobileNet Search Space. The code is available at: https://github.com/VITA-Group/WeakNAS.",https://api.openreview.net/pdf/e1fd2f8bb63212cc5bd489fe51dd5ca9de4b7f35.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Stronger+NAS+with+Weaker+Predictors
ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias,2021,NIPS,"['Yufei Xu', 'Qiming ZHANG', 'Jing Zhang', 'Dacheng Tao']",poster,"['Vision Transformer', 'Classification', 'Convolution', 'Inductive Bias']","Transformers have shown great potential in various computer vision tasks owing to their strong capability in modeling long-range dependency using the self-attention mechanism. Nevertheless, vision transformers treat an image as 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance. Alternatively, they require large-scale training data and longer training schedules to learn the IB implicitly. In this paper, we propose a new Vision Transformer Advanced by Exploring intrinsic IB from convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block in parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. Experiments on ImageNet as well as downstream tasks prove the superiority of ViTAE over the baseline transformer and concurrent works. Source code and pretrained models will be available at https://github.com/Annbless/ViTAE.",https://api.openreview.net/pdf/12fd6ce27076c9dff6592307ff2b89b1a1f7f78e.pdf,transformer;representation;llm,https://scholar.google.com/scholar?q=ViTAE:+Vision+Transformer+Advanced+by+Exploring+Intrinsic+Inductive+Bias
Topology-Imbalance Learning for Semi-Supervised Node Classification,2021,NIPS,"['Deli Chen', 'Yankai Lin', 'Guangxiang Zhao', 'Xuancheng Ren', 'Peng Li', 'Jie Zhou', 'Xu Sun']",poster,"['node classification', 'topology imbalance learning', 'semi-supervised learning', 'graph neural network']","The class imbalance problem, as an important issue in learning node representations, has drawn increasing attention from the community. Although the imbalance considered by existing studies roots from the unequal quantity of labeled examples in different classes (quantity imbalance), we argue that graph data expose a unique source of imbalance from the asymmetric topological properties of the labeled nodes, i.e., labeled nodes are not equal in terms of their structural role in the graph (topology imbalance). In this work, we first probe the previously unknown topology-imbalance issue, including its characteristics, causes, and threats to semisupervised node classification learning. We then provide a unified view to jointly analyzing the quantity- and topology- imbalance issues by considering the node influence shift phenomenon with the Label Propagation algorithm. In light of our analysis, we devise an influence conflict detection–based metric Totoro to measure the degree of graph topology imbalance and propose a model-agnostic method ReNode to address the topology-imbalance issue by re-weighting the influence of labeled nodes adaptively based on their relative positions to class boundaries. Systematic experiments demonstrate the effectiveness and generalizability of our method in relieving topology-imbalance issue and promoting semi-supervised node classification. The further analysis unveils varied sensitivity of different graph neural networks (GNNs) to topology imbalance, which may serve as a new perspective in evaluating GNN architectures.",https://api.openreview.net/pdf/eb121349aab2f5e8d4fc81294ff129010e13ec36.pdf,graph;transformer;representation;adaptive;metric;llm,https://scholar.google.com/scholar?q=Topology-Imbalance+Learning+for+Semi-Supervised+Node+Classification
Edge Representation Learning with Hypergraphs,2021,NIPS,"['Jaehyeong Jo', 'Jinheon Baek', 'Seul Lee', 'Dongki Kim', 'Minki Kang', 'Sung Ju Hwang']",poster,"['Graph Neural Network', 'Edge Representation Learning', 'Graph Pooling', 'Hypergraph']","Graph neural networks have recently achieved remarkable success in representing graph-structured data, with rapid progress in both the node embedding and graph pooling methods. Yet, they mostly focus on capturing information from the nodes considering their connectivity, and not much work has been done in representing the edges, which are essential components of a graph. However, for tasks such as graph reconstruction and generation, as well as graph classification tasks for which the edges are important for discrimination, accurately representing edges of a given graph is crucial to the success of the graph representation learning. To this end, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message-passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. We validate our edge representation learning method with hypergraphs on diverse graph datasets for graph representation and generation performance, on which our method largely outperforms existing graph representation learning methods. Moreover, our edge representation learning and pooling method also largely outperforms state-of-the-art graph pooling methods on graph classification, not only because of its accurate edge representation learning, but also due to its lossless compression of the nodes and removal of irrelevant edges for effective message-passing.",https://api.openreview.net/pdf/33dc403c1622ba56e536d077d5067b09d15c808a.pdf,graph;zero_few-shot;representation;generative model;llm,https://scholar.google.com/scholar?q=Edge+Representation+Learning+with+Hypergraphs
Towards Multi-Grained Explainability for Graph Neural Networks,2021,NIPS,"['Xiang Wang', 'Yingxin Wu', 'An Zhang', 'Xiangnan He', 'Tat-seng Chua']",poster,"['Graph Neural Networks', 'Multi-grained Explainability', 'Feature Attribution']","When a graph neural network (GNN) made a prediction, one raises question about explainability: “Which fraction of the input graph is most inﬂuential to the model’s decision?” Producing an answer requires understanding the model’s inner workings in general and emphasizing the insights on the decision for the instance at hand. Nonetheless, most of current approaches focus only on one aspect: (1) local explainability, which explains each instance independently, thus hardly exhibits the class-wise patterns; and (2) global explainability, which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy limits the ﬂexibility and effectiveness of explainers greatly. A performant paradigm towards multi-grained explainability is until-now lacking and thus a focus of our work. In this work, we exploit the pre-training and ﬁne-tuning idea to develop our explainer and generate multi-grained explanations. Speciﬁcally, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the ﬁne-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classiﬁcation over the leading baselines. Our codes and datasets are available at https://github.com/Wuyxin/ReFine.",https://api.openreview.net/pdf/4a67f991560e7138fce93ffea65e65a8c15ce20c.pdf,graph;llm,https://scholar.google.com/scholar?q=Towards+Multi-Grained+Explainability+for+Graph+Neural+Networks
KALE Flow: A Relaxed KL Gradient Flow for Probabilities with Disjoint Support,2021,NIPS,"['Pierre Glaser', 'Michael Arbel', 'Arthur Gretton']",poster,"['Generative Models', 'Kernel Methods', 'Optimal Transportation', 'Probability Divergences']","We study the gradient flow for a relaxed approximation to the Kullback-Leibler (KL) divergence
between a moving source and a fixed target distribution.
This approximation, termed the
KALE (KL approximate lower-bound estimator), solves a regularized version of
the Fenchel dual problem defining the KL over a restricted class of functions.
When using a Reproducing Kernel Hilbert Space (RKHS) to define the function
class, we show that the KALE continuously interpolates between the KL and the
Maximum Mean Discrepancy (MMD). Like the MMD and other Integral Probability
Metrics, the KALE remains well defined for mutually singular
distributions. Nonetheless, the KALE inherits from the limiting KL a greater 
sensitivity to mismatch in the support of the distributions, compared with the MMD. These two properties make the
KALE gradient flow particularly well suited when the target distribution is supported on a low-dimensional manifold. Under an assumption of sufficient smoothness of the trajectories, we show the global convergence of the KALE flow. We propose a particle implementation of the flow given initial samples from the source and the target distribution, which we use to empirically confirm the KALE's properties.",https://api.openreview.net/pdf/30a424c8c778f44fadb60c9c2e837f36ee1bb8d5.pdf,zero_few-shot;transformer;metric;flow;llm,https://scholar.google.com/scholar?q=KALE+Flow:+A+Relaxed+KL+Gradient+Flow+for+Probabilities+with+Disjoint+Support
Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning,2021,NIPS,"['Christopher Michael Rytting', 'David Wingate']",poster,"['natural language processing', 'reasoning', 'transfer learning', 'generalization']","Large natural language models (LMs) (such as GPT-3 or T5) demonstrate impressive abilities across a range of general NLP tasks. Here, we show that the knowledge embedded in such models provides a useful inductive bias, not just on traditional NLP tasks, but also in the nontraditional task of training a symbolic reasoning engine. We observe that these engines learn quickly and generalize in a natural way that reflects human intuition. For example, training such a system to model block-stacking might naturally generalize to stacking other types of objects because of structure in the real world that has been partially captured by the language describing it. We study several abstract textual reasoning tasks, such as object manipulation and navigation, and demonstrate multiple types of generalization to novel scenarios and the symbols that comprise them. We also demonstrate the surprising utility of $\textit{compositional learning}$, where a learner dedicated to mastering a complicated task gains an advantage by training on relevant simpler tasks instead of jumping straight to the complicated task. ",https://api.openreview.net/pdf/fbc62663adc09bd6ec3d0161eb4fce62427ca9c2.pdf,graph;llm,https://scholar.google.com/scholar?q=Leveraging+the+Inductive+Bias+of+Large+Language+Models+for+Abstract+Textual+Reasoning
Grounding Representation Similarity Through Statistical Testing,2021,NIPS,"['Frances Ding', 'Jean-Stanislas Denain', 'Jacob Steinhardt']",poster,"['representation similarity', 'dissimilarity', 'metric', 'CKA', 'CCA', 'Orthogonal Procrustes', 'benchmark', 'representation learning', 'probing', 'deep networks']","To understand neural network behavior, recent works quantitatively compare different networks' learned representations using canonical correlation analysis (CCA), centered kernel alignment (CKA), and other dissimilarity measures. Unfortunately, these widely used measures often disagree on fundamental observations, such as whether deep networks differing only in random initialization learn similar representations. These disagreements raise the question: which, if any, of these dissimilarity measures should we believe? We provide a framework to ground this question through a concrete test: measures should have \emph{sensitivity} to changes that affect functional behavior, and \emph{specificity} against changes that do not. We quantify this through a variety of functional behaviors including probing accuracy and robustness to distribution shift, and examine changes such as varying random initialization and deleting principal components. We find that current metrics exhibit different weaknesses, note that a classical baseline performs surprisingly well, and highlight settings where all metrics appear to fail, thus providing a challenge set for further improvement.",https://api.openreview.net/pdf/ec6c3b6ce8821e990d5fd7ede903619630c7b090.pdf,zero_few-shot;representation;metric;llm,https://scholar.google.com/scholar?q=Grounding+Representation+Similarity+Through+Statistical+Testing
"The Causal-Neural Connection: Expressiveness, Learnability, and Inference",2021,NIPS,"['Kevin Muyuan Xia', 'Kai-Zhan Lee', 'Yoshua Bengio', 'Elias Bareinboim']",poster,"['causal inference', 'deep learning', 'neural models', 'causal identification', 'causal estimation']","One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal  identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach.",https://api.openreview.net/pdf/5e132f99c4c62af5ba928bb0b5278a8a87bf8a4d.pdf,graph;optimization;inference;llm,"https://scholar.google.com/scholar?q=The+Causal-Neural+Connection:+Expressiveness,+Learnability,+and+Inference"
Bubblewrap: Online tiling and real-time flow prediction on neural manifolds,2021,NIPS,"['Anne Draelos', 'Pranjal Gupta', 'Na Young Jun', 'Chaichontat Sriworarat', 'John Pearson']",poster,"['neuroscience', 'neural populations', 'online', 'real-time']","While most classic studies of function in experimental neuroscience have focused on the coding properties of individual neurons, recent developments in recording technologies have resulted in an increasing emphasis on the dynamics of neural populations. This has given rise to a wide variety of models for analyzing population activity in relation to experimental variables, but direct testing of many neural population hypotheses requires intervening in the system based on current neural state, necessitating models capable of inferring neural state online. Existing approaches, primarily based on dynamical systems, require strong parametric assumptions that are easily violated in the noise-dominated regime and do not scale well to the thousands of data channels in modern experiments. To address this problem, we propose a method that combines fast, stable dimensionality reduction with a soft tiling of the resulting neural manifold, allowing dynamics to be approximated as a probability flow between tiles. This method can be fit efficiently using online expectation maximization, scales to tens of thousands of tiles, and outperforms existing methods when dynamics are noise-dominated or feature multi-modal transition probabilities. The resulting model can be trained at kiloHertz data rates, produces accurate approximations of neural dynamics within minutes, and generates predictions on submillisecond time scales. It retains predictive performance throughout many time steps into the future and is fast enough to serve as a component of closed-loop causal experiments.",https://api.openreview.net/pdf/e26b21dcf1686126fdb1ef08a43b8c2ad96dd0da.pdf,zero_few-shot;online learning;metric;flow;multimodal;llm,https://scholar.google.com/scholar?q=Bubblewrap:+Online+tiling+and+real-time+flow+prediction+on+neural+manifolds
Batch Active Learning at Scale,2021,NIPS,"['Gui Citovsky', 'Giulia DeSalvo', 'Claudio Gentile', 'Lazaros Karydas', 'Anand Rajagopalan', 'Afshin Rostamizadeh', 'Sanjiv Kumar']",poster,"['active learning', 'large scale', 'batch active learning']","The ability to train complex and highly effective models often requires an abundance of training data, which can easily become a bottleneck in cost, time, and computational resources. Batch active learning, which adaptively issues batched queries to a labeling oracle, is a common approach for addressing this problem. The practical benefits of batch sampling come with the downside of less adaptivity and the risk of sampling redundant examples within a batch -- a risk that grows with the batch size. In this work, we analyze an efficient active learning algorithm, which focuses on the large batch setting. In particular, we show that our sampling method, which combines notions of uncertainty and diversity, easily scales to batch sizes (100K-1M) several orders of magnitude larger than used in previous studies and provides significant improvements in model training efficiency compared to recent baselines. Finally, we provide an initial theoretical analysis, proving label complexity guarantees for a related sampling method, which we show is approximately equivalent to our sampling method in specific settings.",https://api.openreview.net/pdf/eb75c47ba97fe1a3a11db6d43e727602f83d1c37.pdf,adaptive;active learning;llm,https://scholar.google.com/scholar?q=Batch+Active+Learning+at+Scale
Weisfeiler and Lehman Go Cellular: CW Networks,2021,NIPS,"['Cristian Bodnar', 'Fabrizio Frasca', 'Nina Otter', 'Yu Guang Wang', 'Pietro Liò', 'Guido Montufar', 'Michael M. Bronstein']",poster,"['graph neural networks', 'graph representation learning', 'simplicial complexes', 'cell complexes', 'cw complexes', 'simplicial neural networks']","Graph Neural Networks (GNNs) are limited in their expressive power, struggle with long-range interactions and lack a principled way to model higher-order structures. These problems can be attributed to the strong coupling between the computational graph and the input graph structure. The recently proposed Message Passing Simplicial Networks naturally decouple these elements by performing message passing on the clique complex of the graph. Nevertheless, these models can be severely constrained by the rigid combinatorial structure of Simplicial Complexes (SCs). In this work, we extend recent theoretical results on SCs to regular Cell Complexes, topological objects that flexibly subsume SCs and graphs. We show that this generalisation provides a powerful set of graph ""lifting"" transformations, each leading to a unique hierarchical message passing procedure. The resulting methods, which we collectively call CW Networks (CWNs), are strictly more powerful than the WL test and not less powerful than the 3-WL test. In particular, we demonstrate the effectiveness of one such scheme, based on rings, when applied to molecular graph problems. The proposed architecture benefits from provably larger expressivity than commonly used GNNs, principled modelling of higher-order signals and from compressing the distances between nodes. We demonstrate that our model achieves state-of-the-art results on a variety of molecular datasets.",https://api.openreview.net/pdf/85e8b2601a55d73861d0e9b15c4d6832b82ff14d.pdf,graph;optimization;llm,https://scholar.google.com/scholar?q=Weisfeiler+and+Lehman+Go+Cellular:+CW+Networks
Towards understanding retrosynthesis by energy-based models,2021,NIPS,"['Ruoxi Sun', 'Hanjun Dai', 'Li Li', 'Steven Kearnes', 'Bo Dai']",poster,"['chemical application', 'energy-based model']","Retrosynthesis is the process of identifying a set of reactants to synthesize a target molecule. It is of vital importance to material design and drug discovery. Existing machine learning approaches based on language models and graph neural networks have achieved encouraging results. However, the inner connections of these models are rarely discussed, and rigorous evaluations of these models are largely in need. In this paper, we propose a framework that unifies sequence- and graph-based methods as energy-based models (EBMs) with different energy functions. This unified view establishes connections and reveals the differences between models, thereby enhancing our understanding of model design. We also provide a comprehensive assessment of performance to the community. Moreover, we present a novel dual variant within the framework that performs consistent training to induce the agreement between forward- and backward-prediction. This model improves the state-of-the-art of template-free methods with or without reaction types.  ",https://api.openreview.net/pdf/7ed1d68f98deab4c27d76b67f4070a6bcf62dce3.pdf,graph;llm,https://scholar.google.com/scholar?q=Towards+understanding+retrosynthesis+by+energy-based+models
Multimodal Few-Shot Learning with Frozen Language Models,2021,NIPS,"['Maria Tsimpoukelli', 'Jacob Menick', 'Serkan Cabi', 'S. M. Ali Eslami', 'Oriol Vinyals', 'Felix Hill']",poster,"['Language Modeling', 'Multimodal', 'Few-shot Learning']","When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model presented with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of any number of interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks.",https://api.openreview.net/pdf/ec87f429a486beb17ac4c8a1a1f0e9107d39fb49.pdf,zero_few-shot;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models
Sifting through the noise: Universal first-order methods for stochastic variational inequalities,2021,NIPS,"['Kimon Antonakopoulos', 'Thomas Pethick', 'Ali Kavis', 'Panayotis Mertikopoulos', 'Volkan Cevher']",poster,"['monotone variational inequalities', 'stochastic variational inequalities', 'adaptive methods', 'improved rates']","We examine a flexible algorithmic framework for solving monotone variational inequalities in the presence of randomness and uncertainty. The proposed template encompasses a wide range of popular first-order methods, including dual averaging, dual extrapolation and optimistic gradient algorithms – both adaptive and non-adaptive. Our first result is that the algorithm achieves the optimal rates of convergence for cocoercive problems when the profile of the randomness is known to the optimizer: $\mathcal{O}(1/\sqrt{T})$ for absolute noise profiles, and $\mathcal{O}(1/T)$ for relative ones. Subsequently, we drop all prior knowledge requirements (the absolute/relative variance of the randomness affecting the problem, the operator's cocoercivity constant, etc.), and we analyze an adaptive instance of the method that gracefully interpolates between the above rates – i.e. it achieves $\mathcal{O}(1/\sqrt{T})$ and $\mathcal{O}(1/T)$ in the absolute and relative cases, respectively. To our knowledge, this is the first universality result of its kind in the literature and, somewhat surprisingly, it shows that an extra-gradient proxy step is not required to achieve optimal rates.",https://api.openreview.net/pdf/defdaeedace228e9a2647d9544c849d05d849208.pdf,graph;adaptive;llm,https://scholar.google.com/scholar?q=Sifting+through+the+noise:+Universal+first-order+methods+for+stochastic+variational+inequalities
Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems,2021,NIPS,"['Jimmy T.H. Smith', 'Scott Linderman', 'David Sussillo']",poster,"['recurrent neural networks', 'RNNs', 'switching linear dynamical systems', 'SLDS', 'interpretability', 'dynamical systems', 'reverse engineering', 'fixed points']","Recurrent neural networks (RNNs) are powerful models for processing time-series data, but it remains challenging to understand how they function. Improving this understanding is of substantial interest to both the machine learning and neuroscience communities. The framework of reverse engineering a trained RNN by linearizing around its fixed points has provided insight, but the approach has significant challenges. These include difficulty choosing which fixed point to expand around when studying RNN dynamics and error accumulation when reconstructing the nonlinear dynamics with the linearized dynamics. We present a new model that overcomes these limitations by co-training an RNN with a novel switching linear dynamical system (SLDS) formulation. A first-order Taylor series expansion of the co-trained RNN and an auxiliary function trained to pick out the RNN's fixed points govern the SLDS dynamics. The results are a trained SLDS variant that closely approximates the RNN, an auxiliary function that can produce a fixed point for each point in state-space, and a trained nonlinear RNN whose dynamics have been regularized such that its first-order terms perform the computation, if possible. This model removes the post-training fixed point optimization and allows us to unambiguously study the learned dynamics of the SLDS at any point in state-space.  It also generalizes SLDS models to continuous manifolds of switching points while sharing parameters across switches. We validate the utility of the model on two synthetic tasks relevant to previous work reverse engineering RNNs. We then show that our model can be used as a drop-in in more complex architectures, such as LFADS, and apply this LFADS hybrid to analyze single-trial spiking activity from the motor system of a non-human primate.",https://api.openreview.net/pdf/90b251d98ad06b782701625e545624b35e2714f0.pdf,graph;optimization;zero_few-shot;online learning;llm,https://scholar.google.com/scholar?q=Reverse+engineering+recurrent+neural+networks+with+Jacobian+switching+linear+dynamical+systems
FLEX: Unifying Evaluation for Few-Shot NLP,2021,NIPS,"['Jonathan Bragg', 'Arman Cohan', 'Kyle Lo', 'Iz Beltagy']",poster,"['benchmarks', 'evaluation', 'few-shot', 'zero-shot', 'nlp', 'prompt-based models', 'pretrained language models', 'statistical analysis']","Few-shot NLP research is highly active, yet conducted in disjoint research threads with evaluation suites that lack challenging-yet-realistic testing setups and fail to employ careful experimental design. Consequently, the community does not know which techniques perform best or even if they outperform simple baselines. In response, we formulate the FLEX Principles, a set of requirements and best practices for unified, rigorous, valid, and cost-sensitive few-shot NLP evaluation. These principles include Sample Size Design, a novel approach to benchmark design that optimizes statistical accuracy and precision while keeping evaluation costs manageable. Following the principles, we release the FLEX benchmark, which includes four few-shot transfer settings, zero-shot evaluation, and a public leaderboard that covers diverse NLP tasks. In addition, we present UniFew, a prompt-based model for few-shot learning that unifies pretraining and finetuning prompt formats, eschewing complex machinery of recent prompt-based approaches in adapting downstream task formats to language model pretraining objectives. We demonstrate that despite simplicity, UniFew achieves results competitive with both popular meta-learning and prompt-based approaches.",https://api.openreview.net/pdf/63d846a513945049eaa41f434daa76d28ed30342.pdf,graph;zero_few-shot;meta-learning;transfer learning;active learning;llm,https://scholar.google.com/scholar?q=FLEX:+Unifying+Evaluation+for+Few-Shot+NLP
Rate-Optimal Subspace Estimation on Random Graphs,2021,NIPS,"['Zhixin Zhou', 'Fan Zhou', 'Ping Li', 'Cun-Hui Zhang']",poster,"['bipartite graph', 'network analysis', 'singular value thresholding', 'minimax theory']","We study the theory of random bipartite graph whose adjacency matrix is generated according to a connectivity matrix $M$. We consider the bipartite graph to be sparse, i.e., the entries of $M$ are upper bounded by certain sparsity parameter. We show that the performance of estimating the connectivity matrix $M$ depends on the sparsity of the graph. We focus on two measurement of performance of estimation: the error of estimating $M$ and the error of estimating the column space of $M$. In the first case, we consider the operator norm and Frobenius norm of the difference between the estimation and the true connectivity matrix. In the second case, the performance will be measured by the difference between the estimated projection matrix and the true projection matrix in operator norm and Frobenius norm. We will show that the estimators we propose achieve the minimax optimal rate.",https://api.openreview.net/pdf/7e9960481be9c0e2f6b6c89ce5ed6798379777ab.pdf,graph;sparse;llm,https://scholar.google.com/scholar?q=Rate-Optimal+Subspace+Estimation+on+Random+Graphs
Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models,2021,NIPS,"['Hannah Rose Kirk', 'Yennie Jun', 'Filippo Volpin', 'Haider Iqbal', 'Elias Benussi', 'Frederic A Dreyer', 'Aleksandar Shtedritski', 'Yuki Asano']",poster,"['Natural language processing (NLP)', 'generative language models', 'bias', 'GPT', 'occupations', 'intersections']","The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied `out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models \textit{should} learn - whether they should reflect or correct for existing inequalities.",https://api.openreview.net/pdf/440fa4da3ab1b67f9e9235c5097452bf825390b2.pdf,graph;generative model;llm,https://scholar.google.com/scholar?q=Bias+Out-of-the-Box:+An+Empirical+Analysis+of+Intersectional+Occupational+Biases+in+Popular+Generative+Language+Models
Nested Counterfactual Identification from Arbitrary Surrogate Experiments,2021,NIPS,"['Juan D. Correa', 'Sanghack Lee', 'Elias Bareinboim']",poster,"['counterfactuals', 'identification', 'causal inference', 'experimental data']","The Ladder of Causation describes three qualitatively different types of activities an agent may be interested in engaging in, namely, seeing (observational), doing (interventional), and imagining (counterfactual) (Pearl and Mackenzie, 2018). The inferential challenge imposed by the causal hierarchy is that data is collected by an agent observing or intervening in a system (layers 1 and 2), while its goal may be to understand what would have happened had it taken a different course of action, contrary to what factually ended up happening (layer 3). While there exists a solid understanding of the conditions under which cross-layer inferences are allowed from observations to interventions, the results are somewhat scarcer when targeting counterfactual quantities. In this paper, we study the identification of nested counterfactuals from an arbitrary combination of observations and experiments. Specifically, building on a more explicit definition of nested counterfactuals, we prove the counterfactual unnesting theorem (CUT), which allows one to map arbitrary nested counterfactuals to unnested ones. For instance, applications in mediation and fairness analysis usually evoke notions of direct, indirect, and spurious effects, which naturally require nesting. Second, we introduce a sufficient and necessary graphical condition for counterfactual identification from an arbitrary combination of observational and experimental distributions. Lastly, we develop an efficient and complete algorithm for identifying nested counterfactuals; failure of the algorithm returning an expression for a query implies it is not identifiable.
",https://api.openreview.net/pdf/aeb8f5859ff68a533adb38888697cbf89816b384.pdf,reinforcement learning;graph;zero_few-shot;inference;llm,https://scholar.google.com/scholar?q=Nested+Counterfactual+Identification+from+Arbitrary+Surrogate+Experiments
A first-order primal-dual method with adaptivity to local smoothness,2021,NIPS,"['Maria-Luiza Vladarean', 'Yura Malitsky', 'Volkan Cevher']",poster,"['adaptive', 'primal-dual', 'convex-concave', 'composite optimization', 'local smoothness', 'local Lipschitz continuity']","We consider the problem of finding a saddle point for the convex-concave objective $\min_x \max_y f(x) + \langle Ax, y\rangle - g^*(y)$, where $f$ is a convex function with locally Lipschitz gradient and $g$ is convex and possibly non-smooth. We propose an adaptive version of the Condat-Vũ algorithm, which alternates between primal gradient steps and dual proximal steps. The method achieves stepsize adaptivity through a simple rule involving $\|A\|$ and the norm of recently computed gradients of $f$. Under standard assumptions, we prove an $\mathcal{O}(k^{-1})$ ergodic convergence rate. Furthermore, when $f$ is also locally strongly convex and $A$ has full row rank we show that our method converges with a linear rate. Numerical experiments are provided for illustrating the practical performance of the algorithm.",https://api.openreview.net/pdf/0fb57956b1415460aa69482a1537423392552347.pdf,adaptive;llm,https://scholar.google.com/scholar?q=A+first-order+primal-dual+method+with+adaptivity+to+local+smoothness
Reinforcement learning for optimization of variational quantum circuit architectures,2021,NIPS,"['Mateusz Ostaszewski', 'Lea Marion Trenkwalder', 'Wojciech Masarczyk', 'Eleanor Scerri', 'Vedran Dunjko']",poster,"['Quantum Computing', 'Reinforcement Learning', 'Quantum Chemistry']","	The study of Variational Quantum Eigensolvers (VQEs) has been in the spotlight in recent times as they may lead to real-world applications of near-term quantum devices. However, their performance depends on the structure of the used variational ansatz, which requires balancing the depth and expressivity of the corresponding circuit. At the same time, near-term restrictions limit the depth of the circuit we can expect to run. Thus, the optimization of the VQE ansatz requires maximizing the expressivity of the circuit while maintaining low depth. In recent years, various methods for VQE structure optimization have been introduced but the capacities of machine learning to aid with this problem have not yet been extensively investigated. In this work, we propose a reinforcement learning algorithm that autonomously explores the space of possible ansatzes, identifying economic circuits which still yield accurate ground energy estimates. The algorithm uses a feedback-driven curriculum learning method that autonomously adapts the complexity of the learning problem to the current performance of the learning algorithm and it incrementally improves the accuracy of the result while minimizing the circuit depth. We showcase the performance of our algorithm on the problem of estimating the ground-state energy of lithium hydride (LiH) in various configurations. In this well-known benchmark problem, we achieve chemical accuracy and state-of-the-art results in terms of circuit depth.",https://api.openreview.net/pdf/1a09aab15ec897dbfe31bae0d05f65107aeb0185.pdf,reinforcement learning;optimization;zero_few-shot;curriculum learning;llm,https://scholar.google.com/scholar?q=Reinforcement+learning+for+optimization+of+variational+quantum+circuit+architectures
Towards Biologically Plausible Convolutional Networks,2021,NIPS,"['Roman Pogodin', 'Yash Mehta', 'Timothy P Lillicrap', 'Peter E. Latham']",poster,"['biologically plausible deep learning', 'convolutional networks', 'Hebbian plasticity']","Convolutional networks are ubiquitous in deep learning. They are particularly useful for images, as they reduce the number of parameters, reduce training time, and increase accuracy. However, as a model of the brain they are seriously problematic, since they require weight sharing - something real neurons simply cannot do. Consequently, while neurons in the brain can be locally connected (one of the features of convolutional networks), they cannot be convolutional. Locally connected but non-convolutional networks, however, significantly underperform convolutional ones. This is troublesome for studies that use convolutional networks to explain activity in the visual system. Here we study plausible alternatives to weight sharing that aim at the same regularization principle, which is to make each neuron within a pool react similarly to identical inputs. The most natural way to do that is by showing the network multiple translations of the same image, akin to saccades in animal vision. However, this approach requires many translations, and doesn't remove the performance gap. We propose instead to add lateral connectivity to a locally connected network, and allow learning via Hebbian plasticity. This requires the network to pause occasionally for a sleep-like phase of ""weight sharing"". This method enables locally connected networks to achieve nearly convolutional performance on ImageNet and improves their fit to the ventral stream data, thus supporting convolutional networks as a model of the visual stream.",https://api.openreview.net/pdf/8656a3bc83fdd3fb807a2611cb077a69455559ab.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Towards+Biologically+Plausible+Convolutional+Networks
Topographic VAEs learn Equivariant Capsules,2021,NIPS,"['T. Anderson Keller', 'Max Welling']",poster,"['Topographic Organization', 'Unsupervised', 'Equivariance', 'Variational Inference', 'Deep Generative Model', 'Disentanglement']","In this work we seek to bridge the concepts of topographic organization and equivariance in neural networks. To accomplish this, we introduce the Topographic VAE: a novel method for efficiently training deep generative models with topographically organized latent variables. We show that such a model indeed learns to organize its activations according to salient characteristics such as digit class, width, and style on MNIST. Furthermore, through topographic organization over time (i.e. temporal coherence), we demonstrate how predefined latent space transformation operators can be encouraged for observed transformed input sequences -- a primitive form of unsupervised learned equivariance. We demonstrate that this model successfully learns sets of approximately equivariant features (i.e. ""capsules"") directly from sequences and achieves higher likelihood on correspondingly transforming test sequences. Equivariance is verified quantitatively by measuring the approximate commutativity of the inference network and the sequence transformations. Finally, we demonstrate approximate equivariance to complex transformations, expanding upon the capabilities of existing group equivariant neural networks. ",https://api.openreview.net/pdf/5bfbfd23b559af61f37beff36c358e11926448a3.pdf,graph;vae;generative model;inference;llm,https://scholar.google.com/scholar?q=Topographic+VAEs+learn+Equivariant+Capsules
Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks,2021,NIPS,"['Jesse Hagenaars', 'Federico Paredes Valles', 'Guido De Croon']",poster,"['spiking neural networks', 'neuromorphic computing', 'event camera', 'self-supervised learning', 'optical flow']","The field of neuromorphic computing promises extremely low-power and low-latency sensing and processing. Challenges in transferring learning algorithms from traditional artificial neural networks (ANNs) to spiking neural networks (SNNs) have so far prevented their application to large-scale, complex regression tasks. Furthermore, realizing a truly asynchronous and fully neuromorphic pipeline that maximally attains the abovementioned benefits involves rethinking the way in which this pipeline takes in and accumulates information. In the case of perception, spikes would be passed as-is and one-by-one between an event camera and an SNN, meaning all temporal integration of information must happen inside the network. In this article, we tackle these two problems. We focus on the complex task of learning to estimate optical flow from event-based camera inputs in a self-supervised manner, and modify the state-of-the-art ANN training pipeline to encode minimal temporal information in its inputs. Moreover, we reformulate the self-supervised loss function for event-based optical flow to improve its convexity. We perform experiments with various types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs, we investigate the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We find that initialization and surrogate gradient width play a crucial part in enabling learning with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve performance. We show that the performance of the proposed ANNs and SNNs are on par with that of the current state-of-the-art ANNs trained in a self-supervised manner.",https://api.openreview.net/pdf/78f79e681ffc7ef22ef17ef47e3f6d752fbc6188.pdf,optimization;zero_few-shot;adaptive;sparse;transfer learning;flow;llm,https://scholar.google.com/scholar?q=Self-Supervised+Learning+of+Event-Based+Optical+Flow+with+Spiking+Neural+Networks
An Improved Analysis of Gradient Tracking for Decentralized Machine Learning,2021,NIPS,"['Anastasia Koloskova', 'Tao Lin', 'Sebastian U Stich']",poster,"['Decentralized stochastic optimization', 'Gradient tracking', 'non-iid data', 'decentralized optimization']","We consider decentralized machine learning over a network where the training data is distributed across $n$ agents, each of which can compute stochastic model updates on their local data. The agent's common goal is to find a model that minimizes the average of all local loss functions. While gradient tracking (GT) algorithms can overcome a key challenge, namely accounting for differences between workers' local data distributions, the known convergence rates for GT algorithms are not optimal with respect to their dependence on the mixing parameter $p$ (related to the spectral gap of the connectivity matrix).
We provide a tighter analysis of the GT method in the stochastic strongly convex, convex and non-convex settings. We improve the dependency on $p$ from $\mathcal{O}(p^{-2})$ to $\mathcal{O}(p^{-1}c^{-1})$ in the noiseless case and from $\mathcal{O}(p^{-3/2})$ to $\mathcal{O}(p^{-1/2}c^{-1})$ in the general stochastic case, where $c \geq p$ is related to the negative eigenvalues of the connectivity matrix (and is a constant in most practical applications). This improvement was possible due to a new proof technique which could be of independent interest.",https://api.openreview.net/pdf/990a94271fb1f1d0f61d688798028b2b1586e02c.pdf,reinforcement learning;llm,https://scholar.google.com/scholar?q=An+Improved+Analysis+of+Gradient+Tracking+for+Decentralized+Machine+Learning
Three-dimensional spike localization and improved motion correction for Neuropixels recordings,2021,NIPS,"['Julien Boussard', 'Erdem Varol', 'Hyun Dong Lee', 'Nishchal Dethe', 'Liam Paninski']",poster,"['Spike-sorting', 'Neuropixels', 'Localization', 'Registration']","Neuropixels (NP) probes are dense linear multi-electrode arrays that have rapidly become essential tools for studying the electrophysiology of large neural populations.  Unfortunately, a number of challenges remain in analyzing the large datasets output by these probes.  
Here we introduce several new methods for extracting useful spiking information from NP probes.  First, we use a simple point neuron model, together with a neural-network denoiser, to efficiently map spikes detected on the probe into three-dimensional localizations.  Previous methods localized spikes in two dimensions only; we show that the new localization approach is significantly more robust and provides an improved feature set for clustering spikes according to neural identity (``spike sorting"").  Next, we apply a Poisson denoising method to the resulting three-dimensional point-cloud representation of the data, and show that the resulting 3D images can be accurately registered over time, leading to improved tracking of time-varying neural activity over the probe, and in turn, crisper estimates of neural clusters over time. The code to reproduce our results and an example neuropixels dataset is provided in the supplementary material.",https://api.openreview.net/pdf/acfd6d3cd865038fa6c913397eac332373fb8766.pdf,representation;3d;llm,https://scholar.google.com/scholar?q=Three-dimensional+spike+localization+and+improved+motion+correction+for+Neuropixels+recordings
"On Plasticity, Invariance, and Mutually Frozen Weights in Sequential Task Learning",2021,NIPS,"['Julian G. Zilly', 'Alessandro Achille', 'Andrea Censi', 'Emilio Frazzoli']",poster,"['Transfer learning', 'Deep learning', 'Sparsity', 'Invariance', 'Sequential learning', 'Critical learning periods', 'Curriculum learning']","Plastic neural networks have the ability to adapt to new tasks. However, in a continual learning setting, the configuration of parameters learned in previous tasks can severely reduce the adaptability to future tasks. In particular, we show that, when using weight decay, weights in successive layers of a deep network may become ""mutually frozen"". This has a double effect: on the one hand, it makes the network updates more invariant to nuisance factors, providing a useful bias for future tasks. On the other hand, it can prevent the network from learning new tasks that require significantly different features. In this context, we find that the local input sensitivity of a deep model is correlated with its ability to adapt, thus leading to an intriguing trade-off between adaptability and invariance when training a deep model more than once. We then show that a simple intervention that ""resets"" the mutually frozen connections can improve transfer learning on a variety of visual classification tasks. The efficacy of ""resetting"" itself depends on the size of the target dataset and the difference of the pre-training and target domains, allowing us to achieve state-of-the-art results on some datasets.",https://api.openreview.net/pdf/d52f3459edceb041fb90a1ac1651149956c4a3b2.pdf,zero_few-shot;transfer learning;llm,"https://scholar.google.com/scholar?q=On+Plasticity,+Invariance,+and+Mutually+Frozen+Weights+in+Sequential+Task+Learning"
Distilling Meta Knowledge on Heterogeneous Graph for Illicit Drug Trafficker Detection on Social Media,2021,NIPS,"['Yiyue Qian', 'Yiming Zhang', 'Yanfang Ye', 'Chuxu Zhang']",poster,"['graph representation learning', 'few-shot learning', 'drug trafficker detection']","Driven by the considerable profits, the crime of drug trafficking (a.k.a. illicit drug trading) has co-evolved with modern technologies, e.g., social media such as Instagram has become a popular platform for marketing and selling illicit drugs. The activities of online drug trafficking are nimble and resilient, which call for novel techniques to effectively detect, disrupt, and dismantle illicit drug trades. In this paper, we propose a holistic framework named MetaHG to automatically detect illicit drug traffickers on social media (i.e., Instagram), by tackling the following two new challenges: (1) different from existing works which merely focus on analyzing post content, MetaHG is capable of jointly modeling multi-modal content and relational structured information on social media for illicit drug trafficker detection; (2) in addition, through the proposed meta-learning technique, MetaHG addresses the issue of requiring sufficient data for model training. More specifically, in our proposed MetaHG, we first build a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on social media.  Then, we employ a relation-based graph convolutional neural network to learn node (i.e., user) representations over the built HG, in which we introduce graph structure refinement to compensate the sparse connection among entities in the HG for more robust node representation learning. Afterwards, we propose a meta-learning algorithm for model optimization. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving the model. Extensive experiments based on the real-world data collected from Instagram demonstrate that the proposed MetaHG outperforms state-of-the-art methods.",https://api.openreview.net/pdf/a177ec106a4d29160d088477c4532d7ebf9ff427.pdf,graph;optimization;zero_few-shot;representation;online learning;meta-learning;sparse;distillation;multimodal;llm,https://scholar.google.com/scholar?q=Distilling+Meta+Knowledge+on+Heterogeneous+Graph+for+Illicit+Drug+Trafficker+Detection+on+Social+Media
Exploring Forensic Dental Identification with Deep Learning,2021,NIPS,"['Yuan Liang', 'Weikun Han', 'Liang Qiu', 'Chen Wu', 'Yiting Shao', 'Kun Wang', 'Lei He']",poster,"['person identification', 'medical imaging', 'forensics']","Dental forensic identification targets to identify persons with dental traces.
The task is vital for the investigation of criminal scenes and mass disasters because of the resistance of dental structures and the wide-existence of dental imaging. 
However, no widely accepted automated solution is available for this labour-costly task. 
In this work, we pioneer to study deep learning for dental forensic identification based on panoramic radiographs. 
We construct a comprehensive benchmark with various dental variations that can adequately reflect the difficulties of the task. 
By considering the task's unique challenges, we propose FoID, a deep learning method featured by: (\textit{i}) clinical-inspired attention localization, (\textit{ii}) domain-specific augmentations that enable instance discriminative learning, and (\textit{iii}) transformer-based self-attention mechanism that dynamically reasons the relative importance of attentions. 
We show that FoID can outperform traditional approaches by at least \textbf{22.98\%} in terms of Rank-1 accuracy, and outperform strong CNN baselines by at least \textbf{10.50\%} in terms of mean Average Precision (mAP). 
Moreover, extensive ablation studies verify the effectiveness of each building blocks of FoID. 
Our work can be a first step towards the automated system for forensic identification among large-scale multi-site databases. 
Also, the proposed techniques, \textit{e.g.}, self-attention mechanism, can also be meaningful for other identification tasks, \textit{e.g.}, pedestrian re-identification.
Related data and codes can be found at \href{https://github.com/liangyuandg/FoID}{https://github.com/liangyuandg/FoID}. ",https://api.openreview.net/pdf/6ce3cf9ee356cafa9b085215ed3404429a097b83.pdf,graph;transformer;augmentation;llm,https://scholar.google.com/scholar?q=Exploring+Forensic+Dental+Identification+with+Deep+Learning
Associative Memories via Predictive Coding,2021,NIPS,"['Tommaso Salvatori', 'Yuhang Song', 'Yujian Hong', 'Lei Sha', 'Simon Frieder', 'Zhenghua Xu', 'Rafal Bogacz', 'Thomas Lukasiewicz']",poster,"['Deep Learning', 'Associative Memory', 'Cognitive Science']","Associative memories in the brain receive and store patterns of activity registered by the sensory neurons, and are able to retrieve them when necessary. Due to their importance in human intelligence, computational models of associative memories have been developed for several decades now. In this paper, we present a novel neural model for realizing associative memories, which is based on a hierarchical generative network that receives external stimuli via sensory neurons. It is trained using predictive coding, an error-based learning algorithm inspired by information processing in the cortex. To test the model's capabilities, we perform multiple retrieval experiments from both corrupted and incomplete data points. In an extensive comparison, we show that this new model outperforms in retrieval accuracy and robustness popular associative memory models, such as  autoencoders trained via backpropagation, and modern Hopfield networks. In particular, in completing partial data points, our model achieves remarkable results on natural image datasets, such as ImageNet, with a surprisingly high accuracy, even when only a tiny fraction of pixels of the original images is presented. Our model provides a plausible framework to study learning and retrieval of memories in the brain, as it closely mimics the behavior of the hippocampus as a memory index and generative model.",https://api.openreview.net/pdf/eff8811a2b36b448752d9e709f3c1c47da02f106.pdf,graph;generative model;llm,https://scholar.google.com/scholar?q=Associative+Memories+via+Predictive+Coding
The Effect of the Intrinsic Dimension on the Generalization of Quadratic Classifiers,2021,NIPS,"['Fabian Latorre', 'Leello Tadesse Dadi', 'Paul Rolland', 'Volkan Cevher']",poster,"['intrinsic dimension', 'generalization', 'rademacher complexity', 'statistical learning theory']","It has been recently observed that neural networks, unlike kernel methods, enjoy a reduced sample complexity when the distribution is isotropic (i.e., when the covariance matrix is the identity). We find that this sensitivity to the data distribution is not exclusive to neural networks, and the same phenomenon can be observed on the class of quadratic classifiers (i.e., the sign of a quadratic polynomial) with a nuclear-norm constraint. We demonstrate this by deriving an upper bound on the Rademacher Complexity that depends on two key quantities: (i) the intrinsic dimension, which is a measure of isotropy, and (ii) the largest eigenvalue of the second moment (covariance) matrix of the distribution. Our result improves the dependence on the dimension over the best previously known bound and precisely quantifies the relation between the sample complexity and the level of isotropy of the distribution.",https://api.openreview.net/pdf/c827813b09650a1f2843ee2b16b3341afceb9a43.pdf,optimization;llm,https://scholar.google.com/scholar?q=The+Effect+of+the+Intrinsic+Dimension+on+the+Generalization+of+Quadratic+Classifiers
Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition,2021,NIPS,"['Yulin Wang', 'Rui Huang', 'Shiji Song', 'Zeyi Huang', 'Gao Huang']",poster,"['Vision Transformer', 'Efficient Inference', 'Image Recognition']","Vision Transformers (ViT) have achieved remarkable success in large-scale image recognition. They split every 2D image into a fixed number of patches, each of which is treated as a token. Generally, representing an image with more tokens would lead to higher prediction accuracy, while it also results in drastically increased computational cost. To achieve a decent trade-off between accuracy and speed, the number of tokens is empirically set to 16x16 or 14x14. In this paper, we argue that every image has its own characteristics, and ideally the token number should be conditioned on each individual input. In fact, we have observed that there exist a considerable number of “easy” images which can be accurately predicted with a mere number of 4x4 tokens, while only a small fraction of “hard” ones need a finer representation. Inspired by this phenomenon, we propose a Dynamic Transformer to automatically configure a proper number of tokens for each input image. This is achieved by cascading multiple Transformers with increasing numbers of tokens, which are sequentially activated in an adaptive fashion at test time, i.e., the inference is terminated once a sufficiently confident prediction is produced. We further design efficient feature reuse and relationship reuse mechanisms across different components of the Dynamic Transformer to reduce redundant computations. Extensive empirical results on ImageNet, CIFAR-10, and CIFAR-100 demonstrate that our method significantly outperforms the competitive baselines in terms of both theoretical computational efficiency and practical inference speed. Code and pre-trained models (based on PyTorch and MindSpore) are available at https://github.com/blackfeather-wang/Dynamic-Vision-Transformer and https://github.com/blackfeather-wang/Dynamic-Vision-Transformer-MindSpore.",https://api.openreview.net/pdf/d792afe79f0d78702921c3518f9c14c2c18aca73.pdf,transformer;representation;adaptive;inference;llm,https://scholar.google.com/scholar?q=Not+All+Images+are+Worth+16x16+Words:+Dynamic+Transformers+for+Efficient+Image+Recognition
Adaptive Proximal Gradient Methods for Structured Neural Networks,2021,NIPS,"['Jihun Yun', 'Aurelie Lozano', 'Eunho Yang']",poster,"['Proximal Gradient Descent for Adaptive Methods', 'Stochastic Optimization', 'Non-convex Optimization']","We consider the training of structured neural networks where the regularizer can be non-smooth and possibly non-convex. While popular machine learning libraries have resorted to stochastic (adaptive) subgradient approaches, the use of proximal gradient methods in the stochastic setting has been little explored and warrants further study, in particular regarding the incorporation of adaptivity. Towards this goal, we present a general framework of stochastic proximal gradient descent methods that allows for arbitrary positive preconditioners and lower semi-continuous regularizers. We derive two important instances of our framework: (i) the first proximal version of \textsc{Adam}, one of the most popular adaptive SGD algorithm, and (ii) a revised version of ProxQuant for quantization-specific regularizers, which improves upon the original approach by incorporating the effect of preconditioners in the proximal mapping computations. We provide convergence guarantees for our framework and show that adaptive gradient methods can have faster convergence in terms of constant than vanilla SGD for sparse data. Lastly, we demonstrate the superiority of stochastic proximal methods compared to subgradient-based approaches via extensive experiments. Interestingly, our results indicate that the benefit of proximal approaches over sub-gradient counterparts is more pronounced for non-convex regularizers than for convex ones.",https://api.openreview.net/pdf/7cffb0ed7627b9e46dcdd64a5c4ce8741fab6296.pdf,graph;zero_few-shot;adaptive;sparse;llm,https://scholar.google.com/scholar?q=Adaptive+Proximal+Gradient+Methods+for+Structured+Neural+Networks
Label-Imbalanced and Group-Sensitive Classification under Overparameterization,2021,NIPS,"['Ganesh Ramachandra Kini', 'Orestis Paraskevas', 'Samet Oymak', 'Christos Thrampoulidis']",poster,"['fairness', 'overparameterization', 'cost-sensitive methods']","The goal in label-imbalanced and group-sensitive classification is to optimize relevant metrics such as balanced error and equal opportunity. Classical methods, such as weighted cross-entropy, fail when training deep nets to the terminal phase of training (TPT), that is training beyond zero training error. This observation has motivated recent flurry of activity in developing heuristic alternatives following the intuitive mechanism of promoting larger margin for minorities. In contrast to previous heuristics, we follow a principled analysis explaining how different loss adjustments affect margins. First, we prove that for all linear classifiers trained in TPT, it is necessary to introduce multiplicative, rather than additive, logit adjustments so that the interclass margins change appropriately. To show this, we discover a connection of the multiplicative CE modification to the cost-sensitive support-vector machines. Perhaps counterintuitively, we also find that, at the start of training, the same multiplicative weights can actually harm the minority classes. Thus, while additive adjustments are ineffective in the TPT, we show that they can speed up convergence by countering the initial negative effect of the multiplicative weights. Motivated by these findings, we formulate the vector-scaling (VS) loss, that captures existing techniques as special cases. Moreover, we introduce a natural extension of the VS-loss to group-sensitive classification, thus treating the two common types of imbalances (label/group) in a unifying way. Importantly, our experiments on state-of-the-art datasets are fully consistent with our theoretical insights and confirm the superior performance of our algorithms. Finally, for imbalanced Gaussian-mixtures data, we perform a generalization analysis, revealing tradeoffs between balanced / standard error and equal opportunity.",https://api.openreview.net/pdf/24c2b3d96a5133f7951351a02e900989f63ca39f.pdf,graph;zero_few-shot;transformer;metric;llm,https://scholar.google.com/scholar?q=Label-Imbalanced+and+Group-Sensitive+Classification+under+Overparameterization
NovelD: A Simple yet Effective Exploration Criterion,2021,NIPS,"['Tianjun Zhang', 'Huazhe Xu', 'Xiaolong Wang', 'Yi Wu', 'Kurt Keutzer', 'Joseph E. Gonzalez', 'Yuandong Tian']",poster,"['RL', 'Exploration']","Efficient exploration under sparse rewards remains a key challenge in deep reinforcement learning. Previous exploration methods (e.g., RND) have achieved strong results in multiple hard tasks. However, if there are multiple novel areas to explore, these methods often focus quickly on one without sufficiently trying others (like a depth-wise first search manner). In some scenarios (e.g., four corridor environment in Sec 4.2), we observe they explore in one corridor for long and fail to cover all the states. On the other hand, in theoretical RL, with optimistic initialization and the inverse square root of visitation count as a bonus, it won't suffer from this and explores different novel regions alternatively (like a breadth-first search manner). In this paper, inspired by this, we propose a simple but effective criterion called NovelD by weighting every novel area approximately equally. Our algorithm is very simple but yet shows comparable performance or even outperforms multiple SOTA exploration methods in many hard exploration tasks. Specifically, NovelD solves all the static procedurally-generated tasks in Mini-Grid with just 120M environment steps, without any curriculum learning. In comparison, the previous SOTA only solves 50% of them. NovelD also achieves SOTA on multiple tasks in NetHack, a rogue-like game that contains more challenging procedurally-generated environments. In multiple Atari games (e.g., MonteZuma's Revenge, Venture, Gravitar), NovelD outperforms RND. We analyze NovelD thoroughly in MiniGrid and found that empirically it helps the agent explore the environment more uniformly with a focus on exploring beyond the boundary.  ",https://api.openreview.net/pdf/a36a0b6118d799891ab137f5e99a1caea9f4cc49.pdf,reinforcement learning;graph;sparse;curriculum learning;llm,https://scholar.google.com/scholar?q=NovelD:+A+Simple+yet+Effective+Exploration+Criterion
Physics-Aware Downsampling with Deep Learning for Scalable Flood Modeling,2021,NIPS,"['Niv Giladi', 'Zvika Ben-Haim', 'Sella Nevo', 'Yossi Matias', 'Daniel Soudry']",poster,"['Partial differential equation', 'Inundation modeling', 'Physics-informed', 'Downsampling', 'Flood modeling']","Background. Floods are the most common natural disaster in the world, affecting the lives of hundreds of millions. Flood forecasting is therefore a vitally important endeavor, typically achieved using physical water flow simulations, which rely on accurate terrain elevation maps. However, such simulations, based on solving partial differential equations, are computationally prohibitive on a large scale. This scalability issue is commonly alleviated using a coarse grid representation of the elevation map, though this representation may distort crucial terrain details, leading to significant inaccuracies in the simulation.\\
Contributions. We train a deep neural network to perform physics-informed downsampling of the terrain map: we optimize the coarse grid representation of the terrain maps, so that the flood prediction will match the fine grid solution. For the learning process to succeed, we configure a dataset specifically for this task. We demonstrate that with this method, it is possible to achieve a significant reduction in computational cost, while maintaining an accurate solution. A reference implementation accompanies the paper as well as documentation and code for dataset reproduction.",https://api.openreview.net/pdf/09eff98937aa01919b1190b296d69be53c308943.pdf,zero_few-shot;representation;flow;llm,https://scholar.google.com/scholar?q=Physics-Aware+Downsampling+with+Deep+Learning+for+Scalable+Flood+Modeling
Best-case lower bounds in online learning,2021,NIPS,"['Cristóbal A Guzmán', 'Nishant A Mehta', 'Ali Mortazavi']",poster,"['online learning', 'online convex optimization', 'learning theory']","Much of the work in online learning focuses on the study of sublinear upper bounds on the regret. In this work, we initiate the study of best-case lower bounds in online convex optimization, wherein we bound the largest \emph{improvement} an algorithm can obtain relative to the single best action in hindsight. This problem is motivated by the goal of better understanding the adaptivity of a learning algorithm. Another motivation comes from fairness: it is known that best-case lower bounds are instrumental in obtaining algorithms for decision-theoretic online learning (DTOL) that satisfy a notion of group fairness. Our contributions are a general method to provide best-case lower bounds in Follow The Regularized Leader (FTRL) algorithms with time-varying regularizers, which we use to show that best-case lower bounds are of the same order as existing upper regret bounds: this includes situations with a fixed learning rate, decreasing learning rates, timeless methods, and adaptive gradient methods. In stark contrast, we show that the linearized version of FTRL can attain negative linear regret. Finally, in DTOL with two experts and binary losses, we fully characterize the best-case sequences, which provides a finer understanding of the best-case lower bounds.",https://api.openreview.net/pdf/f2a0e3ba084ea30b8b8a1df029d14394a7c1443c.pdf,optimization;zero_few-shot;online learning;adaptive;llm,https://scholar.google.com/scholar?q=Best-case+lower+bounds+in+online+learning
A Topological Perspective on Causal Inference,2021,NIPS,"['Duligur Ibeling', 'Thomas F Icard']",poster,"['causal inference', 'statistical learning theory', 'topology']","This paper presents a topological learning-theoretic perspective on causal inference by introducing a series of topologies defined on general spaces of structural causal models (SCMs). As an illustration of the framework we prove a topological causal hierarchy theorem, showing that substantive assumption-free causal inference is possible only in a meager set of SCMs. Thanks to a known correspondence between open sets in the weak topology and statistically verifiable hypotheses, our results show that inductive assumptions sufficient to license valid causal inferences are statistically unverifiable in principle. Similar to no-free-lunch theorems for statistical inference, the present results clarify the inevitability of substantial assumptions for causal inference. An additional benefit of our topological approach is that it easily accommodates SCMs with infinitely many variables. We finally suggest that our framework may be helpful for the positive project of exploring and assessing alternative causal-inductive assumptions.",https://api.openreview.net/pdf/a57f825756fb448e262532dcaaa0384fbcafbbba.pdf,zero_few-shot;inference;llm,https://scholar.google.com/scholar?q=A+Topological+Perspective+on+Causal+Inference
Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph Embedding,2021,NIPS,"['Tengwei Song', 'Jie Luo', 'Lei Huang']",poster,"['knowledge graph embedding', 'transitivity', 'projection', 'relation pattern']","Knowledge graph embedding models learn the representations of entities and relations in the knowledge graphs for predicting missing links (relations) between entities. Their effectiveness are deeply affected by the ability of modeling and inferring different relation patterns such as symmetry, asymmetry, inversion, composition and transitivity. Although existing models are already able to model many of these relations patterns, transitivity, a very common relation pattern, is still not been fully supported. In this paper, we first theoretically show that the transitive relations can be modeled with projections. We then propose the Rot-Pro model which combines the projection and relational rotation together. We prove that Rot-Pro can infer all the above relation patterns. Experimental results show that the proposed Rot-Pro model  effectively learns the transitivity pattern and achieves the state-of-the-art results on the link prediction task in the datasets containing transitive relations.",https://api.openreview.net/pdf/1e72ede870f92cd60e4b97c68a248e6ccf4c8d7c.pdf,graph;representation;llm,https://scholar.google.com/scholar?q=Rot-Pro:+Modeling+Transitivity+by+Projection+in+Knowledge+Graph+Embedding
The Elastic Lottery Ticket Hypothesis,2021,NIPS,"['Xiaohan Chen', 'Yu Cheng', 'Shuohang Wang', 'Zhe Gan', 'Jingjing Liu', 'Zhangyang Wang']",poster,['Lottery Ticket Hypothesis'],"Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse trainable subnetworks, or winning tickets, which can be trained in isolation to achieve similar or even better performance compared to the full models. Despite many efforts being made, the most effective method to identify such winning tickets is still Iterative Magnitude-based Pruning (IMP), which is computationally expensive and has to be run thoroughly for every different network. A natural question that comes in is: can we “transform” the winning ticket found in one network to another with a different architecture, yielding a winning ticket for the latter at the beginning, without re-doing the expensive IMP? Answering this question is not only practically relevant for efficient “once-for-all” winning ticket ﬁnding, but also theoretically appealing for uncovering inherently scalable sparse patterns in networks. We conduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety of strategies to tweak the winning tickets found from different networks of the same model family (e.g., ResNets). Based on these results, we articulate the Elastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or dropping) and re-ordering layers for one network, its corresponding winning ticket could be stretched (or squeezed) into a subnetwork for another deeper (or shallower) network from the same family, whose performance is nearly the same competitive as the latter’s winning ticket directly found by IMP. We have also extensively compared E-LTH with pruning-at-initialization and dynamic sparse training methods, as well as discussed the generalizability of E-LTH to different model families, layer types, and across datasets. Code is available at https://github.com/VITA-Group/ElasticLTH.",https://api.openreview.net/pdf/7791f0ee34f78fe1d23902ddac6f407a456a43fd.pdf,graph;zero_few-shot;transformer;sparse;llm,https://scholar.google.com/scholar?q=The+Elastic+Lottery+Ticket+Hypothesis
Hyperparameter Tuning is All You Need for LISTA,2021,NIPS,"['Xiaohan Chen', 'Jialin Liu', 'Zhangyang Wang', 'Wotao Yin']",poster,"['LISTA', 'unrolling']","Learned Iterative Shrinkage-Thresholding Algorithm (LISTA) introduces the concept of unrolling an iterative algorithm and training it like a neural network. It has had great success on sparse recovery. In this paper, we show that adding momentum to intermediate variables in the LISTA network achieves a better convergence rate and, in particular, the network with instance-optimal parameters is superlinearly convergent. Moreover, our new theoretical results lead to a practical approach of automatically and adaptively calculating the parameters of a LISTA network layer based on its previous layers. Perhaps most surprisingly, such an adaptive-parameter procedure reduces the training of LISTA to tuning only three hyperparameters from data: a new record set in the context of the recent advances on trimming down LISTA complexity. We call this new ultra-light weight network HyperLISTA. Compared to state-of-the-art LISTA models, HyperLISTA achieves almost the same performance on seen data distributions and performs better when tested on unseen distributions (speciﬁcally, those with different sparsity levels and nonzero magnitudes). Code is available: https://github.com/VITA-Group/HyperLISTA.",https://api.openreview.net/pdf/4d58f639350de768f7c8e26381eb4df302fee441.pdf,adaptive;sparse;llm,https://scholar.google.com/scholar?q=Hyperparameter+Tuning+is+All+You+Need+for+LISTA
Topological Relational Learning on Graphs,2021,NIPS,"['Yuzhou Chen', 'Baris Coskunuzer', 'Yulia Gel']",poster,"['Topological relational learning', 'Graph neural networks', 'Persistence based graph representation', 'Robust graph learning']","Graph neural networks (GNNs) have emerged as a powerful tool for graph classification and representation learning. However, GNNs tend to suffer from over-smoothing problems and are vulnerable to graph perturbations. To address these challenges, we propose a novel topological neural framework of topological relational inference (TRI) which allows for integrating higher-order graph information to GNNs and for systematically learning a local graph structure. The key idea is to rewire the original graph by using the persistent homology of the small neighborhoods of the nodes and then to incorporate the extracted topological summaries as the side information into the local algorithm. As a result, the new framework enables us to harness both the conventional information on the graph structure and information on higher order topological properties of the graph. We derive theoretical properties on stability of the new local topological representation of the graph and discuss its implications on the graph algebraic connectivity. The experimental results on node classification tasks demonstrate that the new TRI-GNN outperforms all 14 state-of-the-art baselines on 6 out 7 graphs and exhibit higher robustness to perturbations, yielding up to 10\% better performance under noisy scenarios.",https://api.openreview.net/pdf/f4c1e3b9d4ba57da8e7ae823a23f32af1bd11811.pdf,graph;zero_few-shot;representation;inference;llm,https://scholar.google.com/scholar?q=Topological+Relational+Learning+on+Graphs
You are caught stealing my winning lottery ticket! Making a lottery ticket claim its ownership,2021,NIPS,"['Xuxi Chen', 'Tianlong Chen', 'Zhenyu Zhang', 'Zhangyang Wang']",poster,"['Lottery Ticket Hypothesis', 'Ownership Verification']","Despite tremendous success in many application scenarios, the training and inference costs of using deep learning are also rapidly increasing over time. The lottery ticket hypothesis (LTH) emerges as a promising framework to leverage a special sparse subnetwork (i.e., $\textit{winning ticket}$) instead of a full model for both training and inference, that can lower both costs without sacrificing the performance. The main resource bottleneck of LTH is however the extraordinary cost to find the sparse mask of the winning ticket. That makes the found winning ticket become a valuable asset to the owners, highlighting the necessity of protecting its copyright. Our setting adds a new dimension to the recently soaring interest in protecting against the intellectual property (IP) infringement of deep models and verifying their ownerships, since they take owners' massive/unique resources to develop or train. While existing methods explored encrypted weights or predictions, we investigate a unique way to leverage sparse topological information to perform $\textit{lottery verification}$, by developing several graph-based signatures that can be embedded as credentials. By further combining trigger set-based methods, our proposal can work in both white-box and black-box verification scenarios. Through extensive experiments, we demonstrate the effectiveness of lottery verification in diverse models (ResNet-20, ResNet-18, ResNet-50) on CIFAR-10 and CIFAR-100. Specifically, our verification is shown to be robust to removal attacks such as model fine-tuning and pruning, as well as several ambiguity attacks. Our codes are available at https://github.com/VITA-Group/NO-stealing-LTH.",https://api.openreview.net/pdf/c41cddc397a21766418599dc403f8ce858efc814.pdf,graph;zero_few-shot;inference;sparse;llm,https://scholar.google.com/scholar?q=You+are+caught+stealing+my+winning+lottery+ticket!+Making+a+lottery+ticket+claim+its+ownership
Time-independent Generalization Bounds for SGLD in Non-convex Settings,2021,NIPS,"['Tyler Farghly', 'Patrick Rebeschini']",poster,"['SGLD', 'Langevin', 'stochastic gradient', 'generalization', 'stability', 'non-convex', 'wasserstein', 'optimization']","We establish generalization error bounds for stochastic gradient Langevin dynamics (SGLD) with constant learning rate under the assumptions of dissipativity and smoothness, a setting that has received increased attention in the sampling/optimization literature. Unlike existing bounds for SGLD in non-convex settings, ours are time-independent and decay to zero as the sample size increases. Using the framework of uniform stability, we establish time-independent bounds by exploiting the Wasserstein contraction property of the Langevin diffusion, which also allows us to circumvent the need to bound gradients using Lipschitz-like assumptions. Our analysis also supports variants of SGLD that use different discretization methods, incorporate Euclidean projections, or use non-isotropic noise.",https://api.openreview.net/pdf/edd5602c4607f56519140ad84e3052523cee677c.pdf,optimization;zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Time-independent+Generalization+Bounds+for+SGLD+in+Non-convex+Settings
"Neural Regression, Representational Similarity, Model Zoology & Neural Taskonomy at Scale in Rodent Visual Cortex",2021,NIPS,"['Colin Conwell', 'David Mayo', 'Andrei Barbu', 'Michael A Buice', 'George A. Alvarez', 'Boris Katz']",poster,"['neuro_ai', 'deep neural networks', 'rodent visual cortex', 'mouse brains', 'optical physiology']","How well do deep neural networks fare as models of mouse visual cortex? A majority of research to date suggests results far more mixed than those produced in the modeling of primate visual cortex. Here, we perform a large-scale benchmarking of dozens of deep neural network models in mouse visual cortex with both representational similarity analysis and neural regression. Using the Allen Brain Observatory's 2-photon calcium-imaging dataset of activity in over 6,000 reliable rodent visual cortical neurons recorded in response to natural scenes, we replicate previous findings and resolve previous discrepancies, ultimately demonstrating that modern neural networks can in fact be used to explain activity in the mouse visual cortex to a more reasonable degree than previously suggested. Using our benchmark as an atlas, we offer preliminary answers to overarching questions about levels of analysis (e.g. do models that better predict the representations of individual neurons also predict representational similarity across neural populations?); questions about the properties of models that best predict the visual system overall (e.g. is convolution or category-supervision necessary to better predict neural activity?); and questions about the mapping between biological and artificial representations (e.g. does the information processing hierarchy in deep nets match the anatomical hierarchy of mouse visual cortex?). Along the way, we catalogue a number of models (including vision transformers, MLP-Mixers, normalization free networks, Taskonomy encoders and self-supervised models) outside the traditional circuit of convolutional object recognition. Taken together, our results provide a reference point for future ventures in the deep neural network modeling of mouse visual cortex, hinting at novel combinations of mapping method, architecture, and task to more fully characterize the computational motifs of visual representation in a species so central to neuroscience, but with a perceptual physiology and ecology markedly different from the ones we study in primates.",https://api.openreview.net/pdf/209034fd231a2e958db5b5ffd7380d8c93390d56.pdf,graph;zero_few-shot;transformer;representation;llm,"https://scholar.google.com/scholar?q=Neural+Regression,+Representational+Similarity,+Model+Zoology+&+Neural+Taskonomy+at+Scale+in+Rodent+Visual+Cortex"
Dynamic Trace Estimation,2021,NIPS,"['Prathamesh Dharangutte', 'Christopher P Musco']",poster,"['randomized numerical linear algebra', 'trace estimation', 'dynamic algorithms']","We study a dynamic version of the implicit trace estimation problem. Given access to an oracle for computing matrix-vector multiplications with a dynamically changing matrix A, our goal is to maintain an accurate approximation to A's trace using as few multiplications as possible. We present a practical algorithm for solving this problem and prove that, in a natural setting, its complexity is quadratically better than the standard solution of repeatedly applying Hutchinson's stochastic trace estimator. We also provide an improved algorithm assuming additional common assumptions on A's dynamic updates. We support our theory with empirical results, showing significant computational improvements on three applications in machine learning and network science: tracking moments of the Hessian spectral density during neural network optimization, counting triangles and estimating natural connectivity in a dynamically changing graph.",https://api.openreview.net/pdf/9393dbd730f685b932fd0bedf46690d6643792de.pdf,graph;optimization;llm,https://scholar.google.com/scholar?q=Dynamic+Trace+Estimation
Adapting to function difficulty and growth conditions in private optimization,2021,NIPS,"['Hilal Asi', 'Daniel Asher Nathan Levy', 'John Duchi']",poster,"['privacy', 'differential privacy', 'adaptivity', 'optimization', 'convex', 'learning', 'sco', 'lower bounds', 'minimax', 'growth']","We develop algorithms for private stochastic convex optimization that adapt to the hardness of the specific function we wish to optimize. While previous work provide worst-case bounds for arbitrary convex functions, it is often the case that the function at hand belongs to a smaller class that enjoys faster rates. Concretely, we show that for functions exhibiting $\kappa$-growth around the optimum, i.e., $f(x) \ge f(x^\star) + \lambda \kappa^{-1} \|x-x^\star\|_2^\kappa$ for $\kappa > 1$, our algorithms improve upon the standard ${\sqrt{d}}/{n\varepsilon}$ privacy rate to the faster $({\sqrt{d}}/{n\varepsilon})^{\tfrac{\kappa}{\kappa - 1}}$. Crucially, they achieve these rates without knowledge of the growth constant $\kappa$ of the function. Our algorithms build upon the inverse sensitivity mechanism, which adapts to instance difficulty [2], and recent localization techniques in private optimization [25]. We complement our algorithms with matching lower bounds for these function classes and demonstrate that our adaptive algorithm is simultaneously (minimax) optimal over all $\kappa \ge 1+c$ whenever $c = \Theta(1)$.",https://api.openreview.net/pdf/8e076d289cf24c4ab9031a9798b07e8de01c4d11.pdf,optimization;zero_few-shot;adaptive;llm,https://scholar.google.com/scholar?q=Adapting+to+function+difficulty+and+growth+conditions+in+private+optimization
Revitalizing CNN Attention via Transformers in Self-Supervised Visual Representation Learning,2021,NIPS,"['Chongjian GE', 'Youwei Liang', 'Yibing Song', 'Jianbo Jiao', 'Jue Wang', 'Ping Luo']",poster,"['Self-Supervised Visual Representation Learning', 'Vision Transformers']","Studies on self-supervised visual representation learning (SSL) improve encoder backbones to discriminate training samples without labels. While CNN encoders via SSL achieve comparable recognition performance to those via supervised learning, their network attention is under-explored for further improvement. Motivated by the transformers that explore visual attention effectively in recognition scenarios, we propose a CNN Attention REvitalization (CARE) framework to train attentive CNN encoders guided by transformers in SSL. The proposed CARE framework consists of a CNN stream (C-stream) and a transformer stream (T-stream), where each stream contains two branches. C-stream follows an existing SSL framework with two CNN encoders, two projectors, and a predictor. T-stream contains two transformers, two projectors, and a predictor. T-stream connects to CNN encoders and is in parallel to the remaining C-Stream. During training, we perform SSL in both streams simultaneously and use the T-stream output to supervise C-stream. The features from CNN encoders are modulated in T-stream for visual attention enhancement and become suitable for the SSL scenario. We use these modulated features to supervise C-stream for learning attentive CNN encoders. To this end, we revitalize CNN attention by using transformers as guidance. Experiments on several standard visual recognition benchmarks, including image classification, object detection, and semantic segmentation, show that the proposed CARE framework improves CNN encoder backbones to the state-of-the-art performance. ",https://api.openreview.net/pdf/9354344219eab104fcb43e9e37ea3103136eabf5.pdf,zero_few-shot;transformer;representation;segmentation;llm,https://scholar.google.com/scholar?q=Revitalizing+CNN+Attention+via+Transformers+in+Self-Supervised+Visual+Representation+Learning
Local Signal Adaptivity: Provable Feature Learning in Neural Networks Beyond Kernels,2021,NIPS,"['Stefani Karp', 'Ezra Winston', 'Yuanzhi Li', 'Aarti Singh']",poster,"['deep learning theory', 'neural networks', 'kernels']","Neural networks have been shown to outperform kernel methods in practice (including neural tangent kernels). Most theoretical explanations of this performance gap focus on learning a complex hypothesis class; in some cases, it is unclear whether this hypothesis class captures realistic data. In this work, we propose a related, but alternative, explanation for this performance gap in the image classification setting, based on finding a sparse signal in the presence of noise. Specifically, we prove that, for a simple data distribution with sparse signal amidst high-variance noise, a simple convolutional neural network trained using stochastic gradient descent learns to threshold out the noise and find the signal. On the other hand, the corresponding neural tangent kernel, with a fixed set of predetermined features, is unable to adapt to the signal in this manner. We supplement our theoretical results by demonstrating this phenomenon empirically: in CIFAR-10 and MNIST images with various backgrounds, as the background noise increases in intensity, a CNN's performance stays relatively robust, whereas its corresponding neural tangent kernel sees a notable drop in performance. We therefore propose the ""local signal adaptivity"" (LSA) phenomenon as one explanation for the superiority of neural networks over kernel methods.",https://api.openreview.net/pdf/4f37dcef36b94cd4c87f88b483d774caf611803e.pdf,sparse;llm,https://scholar.google.com/scholar?q=Local+Signal+Adaptivity:+Provable+Feature+Learning+in+Neural+Networks+Beyond+Kernels
On sensitivity of meta-learning to support data,2021,NIPS,"['Mayank Agarwal', 'Mikhail Yurochkin', 'Yuekai Sun']",poster,"['meta-learning', 'sensitivity', 'robustness']","Meta-learning algorithms are widely used for few-shot learning. For example, image recognition systems that readily adapt to unseen classes after seeing only a few labeled examples. Despite their success, we show that modern meta-learning algorithms are extremely sensitive to the data used for adaptation, i.e. support data. In particular, we demonstrate the existence of (unaltered, in-distribution, natural) images that, when used for adaptation, yield accuracy as low as 4\% or as high as 95\% on standard few-shot image classification benchmarks. We explain our empirical findings in terms of class margins, which in turn suggests that robust and safe meta-learning requires larger margins than supervised learning.",https://api.openreview.net/pdf/5092789175fcee4133a9baf4b99eb113ccf0a895.pdf,graph;zero_few-shot;meta-learning;llm,https://scholar.google.com/scholar?q=On+sensitivity+of+meta-learning+to+support+data
When does Contrastive Learning Preserve Adversarial Robustness from Pretraining to Finetuning?,2021,NIPS,"['Lijie Fan', 'Sijia Liu', 'Pin-Yu Chen', 'Gaoyuan Zhang', 'Chuang Gan']",poster,"['Adversarial robustness', 'self-supervised learning', 'pretraining and finetuning']","Contrastive learning (CL) can learn generalizable feature representations and achieve state-of-the-art performance of downstream tasks by finetuning a linear classifier on top of it.  However, as adversarial robustness becomes vital in image classification,  it remains unclear whether or not CL is able to preserve robustness to downstream tasks. The main challenge is that in the self-supervised pretraining + supervised finetuning paradigm, adversarial robustness is easily forgotten due to a learning task mismatch from pretraining to finetuning. We call such challenge 'cross-task robustness transferability'. To address the above problem, in this paper we revisit and advance CL principles through the lens of robustness enhancement.  We show that (1) the design of contrastive views matters: High-frequency components of images are beneficial to improving model robustness; (2) Augmenting CL with pseudo-supervision stimulus (e.g., resorting to feature clustering) helps preserve robustness without forgetting. Equipped with our new designs, we propose AdvCL, a novel  adversarial contrastive pretraining framework. We show that AdvCL is able to enhance cross-task robustness transferability without loss of model accuracy and finetuning efficiency. With a thorough experimental study,  we demonstrate that AdvCL outperforms the state-of-the-art self-supervised robust learning methods across multiple datasets (CIFAR-10, CIFAR-100, and STL-10) and finetuning schemes  (linear evaluation and full model finetuning).",https://api.openreview.net/pdf/1eeaf70f06a89a644fb2e790c0429e40f53da3b8.pdf,zero_few-shot;representation;contrastive learning;transfer learning;llm,https://scholar.google.com/scholar?q=When+does+Contrastive+Learning+Preserve+Adversarial+Robustness+from+Pretraining+to+Finetuning?
Characterizing possible failure modes in physics-informed neural networks,2021,NIPS,"['Aditi Krishnapriyan', 'Amir Gholami', 'Shandian Zhe', 'Robert Kirby', 'Michael W. Mahoney']",poster,"['scientific machine learning', 'physics-informed neural networks', 'regularization', 'constrained optimization', 'unconstrained optimization']","Recent work in scientific machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.",https://api.openreview.net/pdf/28102b70fa8e4ca4c939fdf4aec5054e7e51dcad.pdf,optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Characterizing+possible+failure+modes+in+physics-informed+neural+networks
Reformulating Zero-shot Action Recognition for Multi-label Actions,2021,NIPS,"['Alec Kerrigan', 'Kevin Duarte', 'Yogesh S Rawat', 'Mubarak Shah']",poster,"['zero-shot', 'action recognition', 'computer vision']","The goal of zero-shot action recognition (ZSAR) is to classify action classes which were not previously seen during training. Traditionally, this is achieved by training a network to map, or regress, visual inputs to a semantic space where a nearest neighbor classifier is used to select the closest target class. We argue that this approach is sub-optimal due to the use of nearest neighbor on static semantic space and is ineffective when faced with multi-label videos - where two semantically distinct co-occurring action categories cannot be predicted with high confidence. To overcome these limitations, we propose a ZSAR framework which does not rely on nearest neighbor classification, but rather consists of a pairwise scoring function. Given a video and a set of action classes, our method predicts a set of confidence scores for each class independently. This allows for the prediction of several semantically distinct classes within one video input. Our evaluations show that our method not only achieves strong performance on three single-label action classification datasets (UCF-101, HMDB, and RareAct), but also outperforms previous ZSAR approaches on a challenging multi-label dataset (AVA) and a real-world surprise activity detection dataset (MEVA).",https://api.openreview.net/pdf/5d28d98f2efe5799996a2f97a7b529c15047891a.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Reformulating+Zero-shot+Action+Recognition+for+Multi-label+Actions
Greedy Approximation Algorithms for Active Sequential Hypothesis Testing,2021,NIPS,"['Kyra Gan', 'Su Jia', 'Andrew A Li']",poster,"['Approximation Guarantee', 'Active Learning', 'Sequential Hypothesis Testing', 'Greedy Aglorithm', 'Cancer Detection']","In the problem of \emph{active sequential hypothesis testing} (ASHT), a learner seeks to identify the \emph{true} hypothesis from among a known set of hypotheses. The learner is given a set of actions and knows the random distribution of the outcome of any action under any true hypothesis. Given a target error $\delta>0$, the goal is to sequentially select the fewest number of actions so as to identify the true hypothesis with probability at least $1 - \delta$. Motivated by applications in which the number of hypotheses or actions is massive (e.g., genomics-based cancer detection), we propose efficient (greedy, in fact) algorithms and provide the first approximation guarantees for ASHT, under two types of adaptivity. Both of our guarantees are independent of the number of actions and logarithmic in the number of hypotheses. We numerically evaluate the performance of our algorithms using both synthetic and real-world DNA mutation data, demonstrating that our algorithms outperform previously proposed heuristic policies by large margins.",https://api.openreview.net/pdf/136c89cb1ffed4278791c3b6105a14ecb34b0e31.pdf,graph;active learning;llm,https://scholar.google.com/scholar?q=Greedy+Approximation+Algorithms+for+Active+Sequential+Hypothesis+Testing
Mitigating Forgetting in Online Continual Learning with  Neuron Calibration,2021,NIPS,"['Haiyan Yin', 'Peng Yang', 'Ping Li']",poster,"['Continual Learning', 'Online Learning', 'Deep Learning']","Inspired by human intelligence, the research on online continual learning aims to push the limits of the machine learning models to constantly learn from sequentially encountered tasks, with the data from each task being observed in an online fashion. Though recent studies have achieved remarkable progress in improving the online continual learning performance empowered by the deep neural networks-based models, many of today's approaches still suffer a lot from catastrophic forgetting, a persistent challenge for continual learning. In this paper, we present a novel method which attempts to mitigate catastrophic forgetting in online continual learning from a new perspective, i.e., neuron calibration. In particular, we model the neurons in the deep neural networks-based models as calibrated units under a general formulation. Then we formalize a learning framework to effectively train the calibrated model, where neuron calibration could give ubiquitous benefit to balance the stability and plasticity of online continual learning algorithms through influencing both their forward inference path and backward optimization path.  Our proposed formulation for neuron calibration is lightweight and applicable to general feed-forward neural networks-based models. We perform extensive experiments to evaluate our method on four benchmark continual learning datasets. The results show that neuron calibration plays a vital role in improving online continual learning performance and our method could substantially improve the state-of-the-art performance on all~the~evaluated~datasets.",https://api.openreview.net/pdf/cc3ebd7a4834a4551e0b1f825969f9f51fd06415.pdf,optimization;online learning;inference;llm,https://scholar.google.com/scholar?q=Mitigating+Forgetting+in+Online+Continual+Learning+with++Neuron+Calibration
Chasing Sparsity in Vision Transformers: An End-to-End Exploration,2021,NIPS,"['Tianlong Chen', 'Yu Cheng', 'Zhe Gan', 'Lu Yuan', 'Lei Zhang', 'Zhangyang Wang']",poster,"['Vision Transformer', 'Sparsity', 'Data and Architecture Sparse Co-Training']","Vision transformers (ViTs) have recently received explosive popularity, but their enormous model sizes and training costs remain daunting. Conventional post-training pruning often incurs higher training budgets. In contrast, this paper aims to trim down both the training memory overhead and the inference complexity, without sacrificing the achievable accuracy. We carry out the first-of-its-kind comprehensive exploration, on taking a unified approach of integrating sparsity in ViTs ""from end to end''. Specifically, instead of training full ViTs, we dynamically extract and train sparse subnetworks, while sticking to a fixed small parameter budget. Our approach jointly optimizes model parameters and explores connectivity throughout training, ending up with one sparse network as the final output. The approach is seamlessly extended from unstructured to structured sparsity, the latter by considering to guide the prune-and-grow of self-attention heads inside ViTs. We further co-explore data and architecture sparsity for additional efficiency gains by plugging in a novel learnable token selector to adaptively determine the currently most vital patches. Extensive results on ImageNet with diverse ViT backbones validate the effectiveness of our proposals which obtain significantly reduced computational cost and almost unimpaired generalization. Perhaps most surprisingly, we find that the proposed sparse (co-)training can sometimes \textit{improve the ViT accuracy} rather than compromising it, making sparsity a tantalizing ""free lunch''. For example, our sparsified DeiT-Small at ($5\%$, $50\%$) sparsity for (data, architecture), improves $\mathbf{0.28\%}$ top-1 accuracy, and meanwhile enjoys $\mathbf{49.32\%}$ FLOPs and $\mathbf{4.40\%}$ running time savings. Our codes are available at https://github.com/VITA-Group/SViTE.",https://api.openreview.net/pdf/88bcdb9746be1c049a4bf446ce746c6f96906387.pdf,graph;transformer;adaptive;inference;sparse;llm,https://scholar.google.com/scholar?q=Chasing+Sparsity+in+Vision+Transformers:+An+End-to-End+Exploration
Targeted Neural Dynamical Modeling,2021,NIPS,"['Cole Lincoln Hurwitz', 'Akash Srivastava', 'Kai Xu', 'Justin Jude', 'Matt Perich', 'Lee E. Miller', 'Matthias H. Hennig']",poster,"['neuroscience', 'latent variable model', 'recurrent neural network', 'behavioural neuroscience', 'motor neuroscience', 'dynamics']","Latent dynamics models have emerged as powerful tools for modeling and interpreting neural population activity. Recently, there has been a focus on incorporating simultaneously measured behaviour into these models to further disentangle sources of neural variability in their latent space. These approaches, however, are limited in their ability to capture the underlying neural dynamics (e.g. linear) and in their ability to relate the learned dynamics back to the observed behaviour (e.g. no time lag). To this end, we introduce Targeted Neural Dynamical Modeling (TNDM), a nonlinear state-space model that jointly models the neural activity and external behavioural variables. TNDM decomposes neural dynamics into behaviourally relevant and behaviourally irrelevant dynamics; the relevant dynamics are used to reconstruct the behaviour through a flexible linear decoder and both sets of dynamics are used to reconstruct the neural activity through a linear decoder with no time lag. We implement TNDM as a sequential variational autoencoder and validate it on simulated recordings and recordings taken from the premotor and motor cortex of a monkey performing a center-out reaching task. We show that TNDM is able to learn low-dimensional latent dynamics that are highly predictive of behaviour without sacrificing its fit to the neural data.",https://api.openreview.net/pdf/5b2cd7153188a1527cac9898866370e61f1acfc7.pdf,zero_few-shot;online learning;llm,https://scholar.google.com/scholar?q=Targeted+Neural+Dynamical+Modeling
Learning Disentangled Behavior Embeddings,2021,NIPS,"['Changhao Shi', 'Sivan Schwartz', 'Shahar Levy', 'Shay Achvat', 'Maisan Abboud', 'Amir Ghanayim', 'Jackie Schiller', 'Gal Mishne']",spotlight,"['Neuroscience', 'Animal Behavior', 'Behavioral Videos', 'Generative Models']","To understand the relationship between behavior and neural activity, experiments in neuroscience often include an animal performing a repeated behavior such as a motor task. Recent progress in computer vision and deep learning has shown great potential in the automated analysis of behavior by leveraging large and high-quality video datasets. In this paper, we design Disentangled Behavior Embedding (DBE) to learn robust behavioral embeddings from unlabeled, multi-view, high-resolution behavioral videos across different animals and multiple sessions. We further combine DBE with a stochastic temporal model to propose Variational Disentangled Behavior Embedding (VDBE), an end-to-end approach that learns meaningful discrete behavior representations and generates interpretable behavioral videos. Our models learn consistent behavior representations by explicitly disentangling the dynamic behavioral factors (pose) from time-invariant, non-behavioral nuisance factors (context) in a deep autoencoder, and exploit the temporal structures of pose dynamics. Compared to competing approaches, DBE and VDBE enjoy superior performance on downstream tasks such as fine-grained behavioral motif generation and behavior decoding.",https://api.openreview.net/pdf/e1e68e0afaed7486ffbd91ed1e878bcc0baa6167.pdf,graph;representation;generative model;multi-view;llm,https://scholar.google.com/scholar?q=Learning+Disentangled+Behavior+Embeddings
Two steps to risk sensitivity,2021,NIPS,"['Christopher Gagne', 'Peter Dayan']",spotlight,"['Risk measures', 'Conditional value-at-risk', 'Two-step task', 'Time-consistency', 'Distributional reinforcement learning', 'Anxiety']","Distributional reinforcement learning (RL) – in which agents learn about all the possible long-term consequences of their actions, and not just the expected value – is of great recent interest. One of the most important affordances of a distributional view is facilitating a modern, measured, approach to risk when outcomes are not completely certain. By contrast, psychological and neuroscientific investigations into decision making under risk have utilized a variety of more venerable theoretical models such as prospect theory that lack axiomatically desirable properties such as coherence. Here, we consider a particularly relevant risk measure for modeling human and animal planning, called conditional value-at-risk (CVaR), which quantifies worst-case outcomes (e.g., vehicle accidents or predation). We first adopt a conventional distributional approach to CVaR in a sequential setting and reanalyze the choices of human decision-makers in the well-known two-step task, revealing substantial risk aversion that had been lurking under stickiness and perseveration. We then consider a further critical property of risk sensitivity, namely time consistency, showing alternatives to this form of CVaR that enjoy this desirable characteristic. We use simulations to examine settings in which the various forms differ in ways that have implications for human and animal planning and behavior.",https://api.openreview.net/pdf/e4557754aa084979b86bd331e6bf1cf6d6f5afc8.pdf,reinforcement learning;llm,https://scholar.google.com/scholar?q=Two+steps+to+risk+sensitivity
Probabilistic Tensor Decomposition of Neural Population Spiking Activity,2021,NIPS,"['Hugo Soulat', 'Sepiedeh Keshavarzi', 'Troy William Margrie', 'Maneesh Sahani']",spotlight,"['Probabilistic', 'Tensor Decomposition', 'Neuroscience', 'Spike', 'Population Activity', 'Count']","The firing of neural populations is coordinated across cells, in time, and across experimental
conditions or repeated experimental trials; and so a full understanding of the computational
significance of neural responses must be based on a separation of these different contributions to
structured activity.

Tensor decomposition is an approach to untangling the influence of multiple factors in data that is
common in many fields.  However, despite some recent interest in neuroscience, wider applicability
of the approach is hampered by the lack of a full probabilistic treatment allowing principled
inference of a decomposition from non-Gaussian spike-count data.
Here, we extend the Pólya-Gamma (PG) augmentation, previously used in sampling-based Bayesian
inference, to implement scalable variational inference in non-conjugate spike-count models.

Using this new approach, we develop techniques related to automatic relevance determination to infer
the most appropriate tensor rank, as well as to incorporate priors based on known brain anatomy such
as the segregation of cell response properties by brain area.

We apply the model to neural recordings taken under conditions of visual-vestibular sensory
integration, revealing how the encoding of self- and visual-motion signals is modulated by the
sensory information available to the animal.",https://api.openreview.net/pdf/0219c0b63bfa2e8fa16570221cd68bdf09c41ea0.pdf,zero_few-shot;bayesian;inference;augmentation;llm,https://scholar.google.com/scholar?q=Probabilistic+Tensor+Decomposition+of+Neural+Population+Spiking+Activity
A flow-based latent state generative model of neural population responses to natural images,2021,NIPS,"['Mohammad Bashiri', 'Edgar Y. Walker', 'Konstantin-Klemens Lurz', 'Akshay Kumar Jagadish', 'Taliah Muhammad', 'Zhiwei Ding', 'Zhuokun Ding', 'Andreas S. Tolias', 'Fabian H. Sinz']",spotlight,"['mouse visual cortex', 'neural system identification', 'latent variable models', 'normalizing flow', 'generative models', 'noise correlations']","We present a joint deep neural system identification model for two major sources of neural variability: stimulus-driven and stimulus-conditioned fluctuations. To this end, we combine (1) state-of-the-art deep networks for stimulus-driven activity and (2) a flexible, normalizing flow-based generative model to capture the stimulus-conditioned variability including noise correlations. This allows us to train the model end-to-end without the need for sophisticated probabilistic approximations associated with many latent state models for stimulus-conditioned fluctuations. We train the model on the responses of thousands of neurons from multiple areas of the mouse visual cortex to natural images. We show that our model outperforms previous state-of-the-art models in predicting the distribution of neural population responses to novel stimuli, including shared stimulus-conditioned variability. Furthermore, it successfully learns known latent factors of the population responses that are related to behavioral variables such as pupil dilation, and other factors that vary systematically with brain area or retinotopic location. Overall, our model accurately accounts for two critical sources of neural variability while avoiding several complexities associated with many existing latent state models. It thus provides a useful tool for uncovering the interplay between different factors that contribute to variability in neural activity.",https://api.openreview.net/pdf/4a677fb99d14450a3ed48bcf67829e4ad7965d8f.pdf,zero_few-shot;generative model;flow;llm,https://scholar.google.com/scholar?q=A+flow-based+latent+state+generative+model+of+neural+population+responses+to+natural+images
Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization,2021,NIPS,"['Yusuke Iwasawa', 'Yutaka Matsuo']",spotlight,"['domain generalization', 'test time adaptation', 'prototypical classifier']","This paper presents a new algorithm for domain generalization (DG), \textit{test-time template adjuster (T3A)}, aiming to robustify a model to unknown distribution shift. Unlike existing methods that focus on \textit{training phase}, our method focuses \textit{test phase}, i.e., correcting its prediction by itself during test time. Specifically, T3A adjusts a trained linear classifier (the last layer of deep neural networks) with the following procedure:  (1) compute a pseudo-prototype representation for each class using online unlabeled data augmented by the base classifier trained in the source domains, (2) and then classify each sample based on its distance to the pseudo-prototypes. T3A is back-propagation-free and modifies only the linear layer; therefore, the increase in computational cost during inference is negligible and avoids the catastrophic failure might caused by stochastic optimization. Despite its simplicity, T3A can leverage knowledge about the target domain by using off-the-shelf test-time data and improve performance. We tested our method on four domain generalization benchmarks, namely PACS, VLCS, OfficeHome, and TerraIncognita, along with various backbone networks including ResNet18, ResNet50, Big Transfer (BiT), Vision Transformers (ViT), and MLP-Mixer. The results show T3A stably improves performance on unseen domains across choices of backbone networks, and outperforms existing domain generalization methods. ",https://api.openreview.net/pdf/22a5d6cf8f5a37e348c03a005c67203022e78ed2.pdf,optimization;zero_few-shot;transformer;representation;online learning;inference;transfer learning;augmentation;llm,https://scholar.google.com/scholar?q=Test-Time+Classifier+Adjustment+Module+for+Model-Agnostic+Domain+Generalization
Credit Assignment in Neural Networks through Deep Feedback Control,2021,NIPS,"['Alexander Meulemans', 'Matilde Tristany Farinha', 'Javier Garcia Ordonez', 'Pau Vilimelis Aceituno', 'Joao Sacramento', 'Benjamin F Grewe']",spotlight,"['Biologically-plausible Deep Learning', 'Neuroscience', 'Deep Learning', 'Control theory', 'Optimization theory']","The success of deep learning sparked interest in whether the brain learns by using similar techniques for assigning credit to each synaptic weight for its contribution to the network output. However, the majority of current attempts at biologically-plausible learning methods are either non-local in time, require highly specific connectivity motifs, or have no clear link to any known mathematical optimization method. Here, we introduce Deep Feedback Control (DFC), a new learning method that uses a feedback controller to drive a deep neural network to match a desired output target and whose control signal can be used for credit assignment. The resulting learning rule is fully local in space and time and approximates Gauss-Newton optimization for a wide range of feedback connectivity patterns. To further underline its biological plausibility, we relate DFC to a multi-compartment model of cortical pyramidal neurons with a local voltage-dependent synaptic plasticity rule, consistent with recent theories of dendritic processing. By combining dynamical system theory with mathematical optimization theory, we provide a strong theoretical foundation for DFC that we corroborate with detailed results on toy experiments and standard computer-vision benchmarks.",https://api.openreview.net/pdf/9d0ab63395985e29e1ac0236a796ad79bd5d2ed1.pdf,optimization;llm,https://scholar.google.com/scholar?q=Credit+Assignment+in+Neural+Networks+through+Deep+Feedback+Control
Uniform Sampling over Episode Difficulty,2021,NIPS,"['Sebastien Arnold', 'Guneet Singh Dhillon', 'Avinash Ravichandran', 'Stefano Soatto']",spotlight,"['episode sampling', 'few-shot learning', 'meta-learning']","Episodic training is a core ingredient of few-shot learning to train models on tasks with limited labelled data. Despite its success, episodic training remains largely understudied, prompting us to ask the question: what is the best way to sample episodes? In this paper, we first propose a method to approximate episode sampling distributions based on their difficulty. Building on this method, we perform an extensive analysis and find that sampling uniformly over episode difficulty outperforms other sampling schemes, including curriculum and easy-/hard-mining. As the proposed sampling method is algorithm agnostic, we can leverage these insights to improve few-shot learning accuracies across many episodic training algorithms. We demonstrate the efficacy of our method across popular few-shot learning datasets, algorithms, network architectures, and protocols.
",https://api.openreview.net/pdf/bc0bc5f9e36f18e9bfbb12f39248209be6ff2c32.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Uniform+Sampling+over+Episode+Difficulty
Intriguing Properties of Vision Transformers,2021,NIPS,"['Muzammal Naseer', 'Kanchana Ranasinghe', 'Salman Khan', 'Munawar Hayat', 'Fahad Khan', 'Ming-Hsuan Yang']",spotlight,"['Vision Transformers', 'Auto-segmentation', 'Off-the-shelf-features', 'Robustness', 'Shape-Modeling']","Vision transformers (ViT) have demonstrated impressive performance across numerous machine vision tasks. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility (in attending image-wide context conditioned on a given patch) can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and provide comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a)Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1 accuracy on ImageNet even after randomly occluding 80% of the image content. (b)The robustness towards occlusions is not due to texture bias, instead we show that ViTs are significantly less biased towards local textures, compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c)Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d)Off-the-shelf features from a single ViT model can be combined to create a feature ensemble,  leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms.  We show effective features of ViTs are due to flexible and dynamic receptive fields possible via self-attention mechanisms. Our code will be publicly released.",https://api.openreview.net/pdf/0140d23fc2b190ea54786be5346158ec0c332da4.pdf,zero_few-shot;transformer;representation;segmentation;llm,https://scholar.google.com/scholar?q=Intriguing+Properties+of+Vision+Transformers
Neural Additive Models: Interpretable Machine Learning with Neural Nets,2021,NIPS,"['Rishabh Agarwal', 'Levi Melnick', 'Nicholas Frosst', 'Xuezhou Zhang', 'Ben Lengerich', 'Rich Caruana', 'Geoffrey Hinton']",spotlight,"['Additive Models', 'Interpretability', 'Multitask learning', 'Explainable AI']","Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19. ",https://api.openreview.net/pdf/a7c0719bc57273993d24ab46199bac6d1a76c02f.pdf,zero_few-shot;decision trees;llm,https://scholar.google.com/scholar?q=Neural+Additive+Models:+Interpretable+Machine+Learning+with+Neural+Nets
Across-animal odor decoding by probabilistic manifold alignment,2021,NIPS,"['Pedro Herrero-Vidal', 'Dmitry Rinberg', 'Cristina Savin']",spotlight,"['neuroscience', 'probabilistic models', 'latent dynamical systems', 'decoding', 'neural alignment', 'across-animal neural analysis']","Identifying the common structure of neural dynamics across subjects is key for extracting unifying principles of brain computation and for many brain machine interface applications. Here, we propose a novel probabilistic approach for aligning stimulus-evoked responses from multiple animals in a common low dimensional manifold and use hierarchical inference to identify which stimulus drives neural activity in any given trial. Our probabilistic decoder is robust to a range of features of the neural responses and significantly outperforms existing neural alignment procedures. When applied to recordings from the mouse olfactory bulb, our approach reveals low-dimensional population dynamics that are odor specific and have consistent structure across animals. Thus, our decoder can be used for increasing the robustness and scalability of neural-based chemical detection.",https://api.openreview.net/pdf/b94a04a54ee9131793eb695b13f7de9b80af0645.pdf,zero_few-shot;inference;llm,https://scholar.google.com/scholar?q=Across-animal+odor+decoding+by+probabilistic+manifold+alignment
Shaping embodied agent behavior with activity-context priors from egocentric video,2021,NIPS,"['Tushar Nagarajan', 'Kristen Grauman']",spotlight,"['embodied AI', 'learning from passive videos', 'visual semantic planning', 'interaction exploration', 'object interaction', 'egocentric video', 'human activity understanding', 'computer vision']","Complex physical tasks entail a sequence of object interactions, each with its own preconditions -- which can be difficult for robotic agents to learn efficiently solely through their own experience. We introduce an approach to discover activity-context priors from in-the-wild egocentric video captured with human worn cameras. For a given object, an activity-context prior represents the set of other compatible objects that are required for activities to succeed (e.g., a knife and cutting board brought together with a tomato are conducive to cutting). We encode our video-based prior as an auxiliary reward function that encourages an agent to bring compatible objects together before attempting an interaction. In this way, our model translates everyday human experience into embodied agent skills. We demonstrate our idea using egocentric EPIC-Kitchens video of people performing unscripted kitchen activities to benefit virtual household robotic agents performing various complex tasks in AI2-iTHOR, significantly accelerating agent learning. ",https://api.openreview.net/pdf/ba11f672f19300d6e8d54fa2655d84185c4405c9.pdf,reinforcement learning;llm,https://scholar.google.com/scholar?q=Shaping+embodied+agent+behavior+with+activity-context+priors+from+egocentric+video
Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning,2021,NIPS,"['Colin Wei', 'Sang Michael Xie', 'Tengyu Ma']",spotlight,"['theory', 'representation learning theory', 'self-supervised learning theory', 'pretrained language models', 'natural language processing']","Pretrained language models have achieved state-of-the-art performance when adapted to a downstream NLP task. However, theoretical analysis of these models is scarce and challenging since the pretraining and downstream tasks can be very different. We propose an analysis framework that links the pretraining and downstream tasks with an underlying latent variable generative model of text -- the downstream classifier must recover a function of the posterior distribution over the latent variables. We analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. The generative model in our analysis is either a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component, motivated by long-term dependencies in natural language. We show that 1) under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy conditions, and 3) our recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long-term memory. Experiments on synthetically generated data from HMMs back our theoretical findings.
",https://api.openreview.net/pdf/6360b8480b6e711cab6fa4f501d5ce55d1c771e8.pdf,graph;zero_few-shot;generative model;augmentation;llm,https://scholar.google.com/scholar?q=Why+Do+Pretrained+Language+Models+Help+in+Downstream+Tasks?+An+Analysis+of+Head+and+Prompt+Tuning
Variational Bayesian Optimistic Sampling,2021,NIPS,"[""Brendan O'Donoghue"", 'Tor Lattimore']",spotlight,"['Bayes', 'Variational inference', 'Bandits', 'Decision problems', 'Online learning']","We consider online sequential decision problems where an agent must balance  exploration and exploitation. We derive a set of Bayesian `optimistic' policies  which, in the stochastic multi-armed bandit case, includes the Thompson sampling  policy. We provide a new analysis showing that any algorithm producing policies in the optimistic set enjoys $\tilde O(\sqrt{AT})$ Bayesian regret for a problem with $A$ actions after $T$ rounds. We extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases. In this case we show that Thompson sampling can produce policies outside of the optimistic set and suffer linear regret in some instances. Finding a policy inside the optimistic set amounts to solving a convex optimization problem and we call the resulting algorithm `variational Bayesian optimistic sampling' (VBOS). The procedure works for any posteriors, \ie, it does not require the posterior to have any special properties, such as log-concavity, unimodality, or smoothness. The variational view of the problem has many useful properties, including the ability to tune the exploration-exploitation tradeoff, add regularization, incorporate constraints, and linearly parameterize the policy.",https://api.openreview.net/pdf/01c97b8a67710e6157fac50027ff722a4d02c107.pdf,reinforcement learning;optimization;online learning;bayesian;llm,https://scholar.google.com/scholar?q=Variational+Bayesian+Optimistic+Sampling
Information Directed Sampling for Sparse Linear Bandits,2021,NIPS,"['Botao Hao', 'Tor Lattimore', 'Wei Deng']",spotlight,"['Information-directed sampling', 'sparse linear bandits', 'Bayesian regret']","Stochastic sparse linear bandits offer a practical model for high-dimensional online decision-making problems and have a rich information-regret structure. In this work we explore the use of information-directed sampling (IDS), which naturally balances the information-regret trade-off. We develop a class of information-theoretic Bayesian regret bounds that nearly match existing lower bounds on a variety of problem instances, demonstrating the adaptivity of IDS. To efficiently implement sparse IDS, we propose an empirical Bayesian approach for sparse posterior sampling using a spike-and-slab Gaussian-Laplace prior.  Numerical results demonstrate significant regret reductions by sparse IDS relative to several baselines.
",https://api.openreview.net/pdf/5e1a67215084bf911ced2a7921c6fe1fce36d52c.pdf,zero_few-shot;online learning;bayesian;sparse;llm,https://scholar.google.com/scholar?q=Information+Directed+Sampling+for+Sparse+Linear+Bandits
Determinantal point processes based on orthogonal polynomials for sampling minibatches in SGD,2021,NIPS,"['Rémi Bardenet', 'Subhroshekhar Ghosh', 'Meixia LIN']",spotlight,"['determinantal point processes', 'variance reduction', 'stochastic gradient descent', 'orthogonal polynomials']","Stochastic gradient descent (SGD) is a cornerstone of machine learning. When the number $N$ of data items is large, SGD relies on constructing an unbiased estimator of the gradient of the empirical risk using a small subset of the original dataset, called a minibatch. Default minibatch construction involves uniformly sampling a subset of the desired size, but alternatives have been explored for variance reduction. In particular, experimental evidence suggests drawing minibatches from determinantal point processes (DPPs), tractable distributions over minibatches that favour diversity among selected items. However, like in recent work on DPPs for coresets, providing a systematic and principled understanding of how and why DPPs help has been difficult. In this work, we contribute an orthogonal polynomial-based determinantal point process paradigm for performing minibatch sampling in SGD. Our approach leverages the specific data distribution at hand, which endows it with greater sensitivity and power over existing data-agnostic methods. We substantiate our method via a detailed theoretical analysis of its convergence properties, interweaving between the discrete data set and the underlying continuous domain. In particular, we show how specific DPPs and a string of controlled approximations can lead to gradient estimators with a variance that decays faster with the batchsize than under uniform sampling. Coupled with existing finite-time guarantees for SGD on convex objectives, this entails that, for a large enough batchsize and a fixed budget of item-level gradients to evaluate, DPP minibatches lead to a smaller bound on the mean square approximation error than uniform minibatches. Moreover, our estimators are amenable to a recent algorithm that directly samples linear statistics of DPPs (i.e., the gradient estimator) without sampling the underlying DPP (i.e., the minibatch), thereby reducing computational overhead. We provide detailed synthetic as well as real data experiments to substantiate our theoretical claims.",https://api.openreview.net/pdf/76a9481c7e78719e98baff680fb72cd07586437e.pdf,graph;llm,https://scholar.google.com/scholar?q=Determinantal+point+processes+based+on+orthogonal+polynomials+for+sampling+minibatches+in+SGD
A Normative and Biologically Plausible Algorithm for Independent Component Analysis,2021,NIPS,"['Yanis Bahroun', 'Dmitri Chklovskii', 'Anirvan M. Sengupta']",spotlight,"['Blind source separation', 'independent component analysis', 'neural network', 'local learning rules', 'biologically plausible', 'Hebbian learning']","The brain effortlessly solves blind source separation (BSS) problems, but the algorithm it uses remains elusive. In signal processing, linear BSS problems are often solved by Independent Component Analysis (ICA). To serve as a model of a biological circuit, the ICA neural network (NN) must satisfy at least the following requirements: 1. The algorithm must operate in the online setting where data samples are streamed one at a time, and the NN computes the sources on the fly without storing any significant fraction of the data in memory. 2. The synaptic weight update is local, i.e., it depends only on the biophysical variables present in the vicinity of a synapse. Here, we propose a novel objective function for ICA from which we derive a biologically plausible NN, including both the neural architecture and the synaptic learning rules. Interestingly, our algorithm relies on modulating synaptic plasticity by the total activity of the output neurons. In the brain, this could be accomplished by neuromodulators, extracellular calcium, local field potential, or nitric oxide. ",https://api.openreview.net/pdf/28df2354231b10a5f8528f43042180ed97b52ad1.pdf,zero_few-shot;online learning;llm,https://scholar.google.com/scholar?q=A+Normative+and+Biologically+Plausible+Algorithm+for+Independent+Component+Analysis
Your head is there to move you around: Goal-driven models of the primate dorsal pathway,2021,NIPS,"['Patrick J Mineault', 'Shahab Bakhtiari', 'Blake Aaron Richards', 'Christopher C Pack']",spotlight,"['sensory neuroscience', 'systems identification', 'goal-driven models', 'vision neuroscience', 'motion processing']","Neurons in the dorsal visual pathway of the mammalian brain are selective for motion stimuli, with the complexity of stimulus representations increasing along the hierarchy. This progression is similar to that of the ventral visual pathway, which is well characterized by artificial neural networks (ANNs) optimized for object recognition. In contrast, there are no image-computable models of the dorsal stream with comparable explanatory power. We hypothesized that the properties of dorsal stream neurons could be explained by a simple learning objective: the need for an organism to orient itself during self-motion. To test this hypothesis, we trained a 3D ResNet to predict an agent's self-motion parameters from visual stimuli in a simulated environment. We found that the responses in this network accounted well for the selectivity of neurons in a large database of single-neuron recordings from the dorsal visual stream of non-human primates. In contrast, ANNs trained on an action recognition dataset through supervised or self-supervised learning  could not explain responses in the dorsal stream, despite also being trained on naturalistic videos with moving objects. These results demonstrate that an ecologically relevant cost function can account for dorsal stream properties in the primate brain.",https://api.openreview.net/pdf/7221ecd5ee05c87f7b840ebc3801e70042bb7cae.pdf,reinforcement learning;zero_few-shot;representation;3d;llm,https://scholar.google.com/scholar?q=Your+head+is+there+to+move+you+around:+Goal-driven+models+of+the+primate+dorsal+pathway
Uniform Concentration Bounds toward a Unified  Framework for Robust Clustering,2021,NIPS,"['Debolina Paul', 'Saptarshi Chakraborty', 'Swagatam Das', 'Jason Xu']",spotlight,"['k-means', 'median of means', 'metric entropy bounds', 'Bregman divergences', 'Iterative optimization']","Recent advances in center-based clustering continue to improve upon the drawbacks of Lloyd's celebrated $k$-means algorithm over $60$ years after its introduction. Various methods seek to address poor local minima, sensitivity to outliers, and data that are not well-suited to Euclidean measures of fit, but many are supported largely empirically. Moreover, combining such approaches in a piecemeal manner can result in ad hoc methods, and the limited theoretical results supporting each individual contribution may no longer hold. Toward addressing these issues in a principled way, this paper proposes a cohesive robust framework for center-based clustering under a general class of dissimilarity measures. In particular, we present a rigorous theoretical treatment within a Median-of-Means (MoM) estimation framework, showing that it subsumes several popular $k$-means variants. In addition to unifying existing methods, we derive uniform concentration bounds that complete their analyses, and bridge these results to the MoM framework via Dudley's chaining arguments. Importantly, we neither require any assumptions on the distribution of the outlying observations nor on the relative number of observations $n$ to features $p$. We establish strong consistency and an error rate of $O(n^{-1/2})$ under mild conditions, surpassing the best-known results in the literature. The methods are empirically validated thoroughly on real and synthetic datasets. ",https://api.openreview.net/pdf/453f82f5781b7ab06f4b1abdf295f913a9c0492a.pdf,llm,https://scholar.google.com/scholar?q=Uniform+Concentration+Bounds+toward+a+Unified++Framework+for+Robust+Clustering
Explaining heterogeneity in medial entorhinal cortex with task-driven neural networks,2021,NIPS,"['Aran Nayebi', 'Alexander Attinger', 'Malcolm G. Campbell', 'Kiah Hardcastle', 'Isabel I. C. Low', 'Caitlin S. Mallory', 'Gabriel Carreira Mel', 'Ben Sorscher', 'Alex H Williams', 'Surya Ganguli', 'Lisa Giocomo', 'Daniel LK Yamins']",spotlight,"['neural coding', 'medial entorhinal cortex', 'grid cells', 'biologically-inspired navigation', 'path integration', 'recurrent neural networks']","Medial entorhinal cortex (MEC) supports a wide range of navigational and memory related behaviors.
Well-known experimental results have revealed specialized cell types in MEC --- e.g. grid, border, and head-direction cells --- whose highly stereotypical response profiles are suggestive of the role they might play in supporting MEC functionality. 
However, the majority of MEC neurons do not exhibit stereotypical firing patterns.
How should the response profiles of these more ""heterogeneous"" cells be described, and how do they contribute to behavior?
In this work, we took a computational approach to addressing these questions.
We first performed a statistical analysis that shows that heterogeneous MEC cells are just as reliable in their response patterns as the more stereotypical cell types, suggesting that they have a coherent functional role.
Next, we evaluated a spectrum of candidate models in terms of their ability to describe the response profiles of both stereotypical and heterogeneous MEC cells.
We found that recently developed task-optimized neural network models are substantially better than traditional grid cell-centric models at matching most MEC neuronal response profiles --- including those of grid cells themselves --- despite not being explicitly trained for this purpose.
Specific choices of network architecture (such as gated nonlinearities and an explicit intermediate place cell representation) have an important effect on the ability of the model to generalize to novel scenarios, with the best of these models closely approaching the noise ceiling of the data itself.
We then performed in silico experiments on this model to address questions involving the relative functional relevance of various cell types, finding that heterogeneous cells are likely to be just as involved in downstream functional outcomes (such as path integration) as grid and border cells.
Finally, inspired by recent data showing that, going beyond their spatial response selectivity, MEC cells are also responsive to non-spatial rewards, we introduce a new MEC model that performs reward-modulated path integration.
We find that this unified model matches neural recordings across all variable-reward conditions.
Taken together, our results point toward a conceptually principled goal-driven modeling approach for moving future experimental and computational efforts beyond overly-simplistic single-cell stereotypes.",https://api.openreview.net/pdf/83c988970fb39c6263bc9a1805d7c4581cbdda8b.pdf,representation;online learning;llm,https://scholar.google.com/scholar?q=Explaining+heterogeneity+in+medial+entorhinal+cortex+with+task-driven+neural+networks
Bayesian decision-making under misspecified priors with applications to meta-learning,2021,NIPS,"['Max Simchowitz', 'Christopher Tosh', 'Akshay Krishnamurthy', 'Daniel Hsu', 'Thodoris Lykouris', 'Miroslav Dudík', 'Robert E. Schapire']",spotlight,"['Thompson Sampling', 'Bayesian Bandits', 'Bayesian Decision Making', 'Meta-Learning']","Thompson sampling and other Bayesian sequential decision-making algorithms are among the most popular approaches to tackle explore/exploit trade-offs in (contextual) bandits.  The choice of prior in these algorithms offers flexibility to encode domain knowledge but can also lead to poor performance when misspecified.  In this paper, we demonstrate that performance degrades gracefully with misspecification.  We prove that the expected reward accrued by Thompson sampling (TS) with a misspecified prior differs by at most $\tilde{O}(H^2 \epsilon)$ from TS with a well-specified prior, where $\epsilon$ is the total-variation distance between priors and $H$ is the learning horizon.  

Our bound does not require the prior to have any parametric form.  For priors with bounded support, our bound is independent of the cardinality or structure of the action space, and we show that it is tight up to universal constants in the worst case.

Building on our sensitivity analysis, we establish generic PAC guarantees for algorithms in the recently studied Bayesian meta-learning setting and derive corollaries for various families of priors.  Our results generalize along two axes: (1) they apply to a broader family of Bayesian decision-making algorithms, including a Monte-Carlo implementation of the knowledge gradient algorithm (KG), and (2) they apply to Bayesian POMDPs, the most general Bayesian decision-making setting, encompassing contextual bandits as a special case. Through numerical simulations, we illustrate how prior misspecification and the deployment of one-step look-ahead (as in KG) can impact the convergence of meta-learning in multi-armed and contextual bandits with structured and correlated priors.",https://api.openreview.net/pdf/eb80690d338fca1881fdcbc9e9c27c1489633eac.pdf,bayesian;metric;meta-learning;llm,https://scholar.google.com/scholar?q=Bayesian+decision-making+under+misspecified+priors+with+applications+to+meta-learning
A Geometric Perspective towards Neural Calibration via Sensitivity Decomposition,2021,NIPS,"['Junjiao Tian', 'Dylan Yung', 'Yen-Chang Hsu', 'Zsolt Kira']",spotlight,"['uncertainty estimation', 'calibration', 'sensitivity']","It is well known that vision classification models suffer from poor calibration in the face of data distribution shifts. In this paper, we take a geometric approach to this problem. We propose Geometric Sensitivity Decomposition (GSD) which decomposes the norm of a sample feature embedding and the angular similarity to a target classifier into an instance-dependent and an instance-independent com-ponent. The instance-dependent component captures the sensitive information about changes in the input while the instance-independent component represents the insensitive information serving solely to minimize the loss on the training dataset. Inspired by the decomposition, we analytically derive a simple extension to current softmax-linear models, which learns to disentangle the two components during training. On several common vision models, the disentangled model out-performs other calibration methods on standard calibration metrics in the face of out-of-distribution (OOD) data and corruption with significantly less complexity. Specifically, we surpass the current state of the art by 30.8% relative improvement on corrupted CIFAR100 in Expected Calibration Error.
",https://api.openreview.net/pdf/7932cdc582ef30fdfd42a08521e770129cd4fbf2.pdf,metric;llm,https://scholar.google.com/scholar?q=A+Geometric+Perspective+towards+Neural+Calibration+via+Sensitivity+Decomposition
Hash Layers For Large Sparse Models,2021,NIPS,"['Stephen Roller', 'Sainbayar Sukhbaatar', 'Arthur Szlam', 'Jason E Weston']",spotlight,"['large-scale', 'sparsity', 'Transformers', 'hashing', 'MoE']","We investigate the training of sparse layers that use different parameters for different inputs based on hashing in large Transformer models. Specifically, we modify the feedforward layer to hash to different sets of weights depending on the current token, over all tokens in the sequence. We show that this procedure either outperforms or is competitive with learning-to-route mixture-of-expert methods such as Switch Transformers and BASE Layers, while requiring no routing parameters or extra terms in the objective function such as a load balancing loss, and no sophisticated assignment algorithm. We study the performance of different hashing techniques,  hash sizes and input features,  and  show that  balanced and random hashes focused on the most local features work best, compared to either learning clusters or using longer-range context. We show our approach works well both on large language modeling and dialogue tasks, and on downstream fine-tuning tasks.",https://api.openreview.net/pdf/c722df087549e62aa8f20733964d2fe2a2c94c6e.pdf,transformer;sparse;llm,https://scholar.google.com/scholar?q=Hash+Layers+For+Large+Sparse+Models
Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems,2021,NIPS,"['Suhas S Kowshik', 'Dheeraj Mysore Nagaraj', 'Prateek Jain', 'Praneeth Netrapalli']",spotlight,"['Non-Linear Dynamical Systems', 'Learning with Dependent Data', 'Streaming Algorithms', 'Experience Replay']","We consider the setting of vector valued non-linear dynamical systems $X_{t+1} = \phi(A^{*} X_t) + \eta_t$, where $\eta_t$ is unbiased noise and $\phi : \mathbb{R} \to \mathbb{R}$ is a known link function that satisfies certain {\em expansivity property}. The goal is to learn $A^{*}$ from a single trajectory $X_1,\cdots , X_T$ of {\em dependent or correlated} samples.
	While the problem is well-studied in the linear case, where $\phi$ is identity, with optimal error rates even for non-mixing systems, existing results in the non-linear case hold only for mixing systems. In this work, we improve existing results for learning nonlinear systems in a number of ways: a) we provide the first offline algorithm that can learn non-linear dynamical systems without the mixing assumption, b) we significantly improve upon the sample complexity of existing results for mixing systems, c) in the much harder one-pass, streaming setting we study a SGD with Reverse Experience Replay (SGD-RER) method, and demonstrate that for mixing systems, it achieves the same sample complexity as our offline algorithm, d) we justify the expansivity assumption by showing that for the popular ReLU  link function --- a non-expansive but easy to learn link function with i.i.d. samples --- any method would require exponentially many samples (with respect to dimension of $X_t$) from the dynamical system. We validate our results via. simulations and  demonstrate that a naive application of SGD can be highly sub-optimal. Indeed, our work demonstrates that for correlated data, specialized  methods designed for the dependency structure in data can  significantly outperform  standard SGD based methods. ",https://api.openreview.net/pdf/e59eceb0a8be8b2d1d7ccf8b2067b85ac25ac455.pdf,offline reinforcement learning;online learning;llm,https://scholar.google.com/scholar?q=Near-optimal+Offline+and+Streaming+Algorithms+for+Learning+Non-Linear+Dynamical+Systems
SOFT: Softmax-free Transformer with Linear Complexity,2021,NIPS,"['Jiachen Lu', 'Jinghan Yao', 'Junge Zhang', 'Xiatian Zhu', 'Hang Xu', 'Weiguo Gao', 'Chunjing Xu', 'Tao Xiang', 'Li Zhang']",spotlight,['Transformer'],"Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations.  Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or  SOFT is proposed. To remove softmax in self-attention,  Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank  matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using  a  Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.",https://api.openreview.net/pdf/82d037021c14fd9ad0eaeebebd048f59a1696e5a.pdf,zero_few-shot;transformer;low-rank;llm,https://scholar.google.com/scholar?q=SOFT:+Softmax-free+Transformer+with+Linear+Complexity
Impression learning: Online representation learning with synaptic plasticity,2021,NIPS,"['Colin Bredenberg', 'Benjamin S. H. Lyo', 'Eero P Simoncelli', 'Cristina Savin']",poster,"['computational neuroscience', 'probabilistic computation', 'synaptic plasticity', 'neural sampling', 'Wake-Sleep']","Understanding how the brain constructs statistical models of the sensory world remains a longstanding challenge for computational neuroscience. Here, we derive an unsupervised local synaptic plasticity rule that trains neural circuits to infer latent structure from sensory stimuli via a novel loss function for approximate online Bayesian inference. The learning algorithm is driven by a local error signal computed between two factors that jointly contribute to neural activity: stimulus drive and internal predictions --- the network's 'impression' of the stimulus. Physiologically, we associate these two components with the basal and apical dendrites of pyramidal neurons, respectively. We show that learning can be implemented online, is capable of capturing temporal dependencies in continuous input streams, and generalizes to hierarchical architectures. Furthermore, we demonstrate both analytically and empirically that the algorithm is more data-efficient than a three-factor plasticity alternative, enabling it to learn statistics of high-dimensional, naturalistic inputs. Overall, the model provides a bridge from mechanistic accounts of synaptic plasticity to algorithmic descriptions of unsupervised probabilistic learning and inference.",https://api.openreview.net/pdf/b93384ee77f458627efcf077c169e066919e9512.pdf,representation;online learning;bayesian;inference;llm,https://scholar.google.com/scholar?q=Impression+learning:+Online+representation+learning+with+synaptic+plasticity
Dynamic Analysis of Higher-Order Coordination in Neuronal Assemblies via De-Sparsified Orthogonal Matching Pursuit,2021,NIPS,"['Shoutik Mukherjee', 'Behtash Babadi']",poster,"['neural coding', 'generalized linear models', 'synchrony', 'orthogonal matching pursuit', 'high-dimensional inference']","Coordinated ensemble spiking activity is widely observable in neural recordings and central in the study of population codes, with hypothesized roles including robust stimulus representation, interareal communication of neural information, and learning and memory formation. Model-free measures of synchrony characterize the coherence of pairwise activity, but not higher-order interactions; this limitation is transcended by statistical models of ensemble spiking activity. However, existing model-based analyses often impose assumptions about the relevance of higher-order interactions and require multiple repeated trials in order to characterize dynamics in the correlational structure of ensemble activity. To address these shortcomings, we propose an adaptive greedy filtering algorithm based on a discretized mark point-process model of ensemble spiking and a corresponding precise statistical inference framework to identify significant coordinated higher-order spiking activity. In the course of developing the statistical inference procedures, we also show that confidence intervals can be constructed for greedily estimated parameters. We demonstrate the utility of our proposed methods on simulated neuronal assemblies. Applied to multi-electrode recordings of human cortical ensembles, our proposed methods provide new insights into the dynamics underlying localized population activity during transitions between brain states.",https://api.openreview.net/pdf/eaf6eb7df15ca9004518c692d0f5c91a8cdaae36.pdf,representation;adaptive;inference;llm,https://scholar.google.com/scholar?q=Dynamic+Analysis+of+Higher-Order+Coordination+in+Neuronal+Assemblies+via+De-Sparsified+Orthogonal+Matching+Pursuit
A Contrastive Learning Approach for Training Variational Autoencoder Priors,2021,NIPS,"['Jyoti Aneja', 'Alex Schwing', 'Jan Kautz', 'Arash Vahdat']",poster,"['Variational Autoencoders', 'Noise Contrastive Estimation', 'Sampling']","Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in many domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is simple and can be applied to a wide variety of VAEs to improve the expressivity of their prior distribution.",https://api.openreview.net/pdf/eafa3b3afa6cdbeb444b3e9dd9fb210f140ad0f5.pdf,graph;vae;generative model;contrastive learning;llm,https://scholar.google.com/scholar?q=A+Contrastive+Learning+Approach+for+Training+Variational+Autoencoder+Priors
Towards robust vision by multi-task learning on monkey visual cortex,2021,NIPS,"['Shahd Safarani', 'Arne Nix', 'Konstantin Friedrich Willeke', 'Santiago A Cadena', 'Kelli Restivo', 'George Denfield', 'Andreas S. Tolias', 'Fabian H. Sinz']",poster,"['deep learning', 'neuroscience', 'computer vision', 'robustness']","Deep neural networks set the state-of-the-art across many tasks in computer vision, but their generalization ability to simple image distortions is surprisingly fragile. In contrast, the mammalian visual system is robust to a wide range of perturbations. Recent work suggests that this generalization ability can be explained by useful inductive biases encoded in the representations of visual stimuli throughout the visual cortex. Here, we successfully leveraged these inductive biases with a multi-task learning approach: we jointly trained a deep network to perform image classification and to predict neural activity in macaque primary visual cortex (V1) in response to the same natural stimuli. We measured the out-of-distribution generalization abilities of our resulting network by testing its robustness to common image distortions. We found that co-training on monkey V1 data indeed leads to increased robustness despite the absence of those distortions during training. Additionally, we showed that our network's robustness is often very close to that of an Oracle network where parts of the architecture are directly trained on noisy images. Our results also demonstrated that the network's representations become more brain-like as their robustness improves. Using a novel constrained reconstruction analysis, we investigated what makes our brain-regularized network more robust. We found that our monkey co-trained network is more sensitive to content than noise when compared to a Baseline network that we trained for image classification alone. Using DeepGaze-predicted saliency maps for ImageNet images, we found that the monkey co-trained network tends to be more sensitive to salient regions in a scene, reminiscent of existing theories on the role of V1 in the detection of object borders and bottom-up saliency. Overall, our work expands the promising research avenue of transferring inductive biases from biological to artificial neural networks on the representational level, and provides a novel analysis of the effects of our transfer.",https://api.openreview.net/pdf/385eb34b98f2082c33de8adf0427fd6f5e3a7305.pdf,optimization;representation;transfer learning;multi-task;llm,https://scholar.google.com/scholar?q=Towards+robust+vision+by+multi-task+learning+on+monkey+visual+cortex
Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery Ticket Perspective,2021,NIPS,"['Tianlong Chen', 'Yu Cheng', 'Zhe Gan', 'Jingjing Liu', 'Zhangyang Wang']",poster,"['Data-Efficient GAN Training', 'The Lottery Tickets Hypothesis']","Training generative adversarial networks (GANs) with limited real image data generally results in deteriorated performance and collapsed models. To conquer this challenge, we are inspired by the latest observation, that one can discover independently trainable and highly sparse subnetworks (a.k.a., lottery tickets) from GANs. Treating this as an inductive prior, we suggest a brand-new angle towards data-efficient GAN training: by first identifying the lottery ticket from the original GAN using the small training set of real images; and then focusing on training that sparse subnetwork by re-using the same set. We find our coordinated framework to offer orthogonal gains to existing real image data augmentation methods, and we additionally present a new feature-level augmentation that can be applied together with them. Comprehensive experiments endorse the effectiveness of our proposed framework, across various GAN architectures (SNGAN, BigGAN, and StyleGAN-V2) and diverse datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet, ImageNet, and multiple few-shot generation datasets). Codes are available at: https://github.com/VITA-Group/Ultra-Data-Efficient-GAN-Training.",https://api.openreview.net/pdf/0ad793b14eddc6107d85dbb5e905edb88232e0b8.pdf,graph;generative model;sparse;augmentation;llm,https://scholar.google.com/scholar?q=Data-Efficient+GAN+Training+Beyond+(Just)+Augmentations:+A+Lottery+Ticket+Perspective
Revisiting Model Stitching to Compare Neural Representations,2021,NIPS,"['Yamini Bansal', 'Preetum Nakkiran', 'Boaz Barak']",poster,"['representations', 'deep learning', 'neural network dynamics']","We revisit and extend model stitching (Lenc & Vedaldi 2015) as a methodology to study the internal representations of neural networks. Given two trained and frozen models $A$ and $B$, we consider a ""stitched model"" formed by connecting the bottom-layers of $A$ to the top-layers of $B$, with a simple trainable layer between them.  We argue that model stitching is a powerful and perhaps under-appreciated tool, which reveals aspects of representations that measures such as centered kernel alignment (CKA) cannot. Through extensive experiments, we use model stitching to obtain quantitative verifications for intuitive statements such as ""good networks learn similar representations"", by demonstrating that good networks of the same architecture, but trained in very different ways (eg: supervised vs. self-supervised learning), can be stitched to each other without drop in performance. We also give evidence for the intuition that ""more is better"" by showing that representations learnt with (1) more data, (2) bigger width, or (3) more training time can be ""plugged in"" to weaker models to improve performance. Finally, our experiments reveal a new structural property of SGD which we call ""stitching connectivity"", akin to mode-connectivity: typical minima reached by SGD are all ""stitching-connected"" to each other.
",https://api.openreview.net/pdf/4bd10d6feddeb2a4f3983b18d830cfe2709d355e.pdf,zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Revisiting+Model+Stitching+to+Compare+Neural+Representations
MOMA: Multi-Object Multi-Actor Activity Parsing,2021,NIPS,"['Zelun Luo', 'Wanze Xie', 'Siddharth Kapoor', 'Yiyun Liang', 'Michael Cooper', 'Juan Carlos Niebles', 'Ehsan Adeli', 'L. Fei-Fei']",poster,"['activity recognition', 'video understanding', 'human-object interaction', 'temporal action detection']","Complex activities often involve multiple humans utilizing different objects to complete actions (e.g., in healthcare settings, physicians, nurses, and patients interact with each other and various medical devices). Recognizing activities poses a challenge that requires a detailed understanding of actors' roles, objects' affordances, and their associated relationships. Furthermore, these purposeful activities are composed of multiple achievable steps, including sub-activities and atomic actions, which jointly define a hierarchy of action parts. This paper introduces Activity Parsing as the overarching task of temporal segmentation and classification of activities, sub-activities, atomic actions, along with an instance-level understanding of actors, objects, and their relationships in videos. Involving multiple entities (actors and objects), we argue that traditional pair-wise relationships, often used in scene or action graphs, do not appropriately represent the dynamics between them. Hence, we introduce Action Hypergraph, a spatial-temporal graph containing hyperedges (i.e., edges with higher-order relationships), as a new representation. In addition, we introduce Multi-Object Multi-Actor (MOMA), the first benchmark and dataset dedicated to activity parsing. Lastly, to parse a video, we propose the HyperGraph Activity Parsing (HGAP) network, which outperforms several baselines, including those based on regular graphs and raw video data. ",https://api.openreview.net/pdf/65adf750a6064c8ceea65c0018b705fe98e8bb64.pdf,graph;representation;segmentation;llm,https://scholar.google.com/scholar?q=MOMA:+Multi-Object+Multi-Actor+Activity+Parsing
Adversarial Examples in Multi-Layer Random ReLU Networks,2021,NIPS,"['Peter Bartlett', 'Sebastien Bubeck', 'Yeshwanth Cherapanamjeri']",poster,"['Adversarial examples', 'deep ReLU networks', 'theory']","We consider the phenomenon of adversarial examples in ReLU networks with independent Gaussian parameters.  For networks of constant depth and with a large range of widths (for instance, it suffices if the width of each layer is polynomial in that of any other layer), small perturbations of input vectors lead to large changes of outputs.  This generalizes results of Daniely and Schacham (2020) for networks of rapidly decreasing width and of Bubeck et al (2021) for two-layer networks. Our proof shows that adversarial examples arise in these networks because the functions they compute are \emph{locally} very similar to random linear functions. Bottleneck layers play a key role: the minimal width up to some point in the network determines scales and sensitivities of mappings computed up to that point.  The main result is for networks with constant depth, but we also show that some constraint on depth is necessary for a result of this kind, because there are suitably deep networks that, with constant probability, compute a function that is close to constant.",https://api.openreview.net/pdf/c19bbdc591475f27659471733b98192f8392af03.pdf,optimization;llm,https://scholar.google.com/scholar?q=Adversarial+Examples+in+Multi-Layer+Random+ReLU+Networks
Understanding Interlocking Dynamics of Cooperative Rationalization,2021,NIPS,"['Mo Yu', 'Yang Zhang', 'Shiyu Chang', 'Tommi S. Jaakkola']",poster,"['Rationalization', 'interlocking', 'cooperative game', 'model explainability', 'natural language processing']","Selective rationalization explains the prediction of complex neural networks by finding a small subset of the input that is sufficient to predict the neural model output. The selection mechanism is commonly integrated into the model itself by specifying a two-component cascaded system consisting of a rationale generator, which makes a binary selection of the input features (which is the rationale), and a predictor, which predicts the output based only on the selected features. The components are trained jointly to optimize prediction performance. In this paper, we reveal a major problem with such cooperative rationalization paradigm --- model interlocking. Inter-locking arises when the predictor overfits to the features selected by the generator thus reinforcing the generator's selection even if the selected rationales are sub-optimal. The fundamental cause of the interlocking problem is that the rationalization objective to be minimized is concave with respect to the generator’s selection policy. We propose a new rationalization framework, called A2R, which introduces a third component into the architecture, a predictor driven by soft attention as opposed to selection. The generator now realizes both soft and hard attention over the features and these are fed into the two different predictors. While the generator still seeks to support the original predictor performance, it also minimizes a gap between the two predictors. As we will show theoretically, since the attention-based predictor exhibits a better convexity property, A2R can overcome the concavity barrier. Our experiments on two synthetic benchmarks and two real datasets demonstrate that A2R can significantly alleviate the interlock problem and find explanations that better align with human judgments.",https://api.openreview.net/pdf/14641f9f7ba1434ccbc1f32b9927617f91fd1e35.pdf,graph;transformer;llm,https://scholar.google.com/scholar?q=Understanding+Interlocking+Dynamics+of+Cooperative+Rationalization
How Does it Sound?,2021,NIPS,"['Kun Su', 'Xiulong Liu', 'Eli Shlizerman']",poster,"['audio-visual learning', 'video to music', 'applications', 'multi-modality']","One of the primary purposes of video is to capture people and their unique activities. It is often the case that the experience of watching the video can be enhanced by adding a musical soundtrack that is in-sync with the rhythmic features of these activities. How would this soundtrack sound? Such a problem is challenging since little is known about capturing the rhythmic nature of free body movements. In this work, we explore this problem and propose a novel system, called `RhythmicNet', which takes as an input a video which includes human movements and generates a soundtrack for it. RhythmicNet works directly with human movements by extracting skeleton keypoints and implements a sequence of models which translate the keypoints to rhythmic sounds.
RhythmicNet follows the natural process of music improvisation which includes the prescription of streams of the beat, the rhythm and the melody. In particular, RhythmicNet first infers the music beat and the style pattern from body keypoints per each frame to produce rhythm. Next, it implements a transformer-based model to generate the hits of drum instruments and implements a U-net based model to generate the velocity and the offsets of the instruments. Additional types of instruments are added to the soundtrack by further conditioning on the generated drum sounds. We evaluate RhythmicNet on large scale datasets of videos that include body movements with inherit sound association, such as dance, as well as 'in the wild' internet videos of various movements and actions. We show that the method can generate plausible music that aligns well with different types of human movements.",https://api.openreview.net/pdf/ac13da3b7ee6de84b4f9cfe8d165c5539fe69878.pdf,graph;zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=How+Does+it+Sound?
RelaySum for Decentralized Deep Learning on Heterogeneous Data,2021,NIPS,"['Thijs Vogels', 'Lie He', 'Anastasia Koloskova', 'Sai Praneeth Karimireddy', 'Tao Lin', 'Sebastian U Stich', 'Martin Jaggi']",poster,"['decentralized optimization', 'distributed optimization', 'consensus', 'deep learning']","In decentralized machine learning, workers compute model updates on their local data.
Because the workers only communicate with few neighbors without central coordination, these updates propagate progressively over the network.
This paradigm enables distributed training on networks without all-to-all connectivity, helping to protect data privacy as well as to reduce the communication cost of distributed training in data centers.
A key challenge, primarily in decentralized deep learning, remains the handling of differences between the workers' local data distributions.
To tackle this challenge, we introduce the RelaySum mechanism for information propagation in decentralized learning.
RelaySum uses spanning trees to distribute information exactly uniformly across all workers with finite delays depending on the distance between nodes.
In contrast, the typical gossip averaging mechanism only distributes data uniformly asymptotically while using the same communication volume per step as RelaySum.
We prove that RelaySGD, based on this mechanism, is independent of data heterogeneity and scales to many workers, enabling highly accurate decentralized deep learning on heterogeneous data.",https://api.openreview.net/pdf/da6e95fcacb7887a28841f9ef42065bd18fe04f8.pdf,graph;llm,https://scholar.google.com/scholar?q=RelaySum+for+Decentralized+Deep+Learning+on+Heterogeneous+Data
Unadversarial Examples: Designing Objects for Robust Vision,2021,NIPS,"['Hadi Salman', 'Andrew Ilyas', 'Logan Engstrom', 'Sai Vemprala', 'Aleksander Madry', 'Ashish Kapoor']",poster,"['adversarial robustness', 'adversarial examples', 'computer vision']","We study a class of computer vision settings wherein one can modify the design of the objects being recognized. We develop a framework that leverages this capability---and deep networks' unusual sensitivity to input perturbations---to design ``robust objects,'' i.e., objects that are explicitly optimized to be confidently classified. Our framework yields improved performance on standard benchmarks, a simulated robotics environment, and physical-world experiments. ",https://api.openreview.net/pdf/1cc83c21b173d77ddd6cec1a946b38cd3219c62c.pdf,llm,https://scholar.google.com/scholar?q=Unadversarial+Examples:+Designing+Objects+for+Robust+Vision
Adaptive First-Order Methods Revisited: Convex Minimization without Lipschitz Requirements,2021,NIPS,"['Kimon Antonakopoulos', 'Panayotis Mertikopoulos']",poster,"['convex optimization', 'non-Lipschitz', 'adaptive methods']","We propose a new family of adaptive first-order methods for a class of convex minimization problems that may fail to be Lipschitz continuous or smooth in the standard sense. Specifically, motivated by a recent flurry of activity on non-Lipschitz (NoLips) optimization, we consider problems that are continuous or smooth relative to a reference Bregman function – as opposed to a global, ambient norm (Euclidean or otherwise). These conditions encompass a wide range ofproblems with singular objective, such as Fisher markets, Poisson tomography, D-design, and the like. In this setting, the application of existing order-optimal adaptive methods – like UnixGrad or AcceleGrad – is not possible, especially in the presence of randomness and uncertainty. The proposed method, adaptive mirror descent (AdaMir), aims to close this gap by concurrently achieving min-max optimal rates in problems that are relatively continuous or smooth, including stochastic ones.",https://api.openreview.net/pdf/4cd5c7f82c19f6603d6cafdb2a2b2203f7cceb03.pdf,graph;optimization;adaptive;llm,https://scholar.google.com/scholar?q=Adaptive+First-Order+Methods+Revisited:+Convex+Minimization+without+Lipschitz+Requirements
When Are Solutions Connected in Deep Networks?,2021,NIPS,"['Quynh Nguyen', 'Pierre Bréchet', 'Marco Mondelli']",poster,"['Theory of deep learning', 'mode connectivity']","The question of how and why the phenomenon of mode connectivity occurs in training deep neural networks has gained remarkable attention in the research community. From a theoretical perspective, two possible explanations have been proposed: (i) the loss function has connected sublevel sets, and (ii) the solutions found by stochastic gradient descent are dropout stable. While these explanations provide insights into the phenomenon, their assumptions are not always satisfied in practice. In particular, the first approach requires the network to have one layer with order of $N$ neurons ($N$ being the number of training samples), while the second one requires the loss to be almost invariant after removing half of the neurons at each layer (up to some rescaling of the remaining ones). In this work, we improve both conditions by exploiting the quality of the features at every intermediate layer together with a milder over-parameterization requirement. More specifically, we show that: (i) under generic assumptions on the features of intermediate layers, it suffices that the last two hidden layers have order of $\sqrt{N}$ neurons, and (ii) if subsets of features at each layer are linearly separable, then almost no over-parameterization is needed to show the connectivity. Our experiments confirm that the proposed condition ensures the connectivity of solutions found by stochastic gradient descent, even in settings where the previous requirements do not hold.",https://api.openreview.net/pdf/3acf48656b3e56e3bf9c63f15f177eeaff4c37f0.pdf,transformer;llm,https://scholar.google.com/scholar?q=When+Are+Solutions+Connected+in+Deep+Networks?
Directed Spectrum Measures Improve Latent Network Models Of Neural Populations,2021,NIPS,"['Neil Gallagher', 'Kafui Dzirasa', 'David Carlson']",poster,"['functional brain networks', 'linear factor models', 'directed connectivity', 'directed spectrum']","Systems neuroscience aims to understand how networks of neurons distributed throughout the brain mediate computational tasks. One popular approach to identify those networks is to first calculate measures of neural activity (e.g. power spectra) from multiple brain regions, and then apply a linear factor model to those measures. Critically, despite the established role of directed communication between brain regions in neural computation, measures of directed communication have been rarely utilized in network estimation because they are incompatible with the implicit assumptions of the linear factor model approach. Here, we develop a novel spectral measure of directed communication called the Directed Spectrum (DS). We prove that it is compatible with the implicit assumptions of linear factor models, and we provide a method to estimate the DS. We demonstrate that latent linear factor models of DS measures better capture underlying brain networks in both simulated and real neural recording data compared to available alternatives. Thus, linear factor models of the Directed Spectrum offer neuroscientists a simple and effective way to explicitly model directed communication in networks of neural populations.",https://api.openreview.net/pdf/5d5942b8a57a1b4eca409476bdd28481b4329155.pdf,llm,https://scholar.google.com/scholar?q=Directed+Spectrum+Measures+Improve+Latent+Network+Models+Of+Neural+Populations
Delayed Propagation Transformer: A Universal Computation Engine towards Practical Control in Cyber-Physical Systems,2021,NIPS,"['Wenqing Zheng', 'Qiangqiang Guo', 'Hao Frank Yang', 'Peihao Wang', 'Zhangyang Wang']",poster,"['Transformers', 'Control', 'Reinforcement Learning', 'Cyber-Physical System']","Multi-agent control is a central theme in the Cyber-Physical Systems (CPS). However, current control methods either receive non-Markovian states due to insufficient sensing and decentralized design, or suffer from poor convergence. This paper presents the Delayed Propagation Transformer (DePT), a new transformer-based model that specializes in the global modeling of CPS while taking into account the immutable constraints from the physical world. DePT induces a cone-shaped spatial-temporal attention prior, which injects the information propagation and aggregation principles and enables a global view. With physical constraint inductive bias baked into its design, our DePT is ready to plug and play for a broad class of multi-agent systems. The experimental results on one of the most challenging CPS -- network-scale traffic signal control system in the open world -- show that our model outperformed the state-of-the-art expert methods on synthetic and real-world datasets. Our codes are released at: https://github.com/VITA-Group/DePT.",https://api.openreview.net/pdf/7c80a7a6ac60abc18c9bc0ddbe500bebd69caff9.pdf,reinforcement learning;graph;optimization;transformer;multi-agent;llm,https://scholar.google.com/scholar?q=Delayed+Propagation+Transformer:+A+Universal+Computation+Engine+towards+Practical+Control+in+Cyber-Physical+Systems
Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics,2021,NIPS,"['Bhavin Choksi', 'Milad Mozafari', ""Callum Biggs O'May"", 'B. ADOR', 'Andrea Alamia', 'Rufin VanRullen']",poster,"['predictive coding', 'neuroscience', 'deep learning', 'robustness', 'noisy image classification', 'python package']","Deep neural networks excel at image classification, but their performance is far less robust to input perturbations than human perception. In this work we explore whether this shortcoming may be partly addressed by incorporating brain-inspired recurrent dynamics in deep convolutional networks. We take inspiration from a popular framework in neuroscience: ""predictive coding"". At each layer of the hierarchical model, generative feedback ""predicts"" (i.e., reconstructs) the pattern of activity in the previous layer. The reconstruction errors are used to iteratively update the network’s representations across timesteps, and to optimize the network's feedback weights over the natural image dataset--a form of unsupervised training. We show that implementing this strategy into two popular networks, VGG16 and EfficientNetB0, improves their robustness against various corruptions and adversarial attacks. We hypothesize that other feedforward networks could similarly benefit from the proposed framework. To promote research in this direction, we provide an open-sourced PyTorch-based package called \textit{Predify}, which can be used to implement and investigate the impacts of the predictive coding dynamics in any convolutional neural network. ",https://api.openreview.net/pdf/39fa134e6c995b9898807151dccea3304a691e5c.pdf,representation;generative model;llm,https://scholar.google.com/scholar?q=Predify:+Augmenting+deep+neural+networks+with+brain-inspired+predictive+coding+dynamics
Instance-optimal Mean Estimation Under Differential Privacy,2021,NIPS,"['Ziyue Huang', 'Yuting Liang', 'Ke Yi']",poster,"['Mean estimation', 'differential privacy', 'instance optimality']","Mean estimation under differential privacy is a fundamental problem, but worst-case optimal mechanisms do not offer meaningful utility guarantees in practice when the global sensitivity is very large.  Instead, various heuristics have been proposed to reduce the error on real-world data that do not resemble the worst-case instance.  This paper takes a principled approach, yielding a mechanism that is instance-optimal in a strong sense.  In addition to its theoretical optimality, the mechanism is also simple and practical, and adapts to a variety of data characteristics without the need of parameter tuning.  It easily extends to the local and shuffle model as well.",https://api.openreview.net/pdf/3552edcb4c56eb621531e15c8bb69b6cb280f64a.pdf,llm,https://scholar.google.com/scholar?q=Instance-optimal+Mean+Estimation+Under+Differential+Privacy
Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence,2021,NIPS,"['Antoine Labatie', 'Dominic Masters', 'Zach Eaton-Rosen', 'Carlo Luschi']",poster,"['Batch-Independent Normalization', 'Batch Normalization', 'Layer Normalization', 'Group Normalization', 'Instance Normalization', 'Deep Learning Theory', 'CNNs', 'ResNets', 'ResNeXts', 'EfficientNets']","We investigate the reasons for the performance degradation incurred with batch-independent normalization. We find that the prototypical techniques of layer normalization and instance normalization both induce the appearance of failure modes in the neural network's pre-activations: (i) layer normalization induces a collapse towards channel-wise constant functions; (ii) instance normalization induces a lack of variability in instance statistics, symptomatic of an alteration of the expressivity. To alleviate failure mode (i) without aggravating failure mode (ii), we introduce the technique ""Proxy Normalization"" that normalizes post-activations using a proxy distribution. When combined with layer normalization or group normalization, this batch-independent normalization emulates batch normalization's behavior and consistently matches or exceeds its performance.",https://api.openreview.net/pdf/7261daa0c245e94148ba4496f7ae3a06a7457b80.pdf,llm,https://scholar.google.com/scholar?q=Proxy-Normalizing+Activations+to+Match+Batch+Normalization+while+Removing+Batch+Dependence
Detecting Moments and Highlights in Videos via Natural Language Queries,2021,NIPS,"['Jie Lei', 'Tamara Lee Berg', 'Mohit Bansal']",poster,"['query-based video moment retrieval', 'query-based video highlight detection']","Detecting customized moments and highlights from videos given natural language (NL) user queries is an important but under-studied topic. One of the challenges in pursuing this direction is the lack of annotated data. To address this issue, we present the Query-based Video Highlights (QVHighlights) dataset. It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips. This comprehensive annotation enables us to develop and evaluate systems that detect relevant moments as well as salient highlights for diverse, flexible user queries. We also present a strong baseline for this task, Moment-DETR, a transformer encoder-decoder model that views moment retrieval as a direct set prediction problem, taking extracted video and query representations as inputs and predicting moment coordinates and saliency scores end-to-end. While our model does not utilize any human prior, we show that it performs competitively when compared to well-engineered architectures. With weakly supervised pretraining using ASR captions, Moment-DETR substantially outperforms previous methods. Lastly, we present several ablations and visualizations of Moment-DETR. Data and code is publicly available at https://github.com/jayleicn/moment_detr.",https://api.openreview.net/pdf/cf987aacb8113a5cc2013b7f8d6e324cc979c7c0.pdf,graph;zero_few-shot;transformer;representation;llm,https://scholar.google.com/scholar?q=Detecting+Moments+and+Highlights+in+Videos+via+Natural+Language+Queries
Flattening Sharpness for Dynamic Gradient Projection Memory Benefits Continual Learning,2021,NIPS,"['Danruo DENG', 'Guangyong Chen', 'Jianye HAO', 'Qiong Wang', 'Pheng-Ann Heng']",poster,"['continual learning', 'weight loss landscape', 'dynamic gradient projection memory', 'sharpness flatten']","The backpropagation networks are notably susceptible to catastrophic forgetting, where networks tend to forget previously learned skills upon learning new ones. To address such the 'sensitivity-stability' dilemma, most previous efforts have been contributed to minimizing the empirical risk with different parameter regularization terms and episodic memory, but rarely exploring the usages of the weight loss landscape. In this paper, we investigate the relationship between the weight loss landscape and sensitivity-stability in the continual learning scenario, based on which, we propose a novel method, Flattening Sharpness for Dynamic Gradient Projection Memory (FS-DGPM). In particular, we introduce a soft weight to represent the importance of each basis representing past tasks in GPM, which can be adaptively learned during the learning process, so that less important bases can be dynamically released to improve the sensitivity of new skill learning. We further introduce Flattening Sharpness (FS) to reduce the generalization gap by explicitly regulating the flatness of the weight loss landscape of all seen tasks. As demonstrated empirically, our proposed method consistently outperforms baselines with the superior ability to learn new skills while alleviating forgetting effectively.",https://api.openreview.net/pdf/2782adb812cb75e925462317cf84e63e1bac40f0.pdf,adaptive;llm,https://scholar.google.com/scholar?q=Flattening+Sharpness+for+Dynamic+Gradient+Projection+Memory+Benefits+Continual+Learning
Densely connected normalizing flows,2021,NIPS,"['Matej Grcić', 'Ivan Grubišić', 'Siniša Šegvić']",poster,"['Density estimation', 'Normalizing flows', 'Image generation', 'DenseFlow', 'Densely connected couplings']","Normalizing flows are bijective mappings between inputs and latent representations with a fully factorized distribution. They are very attractive due to exact likelihood evaluation and efficient sampling. However, their effective capacity is often insufficient since the bijectivity constraint limits the model width. We address this issue by incrementally padding intermediate representations with noise. We precondition the noise in accordance with previous invertible units, which we describe as cross-unit coupling. Our invertible glow-like modules increase the model expressivity by fusing a densely connected block with Nyström self-attention. We refer to our architecture as DenseFlow since both cross-unit and intra-module couplings rely on dense connectivity. Experiments show significant improvements due to the proposed contributions and reveal state-of-the-art density estimation under moderate computing budgets.",https://api.openreview.net/pdf/6a32d0b7e14c2a814cb39f235e44cdf9cbc9e05b.pdf,optimization;zero_few-shot;transformer;representation;active learning;flow;llm,https://scholar.google.com/scholar?q=Densely+connected+normalizing+flows
Revisiting Hilbert-Schmidt Information Bottleneck for Adversarial Robustness,2021,NIPS,"['Zifeng Wang', 'Tong Jian', 'Aria Masoomi', 'Stratis Ioannidis', 'Jennifer Dy']",poster,"['Adversarial Robustness', 'Hilbert-Schmidt Independence Criterion', 'Information Bottleneck', 'Deep Learning']","We investigate the HSIC (Hilbert-Schmidt independence criterion) bottleneck as a regularizer for learning an adversarially robust deep neural network classifier. In addition to the usual cross-entropy loss, we add regularization terms for every intermediate layer to ensure that the latent representations retain useful information for output prediction while reducing redundant information. We show that the HSIC bottleneck enhances robustness to adversarial attacks both theoretically and experimentally. In particular, we prove that the HSIC bottleneck regularizer reduces the sensitivity of the classifier to adversarial examples. Our experiments on multiple benchmark datasets and architectures demonstrate that incorporating an HSIC bottleneck regularizer attains competitive natural accuracy and improves adversarial robustness, both with and without adversarial examples during training. Our code and adversarially robust models are publicly available.",https://api.openreview.net/pdf/6bb981b744bf0e335ae95e84d8d082051e50defd.pdf,transformer;representation;llm,https://scholar.google.com/scholar?q=Revisiting+Hilbert-Schmidt+Information+Bottleneck+for+Adversarial+Robustness
Towards Unifying Behavioral and Response Diversity for Open-ended Learning in Zero-sum Games,2021,NIPS,"['Xiangyu Liu', 'Hangtian Jia', 'Ying Wen', 'Yujing Hu', 'Yingfeng Chen', 'Changjie Fan', 'Zhipeng Hu', 'Yaodong Yang']",poster,"['reinforcememt learning', 'multi-agent learning', 'diverse policies']","Measuring and promoting policy diversity is critical for solving games with strong non-transitive dynamics where strategic cycles exist, and there is no consistent winner (e.g., Rock-Paper-Scissors). With that in mind, maintaining a pool of diverse policies via open-ended learning is an attractive solution, which can generate auto-curricula to avoid being exploited. However, in conventional open-ended learning algorithms, there are no widely accepted definitions for diversity, making it hard to construct and evaluate the diverse policies. In this work, we summarize previous concepts of diversity and work towards offering a unified measure of diversity in multi-agent open-ended learning to include all elements in Markov games, based on both Behavioral Diversity (BD) and Response Diversity (RD). At the trajectory distribution level, we re-define BD in the state-action space as the discrepancies of occupancy measures. For the reward dynamics, we propose RD to characterize diversity through the responses of policies when encountering different opponents. We also show that many current diversity measures fall in one of the categories of BD or RD but not both. With this unified diversity measure, we design the corresponding diversity-promoting objective and population effectivity when seeking the best responses in open-ended learning. We validate our methods in both relatively simple games like matrix game, non-transitive mixture model, and the complex \textit{Google Research Football} environment. The population found by our methods reveals the lowest exploitability, highest population effectivity in matrix game and non-transitive mixture model, as well as the largest goal difference when interacting with opponents of various levels in \textit{Google Research Football}.",https://api.openreview.net/pdf/038077829daa4fa523c1cd5b29a28b68c4d5844a.pdf,reinforcement learning;zero_few-shot;active learning;multi-agent;llm,https://scholar.google.com/scholar?q=Towards+Unifying+Behavioral+and+Response+Diversity+for+Open-ended+Learning+in+Zero-sum+Games
Hierarchical Skills for Efficient Exploration,2021,NIPS,"['Jonas Gehring', 'Gabriel Synnaeve', 'Andreas Krause', 'Nicolas Usunier']",poster,"['Hierarchical Reinforcement Learning', 'Skill Discovery']","In reinforcement learning, pre-trained low-level skills have the potential to greatly facilitate exploration. However, prior knowledge of the downstream task is required to strike the right balance between generality (fine-grained control) and specificity (faster learning) in skill design. In previous work on continuous control, the sensitivity of methods to this trade-off has not been addressed explicitly, as locomotion provides a suitable prior for navigation tasks, which have been of foremost interest. In this work, we analyze this trade-off for low-level policy pre-training with a new benchmark suite of  diverse, sparse-reward tasks for bipedal robots. We alleviate the need for prior knowledge by proposing a hierarchical skill learning framework that acquires skills of varying complexity in an unsupervised manner. For utilization on downstream tasks, we present a three-layered hierarchical learning algorithm to automatically trade off between general and specific skills as required by the respective task. In our experiments, we show that our approach performs this trade-off effectively and achieves better results than current state-of-the-art methods for end-to-end hierarchical reinforcement learning and unsupervised skill discovery.",https://api.openreview.net/pdf/93943fcf2bea91b001a399927003a94df0b39a73.pdf,reinforcement learning;zero_few-shot;sparse;llm,https://scholar.google.com/scholar?q=Hierarchical+Skills+for+Efficient+Exploration
Dual Adaptivity: A Universal Algorithm for Minimizing the Adaptive Regret of Convex Functions,2021,NIPS,"['Lijun Zhang', 'Guanghui Wang', 'Wei-Wei Tu', 'Wei Jiang', 'Zhi-hua Zhou']",poster,"['Online Convex Optimization', 'Adaptive Regret', 'Convex Functions', 'Strongly Convex Functions', 'Exponentially Concave Functions']","To deal with changing environments, a new performance measure—adaptive regret, defined as the maximum static regret over any interval, was proposed in online learning. Under the setting of online convex optimization, several algorithms have been successfully developed to minimize the adaptive regret. However, existing algorithms lack universality in the sense that they can only handle one type of convex functions and need apriori knowledge of parameters. By contrast, there exist universal algorithms, such as MetaGrad, that attain optimal static regret for multiple types of convex functions simultaneously. Along this line of research, this paper presents the first universal algorithm for minimizing the adaptive regret of convex functions. Specifically, we borrow the idea of maintaining multiple learning rates in MetaGrad to handle the uncertainty of functions, and utilize the technique of sleeping experts to capture changing environments. In this way, our algorithm automatically adapts to the property of functions (convex, exponentially concave, or strongly convex), as well as the nature of environments (stationary or changing). As a by product, it also allows the type of functions to switch between rounds.
",https://api.openreview.net/pdf/cce0dd97461e5a018181b0eb957d57672ee89082.pdf,graph;optimization;zero_few-shot;online learning;adaptive;meta-learning;llm,https://scholar.google.com/scholar?q=Dual+Adaptivity:+A+Universal+Algorithm+for+Minimizing+the+Adaptive+Regret+of+Convex+Functions
R-Drop: Regularized Dropout for Neural Networks,2021,NIPS,"['xiaobo liang', 'Lijun Wu', 'Juntao Li', 'Yue Wang', 'Qi Meng', 'Tao Qin', 'Wei Chen', 'Min Zhang', 'Tie-Yan Liu']",poster,"['Regularization', 'Dropout', 'Simple']","Dropout is a powerful and widely used technique to regularize the training of deep neural networks. Though effective and performing well, the randomness introduced by dropout causes unnegligible inconsistency between training and inference. In this paper, we introduce a simple consistency training strategy to regularize dropout, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the above inconsistency. Experiments on $\bf{5}$ widely used deep learning tasks ($\bf{18}$ datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English$\to$German translation ($\bf{30.91}$ BLEU) and WMT14 English$\to$French translation ($\bf{43.95}$ BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub\footnote{\url{https://github.com/dropreg/R-Drop}}.",https://api.openreview.net/pdf/8baa3284b9976ff1465bc6ed22efcc777822b321.pdf,transformer;inference;active learning;llm,https://scholar.google.com/scholar?q=R-Drop:+Regularized+Dropout+for+Neural+Networks
Learning Theory Can (Sometimes) Explain Generalisation in Graph Neural Networks,2021,NIPS,"['Pascal Esser', 'Leena Chennuru Vankadara', 'Debarghya Ghoshdastidar']",poster,"['Learning Theory', 'Generalisation', 'Graph Neural Networks', 'Transductive Learning', 'Rademacher Complexity']","In recent years, several results in the supervised learning setting suggested that classical statistical learning-theoretic measures, such as VC dimension, do not adequately explain the performance of deep learning models which prompted a slew of work in the infinite-width and iteration regimes. However, there is little theoretical explanation for the success of neural networks beyond the supervised setting. In this paper we argue that, under some distributional assumptions, classical learning-theoretic measures can sufficiently explain generalization for graph neural networks in the transductive setting. In particular, we provide a rigorous analysis of the performance of neural networks in the context of transductive inference, specifically by analysing the generalisation properties of graph convolutional networks for the problem of node classification. While VC-dimension does result in trivial generalisation error bounds in this setting as well, we show that transductive Rademacher complexity can explain the generalisation properties of graph convolutional networks for stochastic block models. We further use the generalisation error bounds based on transductive Rademacher complexity to demonstrate the role of graph convolutions and network architectures in achieving smaller generalisation error and provide insights into when the graph structure can help in learning. The findings of this paper could re-new the interest in studying generalisation in neural networks in terms of learning-theoretic measures, albeit in specific problems.",https://api.openreview.net/pdf/18e30096c5bf61e6b36adc0d77efdbc03964c18d.pdf,graph;inference;llm,https://scholar.google.com/scholar?q=Learning+Theory+Can+(Sometimes)+Explain+Generalisation+in+Graph+Neural+Networks
Do Input Gradients Highlight Discriminative Features? ,2021,NIPS,"['Harshay Shah', 'Prateek Jain', 'Praneeth Netrapalli']",poster,"['instance-specific explanations', 'post-hoc interpretability', 'feature attributions', 'input gradients', 'adversarial robustness']","Post-hoc gradient-based interpretability methods [Simonyan et al., 2013, Smilkov et al., 2017] that provide instance-specific explanations of model predictions are often based on assumption (A): magnitude of input gradients—gradients of logits with respect to input—noisily highlight discriminative task-relevant features. In this work, we test the validity of assumption (A) using a three-pronged approach:

1. We develop an evaluation framework, DiffROAR, to test assumption (A) on four image classification benchmarks. Our results suggest that (i) input gradients of standard models (i.e., trained on original data) may grossly violate (A), whereas (ii) input gradients of adversarially robust models satisfy (A).

2. We then introduce BlockMNIST, an MNIST-based semi-real dataset, that by design encodes a priori knowledge of discriminative features. Our analysis on BlockMNIST leverages this information to validate as well as characterize differences between input gradient attributions of standard and robust models.

3. Finally, we theoretically prove that our empirical findings hold on a simplified version of the BlockMNIST dataset. Specifically, we prove that input gradients of standard one-hidden-layer MLPs trained on this dataset do not highlight instance-specific signal coordinates, thus grossly violating assumption (A).

Our findings motivate the need to formalize and test common assumptions in interpretability in a falsifiable manner [Leavitt and Morcos, 2020]. We believe that the DiffROAR evaluation framework and BlockMNIST-based datasets can serve as sanity checks to audit instance-specific interpretability methods; code and data available at https://github.com/harshays/inputgradients.",https://api.openreview.net/pdf/30ee20ffcfd0b67209e09f4e3429b3742c22527d.pdf,graph;llm,https://scholar.google.com/scholar?q=Do+Input+Gradients+Highlight+Discriminative+Features?+
A Note on Sparse Generalized Eigenvalue Problem,2021,NIPS,"['YUNFENG CAI', 'Guanhua Fang', 'Ping Li']",poster,"['sparse generalized eigenvalue problem', 'perturbation', 'penalization', 'ADMM']","The sparse generalized eigenvalue problem (SGEP) aims to find the leading eigenvector with sparsity structure. SGEP plays an important role in statistical learning and has wide applications including, but not limited to, sparse principal component analysis, sparse canonical correlation analysis  and sparse Fisher discriminant analysis, etc. Due to the sparsity constraint, the solution of SGEP entails interesting properties from both numerical and statistical perspectives. In this paper, we provide a detailed sensitivity analysis for SGEP and establish the rate-optimal perturbation bound under the sparse setting. Specifically, we show that the bound is related to the perturbation/noise level and the recovery of the true support of the leading eigenvector as well. We also investigate the estimator of SGEP via imposing a non-convex regularization. Such estimator can achieve the optimal error rate and can recover the sparsity structure as well. Extensive numerical experiments corroborate our theoretical findings via using alternating direction method of multipliers (ADMM)-based computational method.",https://api.openreview.net/pdf/ccdb67bd26776ee2b39ad59d7004df9de4b146eb.pdf,optimization;sparse;llm,https://scholar.google.com/scholar?q=A+Note+on+Sparse+Generalized+Eigenvalue+Problem
Emergent Communication under Varying Sizes and Connectivities,2021,NIPS,"['Jooyeon Kim', 'Alice Oh']",poster,"['Emergent communication', 'emergence of language', 'group communication', 'communication graph', 'graph optimization']","Recent advances in deep neural networks allowed artificial agents to derive their own emergent languages that promote interaction, coordination, and collaboration within a group. Just as we humans have succeeded in creating a shared language that allows us to interact within a large group, can the emergent communication within an artificial group converge to a shared, agreed language? This research provides an analytical study of the shared emergent language within the group communication settings of different sizes and connectivities. As the group size increases up to hundreds, agents start to speak dissimilar languages, but the rate at which they successfully communicate is maintained. We observe the emergence of different dialects when we restrict the group communication to have local connectivities only. Finally, we provide optimization results of group communication graphs when the number of agents one can communicate with is restricted or when we penalize communication between distant agent pairs. The optimized communication graphs show superior communication success rates compared to graphs with same number of links as well as the emergence of hub nodes and scale-free networks. ",https://api.openreview.net/pdf/daceb64281c70307d073d82a5e69bd59a91608a5.pdf,reinforcement learning;graph;optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Emergent+Communication+under+Varying+Sizes+and+Connectivities
Searching the Search Space of Vision Transformer,2021,NIPS,"['Minghao Chen', 'Kan Wu', 'Bolin Ni', 'Houwen Peng', 'Bei Liu', 'Jianlong Fu', 'Hongyang Chao', 'Haibin Ling']",poster,"['Vision Transformer', 'Neural Architecture Search', 'Design Space']","Vision Transformer has shown great visual representation power in substantial vision tasks such as recognition and detection, and thus been attracting fast-growing efforts on manually designing more effective architectures. In this paper, we propose to use neural architecture search to automate this process, by searching not only the architecture but also the search space. The central idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. Moreover, we provide design guidelines of general vision transformers with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. Remarkably, the searched models, named S3 (short for Searching the Search Space), from the searched space achieve superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering, demonstrating its generality to downstream vision and vision-language tasks. Code and models will be available at https://github.com/microsoft/Cream.",https://api.openreview.net/pdf/f487d25d82223dafdd4f6cf9e8cbbbeb53883b69.pdf,transformer;representation;segmentation;llm,https://scholar.google.com/scholar?q=Searching+the+Search+Space+of+Vision+Transformer
HNPE: Leveraging Global Parameters for Neural Posterior Estimation,2021,NIPS,"['Pedro L. C. Rodrigues', 'Thomas Moreau', 'Gilles Louppe', 'Alexandre Gramfort']",poster,"['Simulation-based Inference', 'Bayesian Inference', 'Hierarchical Models', 'Normalizing Flows', 'Computational Neuroscience', 'Approximate Inference']","Inferring the parameters of a stochastic model based on experimental observations is central to the scientific method. A particularly challenging setting is when the model is strongly indeterminate, i.e. when distinct sets of parameters yield identical observations. This arises in many practical situations, such as when inferring the distance and power of a radio source (is the source close and weak or far and strong?) or when estimating the amplifier gain and underlying brain activity of an electrophysiological experiment. In this work, we present hierarchical neural posterior estimation (HNPE), a novel method for cracking such indeterminacy by exploiting additional information conveyed by an auxiliary set of observations sharing global parameters. Our method extends recent developments in simulation-based inference (SBI) based on normalizing flows to Bayesian hierarchical models. We validate quantitatively our proposal on a motivating example amenable to analytical solutions and then apply it to invert a well known non-linear model from computational neuroscience, using both simulated and real EEG data.",https://api.openreview.net/pdf/e9b0e32125b06b38a1dc480387f3699cae995eb9.pdf,graph;zero_few-shot;bayesian;inference;flow;llm,https://scholar.google.com/scholar?q=HNPE:+Leveraging+Global+Parameters+for+Neural+Posterior+Estimation
Detecting Individual Decision-Making Style: Exploring Behavioral Stylometry in Chess,2021,NIPS,"['Reid McIlroy-Young', 'Russell Wang', 'Siddhartha Sen', 'Jon Kleinberg', 'Ashton Anderson']",poster,"['behavioral stylometry', 'player identification', 'chess', 'few-shot classification', 'transformer']","The advent of machine learning models that surpass human decision-making ability in complex domains has initiated a movement towards building AI systems that interact with humans. Many building blocks are essential for this activity, with a central one being the algorithmic characterization of human behavior. While much of the existing work focuses on aggregate human behavior, an important long-range goal is to develop behavioral models that specialize to individual people and can differentiate among them.

To formalize this process, we study the problem of behavioral stylometry, in which the task is to identify a decision-maker from their decisions alone. We present a transformer-based approach to behavioral stylometry in the context of chess, where one attempts to identify the player who played a set of games. Our method operates in a few-shot classification framework, and can correctly identify a player from among thousands of candidate players with 98% accuracy given only 100 labeled games. Even when trained on amateur play, our method generalises to out-of-distribution samples of Grandmaster players, despite the dramatic differences between amateur and world-class players. Finally, we consider more broadly what our resulting embeddings reveal about human style in chess, as well as the potential ethical implications of powerful methods for identifying individuals from behavioral data. ",https://api.openreview.net/pdf/4d2d5f892d50cc87e802edca3a228485dbb00875.pdf,transformer;llm,https://scholar.google.com/scholar?q=Detecting+Individual+Decision-Making+Style:+Exploring+Behavioral+Stylometry+in+Chess
Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity,2021,NIPS,"['Nicolas Loizou', 'Hugo berard', 'Gauthier Gidel', 'Ioannis Mitliagkas', 'Simon Lacoste-Julien']",poster,"['Smooth Games', 'Min-max optimization', 'Unconstrained Stochastic Variational Inequality', 'Stochastic algorithms', 'Convergence analysis', 'Stochastic Gradient Descent-Ascent (SGDA)', 'Stochastic Consensus Optimization']","Two of the most prominent algorithms for solving unconstrained smooth games are the classical stochastic gradient descent-ascent (SGDA) and the recently introduced stochastic consensus optimization (SCO) [Mescheder et al., 2017]. SGDA is known to converge to a stationary point for specific classes of games, but current convergence analyses require a bounded variance assumption. SCO is used successfully for solving large-scale adversarial problems, but its convergence guarantees are limited to its deterministic variant. In this work, we introduce the expected co-coercivity condition, explain its benefits, and provide the first last-iterate convergence guarantees of SGDA and SCO under this condition for solving a class of stochastic variational inequality problems that are potentially non-monotone. We prove linear convergence of both methods to a neighborhood of the solution when they use constant step-size, and we propose insightful stepsize-switching rules to guarantee convergence to the exact solution. In addition, our convergence guarantees hold under the arbitrary sampling paradigm, and as such, we give insights into the complexity of minibatching. 
",https://api.openreview.net/pdf/05b4a70e4632218c09166ef08655cc2724fb8f05.pdf,optimization;llm,https://scholar.google.com/scholar?q=Stochastic+Gradient+Descent-Ascent+and+Consensus+Optimization+for+Smooth+Games:+Convergence+Analysis+under+Expected+Co-coercivity
Adversarial Neuron Pruning Purifies Backdoored Deep Models,2021,NIPS,"['Dongxian Wu', 'Yisen Wang']",poster,"['backdoor attacks', 'backdoor defense', 'AI security', 'trustworthy machine learning']","As deep neural networks (DNNs) are growing larger, their requirements for computational resources become huge, which makes outsourcing training more popular. Training in a third-party platform, however, may introduce potential risks that a malicious trainer will return backdoored DNNs, which behave normally on clean samples but output targeted misclassifications whenever a trigger appears at the test time. Without any knowledge of the trigger, it is difficult to distinguish or recover benign DNNs from backdoored ones. In this paper, we first identify an unexpected sensitivity of backdoored DNNs, that is, they are much easier to collapse and tend to predict the target label on clean samples when their neurons are adversarially perturbed. Based on these observations, we propose a novel model repairing method, termed Adversarial Neuron Pruning (ANP), which prunes some sensitive neurons to purify the injected backdoor. Experiments show, even with only an extremely small amount of clean data (e.g., 1%), ANP effectively removes the injected backdoor without causing obvious performance degradation.",https://api.openreview.net/pdf/d24a110cbbd5437697488a5e5906b77e31f725b9.pdf,llm,https://scholar.google.com/scholar?q=Adversarial+Neuron+Pruning+Purifies+Backdoored+Deep+Models
Test-time Collective Prediction,2021,NIPS,"['Celestine Mendler-Dünner', 'Wenshuo Guo', 'Stephen Bates', 'Michael Jordan']",poster,"['degroot', 'consenesus finding', 'combining expert opinion', 'opinion pool', 'adaptive model weighting', 'data heterogeneity', 'social sciences', 'federated learning', 'ensemble']","An increasingly common setting in machine learning involves multiple parties, each with their own data, who want to jointly make predictions on future test points. Agents wish to benefit from the collective expertise of the full set of agents to make better predictions than they would individually, but may not be willing to release labeled data or model parameters. In this work, we explore a decentralized mechanism to make collective predictions at test time, that is inspired by the literature in social science on human consensus-making. Building on a query model to facilitate information exchange among agents, our approach leverages each agent’s pre-trained model without relying on external validation, model retraining, or data pooling. A theoretical analysis shows that our approach recovers inverse mean-squared-error (MSE) weighting in the large-sample limit which is known to be the optimal way to combine independent, unbiased estimators. Empirically, we demonstrate that our scheme effectively combines models with differing quality across the input space: the proposed consensus prediction achieves significant gains over classical model averaging, and even outperforms weighted averaging schemes that have access to additional validation data. Finally, we propose a decentralized Jackknife procedure as a tool to evaluate the sensitivity of the collective predictions with respect to a single agent's opinion.",https://api.openreview.net/pdf/76df8c8376bc642e761e292b252279526e748bc2.pdf,reinforcement learning;graph;llm,https://scholar.google.com/scholar?q=Test-time+Collective+Prediction
BAST: Bayesian Additive Regression Spanning Trees for Complex Constrained Domain,2021,NIPS,"['Zhao Tang Luo', 'Huiyan Sang', 'Bani Mallick']",poster,"['Bayesian nonparametric regression', 'Constrained domain', 'Ensemble learning', 'Manifold', 'Random spanning trees']","Nonparametric regression on complex domains has been a challenging task as most existing methods, such as ensemble models based on binary decision trees, are not designed to account for intrinsic geometries and domain boundaries. This article proposes a Bayesian additive regression spanning trees (BAST) model for nonparametric regression on manifolds, with an emphasis on complex constrained domains or irregularly shaped spaces embedded in Euclidean spaces. Our model is built upon a random spanning tree manifold partition model as each weak learner, which is capable of capturing any irregularly shaped spatially contiguous partitions while respecting intrinsic geometries and domain boundary constraints. Utilizing many nice properties of spanning tree structures, we design an efficient Bayesian inference algorithm. Equipped with a soft prediction scheme, BAST is demonstrated to significantly outperform other competing methods in simulation experiments and in an application to the chlorophyll data in Aral Sea, due to its strong local adaptivity to different levels of smoothness. ",https://api.openreview.net/pdf/24ffffa07231b6910022def384131c20ed0a58cd.pdf,graph;optimization;zero_few-shot;bayesian;inference;metric;decision trees;llm,https://scholar.google.com/scholar?q=BAST:+Bayesian+Additive+Regression+Spanning+Trees+for+Complex+Constrained+Domain
Formalizing Generalization and Adversarial Robustness of Neural Networks to Weight Perturbations,2021,NIPS,"['Yu-Lin Tsai', 'Chia-Yi Hsu', 'Chia-Mu Yu', 'Pin-Yu Chen']",poster,"['Generalization under weight perturbation', 'Robustness', 'Neural Network', 'Weight Perturbation', 'Rademacher complexity']","Studying the sensitivity of weight perturbation in neural networks and its impacts on model performance, including generalization and robustness, is an active research topic due to its implications on a wide range of machine learning tasks such as model compression, generalization gap assessment, and adversarial attacks. In this paper, we provide the first integral study and analysis for feed-forward neural networks in terms of the robustness in pairwise class margin and its generalization behavior under weight perturbation. We further design a new theory-driven loss function for training generalizable and robust neural networks against weight perturbations. Empirical experiments are conducted to validate our theoretical analysis. Our results offer fundamental insights for characterizing the generalization and robustness of neural networks against weight perturbations.",https://api.openreview.net/pdf/7b43aa14ba2c7d70c413c38e2c65446664d6cdff.pdf,graph;active learning;llm,https://scholar.google.com/scholar?q=Formalizing+Generalization+and+Adversarial+Robustness+of+Neural+Networks+to+Weight+Perturbations
SurvITE: Learning Heterogeneous Treatment Effects from Time-to-Event Data,2021,NIPS,"['Alicia Curth', 'Changhee Lee', 'Mihaela van der Schaar']",poster,"['Heterogeneous Treatment Effects', 'Survival Analysis', 'Counterfactual Estimation', 'Causal inference']","We study the problem of inferring heterogeneous treatment effects from time-to-event data. While both the related problems of (i) estimating treatment effects for binary or continuous outcomes and (ii) predicting survival outcomes have been well studied in the recent machine learning literature, their combination -- albeit of high practical relevance -- has received considerably less attention. With the ultimate goal of reliably estimating the effects of treatments on instantaneous risk and survival probabilities, we focus on the problem of learning (discrete-time) treatment-specific conditional hazard functions. We find that unique challenges arise in this context due to a variety of covariate shift issues that go beyond a mere combination of well-studied confounding and censoring biases. We theoretically analyse their effects by adapting recent generalization bounds from domain adaptation and treatment effect estimation to our setting and discuss implications for model design. We use the resulting insights to propose a novel deep learning method for treatment-specific hazard estimation based on balancing representations. We investigate performance across a range of experimental settings and empirically confirm that our method outperforms baselines by addressing covariate shifts from various sources.",https://api.openreview.net/pdf/5b2ef03d4dd49845586b98f98e7e1fa5e53d77a4.pdf,transformer;representation;llm,https://scholar.google.com/scholar?q=SurvITE:+Learning+Heterogeneous+Treatment+Effects+from+Time-to-Event+Data
Can fMRI reveal the representation of syntactic structure in the brain?,2021,NIPS,"['Aniketh Janardhan Reddy', 'Leila Wehbe']",poster,"['neuroscience', 'fMRI', 'syntactic representations', 'graph embeddings']","While studying semantics in the brain, neuroscientists use two approaches. One is to identify areas that are correlated with semantic processing load. Another is to find areas that are predicted by the semantic representation of the stimulus words. However, most studies of syntax have focused only on identifying areas correlated with syntactic processing load.  One possible reason for this discrepancy is that representing syntactic structure in an embedding space such that it can be used to model brain activity is a non-trivial computational problem. Another possible reason is that it is unclear if the low signal-to-noise ratio of neuroimaging tools such as functional Magnetic Resonance Imaging (fMRI) can allow us to reveal the correlates of complex (and perhaps subtle) syntactic representations. In this study, we propose novel multi-dimensional features that encode information about the syntactic structure of sentences. Using these features and fMRI recordings of participants reading a natural text, we model the brain representation of syntax. First, we find that our syntactic structure-based features explain additional variance in the brain activity of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, we see that regions well-predicted by syntactic features are distributed in the language system and are not distinguishable from those processing semantics. Our code and data will be available at https://github.com/anikethjr/brain_syntactic_representations.",https://api.openreview.net/pdf/192a0df1cab7cd93eb7115547e1d3dc2a8c13cd2.pdf,graph;zero_few-shot;representation;metric;llm,https://scholar.google.com/scholar?q=Can+fMRI+reveal+the+representation+of+syntactic+structure+in+the+brain?
Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis,2021,NIPS,"['Thomas FEL', 'Remi Cadene', 'Mathieu Chalvidal', 'Matthieu Cord', 'David Vigouroux', 'Thomas Serre']",poster,"['deep learning', 'machine learning', 'explainability', 'interpretability', 'computer vision', 'nlp', 'black box', 'sobol']","We describe a novel attribution method which is grounded in Sensitivity Analysis and uses  Sobol indices. Beyond modeling the individual contributions of image regions,  Sobol indices provide an efficient way to capture higher-order interactions between image regions and their contributions to a neural network's prediction through the lens of variance.
We describe an approach that makes the computation of these indices efficient for high-dimensional problems by using perturbation masks coupled with efficient estimators to handle the high dimensionality of images.
Importantly, we show that the proposed method leads to favorable scores on standard benchmarks for vision (and language models) while drastically reducing the computing time compared to other black-box methods -- even surpassing the accuracy of state-of-the-art white-box methods which require access to internal representations. Our code is freely available:
github.com/fel-thomas/Sobol-Attribution-Method.",https://api.openreview.net/pdf/eb5f330c4e98ecc29dbc87867e1c41a8389897de.pdf,representation;llm,https://scholar.google.com/scholar?q=Look+at+the+Variance!+Efficient+Black-box+Explanations+with+Sobol-based+Sensitivity+Analysis
Local plasticity rules can learn deep representations using self-supervised contrastive predictions,2021,NIPS,"['Bernd Illing', 'Jean Robin Ventura', 'Guillaume Bellec', 'Wulfram Gerstner']",poster,"['Synaptic plasticity', 'Hebbian learning', 'deep learning', 'self-supervised learning', 'contrastive predictive coding']","Learning in the brain is poorly understood and learning rules that respect biological constraints, yet yield deep hierarchical representations, are still unknown. Here, we propose a learning rule that takes inspiration from neuroscience and recent advances in self-supervised deep learning. Learning minimizes a simple layer-specific loss function and does not need to back-propagate error signals within or between layers. Instead, weight updates follow a local, Hebbian, learning rule that only depends on pre- and post-synaptic neuronal activity, predictive dendritic input and widely broadcasted modulation factors which are identical for large groups of neurons. The learning rule applies contrastive predictive learning to a causal, biological setting using saccades (i.e. rapid shifts in gaze direction). We find that networks trained with this self-supervised and local rule build deep hierarchical representations of images, speech and video. ",https://api.openreview.net/pdf/5c025661da77b716deaf8aae79f78a2d5355d8e7.pdf,optimization;zero_few-shot;representation;contrastive learning;llm,https://scholar.google.com/scholar?q=Local+plasticity+rules+can+learn+deep+representations+using+self-supervised+contrastive+predictions
Algorithmic stability and generalization of an unsupervised feature selection algorithm,2021,NIPS,"['Xinxing Wu', 'Qiang Cheng']",poster,"['Feature selection', 'Algorithmic stability', 'generalization', 'neural network']","Feature selection, as a vital dimension reduction technique, reduces data dimension by identifying an essential subset of input features, which can facilitate interpretable insights into learning and inference processes. Algorithmic stability is a key characteristic of an algorithm regarding its sensitivity to perturbations of input samples. In this paper, we propose an innovative unsupervised feature selection algorithm attaining this stability with provable guarantees. The architecture of our algorithm consists of a feature scorer and a feature selector. The scorer trains a neural network (NN) to globally score all the features, and the selector adopts a dependent sub-NN to locally evaluate the representation abilities for selecting features. Further, we present algorithmic stability analysis and show that our algorithm has a performance guarantee via a generalization error bound. Extensive experimental results on real-world datasets demonstrate superior generalization performance of our proposed algorithm to strong baseline methods. Also, the properties revealed by our theoretical analysis and the stability of our algorithm-selected features are empirically confirmed.",https://api.openreview.net/pdf/1f26ae43ab31ecbcf242c153e6fec02fa4125532.pdf,representation;inference;llm,https://scholar.google.com/scholar?q=Algorithmic+stability+and+generalization+of+an+unsupervised+feature+selection+algorithm
Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention,2021,NIPS,"['Byung-Hoon Kim', 'Jong Chul Ye', 'Jae-Jin Kim']",poster,"['neuroimaging', 'connectome', 'fmri', 'graph neural network', 'attention', 'explainability']","Functional connectivity (FC) between regions of the brain can be assessed by the degree of temporal correlation measured with functional neuroimaging modalities. Based on the fact that these connectivities build a network, graph-based approaches for analyzing the brain connectome have provided insights into the functions of the human brain. The development of graph neural networks (GNNs) capable of learning representation from graph structured data has led to increased interest in learning the graph representation of the brain connectome. Although recent attempts to apply GNN to the FC network have shown promising results, there is still a common limitation that they usually do not incorporate the dynamic characteristics of the FC network which fluctuates over time. In addition, a few studies that have attempted to use dynamic FC as an input for the GNN reported a reduction in performance compared to static FC methods, and did not provide temporal explainability. Here, we propose STAGIN, a method for learning dynamic graph representation of the brain connectome with spatio-temporal attention. Specifically, a temporal sequence of brain graphs is input to the STAGIN to obtain the dynamic graph representation, while novel READOUT functions and the Transformer encoder provide spatial and temporal explainability with attention, respectively. Experiments on the HCP-Rest and the HCP-Task datasets demonstrate exceptional performance of our proposed method. Analysis of the spatio-temporal attention also provide concurrent interpretation with the neuroscientific knowledge, which further validates our method. Code is available at https://github.com/egyptdj/stagin",https://api.openreview.net/pdf/3b516cba07cac0bb0c489fe48364bffc309dfc40.pdf,graph;transformer;representation;llm,https://scholar.google.com/scholar?q=Learning+Dynamic+Graph+Representation+of+Brain+Connectome+with+Spatio-Temporal+Attention
Distributed Zero-Order Optimization under Adversarial Noise,2021,NIPS,"['Arya Akhavan', 'massimiliano pontil', 'Alexandre B Tsybakov']",poster,"['Zero-order optimization', 'statistical learning theory', 'online learning']","We study the problem of distributed zero-order optimization for a class of strongly convex functions. They are formed by the average of local objectives, associated to different nodes in a prescribed network. We propose a distributed zero-order projected gradient descent algorithm to solve the problem. Exchange of information within the network is permitted only between neighbouring nodes. An important feature of our procedure is that it can query only function values, subject to a general noise model, that does not require zero mean or independent errors.  We derive upper bounds for the average cumulative regret and optimization error of the algorithm  which highlight the role played by a network connectivity parameter, the number of variables, the noise level, the strong convexity parameter, and smoothness properties of the local objectives. The bounds indicate some key improvements of our method over the state-of-the-art, both in the distributed and standard zero-order optimization settings.",https://api.openreview.net/pdf/f55c5497e60ece0bb5ab98db5d96e8dee5e91d3e.pdf,optimization;llm,https://scholar.google.com/scholar?q=Distributed+Zero-Order+Optimization+under+Adversarial+Noise
Fitting summary statistics of neural data with a differentiable spiking network simulator,2021,NIPS,"['Guillaume Bellec', 'Shuqi Wang', 'Alireza Modirshanechi', 'Johanni Brea', 'Wulfram Gerstner']",poster,"['fitting', 'spikes', 'MLE', 'likelihood', 'GLM', 'spiking', 'recurrent', 'RNN', 'RSNN', 'neural data', 'surrogate gradients', 'straight through', 'Bayesian', 'prior', 'statistics', 'binary networks']","Fitting network models to neural activity is an important tool in neuroscience. A popular approach is to model a brain area with a probabilistic recurrent spiking network whose parameters maximize the likelihood of the recorded activity. Although this is widely used, we show that the resulting model does not produce realistic neural activity. To correct for this, we suggest to augment the log-likelihood with terms that measure the dissimilarity between simulated and recorded activity. This dissimilarity is defined via summary statistics commonly used in neuroscience and the optimization is efficient because it relies on back-propagation through the stochastically simulated spike trains. We analyze this method theoretically and show empirically that it generates more realistic activity statistics. We find that it improves upon other fitting algorithms for spiking network models like GLMs (Generalized Linear Models) which do not usually rely on back-propagation. This new fitting algorithm also enables the consideration of hidden neurons which is otherwise notoriously hard, and we show that it can be crucial when trying to infer the network connectivity from spike recordings.",https://api.openreview.net/pdf/fad72ce7f61ee3c9eaa12c22ad28f3986fe3df06.pdf,optimization;llm,https://scholar.google.com/scholar?q=Fitting+summary+statistics+of+neural+data+with+a+differentiable+spiking+network+simulator
History Aware Multimodal Transformer for Vision-and-Language Navigation,2021,NIPS,"['Shizhe Chen', 'Pierre-Louis Guhur', 'Cordelia Schmid', 'Ivan Laptev']",poster,"['vision-and-language navigation', 'transformer']","Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories. ",https://api.openreview.net/pdf/7a98b07d97723ec77f2aec51cbb7b20521aeab98.pdf,reinforcement learning;zero_few-shot;transformer;multimodal;llm,https://scholar.google.com/scholar?q=History+Aware+Multimodal+Transformer+for+Vision-and-Language+Navigation
Encoding Spatial Distribution of Convolutional Features for Texture Representation,2021,NIPS,"['Yong Xu', 'Feng Li', 'Zhile Chen', 'Jinxiu Liang', 'Yuhui Quan']",poster,"['Deep Learning', 'Feature Encoding', 'Global Pooling', 'Texture Representation']","Existing convolutional neural networks (CNNs) often use global average pooling (GAP) to aggregate feature maps into a single representation. However, GAP cannot well characterize complex distributive patterns of spatial features while such patterns play an important role in texture-oriented applications, e.g., material recognition and ground terrain classification. In the context of texture representation, this paper addressed the issue by proposing Fractal Encoding (FE), a feature encoding module grounded by multi-fractal geometry. Considering a CNN feature map as a union of level sets of points lying in the 2D space, FE characterizes their spatial layout via a local-global hierarchical fractal analysis which examines the multi-scale power behavior on each level set. This enables a CNN to encode the regularity on the spatial arrangement of image features, leading to a robust yet discriminative spectrum descriptor. In addition, FE has trainable parameters for data adaptivity and can be easily incorporated into existing CNNs for end-to-end training. We applied FE to ResNet-based texture classification and retrieval, and demonstrated its effectiveness on several benchmark datasets.",https://api.openreview.net/pdf/202c085d9d94376dabe34774a1900fde09f0891a.pdf,representation;llm,https://scholar.google.com/scholar?q=Encoding+Spatial+Distribution+of+Convolutional+Features+for+Texture+Representation
An Analysis of Constant Step Size SGD in the Non-convex Regime: Asymptotic Normality and Bias,2021,NIPS,"['Lu Yu', 'Krishna Balasubramanian', 'Stanislav Volgushev', 'Murat A Erdogdu']",poster,"['Constant Step Size SGD', 'Non-convex optimization', 'Polyak-Ruppert averaging', 'CLT']"," Structured non-convex learning problems, for which critical points have favorable statistical properties, arise frequently in statistical machine learning. Algorithmic convergence and statistical estimation rates are well-understood for such problems. However, quantifying the uncertainty associated with the underlying training algorithm is not well-studied in the non-convex setting. In order to address this shortcoming, in this work, we establish an asymptotic normality result for the constant step size stochastic gradient descent (SGD)  algorithm---a widely used algorithm in practice. Specifically, based on the relationship between SGD and Markov Chains  [DDB19], we show that the average of SGD iterates is asymptotically normally distributed around the expected value of their unique invariant distribution, as long as the non-convex and non-smooth objective function satisfies a dissipativity property. We also characterize the bias between this expected value and the critical points of the objective function under various local regularity conditions. Together, the above two results could be leveraged to construct confidence intervals for non-convex problems that are trained using the SGD algorithm.",https://api.openreview.net/pdf/ba96ed2cb9ba54ae274bb1c0f3ef7be07c8a2a8f.pdf,llm,https://scholar.google.com/scholar?q=An+Analysis+of+Constant+Step+Size+SGD+in+the+Non-convex+Regime:+Asymptotic+Normality+and+Bias
Remember What You Want to Forget: Algorithms for Machine Unlearning,2021,NIPS,"['Ayush Sekhari', 'Jayadev Acharya', 'Gautam Kamath', 'Ananda Theertha Suresh']",poster,"['Machine unlearning', 'theory', 'differential privacy', 'generalization guarantee', 'right to be forgotten']","We study the problem of unlearning datapoints from a learnt model. The learner first receives a dataset $S$ drawn i.i.d. from an unknown distribution, and outputs a model $\widehat{w}$ that performs well on  unseen samples from the same distribution. However, at some point in the future, any training datapoint $z \in S$ can request to be unlearned, thus prompting the learner to modify its output model while still ensuring the same accuracy guarantees.  We initiate a rigorous study of generalization in machine unlearning, where the goal is to perform well on previously unseen datapoints. Our focus is on both computational and storage complexity. 

For the setting of convex losses, we provide an unlearning algorithm that can unlearn up to $O(n/d^{1/4})$ samples, where $d$ is the problem dimension. In comparison, in general, differentially private learning (which implies unlearning) only guarantees deletion of $O(n/d^{1/2})$ samples. This demonstrates a novel separation between differential privacy and machine unlearning. ",https://api.openreview.net/pdf/9ffa59f7e3575ede5ea770a7fcc42de14ca52b44.pdf,llm,https://scholar.google.com/scholar?q=Remember+What+You+Want+to+Forget:+Algorithms+for+Machine+Unlearning
Information-constrained optimization: can adaptive processing of gradients help?,2021,NIPS,"['Jayadev Acharya', 'Clement Louis Canonne', 'Prathamesh Mayekar', 'Himanshu Tyagi']",poster,"['optimization', 'convex optimization', 'adaptivity', 'information constraints', 'local privacy', 'communication constraints', 'lower bounds', 'random coordinate descent']","We revisit first-order optimization under local information constraints such as local privacy, gradient quantization, and computational constraints limiting access to a few coordinates of the gradient. In this setting, the optimization algorithm is not allowed to directly access the complete output of the gradient oracle, but only gets limited information about it subject to the local information constraints.   We study the role of adaptivity in processing the gradient output to obtain this limited information from it, and obtain tight or nearly tight bounds for both convex and strongly convex optimization when adaptive gradient processing is allowed.",https://api.openreview.net/pdf/b152a4ccce001c5ea1d60d93da986f5b9a4af105.pdf,optimization;zero_few-shot;adaptive;llm,https://scholar.google.com/scholar?q=Information-constrained+optimization:+can+adaptive+processing+of+gradients+help?
Gradient-Free Adversarial Training Against Image Corruption for Learning-based Steering,2021,NIPS,"['Yu Shen', 'Laura Yu Zheng', 'Manli Shu', 'Weizi Li', 'Tom Goldstein', 'Ming Lin']",poster,"['robustness', 'autonomous driving']","We introduce a simple yet effective framework for improving the robustness of learning algorithms against image corruptions for autonomous driving. These corruptions can occur due to both internal (e.g., sensor noises and hardware abnormalities) and external factors (e.g., lighting, weather, visibility, and other environmental effects). Using sensitivity analysis with FID-based parameterization, we propose a novel algorithm exploiting basis perturbations to improve the overall performance of autonomous steering and other image processing tasks, such as classification and detection, for self-driving cars. Our model not only improves the performance on the original dataset, but also achieves significant performance improvement on datasets with multiple and unseen perturbations, up to 87% and 77%, respectively. A comparison between our approach and other SOTA techniques confirms the effectiveness of our technique in improving the robustness of neural network training for learning-based steering and other image processing tasks.",https://api.openreview.net/pdf/da1c9d50332587296ed9eac26953e2c6d18aa8d5.pdf,graph;llm,https://scholar.google.com/scholar?q=Gradient-Free+Adversarial+Training+Against+Image+Corruption+for+Learning-based+Steering
"TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up",2021,NIPS,"['Yifan Jiang', 'Shiyu Chang', 'Zhangyang Wang']",poster,"['Transformer', 'GAN']","The recent explosive interest on transformers has suggested their potential to become powerful ``universal"" models for computer vision tasks, such as classification, detection, and segmentation. While those attempts mainly study the discriminative models, we explore transformers on some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs).  Our goal is to conduct the first pilot study in building a GAN \textit{completely free of convolutions}, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed \textbf{TransGAN}, consists of a memory-friendly transformer-based generator that progressively increases feature resolution, and correspondingly a multi-scale discriminator to capture simultaneously semantic contexts and low-level textures. On top of them, we introduce the new module of grid self-attention for alleviating the memory bottleneck further, in order to scale up TransGAN to high-resolution generation. We also develop a unique training recipe including a series of techniques that can mitigate the training instability issues of TransGAN, such as data augmentation, modified normalization, and relative position encoding. Our best architecture achieves highly competitive performance compared to current state-of-the-art GANs using convolutional backbones. Specifically, TransGAN sets \textbf{new state-of-the-art} inception score of 10.43 and FID of 18.28 on STL-10. It also reaches the inception score of 9.02 and FID of 9.26  on CIFAR-10, and 5.28 FID on CelebA $\mathbf{128} \times \mathbf{128}$, respectively: both on par with the current best results and outperforming StyleGAN-V2. When it comes to higher-resolution (e.g. $\mathbf{256} \times \mathbf{256}$) generation tasks, such as on CelebA-HQ and LSUN-Church, TransGAN continues to produce diverse visual examples with high fidelity and impressive texture details. In addition, we dive deep into the transformer-based generation models to understand how their behaviors differ from convolutional ones, by visualizing training dynamics. The code is available at: https://github.com/VITA-Group/TransGAN.",https://api.openreview.net/pdf/127840dc12357b670f1e491ac867b01efd1914ea.pdf,zero_few-shot;transformer;generative model;augmentation;segmentation;llm,"https://scholar.google.com/scholar?q=TransGAN:+Two+Pure+Transformers+Can+Make+One+Strong+GAN,+and+That+Can+Scale+Up"
Shift Invariance Can Reduce Adversarial Robustness,2021,NIPS,"['Vasu Singla', 'Songwei Ge', 'Ronen Basri', 'David Jacobs']",poster,"['adversarial robustness', 'adversarial examples', 'adversarial machine learning', 'shift invariance']","Shift invariance is a critical property of CNNs that improves performance on classification.  However, we show that invariance to circular shifts can also lead to greater sensitivity to adversarial attacks.  We first characterize the margin between classes when a shift-invariant {\em linear} classifier is used. We show that the margin can only depend on the DC component of the signals.  Then, using results about infinitely wide networks, we show that in some simple cases, fully connected and shift-invariant neural networks produce linear decision boundaries.  Using this, we prove that shift invariance in neural networks produces adversarial examples for the simple case of two classes, each consisting of a single image with a black or white dot on a gray background.  This is more than a curiosity; we show empirically that with real datasets and realistic architectures, shift invariance reduces adversarial robustness.  Finally, we describe initial experiments using synthetic data to probe the source of this connection.",https://api.openreview.net/pdf/a70ca1f0559d88d6d13defbe4f49d56c0c832167.pdf,graph;llm,https://scholar.google.com/scholar?q=Shift+Invariance+Can+Reduce+Adversarial+Robustness
"Think Big, Teach Small: Do Language Models Distil Occam’s Razor?",2021,NIPS,"['Gonzalo Jaimovitch-Lopez', 'David Castellano Falcón', 'Cesar Ferri', 'Jose Hernandez-Orallo']",poster,"['Humans and AI', 'Cognitive Systems', 'Explainability', 'Language Models', 'Inductive Programming', ""Occam's razor"", 'Machine Teaching']","Large language models have recently shown a remarkable ability for few-shot learning, including patterns of algorithmic nature. However, it is still an open question to determine what kind of patterns these models can capture and how many examples they need in their prompts. We frame this question as a teaching problem with strong priors, and study whether language models can identify simple algorithmic concepts from small witness sets. In particular, we explore how several GPT architectures, program induction systems and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs. This first joint analysis of language models and machine teaching can address key questions for artificial intelligence and machine learning, such as whether some strong priors, and Occam’s razor in particular, can be distilled from data, making learning from a few examples possible.",https://api.openreview.net/pdf/555b549c0543b9df9125693f03dca14c9f00d1f0.pdf,zero_few-shot;distillation;llm,"https://scholar.google.com/scholar?q=Think+Big,+Teach+Small:+Do+Language+Models+Distil+Occam’s+Razor?"
Online Learning Of Neural Computations From Sparse Temporal Feedback,2021,NIPS,"['Lukas Braun', 'Tim P. Vogels']",poster,"['spiking neural networks', 'event-dependent scaling', 'intrinsic parameters', 'online learning', 'teacher-student paradigm', 'surrogate gradient']","Neuronal computations depend on synaptic connectivity and intrinsic electrophysiological properties. Synaptic connectivity determines which inputs from presynaptic neurons are integrated, while cellular properties determine how inputs are filtered over time. Unlike their biological counterparts, most computational approaches to learning in simulated neural networks are limited to changes in synaptic connectivity. However, if intrinsic parameters change, neural computations are altered drastically. Here, we include the parameters that determine the intrinsic properties, e.g., time constants and reset potential, into the learning paradigm. Using sparse feedback signals that indicate target spike times, and gradient-based parameter updates, we show that the intrinsic parameters can be learned along with the synaptic weights to produce specific input-output functions. Specifically, we use  a teacher-student paradigm in which a randomly initialised leaky integrate-and-fire or resonate-and-fire neuron must recover the parameters of a teacher neuron. We show that complex temporal functions can be learned online and without backpropagation through time, relying on event-based updates only. Our results are a step towards online learning of neural computations from ungraded and unsigned sparse feedback signals with a biologically inspired learning mechanism.",https://api.openreview.net/pdf/ddf25c2d3d352b6a38fc6ad3a719e64e8ba5ef94.pdf,online learning;sparse;llm,https://scholar.google.com/scholar?q=Online+Learning+Of+Neural+Computations+From+Sparse+Temporal+Feedback
Scalable Rule-Based Representation Learning for Interpretable Classification,2021,NIPS,"['Zhuo Wang', 'Wei Zhang', 'Ning Liu', 'Jianyong Wang']",poster,"['interpretable classification', 'rule-based model', 'representation learning', 'scalability']","Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on nine small and four large data sets show that RRL outperforms the competitive interpretable approaches and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios. Our code is available at: https://github.com/12wang3/rrl.",https://api.openreview.net/pdf/bbb389f0fb02d898b6c84fbb75ba6154af828831.pdf,representation;decision trees;llm,https://scholar.google.com/scholar?q=Scalable+Rule-Based+Representation+Learning+for+Interpretable+Classification
Stylized Dialogue Generation with Multi-Pass Dual Learning,2021,NIPS,"['Jinpeng Li', 'Yingce Xia', 'Rui Yan', 'Hongda Sun', 'Dongyan Zhao', 'Tie-Yan Liu']",poster,"['Dual learning', 'Stylized dialogue generation', 'TCFC', 'SDGC']","Stylized dialogue generation, which aims to generate a given-style response for an input context, plays a vital role in intelligent dialogue systems. Considering there is no parallel data between the contexts and the responses of target style S_1, existing works mainly use back translation to generate stylized synthetic data for training, where the data about context, target style S_1 and an intermediate style S_0 is used. However, the interaction among these texts is not fully exploited, and the pseudo contexts are not adequately modeled. To overcome the above difficulties, we propose multi-pass dual learning (MPDL), which leverages the duality among the context, response of style S_1 and response of style S_0. MPDL builds mappings among the above three domains, where the context should be reconstructed by the MPDL framework, and the reconstruction error is used as the training signal. To evaluate the quality of synthetic data, we also introduce discriminators that effectively measure how a pseudo sequence matches the specific domain, and the evaluation result is used as the weight for that data. Evaluation results indicate that our method obtains significant improvement over previous baselines.
",https://api.openreview.net/pdf/a9527dc7cdb2aae8b7bd5b78dc093edab05251e3.pdf,generative model;llm,https://scholar.google.com/scholar?q=Stylized+Dialogue+Generation+with+Multi-Pass+Dual+Learning
"Localization, Convexity, and Star Aggregation",2021,NIPS,['Suhas Vijaykumar'],poster,"['Statistical Learning Theory', 'Improper Learning', 'Empirical Risk Minimization', 'Fast Rates']","Offset Rademacher complexities have been shown to provide tight upper bounds for the square loss in a broad class of problems including improper statistical learning and online learning. We show that the offset complexity can be generalized to any loss that satisfies a certain general convexity condition. Further, we show that this condition is closely related to both exponential concavity and self-concordance, unifying apparently disparate results. By a novel geometric argument, many of our bounds translate to improper learning in a non-convex class with Audibert's star algorithm. Thus, the offset complexity provides a versatile analytic tool that covers both convex empirical risk minimization and improper learning under entropy conditions. Applying the method, we recover the optimal rates for proper and improper learning with the $p$-loss for $1 < p < \infty$, and show that improper variants of empirical risk minimization can attain fast rates for logistic regression and other generalized linear models.",https://api.openreview.net/pdf/4d5d66d69bc00ebd5710474814e8bd76f4961057.pdf,transformer;online learning;metric;llm,"https://scholar.google.com/scholar?q=Localization,+Convexity,+and+Star+Aggregation"
Delayed Gradient Averaging: Tolerate the Communication Latency for Federated Learning,2021,NIPS,"['Ligeng Zhu', 'Hongzhou Lin', 'Yao Lu', 'Yujun Lin', 'song han']",poster,"['distributed optimization', 'distributed training', 'federated learning', 'latency']","Federated Learning is an emerging direction in distributed machine learning that en-ables jointly training a model without sharing the data. Since the data is distributed across many edge devices through wireless / long-distance connections, federated learning suffers from inevitable high communication latency. However, the latency issues are undermined in the current literature [15] and existing approaches suchas FedAvg [27] become less efficient when the latency increases.  To over comethe problem, we propose \textbf{D}elayed \textbf{G}radient \textbf{A}veraging (DGA), which delays the averaging step to improve efficiency and allows local computation in parallel tocommunication. We theoretically prove that DGA attains a similar convergence rate as FedAvg, and empirically show that our algorithm can tolerate high network latency without compromising accuracy. Specifically, we benchmark the training speed on various vision (CIFAR, ImageNet) and language tasks (Shakespeare),with both IID and non-IID partitions, and show DGA can bring 2.55$\times$ to 4.07$\times$ speedup. Moreover, we built a 16-node Raspberry Pi cluster and show that DGA can consistently speed up real-world federated learning applications.",https://api.openreview.net/pdf/e3c99dd36b8d594af55a4a6363a2f3ee9d235fec.pdf,graph;zero_few-shot;federated learning;llm,https://scholar.google.com/scholar?q=Delayed+Gradient+Averaging:+Tolerate+the+Communication+Latency+for+Federated+Learning
Discrete-Valued Neural Communication,2021,NIPS,"['Dianbo Liu', 'Alex Lamb', 'Kenji Kawaguchi', 'Anirudh Goyal', 'Chen Sun', 'Michael Curtis Mozer', 'Yoshua Bengio']",poster,"['structured architecture', 'discretization', 'communication', 'specialist components', 'system 2']","Deep learning has advanced from fully connected architectures to structured models organized into components, e.g., the transformer composed of positional elements, modular architectures divided into slots, and graph neural nets made up of nodes. The nature of structured models is that communication among the components has a bottleneck, typically achieved by restricted connectivity and attention. In this work, we further tighten the bottleneck via discreteness of the representations transmitted between components. We hypothesize that this constraint serves as a useful form of inductive bias. Our hypothesis is motivated by past empirical work showing the benefits of discretization in non-structured architectures as well as our own theoretical results showing that discretization increases noise robustness and reduces the underlying dimensionality of the model. Building on an existing technique for discretization from the VQ-VAE, we consider multi-headed discretization with shared codebooks as the output of each architectural component. One motivating intuition is human language in which communication occurs through multiple discrete symbols. This form of communication is hypothesized to facilitate transmission of information between functional components of the brain by providing a common interlingua, just as it does for human-to-human communication. Our experiments show that discrete-valued neural communication (DVNC) substantially improves systematic generalization in a variety of architectures—transformers, modular architectures, and graph neural networks. We also show that the DVNC is robust to the choice of hyperparameters, making the method useful in practice.",https://api.openreview.net/pdf/50c7d42bbb8a72e6d99ee1db8057ad056d26f0b4.pdf,graph;optimization;transformer;representation;vae;llm,https://scholar.google.com/scholar?q=Discrete-Valued+Neural+Communication
All Tokens Matter: Token Labeling for Training Better Vision Transformers,2021,NIPS,"['Zihang Jiang', 'Qibin Hou', 'Li Yuan', 'Zhou Daquan', 'Yujun Shi', 'Xiaojie Jin', 'Anran Wang', 'Jiashi Feng']",poster,"['Image classification', 'neural network', 'vision transformer', 'semantic segmentation']","In this paper, we present token labeling---a new training objective for training high-performance vision transformers (ViTs). Different from the standard training objective of ViTs that computes the classification loss on an additional trainable class token, our proposed one takes advantage of all the image patch tokens to compute the training loss in a dense manner. Specifically, token labeling reformulates the image classification problem into multiple token-level recognition problems and assigns each patch token with an individual location-specific supervision generated by a machine annotator. Experiments show that token labeling can clearly and consistently improve the performance of various ViT models across a wide spectrum. For a vision transformer with 26M learnable parameters serving as an example, with token labeling, the model can achieve 84.4% Top-1 accuracy on ImageNet. The result can be further increased to 86.4% by slightly scaling the model size up to 150M, delivering the minimal-sized model among previous models (250M+) reaching 86%. We also show that token labeling can clearly improve the generalization capability of the pretrained models on downstream tasks with dense prediction, such as semantic segmentation.  Our code and model are publicly
available at https://github.com/zihangJiang/TokenLabeling.",https://api.openreview.net/pdf/c1d51d952267c0d7e7c57d5b325f480e56400458.pdf,transformer;segmentation;llm,https://scholar.google.com/scholar?q=All+Tokens+Matter:+Token+Labeling+for+Training+Better+Vision+Transformers
Scatterbrain: Unifying Sparse and Low-rank Attention,2021,NIPS,"['Beidi Chen', 'Tri Dao', 'Eric Winsor', 'Zhao Song', 'Atri Rudra', 'Christopher Ré']",poster,"['Sparsity', 'Low-rank', 'Locality Sensitive Hashing', 'Kernel Approximation', 'Efficient Transformers']","Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve $2.1 \times$ lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce $98\%$ of attention memory at the cost of only $1\%$ drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to $4$ points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.",https://api.openreview.net/pdf/284046220125b56ea638e88cb8814a53c0bf29e3.pdf,graph;zero_few-shot;transformer;generative model;sparse;low-rank;llm,https://scholar.google.com/scholar?q=Scatterbrain:+Unifying+Sparse+and+Low-rank+Attention
Single Layer Predictive Normalized Maximum Likelihood for Out-of-Distribution Detection,2021,NIPS,"['Koby Bibas', 'Meir Feder', 'Tal Hassner']",poster,"['out-of-distribution', 'normalized maximum likelihood', 'information theory', 'deep neural network', 'regret', 'generalization error']","Detecting out-of-distribution (OOD) samples is vital for developing machine learning based models for critical safety systems. Common approaches for OOD detection assume access to some OOD samples during training which may not be available in a real-life scenario. Instead, we utilize the {\em predictive normalized maximum likelihood} (pNML) learner, in which no assumptions are made on the tested input. We derive an explicit expression of the pNML and its generalization error, denoted as the regret, for a single layer neural network (NN). We show that this learner generalizes well when (i) the test vector resides in a subspace spanned by the eigenvectors associated with the large eigenvalues of the empirical correlation matrix of the training data, or (ii) the test sample is far from the decision boundary. Furthermore, we describe how to efficiently apply the derived pNML regret to any pretrained deep NN, by employing the explicit pNML for the last layer, followed by the softmax function. Applying the derived regret to deep NN requires neither additional tunable parameters nor extra data. We extensively evaluate our approach on 74 OOD detection benchmarks using DenseNet-100, ResNet-34, and WideResNet-40 models trained with CIFAR-100, CIFAR-10, SVHN, and ImageNet-30 showing a significant improvement of up to 15.6% over recent leading methods.",https://api.openreview.net/pdf/2f88e6ed66de0f26e0d22ec6ca98a486547be244.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Single+Layer+Predictive+Normalized+Maximum+Likelihood+for+Out-of-Distribution+Detection
Interpreting Representation Quality of DNNs for 3D Point Cloud Processing,2021,NIPS,"['Wen Shen', 'Qihan Ren', 'Dongrui Liu', 'Quanshi Zhang']",poster,"['3D point cloud processing', 'Explainable AI']","In this paper, we evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. We propose a method to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, we also propose metrics to evaluate the spatial smoothness of encoding 3D structures, and the representation complexity of the DNN. Based on such analysis, experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training. The code will be released when this paper is accepted.",https://api.openreview.net/pdf/3804370079fa1ed6736862c85763e4c5a07c5e3d.pdf,representation;metric;3d;llm,https://scholar.google.com/scholar?q=Interpreting+Representation+Quality+of+DNNs+for+3D+Point+Cloud+Processing
Early Convolutions Help Transformers See Better,2021,NIPS,"['Tete Xiao', 'Mannat Singh', 'Eric Mintun', 'Trevor Darrell', 'Piotr Dollar', 'Ross Girshick']",poster,"['Vision Transformer', 'CNN', 'Model Optimizability']","Vision transformer (ViT) models exhibit substandard optimizability. In particular, they are sensitive to the choice of optimizer (AdamW vs. SGD), optimizer hyperparameters, and training schedule length. In comparison, modern convolutional neural networks are easier to optimize. Why is this the case? In this work, we conjecture that the issue lies with the patchify stem of ViT models, which is implemented by a stride-p p×p convolution (p = 16 by default) applied to the input image. This large-kernel plus large-stride convolution runs counter to typical design choices of convolutional layers in neural networks. To test whether this atypical design choice causes an issue, we analyze the optimization behavior of ViT models with their original patchify stem versus a simple counterpart where we replace the ViT stem by a small number of stacked stride-two 3×3 convolutions. While the vast majority of computation in the two ViT designs is identical, we find that this small change in early visual processing results in markedly different training behavior in terms of the sensitivity to optimization settings as well as the final model accuracy. Using a convolutional stem in ViT dramatically increases optimization stability and also improves peak performance (by ∼1-2% top-1 accuracy on ImageNet-1k), while maintaining flops and runtime. The improvement can be observed across the wide spectrum of model complexities (from 1G to 36G flops) and dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us to recommend using a standard, lightweight convolutional stem for ViT models in this regime as a more robust architectural choice compared to the original ViT model design.",https://api.openreview.net/pdf/9f0a36e698f94bbf444e8f2cbd7052654fa84686.pdf,graph;optimization;transformer;llm,https://scholar.google.com/scholar?q=Early+Convolutions+Help+Transformers+See+Better
Beyond the Signs: Nonparametric Tensor Completion via Sign Series,2021,NIPS,"['Chanwoo Lee', 'Miaoyan Wang']",poster,"['Tensor completion', 'high dimension', 'nonparametric learning', 'classification.']","We consider the problem of tensor estimation from noisy observations with possibly missing entries. A nonparametric approach to tensor completion is developed based on a new model which we coin as sign representable tensors. The model represents the signal tensor of interest using a series of structured sign tensors. Unlike earlier methods, the sign series representation effectively addresses both low- and high-rank signals, while encompassing many existing tensor models---including CP models, Tucker models, single index models, structured tensors with repeating entries---as special cases. We provably reduce the tensor estimation problem to a series of structured classification tasks, and we develop a learning reduction machinery to empower existing low-rank tensor algorithms for more challenging high-rank estimation. Excess risk bounds, estimation errors, and sample complexities are established. We demonstrate the outperformance of our approach over previous methods on two datasets, one on human brain connectivity networks and the other on topic data mining. ",https://api.openreview.net/pdf/54ca989c6e394420ec09428c29e2bfd206a66022.pdf,graph;zero_few-shot;representation;metric;low-rank;llm,https://scholar.google.com/scholar?q=Beyond+the+Signs:+Nonparametric+Tensor+Completion+via+Sign+Series
Best of Both Worlds: Practical and Theoretically Optimal Submodular Maximization in Parallel,2021,NIPS,"['Yixin Chen', 'Tonmoy Dey', 'Alan Kuhnle']",poster,"['submodular maximization', 'parallelizable algorithms', 'adaptive complexity']","For the problem of maximizing a monotone, submodular function with respect to a cardinality constraint $k$ on a ground set of size $n$, we provide an algorithm that achieves the state-of-the-art in both its empirical performance and its theoretical properties, in terms of adaptive complexity, query complexity, and approximation ratio; that is, it obtains, with high probability, query complexity of $O(n)$ in expectation, adaptivity of $O(\log(n))$, and approximation ratio of nearly $1-1/e$. The main algorithm is assembled from two components which may be of independent interest. The first component of our algorithm, LINEARSEQ, is useful as a preprocessing algorithm to improve the query complexity of many algorithms. Moreover, a variant of LINEARSEQ is shown to have adaptive complexity of $O( \log (n / k) )$ which is smaller than that of any previous algorithm in the literature. The second component is a parallelizable thresholding procedure THRESHOLDSEQ for adding elements with gain above a constant threshold. Finally, we demonstrate that our main algorithm empirically outperforms, in terms of runtime, adaptive rounds, total queries, and objective values, the previous state-of-the-art algorithm FAST in a comprehensive evaluation with six submodular objective functions.",https://api.openreview.net/pdf/0d7dd16edea0cb04aa5a95937bcab406ddecf2b6.pdf,optimization;adaptive;llm,https://scholar.google.com/scholar?q=Best+of+Both+Worlds:+Practical+and+Theoretically+Optimal+Submodular+Maximization+in+Parallel
Dynamical Wasserstein Barycenters for Time-series Modeling,2021,NIPS,"['Kevin C Cheng', 'Shuchin Aeron', 'Michael C Hughes', 'Eric Miller']",poster,"['time series', 'transition', 'optimal transport', 'Wasserstein barycenter', 'displacement interpolation', 'simplex random walk', 'state space', 'dynamical model', 'Riemannian optimization', 'human activity recognition']","Many time series can be modeled as a sequence of segments representing high-level discrete states, such as running and walking in a human activity application. Flexible models should describe the system state and observations in stationary ``pure-state'' periods as well as transition periods between adjacent segments, such as a gradual slowdown between running and walking. However, most prior work assumes instantaneous transitions between pure discrete states. We propose a dynamical Wasserstein barycentric (DWB) model that estimates the system state over time as well as the data-generating distributions of pure states in an unsupervised manner. Our model assumes each pure state generates data from a multivariate normal distribution, and characterizes transitions between states via displacement-interpolation specified by the Wasserstein barycenter. The system state is represented by a barycentric weight vector which evolves over time via a random walk on the simplex. Parameter learning leverages the natural Riemannian geometry of Gaussian distributions under the Wasserstein distance, which leads to improved convergence speeds. Experiments on several human activity datasets show that our proposed DWB model accurately learns the generating distribution of pure states while improving state estimation for transition periods compared to the commonly used linear interpolation mixture models.",https://api.openreview.net/pdf/dc30f437025912393f076e5f19cbf7a43eb4afbc.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Dynamical+Wasserstein+Barycenters+for+Time-series+Modeling
Derivative-Free Policy Optimization for Linear Risk-Sensitive and Robust Control Design: Implicit Regularization and Sample Complexity,2021,NIPS,"['Kaiqing Zhang', 'Xiangyuan Zhang', 'Bin Hu', 'Tamer Basar']",poster,"['Robust Control', 'Policy Gradient', 'Reinforcement Learning', 'Nonconvex-Nonconcave Optimization']","Direct policy search serves as one of the workhorses in modern reinforcement learning (RL), and its applications in continuous control tasks have recently attracted increasing attention. In this work, we investigate the convergence theory of policy gradient (PG) methods for learning the linear risk-sensitive and robust controller. In particular, we develop PG methods that can be implemented in a derivative-free fashion by sampling system trajectories, and establish both global convergence and sample complexity results in the solutions of two fundamental settings in risk-sensitive and robust control: the finite-horizon linear exponential quadratic Gaussian, and the finite-horizon linear-quadratic disturbance attenuation problems. As a by-product, our results also provide the first sample complexity for the global convergence of PG methods on solving zero-sum linear-quadratic dynamic games, a nonconvex-nonconcave minimax optimization problem that serves as a baseline setting in multi-agent reinforcement learning (MARL) with continuous spaces. One feature of our algorithms is that during the learning phase, a certain level of robustness/risk-sensitivity of the controller is preserved, which we termed as the implicit regularization property, and is an essential requirement in safety-critical control systems. ",https://api.openreview.net/pdf/e32978d0c09b3d44ec62c1c0e0fd00e915c7f19b.pdf,reinforcement learning;optimization;transformer;multi-agent;llm,https://scholar.google.com/scholar?q=Derivative-Free+Policy+Optimization+for+Linear+Risk-Sensitive+and+Robust+Control+Design:+Implicit+Regularization+and+Sample+Complexity
Neighborhood Reconstructing Autoencoders,2021,NIPS,"['Yonghyeon LEE', 'Hyeokjun Kwon', 'Frank C. Park']",poster,"['Autoencoders', 'Manifold Learning', 'Neighborhood Graph', 'Geometry']","Vanilla autoencoders often produce manifolds that overfit to noisy training data, or have the wrong local connectivity and geometry. Autoencoder regularization techniques, e.g., the denoising autoencoder, have had some success in reducing overfitting, whereas recent graph-based methods that exploit local connectivity information provided by neighborhood graphs have had some success in mitigating local connectivity errors. Neither of these two approaches satisfactorily reduce both overfitting and connectivity errors; moreover, graph-based methods typically involve considerable preprocessing and tuning. To simultaneously address the two issues of overfitting and local connectivity, we propose a new graph-based autoencoder, the Neighborhood Reconstructing Autoencoder (NRAE). Unlike existing graph-based methods that attempt to encode the training data to some prescribed latent space distribution -- one consequence being that only the encoder is the object of the regularization -- NRAE merges local connectivity information contained in the neighborhood graphs with local quadratic approximations of the decoder function to formulate a new neighborhood reconstruction loss. Compared to existing graph-based methods, our new loss function is simple and easy to implement, and the resulting algorithm is scalable and computationally efficient; the only required preprocessing step is the construction of the neighborhood graph. Extensive experiments with standard datasets demonstrate that, compared to existing methods, NRAE improves both overfitting and local connectivity in the learned manifold, in some cases by significant margins. Code for NRAE is available at https://github.com/Gabe-YHLee/NRAE-public.",https://api.openreview.net/pdf/826e58f1817df457ac0d69060a6a9411731e1a5b.pdf,graph;llm,https://scholar.google.com/scholar?q=Neighborhood+Reconstructing+Autoencoders
Meta-Learning for Relative Density-Ratio Estimation,2021,NIPS,"['Atsutoshi Kumagai', 'Tomoharu Iwata', 'Yasuhiro Fujiwara']",poster,"['density-ratio estimation', 'relative density-ratio estimation', 'meta-learning', 'neural networks']","The ratio of two probability densities, called a density-ratio, is a vital quantity in machine learning. In particular, a relative density-ratio, which is a bounded extension of the density-ratio, has received much attention due to its stability and has been used in various applications such as outlier detection and dataset comparison. Existing methods for (relative) density-ratio estimation (DRE) require many instances from both densities. However, sufficient instances are often unavailable in practice. In this paper, we propose a meta-learning method for relative DRE, which estimates the relative density-ratio from a few instances by using knowledge in related datasets. Specifically, given two datasets that consist of a few instances, our model extracts the datasets' information by using neural networks and uses it to obtain instance embeddings appropriate for the relative DRE. We model the relative density-ratio by a linear model on the embedded space, whose global optimum solution can be obtained as a closed-form solution. The closed-form solution enables fast and effective adaptation to a few instances, and its differentiability enables us to train our model such that the expected test error for relative DRE can be explicitly minimized after adapting to a few instances. We empirically demonstrate the effectiveness of the proposed method by using three problems: relative DRE, dataset comparison, and outlier detection.",https://api.openreview.net/pdf/d826c7f504ccb63f11cc0e2c7f9763f7ea1d9cfc.pdf,transformer;meta-learning;llm,https://scholar.google.com/scholar?q=Meta-Learning+for+Relative+Density-Ratio+Estimation
AugMax: Adversarial Composition of Random Augmentations for Robust Training,2021,NIPS,"['Haotao Wang', 'Chaowei Xiao', 'Jean Kossaifi', 'Zhiding Yu', 'Anima Anandkumar', 'Zhangyang Wang']",poster,"['Model robustness', 'natural corruptions', 'data augmentation', 'adversarial training']","Data augmentation is a simple yet effective way to improve the robustness of deep neural networks (DNNs). Diversity and hardness are two complementary dimensions of data augmentation to achieve robustness. For example, AugMix explores random compositions of a diverse set of augmentations to enhance broader coverage, while adversarial training generates adversarially hard samples to spot the weakness. Motivated by this, we propose a data augmentation framework, termed AugMax, to unify the two aspects of diversity and hardness. AugMax first randomly samples multiple augmentation operators and then learns an adversarial mixture of the selected operators. Being a stronger form of data augmentation, AugMax leads to a significantly augmented input distribution which makes model training more challenging. To solve this problem, we further design a disentangled normalization module, termed DuBIN (Dual-Batch-and-Instance Normalization), that disentangles the instance-wise feature heterogeneity arising from AugMax. Experiments show that AugMax-DuBIN leads to significantly improved out-of-distribution robustness, outperforming prior arts by 3.03%, 3.49%, 1.82% and 0.71% on CIFAR10-C, CIFAR100-C, Tiny ImageNet-C and ImageNet-C. Codes and pretrained models are available: https://github.com/VITA-Group/AugMax.",https://api.openreview.net/pdf/125770c5dd1400ab684f2111551accfe04e0ebfb.pdf,graph;zero_few-shot;augmentation;llm,https://scholar.google.com/scholar?q=AugMax:+Adversarial+Composition+of+Random+Augmentations+for+Robust+Training
Functionally Regionalized Knowledge Transfer for Low-resource Drug Discovery,2021,NIPS,"['Huaxiu Yao', 'Ying Wei', 'Long-Kai Huang', 'Ding Xue', 'Junzhou Huang', 'Zhenhui Li']",poster,"['low-resource drug discovery', 'knowledge transfer', 'functional regions']","More recently, there has been a surge of interest in employing machine learning approaches to expedite the drug discovery process where virtual screening for hit discovery and ADMET prediction for lead optimization play essential roles. One of the main obstacles to the wide success of machine learning approaches in these two tasks is that the number of compounds labeled with activities or ADMET properties is too small to build an effective predictive model. This paper seeks to remedy the problem by transferring the knowledge from previous assays, namely in-vivo experiments, by different laboratories and against various target proteins. To accommodate these wildly different assays and capture the similarity between assays, we propose a functional rationalized meta-learning algorithm FRML for such knowledge transfer. FRML constructs the predictive model with layers of neural sub-networks or so-called functional regions. Building on this, FRML shares an initialization for the weights of the predictive model across all assays, while customizes it to each assay with a region localization network choosing the pertinent regions. The compositionality of the model improves the capacity of generalization to various and even out-of-distribution tasks. Empirical results on both virtual screening and ADMET prediction validate the superiority of FRML over state-of-the-art baselines powered with interpretability in assay relationship.",https://api.openreview.net/pdf/c231a7e19e5c44409feb5d33f8b42a2d60d03974.pdf,optimization;zero_few-shot;meta-learning;transfer learning;llm,https://scholar.google.com/scholar?q=Functionally+Regionalized+Knowledge+Transfer+for+Low-resource+Drug+Discovery
Are Transformers more robust than CNNs? ,2021,NIPS,"['Yutong Bai', 'Jieru Mei', 'Alan Yuille', 'Cihang Xie']",poster,"['Transformer', 'Robustness']","Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks,  recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair & in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations. 

With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at: https://github.com/ytongbai/ViTs-vs-CNNs. 
",https://api.openreview.net/pdf/305acc7d4813f6e9d63965d901cbcd404717c40a.pdf,transformer;llm,https://scholar.google.com/scholar?q=Are+Transformers+more+robust+than+CNNs?+
End-to-end Multi-modal Video Temporal Grounding,2021,NIPS,"['Yi-Wen Chen', 'Yi-Hsuan Tsai', 'Ming-Hsuan Yang']",poster,"['Computer Vision', 'Vision and Language', 'Video Temporal Grounding', 'Multi-modal Learning', 'Transformer', 'Contrastive Learning']","We address the problem of text-guided video temporal grounding, which aims to identify the time interval of a certain event based on a natural language description. Different from most existing methods that only consider RGB images as visual features, we propose a multi-modal framework to extract complementary information from videos. Specifically, we adopt RGB images for appearance, optical flow for motion, and depth maps for image structure. While RGB images provide abundant visual cues of certain events, the performance may be affected by background clutters. Therefore, we use optical flow to focus on large motion and depth maps to infer the scene configuration when the action is related to objects recognizable with their shapes. To integrate the three modalities more effectively and enable inter-modal learning, we design a dynamic fusion scheme with transformers to model the interactions between modalities. Furthermore, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, which also facilitates multi-modal learning. We conduct extensive experiments on the Charades-STA and ActivityNet Captions datasets, and show that the proposed method performs favorably against state-of-the-art approaches.",https://api.openreview.net/pdf/719103046bcca6b7ca0f8be128f591dcb4cccbd9.pdf,zero_few-shot;transformer;representation;flow;multimodal;llm,https://scholar.google.com/scholar?q=End-to-end+Multi-modal+Video+Temporal+Grounding
Learning Robust Hierarchical Patterns of Human Brain across Many fMRI Studies,2021,NIPS,"['Dushyant Sahoo', 'Christos Davatzikos']",poster,"['Hierarchical Latent Factor Modeling', 'Matrix Factorization', 'Domain Adaptation', 'fMRI analysis']","Multi-site fMRI studies face the challenge that the pooling introduces systematic non-biological site-specific variance due to hardware, software, and environment. In this paper, we propose to reduce site-specific variance in the estimation of hierarchical Sparsity Connectivity Patterns (hSCPs) in fMRI data via a simple yet effective matrix factorization while preserving biologically relevant variations. Our method leverages unsupervised adversarial learning to improve the reproducibility of the components. Experiments on simulated datasets display that the proposed method can estimate components with higher accuracy and reproducibility, while preserving age-related variation on a multi-center clinical data set. ",https://api.openreview.net/pdf/228f3c1726ff36275eb6606f0cd35966b4ceaad4.pdf,llm,https://scholar.google.com/scholar?q=Learning+Robust+Hierarchical+Patterns+of+Human+Brain+across+Many+fMRI+Studies
True Few-Shot Learning with Language Models,2021,NIPS,"['Ethan Perez', 'Douwe Kiela', 'Kyunghyun Cho']",poster,"['Few-Shot Learning', 'Natural Language Processing']","Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (""prompts""). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.",https://api.openreview.net/pdf/3c5e8dbdf04926abc5f2d08119558105d516f81d.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=True+Few-Shot+Learning+with+Language+Models
Improving Contrastive Learning on Imbalanced Data via Open-World Sampling,2021,NIPS,"['Ziyu Jiang', 'Tianlong Chen', 'Ting Chen', 'Zhangyang Wang']",poster,"['contrastive learning', 'sampling', 'balancedness']","Contrastive learning approaches have achieved great success in learning visual representations with few labels of the target classes. That implies a tantalizing possibility of scaling them up beyond a curated “seed"" benchmark, to incorporating more unlabeled images from the internet-scale external sources to enhance its performance. However, in practice, larger amount of unlabeled data will require more computing resources due to the bigger model size and longer training needed. Moreover, open-world unlabeled data usually follows an implicit long-tail class or attribute distribution, many of which also do not belong to the target classes. Blindly leveraging all unlabeled data hence can lead to the data imbalance as well as distraction issues. This motivates us to seek a principled approach to strategically select unlabeled data from an external source, in order to learn generalizable, balanced and diverse representations for relevant classes. In this work, we present an open-world unlabeled data sampling framework called Model-Aware K-center (MAK), which follows three simple principles: (1) tailness, which encourages sampling of examples from tail classes, by sorting the empirical contrastive loss expectation (ECLE) of samples over random data augmentations; (2) proximity, which rejects the out-of-distribution outliers that may distract training; and (3) diversity, which ensures diversity in the set of sampled examples. Empirically, using ImageNet-100-LT (without labels) as the seed dataset and two “noisy” external data sources, we demonstrate that MAK can consistently improve both the overall representation quality and the class balancedness of the learned features, as evaluated via linear classifier evaluation on full-shot and few-shot settings. The
code is available at: https://github.com/VITA-Group/MAK.",https://api.openreview.net/pdf/58f305020e4e7fed95cebf7a19fe2563d411430b.pdf,graph;zero_few-shot;transformer;representation;contrastive learning;augmentation;llm,https://scholar.google.com/scholar?q=Improving+Contrastive+Learning+on+Imbalanced+Data+via+Open-World+Sampling
Deep Molecular Representation Learning via Fusing Physical and Chemical Information,2021,NIPS,"['Shuwen Yang', 'Ziyao Li', 'Guojie Song', 'Lingsheng Cai']",poster,"['molecular representation', 'molecular fingerprints', 'neural molecular dynamics', 'graph neural networks']","Molecular representation learning is the first yet vital step in combining deep learning and molecular science. To push the boundaries of molecular representation learning, we present PhysChem, a novel neural architecture that learns molecular representations via fusing physical and chemical information of molecules. PhysChem is composed of a physicist network (PhysNet) and a chemist network (ChemNet). PhysNet is a neural physical engine that learns molecular conformations through simulating molecular dynamics with parameterized forces; ChemNet implements geometry-aware deep message-passing to learn chemical / biomedical properties of molecules. Two networks specialize in their own tasks and cooperate by providing expertise to each other. By fusing physical and chemical information, PhysChem achieved state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark. The effectiveness of PhysChem was further corroborated on cutting-edge datasets of SARS-CoV-2.",https://api.openreview.net/pdf/1e943e93239487d0f66ec90764637ba67b4b0f58.pdf,graph;representation;llm,https://scholar.google.com/scholar?q=Deep+Molecular+Representation+Learning+via+Fusing+Physical+and+Chemical+Information
Estimating the Unique Information of Continuous Variables,2021,NIPS,"['Ari Pakman', 'Amin Nejatbakhsh', 'Dar Gilboa', 'Abdullah Makkeh', 'Luca Mazzucato', 'Michael Wibral', 'Elad Schneidman']",poster,"['Recurrent Neural Networks', 'Partial Information Decomposition']","The integration and transfer of information from multiple sources to multiple targets is a core motive of neural systems. The emerging field of partial information decomposition (PID) provides a novel information-theoretic lens into these mechanisms by identifying synergistic, redundant, and unique contributions to the mutual information between one and several variables. While many works have studied aspects of PID for Gaussian and discrete distributions, the case of general continuous distributions is still uncharted territory. In this work we present a method for estimating the unique information in continuous distributions, for the case of one versus two variables. Our method solves the associated optimization problem over the space of distributions with fixed bivariate  marginals by combining copula decompositions and techniques developed to optimize variational autoencoders. We obtain excellent agreement with known analytic results for Gaussians, and  illustrate the power of our new approach in several brain-inspired neural models. Our method is capable of recovering the effective connectivity of a chaotic network of rate neurons, and uncovers a complex trade-off between redundancy, synergy and unique information in recurrent networks trained to solve a generalized XOR~task.
",https://api.openreview.net/pdf/4c4deb496a8bce23f1a518b87d44e9ae07a7af6b.pdf,graph;optimization;vae;transfer learning;llm,https://scholar.google.com/scholar?q=Estimating+the+Unique+Information+of+Continuous+Variables
Trustworthy Multimodal Regression with Mixture of Normal-inverse Gamma Distributions,2021,NIPS,"['Huan Ma', 'Zongbo Han', 'Changqing Zhang', 'Huazhu Fu', 'Joey Tianyi Zhou', 'Qinghua Hu']",poster,"['Multimodal Regression', 'Trustworthy', 'Uncertainty', 'Evidential']","Multimodal regression is a fundamental task, which integrates the information from different sources to improve the performance of follow-up applications. However, existing methods mainly focus on improving the performance and often ignore the confidence of prediction for diverse situations. In this study, we are devoted to trustworthy multimodal regression which is critical in cost-sensitive domains. To this end, we introduce a novel Mixture of Normal-Inverse Gamma distributions (MoNIG) algorithm, which efficiently estimates uncertainty in principle for adaptive integration of different modalities and produces a trustworthy regression result. Our model can be dynamically aware of uncertainty for each modality, and also robust for corrupted modalities. Furthermore, the proposed MoNIG ensures explicitly representation of (modality-specific/global) epistemic and aleatoric uncertainties, respectively. Experimental results on both synthetic and different real-world data demonstrate the effectiveness and trustworthiness of our method on various multimodal regression tasks (e.g., temperature prediction for superconductivity, relative location prediction for CT slices, and multimodal sentiment analysis).",https://api.openreview.net/pdf/0cdd71f714a740149b60154b6833f521e03ec39d.pdf,zero_few-shot;representation;adaptive;multimodal;llm,https://scholar.google.com/scholar?q=Trustworthy+Multimodal+Regression+with+Mixture+of+Normal-inverse+Gamma+Distributions
Scalable Bayesian GPFA with automatic relevance determination and discrete noise models,2021,NIPS,"['Kristopher T Jensen', 'Ta-Chu Kao', 'Jasmine Talia Stone', 'Guillaume Hennequin']",poster,"['neuroscience', 'GPFA', 'latent variable models', 'motor control', 'neural dynamics', 'neural data analysis', 'variational inference']","Latent variable models are ubiquitous in the exploratory analysis of neural population recordings, where they allow researchers to summarize the activity of large populations of neurons in lower dimensional ‘latent’ spaces. Existing methods can generally be categorized into (i) Bayesian methods that facilitate flexible incorporation of prior knowledge and uncertainty estimation, but which typically do not scale to large datasets; and (ii) highly parameterized methods without explicit priors that scale better but often struggle in the low-data regime. Here, we bridge this gap by developing a fully Bayesian yet scalable version of Gaussian process factor analysis (bGPFA), which models neural data as arising from a set of inferred latent processes with a prior that encourages smoothness over time. Additionally, bGPFA uses automatic relevance determination to infer the dimensionality of neural activity directly from the training data during optimization. To enable the analysis of continuous recordings without trial structure, we introduce a novel variational inference strategy that scales near-linearly in time and also allows for non-Gaussian noise models appropriate for electrophysiological recordings. We apply bGPFA to continuous recordings spanning 30 minutes with over 14 million data points from primate motor and somatosensory cortices during a self-paced reaching task. We show that neural activity progresses from an initial state at target onset to a reach- specific preparatory state well before movement onset. The distance between these initial and preparatory latent states is predictive of reaction times across reaches, suggesting that such preparatory dynamics have behavioral relevance despite the lack of externally imposed delay periods. Additionally, bGPFA discovers latent processes that evolve over slow timescales on the order of several seconds and contain complementary information about reaction time. These timescales are longer than those revealed by methods which focus on individual movement epochs and may reflect fluctuations in e.g. task engagement.",https://api.openreview.net/pdf/485efe225be1eb8b13d5172f95615b902e16462d.pdf,optimization;zero_few-shot;bayesian;inference;llm,https://scholar.google.com/scholar?q=Scalable+Bayesian+GPFA+with+automatic+relevance+determination+and+discrete+noise+models
TransMatcher: Deep Image Matching Through Transformers for Generalizable Person Re-identification,2021,NIPS,"['Shengcai Liao', 'Ling Shao']",poster,"['Transformer', 'Deep Image Matching', 'Deep Metric Learning', 'Person Re-Identification', 'Generalizable Person Re-Identification', 'Domain Generalization']","Transformers have recently gained increasing attention in computer vision. However, existing studies mostly use Transformers for feature representation learning, e.g. for image classification and dense predictions, and the generalizability of Transformers is unknown. In this work, we further investigate the possibility of applying Transformers for image matching and metric learning given pairs of images. We find that the Vision Transformer (ViT) and the vanilla Transformer with decoders are not adequate for image matching due to their lack of image-to-image attention. Thus, we further design two naive solutions, i.e. query-gallery concatenation in ViT, and query-gallery cross-attention in the vanilla Transformer. The latter improves the performance, but it is still limited. This implies that the attention mechanism in Transformers is primarily designed for global feature aggregation, which is not naturally suitable for image matching. Accordingly, we propose a new simplified decoder, which drops the full attention implementation with the softmax weighting, keeping only the query-key similarity computation. Additionally, global max pooling and a multilayer perceptron (MLP) head are applied to decode the matching result. This way, the simplified decoder is computationally more efficient, while at the same time more effective for image matching. The proposed method, called TransMatcher, achieves state-of-the-art performance in generalizable person re-identification, with up to 6.1% and 5.7% performance gains in Rank-1 and mAP, respectively, on several popular datasets. Code is available at https://github.com/ShengcaiLiao/QAConv.",https://api.openreview.net/pdf/29e0859b64bc8d806feafad30c3cd0e4ebefbfb9.pdf,transformer;representation;metric;llm,https://scholar.google.com/scholar?q=TransMatcher:+Deep+Image+Matching+Through+Transformers+for+Generalizable+Person+Re-identification
Learning High-Precision Bounding Box for Rotated Object Detection via Kullback-Leibler Divergence,2021,NIPS,"['Xue Yang', 'Xiaojiang Yang', 'Jirui Yang', 'Qi Ming', 'Wentao Wang', 'Qi Tian', 'Junchi Yan']",poster,"['Rotated Object Detection', 'High-Precision', 'Kullback-Leibler Divergence']","Existing rotated object detectors are mostly inherited from the horizontal detection paradigm, as the latter has evolved into a well-developed area. However, these detectors are difficult to perform prominently in high-precision detection due to the limitation of current regression loss design, especially for objects with large aspect ratios. Taking the perspective that horizontal detection is a special case for rotated object detection, in this paper, we are motivated to change the design of rotation regression loss from induction paradigm to deduction methodology, in terms of the relation between rotation and horizontal detection. We show that one essential challenge is how to modulate the coupled parameters in the rotation regression loss, as such the estimated parameters can influence to each other during the dynamic joint optimization, in an adaptive and synergetic way. Specifically, we first convert the rotated bounding box into a 2-D Gaussian distribution, and then calculate the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the regression loss. By analyzing the gradient of each parameter, we show that KLD (and its derivatives) can dynamically adjust the parameter gradients according to the characteristics of the object. For instance, it will adjust the importance (gradient weight) of the angle parameter according to the aspect ratio. This mechanism can be vital for high-precision detection as a slight angle error would cause a serious accuracy drop for large aspect ratios objects. More importantly, we have proved that KLD is scale invariant. We further show that the KLD loss can be degenerated into the popular Ln-norm loss for horizontal detection. Experimental results on seven datasets using different detectors show its consistent superiority, and codes are available at https://github.com/yangxue0827/RotationDetection.",https://api.openreview.net/pdf/5c56f94864ca7261074e4beb59559be5da38cf96.pdf,optimization;adaptive;llm,https://scholar.google.com/scholar?q=Learning+High-Precision+Bounding+Box+for+Rotated+Object+Detection+via+Kullback-Leibler+Divergence
Passive attention in artificial neural networks predicts human visual selectivity,2021,NIPS,"['Thomas A Langlois', 'Haicheng Charles Zhao', 'Erin Grant', 'Ishita Dasgupta', 'Thomas L. Griffiths', 'Nori Jacoby']",oral,"['Cognition', 'Attention', 'Interpretable AI', 'Computer Vision', 'Human Visual Perception']","Developments in machine learning interpretability techniques over the past decade have provided new tools to observe the image regions that are most informative for classification and localization in artificial neural networks (ANNs). Are the same regions similarly informative to human observers? Using data from 79 new experiments and 7,810 participants, we show that passive attention techniques reveal a significant overlap with human visual selectivity estimates derived from 6 distinct behavioral tasks including visual discrimination, spatial localization, recognizability, free-viewing, cued-object search, and saliency search fixations. We find that input visualizations derived from relatively simple ANN architectures probed using guided backpropagation methods are the best predictors of a shared component in the joint variability of the human measures. We validate these correlational results with causal manipulations using recognition experiments. We show that images masked with ANN attention maps were easier for humans to classify than control masks in a speeded recognition experiment. Similarly, we find that recognition performance in the same ANN models was likewise influenced by masking input images using human visual selectivity maps. This work contributes a new approach to evaluating the biological and psychological validity of leading ANNs as models of human vision: by examining their similarities and differences in terms of their visual selectivity to the information contained in images.",https://api.openreview.net/pdf/219379184533d639a61682c69c38ab129899fbc7.pdf,transformer;llm,https://scholar.google.com/scholar?q=Passive+attention+in+artificial+neural+networks+predicts+human+visual+selectivity
An Exponential Lower Bound for Linearly Realizable MDP with Constant Suboptimality Gap,2021,NIPS,"['Yuanhao Wang', 'Ruosong Wang', 'Sham M. Kakade']",oral,"['Reinforcement Learning', 'Linear Function Approximation', 'Lower Bound']","A fundamental question in the theory of reinforcement learning is: suppose the optimal $Q$-function lies in the linear span of a given $d$ dimensional feature mapping, is sample-efficient reinforcement learning (RL) possible? The recent and remarkable result of Weisz et al. (2020) resolves this question in the negative, providing an exponential (in $d$) sample size lower bound, which holds even if the agent has access to a generative model of the environment. One may hope that such a lower can be circumvented with an even stronger assumption that there is a \emph{constant gap} between the optimal $Q$-value of the best action and that of the second-best action (for all states); indeed, the construction in Weisz et al. (2020) relies on having an exponentially small gap. This work resolves this subsequent question, showing that an exponential sample complexity lower bound still holds even if a constant gap is assumed.  Perhaps surprisingly, this result implies an exponential separation between the online RL setting and the generative model setting, where sample-efficient RL is in fact possible in the latter setting with a constant gap. Complementing our negative hardness result, we give two positive results showing that provably sample-efficient RL is possible either under an additional low-variance assumption or under a novel hypercontractivity assumption.",https://api.openreview.net/pdf/715828e0d27f31124023770d924f73776d1ca465.pdf,reinforcement learning;zero_few-shot;generative model;online learning;llm,https://scholar.google.com/scholar?q=An+Exponential+Lower+Bound+for+Linearly+Realizable+MDP+with+Constant+Suboptimality+Gap
Bellman-consistent Pessimism for Offline Reinforcement Learning,2021,NIPS,"['Tengyang Xie', 'Ching-An Cheng', 'Nan Jiang', 'Paul Mineiro', 'Alekh Agarwal']",oral,"['offline reinforcement learning', 'Bellman-consistent pessimism', 'sample complexity bounds', 'linear MDP', 'function approximation']","The use of pessimism, when reasoning about datasets lacking exhaustive exploration has recently gained prominence in offline reinforcement learning. Despite the robustness it adds to the algorithm, overly pessimistic reasoning can be equally damaging in precluding the discovery of good policies, which is an issue for the popular bonus-based pessimism. In this paper, we introduce the notion of Bellman-consistent pessimism for general function approximation: instead of calculating a point-wise lower bound for the value function, we implement pessimism at the initial state over the set of functions consistent with the Bellman equations. Our theoretical guarantees only require Bellman closedness as standard in the exploratory setting, in which case bonus-based pessimism fails to provide guarantees.  Even in the special case of linear function approximation where stronger expressivity assumptions hold, our result improves upon a recent bonus-based approach by $\mathcal O(d)$ in its sample complexity (when the action space is finite). Remarkably, our algorithms automatically adapt to the best bias-variance tradeoff in the hindsight, whereas most prior approaches require tuning extra hyperparameters a priori.",https://api.openreview.net/pdf/172b82a8a2edef3d7d2d14d6f27e4f9c3818c44b.pdf,reinforcement learning;offline reinforcement learning;graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Bellman-consistent+Pessimism+for+Offline+Reinforcement+Learning
"Drop, Swap, and Generate: A Self-Supervised Approach for Generating Neural Activity",2021,NIPS,"['Ran Liu', 'Mehdi Azabou', 'Max Dabagia', 'Chi-Heng Lin', 'Mohammad Gheshlaghi Azar', 'Keith B Hengen', 'Michal Valko', 'Eva L Dyer']",oral,"['Generative modeling', 'Self-supervised learning', 'Neural decoding', 'Neural population activity', 'Representation learning']","Meaningful and simplified representations of neural activity can yield insights into how and what information is being processed within a neural circuit. However, without labels, finding representations that reveal the link between the brain and behavior can be challenging. Here, we introduce a novel unsupervised approach for learning disentangled representations of neural activity called Swap-VAE. Our approach combines a generative modeling framework with an instance-specific alignment loss that tries to maximize the representational similarity between transformed views of the input (brain state). These transformed (or augmented) views are created by dropping out neurons and jittering samples in time, which intuitively should lead the network to a representation that maintains both temporal consistency and invariance to the specific neurons used to represent the neural state. Through evaluations on both synthetic data and neural recordings from hundreds of neurons in different primate brains, we show that it is possible to build representations that disentangle neural datasets along relevant latent dimensions linked to behavior.",https://api.openreview.net/pdf/044cf9bcd955efa27858483633cd8ca4cc00d1d8.pdf,graph;zero_few-shot;representation;vae;generative model;augmentation;llm,"https://scholar.google.com/scholar?q=Drop,+Swap,+and+Generate:+A+Self-Supervised+Approach+for+Generating+Neural+Activity"
Stability and Deviation Optimal Risk Bounds with Convergence Rate $O(1/n)$,2021,NIPS,"['Yegor Klochkov', 'Nikita Zhivotovskiy']",oral,"['algorithmic stability', 'generalization bounds', 'concentration inequalities', 'stochastic convex optimization']","The sharpest known high probability generalization bounds for uniformly stable algorithms (Feldman, Vondrak, NeurIPS 2018, COLT, 2019), (Bousquet, Klochkov, Zhivotovskiy, COLT, 2020) contain a generally inevitable sampling error term of order $\Theta(1/\sqrt{n})$. When applied to excess risk bounds, this leads to suboptimal results in several standard stochastic convex optimization problems. We show that if the so-called Bernstein condition is satisfied, the term $\Theta(1/\sqrt{n})$ can be avoided, and high probability excess risk bounds of order up to $O(1/n)$ are possible via uniform stability. Using this result, we show a high probability excess risk bound with the rate $O(\log n/n)$ for strongly convex and Lipschitz losses valid for \emph{any} empirical risk minimization method. This resolves a question of Shalev-Shwartz, Shamir, Srebro, and Sridharan (COLT, 2009). We discuss how $O(\log n/n)$ high probability excess risk bounds are possible for projected gradient descent in the case of strongly convex and Lipschitz losses without the usual smoothness assumption.",https://api.openreview.net/pdf/4bfb0b2d40503eb1f86bc6fbddb303b0f035fff7.pdf,optimization;llm,https://scholar.google.com/scholar?q=Stability+and+Deviation+Optimal+Risk+Bounds+with+Convergence+Rate+$O(1/n)$
On the Expressivity of Markov Reward,2021,NIPS,"['David Abel', 'Will Dabney', 'Anna Harutyunyan', 'Mark K Ho', 'Michael Littman', 'Doina Precup', 'Satinder Singh']",oral,"['Reinforcement Learning', 'Reward Functions', 'Reward', 'Reward Hypothesis', 'Markov Decision Process']","Reward is the driving force for reinforcement-learning agents. This paper is dedicated to understanding the expressivity of reward as a way to capture tasks that we would want an agent to perform. We frame this study around three new abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) a partial ordering over trajectories. Our main results prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. We then provide a set of polynomial-time algorithms that construct a Markov reward function that allows an agent to optimize tasks of each of these three types, and correctly determine when no such reward function exists. We conclude with an empirical study that corroborates and illustrates our theoretical findings.",https://api.openreview.net/pdf/f38ec47becfb7e6c3c6be1f07458f010ab06c3da.pdf,reinforcement learning;zero_few-shot;llm,https://scholar.google.com/scholar?q=On+the+Expressivity+of+Markov+Reward
Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos,2022,CVPR,Muheng Li;Lei Chen;Yueqi Duan;Zhilan Hu;Jianjiang Feng;Jie Zhou;Jiwen Lu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Bridge-Prompt_Towards_Ordinal_Action_Understanding_in_Instructional_Videos_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Bridge-Prompt:+Towards+Ordinal+Action+Understanding+in+Instructional+Videos
Prompt Distribution Learning,2022,CVPR,Yuning Lu;Jianzhuang Liu;Yonggang Zhang;Yajing Liu;Xinmei Tian,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Prompt+Distribution+Learning
Splicing ViT Features for Semantic Appearance Transfer,2022,CVPR,Narek Tumanyan;Omer Bar-Tal;Shai Bagon;Tali Dekel,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Tumanyan_Splicing_ViT_Features_for_Semantic_Appearance_Transfer_CVPR_2022_paper.pdf,transfer learning;llm,https://scholar.google.com/scholar?q=Splicing+ViT+Features+for+Semantic+Appearance+Transfer
Learning To Prompt for Continual Learning,2022,CVPR,Zifeng Wang;Zizhao Zhang;Chen-Yu Lee;Han Zhang;Ruoxi Sun;Xiaoqi Ren;Guolong Su;Vincent Perot;Jennifer Dy;Tomas Pfister,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Learning_To_Prompt_for_Continual_Learning_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Learning+To+Prompt+for+Continual+Learning
Align and Prompt: Video-and-Language Pre-Training With Entity Prompts,2022,CVPR,Dongxu Li;Junnan Li;Hongdong Li;Juan Carlos Niebles;Steven C.H. Hoi,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Align_and_Prompt_Video-and-Language_Pre-Training_With_Entity_Prompts_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Align+and+Prompt:+Video-and-Language+Pre-Training+With+Entity+Prompts
Learning To Recognize Procedural Activities With Distant Supervision,2022,CVPR,Xudong Lin;Fabio Petroni;Gedas Bertasius;Marcus Rohrbach;Shih-Fu Chang;Lorenzo Torresani,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_Learning_To_Recognize_Procedural_Activities_With_Distant_Supervision_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Learning+To+Recognize+Procedural+Activities+With+Distant+Supervision
MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition,2022,CVPR,Chao-Yuan Wu;Yanghao Li;Karttikeya Mangalam;Haoqi Fan;Bo Xiong;Jitendra Malik;Christoph Feichtenhofer,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_MeMViT_Memory-Augmented_Multiscale_Vision_Transformer_for_Efficient_Long-Term_Video_Recognition_CVPR_2022_paper.pdf,transformer;augmentation;llm,https://scholar.google.com/scholar?q=MeMViT:+Memory-Augmented+Multiscale+Vision+Transformer+for+Efficient+Long-Term+Video+Recognition
Gravitationally Lensed Black Hole Emission Tomography,2022,CVPR,Aviad Levis;Pratul P. Srinivasan;Andrew A. Chael;Ren Ng;Katherine L. Bouman,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Levis_Gravitationally_Lensed_Black_Hole_Emission_Tomography_CVPR_2022_paper.pdf,graph;llm,https://scholar.google.com/scholar?q=Gravitationally+Lensed+Black+Hole+Emission+Tomography
Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities,2022,CVPR,Fadime Sener;Dibyadip Chatterjee;Daniel Shelepov;Kun He;Dipika Singhania;Robert Wang;Angela Yao,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Sener_Assembly101_A_Large-Scale_Multi-View_Video_Dataset_for_Understanding_Procedural_Activities_CVPR_2022_paper.pdf,multi-view;llm,https://scholar.google.com/scholar?q=Assembly101:+A+Large-Scale+Multi-View+Video+Dataset+for+Understanding+Procedural+Activities
Learning To Prompt for Open-Vocabulary Object Detection With Vision-Language Model,2022,CVPR,Yu Du;Fangyun Wei;Zihe Zhang;Miaojing Shi;Yue Gao;Guoqi Li,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Du_Learning_To_Prompt_for_Open-Vocabulary_Object_Detection_With_Vision-Language_Model_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Learning+To+Prompt+for+Open-Vocabulary+Object+Detection+With+Vision-Language+Model
GradViT: Gradient Inversion of Vision Transformers,2022,CVPR,Ali Hatamizadeh;Hongxu Yin;Holger R. Roth;Wenqi Li;Jan Kautz;Daguang Xu;Pavlo Molchanov,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Hatamizadeh_GradViT_Gradient_Inversion_of_Vision_Transformers_CVPR_2022_paper.pdf,transformer;llm,https://scholar.google.com/scholar?q=GradViT:+Gradient+Inversion+of+Vision+Transformers
Image Segmentation Using Text and Image Prompts,2022,CVPR,Timo Lüddecke;Alexander Ecker,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Luddecke_Image_Segmentation_Using_Text_and_Image_Prompts_CVPR_2022_paper.pdf,segmentation;llm,https://scholar.google.com/scholar?q=Image+Segmentation+Using+Text+and+Image+Prompts
UnweaveNet: Unweaving Activity Stories,2022,CVPR,Will Price;Carl Vondrick;Dima Damen,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Price_UnweaveNet_Unweaving_Activity_Stories_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=UnweaveNet:+Unweaving+Activity+Stories
"A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes",2022,CVPR,Mazda Moayeri;Phillip Pope;Yogesh Balaji;Soheil Feizi,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Moayeri_A_Comprehensive_Study_of_Image_Classification_Model_Sensitivity_to_Foregrounds_CVPR_2022_paper.pdf,llm,"https://scholar.google.com/scholar?q=A+Comprehensive+Study+of+Image+Classification+Model+Sensitivity+to+Foregrounds,+Backgrounds,+and+Visual+Attributes"
Dual-AI: Dual-Path Actor Interaction Learning for Group Activity Recognition,2022,CVPR,Mingfei Han;David Junhao Zhang;Yali Wang;Rui Yan;Lina Yao;Xiaojun Chang;Yu Qiao,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Dual-AI_Dual-Path_Actor_Interaction_Learning_for_Group_Activity_Recognition_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Dual-AI:+Dual-Path+Actor+Interaction+Learning+for+Group+Activity+Recognition
Conditional Prompt Learning for Vision-Language Models,2022,CVPR,Kaiyang Zhou;Jingkang Yang;Chen Change Loy;Ziwei Liu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Conditional+Prompt+Learning+for+Vision-Language+Models
Audio-Adaptive Activity Recognition Across Video Domains,2022,CVPR,Yunhua Zhang;Hazel Doughty;Ling Shao;Cees G. M. Snoek,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Audio-Adaptive_Activity_Recognition_Across_Video_Domains_CVPR_2022_paper.pdf,adaptive;llm,https://scholar.google.com/scholar?q=Audio-Adaptive+Activity+Recognition+Across+Video+Domains
MViTv2: Improved Multiscale Vision Transformers for Classification and Detection,2022,CVPR,Yanghao Li;Chao-Yuan Wu;Haoqi Fan;Karttikeya Mangalam;Bo Xiong;Jitendra Malik;Christoph Feichtenhofer,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.pdf,transformer;llm,https://scholar.google.com/scholar?q=MViTv2:+Improved+Multiscale+Vision+Transformers+for+Classification+and+Detection
Bootstrapping ViTs: Towards Liberating Vision Transformers From Pre-Training,2022,CVPR,Haofei Zhang;Jiarui Duan;Mengqi Xue;Jie Song;Li Sun;Mingli Song,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Bootstrapping_ViTs_Towards_Liberating_Vision_Transformers_From_Pre-Training_CVPR_2022_paper.pdf,transformer;llm,https://scholar.google.com/scholar?q=Bootstrapping+ViTs:+Towards+Liberating+Vision+Transformers+From+Pre-Training
Meta-Attention for ViT-Backed Continual Learning,2022,CVPR,Mengqi Xue;Haofei Zhang;Jie Song;Mingli Song,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Xue_Meta-Attention_for_ViT-Backed_Continual_Learning_CVPR_2022_paper.pdf,transformer;meta-learning;llm,https://scholar.google.com/scholar?q=Meta-Attention+for+ViT-Backed+Continual+Learning
ADAPT: Vision-Language Navigation With Modality-Aligned Action Prompts,2022,CVPR,Bingqian Lin;Yi Zhu;Zicong Chen;Xiwen Liang;Jianzhuang Liu;Xiaodan Liang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_ADAPT_Vision-Language_Navigation_With_Modality-Aligned_Action_Prompts_CVPR_2022_paper.pdf,multimodal;llm,https://scholar.google.com/scholar?q=ADAPT:+Vision-Language+Navigation+With+Modality-Aligned+Action+Prompts
Using 3D Topological Connectivity for Ghost Particle Reduction in Flow Reconstruction,2022,CVPR,Christina Tsalicoglou;Thomas Rösgen,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Tsalicoglou_Using_3D_Topological_Connectivity_for_Ghost_Particle_Reduction_in_Flow_CVPR_2022_paper.pdf,zero_few-shot;flow;3d;llm,https://scholar.google.com/scholar?q=Using+3D+Topological+Connectivity+for+Ghost+Particle+Reduction+in+Flow+Reconstruction
GroupViT: Semantic Segmentation Emerges From Text Supervision,2022,CVPR,Jiarui Xu;Shalini De Mello;Sifei Liu;Wonmin Byeon;Thomas Breuel;Jan Kautz;Xiaolong Wang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_GroupViT_Semantic_Segmentation_Emerges_From_Text_Supervision_CVPR_2022_paper.pdf,segmentation;llm,https://scholar.google.com/scholar?q=GroupViT:+Semantic+Segmentation+Emerges+From+Text+Supervision
Detector-Free Weakly Supervised Group Activity Recognition,2022,CVPR,Dongkeun Kim;Jinsung Lee;Minsu Cho;Suha Kwak,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_Detector-Free_Weakly_Supervised_Group_Activity_Recognition_CVPR_2022_paper.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Detector-Free+Weakly+Supervised+Group+Activity+Recognition
Pyramid Adversarial Training Improves ViT Performance,2022,CVPR,Charles Herrmann;Kyle Sargent;Lu Jiang;Ramin Zabih;Huiwen Chang;Ce Liu;Dilip Krishnan;Deqing Sun,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Herrmann_Pyramid_Adversarial_Training_Improves_ViT_Performance_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Pyramid+Adversarial+Training+Improves+ViT+Performance
MiniViT: Compressing Vision Transformers With Weight Multiplexing,2022,CVPR,Jinnian Zhang;Houwen Peng;Kan Wu;Mengchen Liu;Bin Xiao;Jianlong Fu;Lu Yuan,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_MiniViT_Compressing_Vision_Transformers_With_Weight_Multiplexing_CVPR_2022_paper.pdf,transformer;llm,https://scholar.google.com/scholar?q=MiniViT:+Compressing+Vision+Transformers+With+Weight+Multiplexing
DenseCLIP: Language-Guided Dense Prediction With Context-Aware Prompting,2022,CVPR,Yongming Rao;Wenliang Zhao;Guangyi Chen;Yansong Tang;Zheng Zhu;Guan Huang;Jie Zhou;Jiwen Lu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=DenseCLIP:+Language-Guided+Dense+Prediction+With+Context-Aware+Prompting
"JRDB-Act: A Large-Scale Dataset for Spatio-Temporal Action, Social Group and Activity Detection",2022,CVPR,Mahsa Ehsanpour;Fatemeh Saleh;Silvio Savarese;Ian Reid;Hamid Rezatofighi,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Ehsanpour_JRDB-Act_A_Large-Scale_Dataset_for_Spatio-Temporal_Action_Social_Group_and_CVPR_2022_paper.pdf,llm,"https://scholar.google.com/scholar?q=JRDB-Act:+A+Large-Scale+Dataset+for+Spatio-Temporal+Action,+Social+Group+and+Activity+Detection"
A-ViT: Adaptive Tokens for Efficient Vision Transformer,2022,CVPR,Hongxu Yin;Arash Vahdat;Jose M. Alvarez;Arun Mallya;Jan Kautz;Pavlo Molchanov,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Yin_A-ViT_Adaptive_Tokens_for_Efficient_Vision_Transformer_CVPR_2022_paper.pdf,transformer;adaptive;llm,https://scholar.google.com/scholar?q=A-ViT:+Adaptive+Tokens+for+Efficient+Vision+Transformer
A Hybrid Egocentric Activity Anticipation Framework via Memory-Augmented Recurrent and One-Shot Representation Forecasting,2022,CVPR,Tianshan Liu;Kin-Man Lam,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_Hybrid_Egocentric_Activity_Anticipation_Framework_via_Memory-Augmented_Recurrent_and_CVPR_2022_paper.pdf,representation;augmentation;llm,https://scholar.google.com/scholar?q=A+Hybrid+Egocentric+Activity+Anticipation+Framework+via+Memory-Augmented+Recurrent+and+One-Shot+Representation+Forecasting
AdaViT: Adaptive Vision Transformers for Efficient Image Recognition,2022,CVPR,Lingchen Meng;Hengduo Li;Bor-Chun Chen;Shiyi Lan;Zuxuan Wu;Yu-Gang Jiang;Ser-Nam Lim,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Meng_AdaViT_Adaptive_Vision_Transformers_for_Efficient_Image_Recognition_CVPR_2022_paper.pdf,transformer;adaptive;llm,https://scholar.google.com/scholar?q=AdaViT:+Adaptive+Vision+Transformers+for+Efficient+Image+Recognition
MPViT: Multi-Path Vision Transformer for Dense Prediction,2022,CVPR,Youngwan Lee;Jonghee Kim;Jeffrey Willette;Sung Ju Hwang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_MPViT_Multi-Path_Vision_Transformer_for_Dense_Prediction_CVPR_2022_paper.pdf,transformer;llm,https://scholar.google.com/scholar?q=MPViT:+Multi-Path+Vision+Transformer+for+Dense+Prediction
Estimating the Impact of Coordinated Inauthentic Behavior on Content Recommendations in Social Networks,2022,ICML,"['Swapneel S Mehta', 'Atilim Gunes Baydin', 'Bogdan State', 'Richard Bonneau', 'Jonathan Nagler', 'Philip Torr']",oral,"['coordinated inauthentic behavior', 'agent based modeling', 'recommender systems', 'social networks']","Online disinformation is a dynamic and pervasive problem on social networks as evidenced by a spate of public disasters in light of active efforts to combat it.
Since the massive amounts of content generated each day on these platforms is impossible to manually curate, ranking and recommendation algorithms are a key apparatus that drive user interactions.
However, the vulnerability of ranking and recommendation algorithms to attack from coordinated campaigns spreading misleading information has been established both theoretically and anecdotally.
Unfortunately it is unclear how effective countermeasures to disinformation are in practice due to the limited view we have into the operation of such platforms.
We develop a multiagent simulation of a popular social network, Reddit, that aligns with the state-action space available to real users based on the platform's affordances. 
We collect millions of real-world interactions from Reddit to estimate the network for each user in our dataset and utilise Reddit's self-described content ranking strategies to compare the impact of coordinated activity on content spread by each algorithm.
We expect that this will inform the design of robust content distribution systems that are resilient against targeted attacks by groups of malicious actors.
",https://api.openreview.net/pdf/93eb841cc83e32617d408975da07ed6750af898e.pdf,reinforcement learning;online learning;active learning;llm,https://scholar.google.com/scholar?q=Estimating+the+Impact+of+Coordinated+Inauthentic+Behavior+on+Content+Recommendations+in+Social+Networks
Causal Prediction Can Induce Performative Stability,2022,ICML,['Bogdan Kulynych'],poster,"['performative prediction', 'causal features']","Predictive models affect the world through inducing a strategic response or reshaping the environment in which they are deployed---a property called performativity. This results in the need to constantly adapt and re-design the model. We formalize one possible mechanism through which performativity can arise using the language of causal modeling. We show that using features which form a Markov blanket of the target variable for prediction closes the feedback loop in this setting. Thus, a predictive model that takes as input such causal features might not require any further adaptation after deployment even if it changes the environment.",https://api.openreview.net/pdf/1a0415d75c8525c31b22306ebc9d5c87a0df977b.pdf,llm,https://scholar.google.com/scholar?q=Causal+Prediction+Can+Induce+Performative+Stability
Selection Bias Induced Spurious Correlations in Large Language Models,2022,ICML,['Emily McMilin'],poster,"['spurious correlations', 'large language models', 'causal inference']","In this work we show how large language models (LLMs) can learn statistical dependencies between otherwise unconditionally independent variables due to dataset selection bias. To demonstrate the effect, we developed a masked gender task that can be applied to BERT-family models to reveal spurious correlations between predicted gender pronouns and a variety of seemingly gender-neutral variables like date and location, on pre-trained (unmodified) BERT and RoBERTa large models. Finally, we provide an online demo, inviting readers to experiment further.",https://api.openreview.net/pdf/ece713570db8eebb68eaaebdd685359675f9ab47.pdf,transformer;online learning;llm,https://scholar.google.com/scholar?q=Selection+Bias+Induced+Spurious+Correlations+in+Large+Language+Models
A Study of Causal Confusion in Preference-Based Reward Learning,2022,ICML,"['Jeremy Tien', 'Jerry Zhi-Yang He', 'Zackory Erickson', 'Anca Dragan', 'Daniel S. Brown']",poster,"['causal reward confusion', 'preference learning', 'reward learning']","While there is much empirical and theoretical analysis of causal confusion and reward gaming behaviors in reinforcement learning and behavioral cloning approaches, we provide the first systematic study of causal confusion in the context of learning reward functions from preferences. We identify a set of three benchmark domains where we observe causal confusion when learning reward functions from offline datasets of pairwise trajectory preferences: a simple reacher domain, an assistive feeding domain, and an itch-scratching domain. To gain insight into this observed causal confusion, we perform a sensitivity analysis on the effect of different factors---the reward model capacity and feature dimensionality---on the robustness of rewards learned from preferences. We find evidence that learning rewards from preferences is highly sensitive and non-robust to spurious features and increasing model capacity. %, but not as sensitive to the type of training data. Videos, code, and supplemental results are available at https://sites.google.com/view/causal-reward-confusion.",https://api.openreview.net/pdf/4a3efd2be712ede94d4419990a93203e0b8fa932.pdf,reinforcement learning;offline reinforcement learning;llm,https://scholar.google.com/scholar?q=A+Study+of+Causal+Confusion+in+Preference-Based+Reward+Learning
Evaluating Systemic Error Detection Methods using Synthetic Images,2022,ICML,"['Gregory Plumb', 'Nari Johnson', 'Ángel Cabrera', 'Marco Tulio Ribeiro', 'Ameet Talwalkar']",poster,[],"We introduce SpotCheck, a framework for generating synthetic datasets to use for evaluating methods for discovering blindspots (i.e., systemic errors) in image classifiers. We use SpotCheck to run controlled studies of how various factors influence the performance of blindspot discovery methods. Our experiments reveal several shortcomings of existing methods, such as relatively poor performance in settings with multiple blindspots and sensitivity to hyperparameters. Further, we find that a method based on dimensionality reduction, PlaneSpot, is competitive with existing methods, which has promising implications for the development of interactive tools.",https://api.openreview.net/pdf/0e30585048ed4443b2b89962f27a994f8de87170.pdf,active learning;llm,https://scholar.google.com/scholar?q=Evaluating+Systemic+Error+Detection+Methods+using+Synthetic+Images
Automated Invariance Testing for Machine Learning Models Using Sparse Linear Layers,2022,ICML,"['Zukang Liao', 'Michael Cheung']",poster,"['Automated invariance testing', 'sparse linear layers', 'feature selection', 'neural networks']","Machine learning testing and evaluation are largely overlooked by the community. In many cases, the only way to conduct testing is through formula-based scores, e.g., accuracy, f1, etc. However, these simple statistical scores cannot fully represent the performance of ML model. Therefore, new testing frameworks are attracting more attention. In this work, we propose a novel invariance testing approach that does not utilise traditional statistical scores. Instead, we train a series of sparse linear layers which are more easily to be compared due to their sparsity. We then use different divergence functions to numerically compare them and fuse the difference scores into a visual matrix. Additionally, testing using sparse linear layers allows us to conduct a novel testing oracle: associativity: by comparing merged weights and weights obtained by combined augmentation. Finally, we assess whether a model is invariant by checking the visual matrix, the associativity, and its sparse layers. We show that by using our testing framework, inter-rater reliability can be significantly improved.",https://api.openreview.net/pdf/2591fda8c13b39ceb935e852fc99b552d0ddc574.pdf,zero_few-shot;transformer;sparse;augmentation;llm,https://scholar.google.com/scholar?q=Automated+Invariance+Testing+for+Machine+Learning+Models+Using+Sparse+Linear+Layers
Are Vision Transformers Robust to Spurious Correlations ?,2022,ICML,"['Soumya Suvra Ghosal', 'Yifei Ming', 'Yixuan Li']",poster,[],"Deep neural networks may be susceptible to learning spurious correlations that hold on average but not in atypical test samples. As with the recent emergence of vision transformer (ViT) models, it remains underexplored how spurious correlations are manifested in such architectures. In this paper, we systematically investigate the robustness of vision transformers to spurious correlations on three challenging benchmark datasets and compare their performance with popular CNNs. Our study reveals that when pre-trained on a sufficiently large dataset, ViT models are more robust to spurious correlations than CNNs. Key to their success is the ability to generalize better from the examples where spurious correlations do not hold. ",https://api.openreview.net/pdf/36aac57b60c8cde28b9ed16a6827988a46031925.pdf,graph;transformer;llm,https://scholar.google.com/scholar?q=Are+Vision+Transformers+Robust+to+Spurious+Correlations+?
Linear Connectivity Reveals Generalization Strategies,2022,ICML,"['Jeevesh Juneja', 'Rachit Bansal', 'Kyunghyun Cho', 'João Sedoc', 'Naomi Saphra']",poster,"['loss landscapes', 'OOD generalization', 'NLI', 'text classification', 'loss surfaces', 'transfer learning', 'challenge sets', 'NLP']","It is widely accepted in the mode connectivity literature that when two neural networks are trained similarly on the same data, they are connected by a path through parameter space over which test set accuracy is maintained. Under some circumstances, including transfer learning from pretrained models, these paths are presumed to be linear. In contrast to existing results, we find that among text classifiers (trained on MNLI, QQP, and CoLA), some pairs of finetuned models have large barriers of increasing loss on the linear paths between them. On each task, we find distinct clusters of models which are linearly connected on the test loss surface, but are disconnected from models outside the cluster---models that occupy separate basins on the surface. By measuring performance on existing diagnostic datasets, we find that these clusters correspond to different generalization strategies: one cluster behaves like a bag of words model under domain shift, while another cluster uses syntactic heuristics. Our work demonstrates how the geometry of the loss surface can guide models towards different heuristic functions.",https://api.openreview.net/pdf/410119f42a0169f646a38e2e72f1415e840804b1.pdf,transfer learning;llm,https://scholar.google.com/scholar?q=Linear+Connectivity+Reveals+Generalization+Strategies
A Density Functional Recommendation Approach for Accurate Predictions of Vertical Spin Splitting of Transition Metal Complexes,2022,ICML,"['Chenru Duan', 'Aditya Nandy', 'Heather Kulik']",oral,"['density functional theory', 'transition metal chemistry', 'recommender', 'high throughput computation']","Both conventional and machine learning-based density functional approximations (DFAs) have emerged as versatile approaches for virtual high-throughput screening and chemical discovery. To date, however, no single DFA is universally accurate for different chemical spaces. This DFA sensitivity is particularly high for open-shell transition-metal-containing systems, where strong static correlation may dominate. With electron density fitting and transfer learning, we build a DFA recommender that selects the DFA with the lowest expected error in a system-dependent manner. We demonstrate this recommender approach on the prediction of vertical spin-splitting energies (i.e., the electronic energy difference between the high-spin and low-spin state) of challenging transition metal complexes. This recommender yields relatively small errors (i.e., 2.1 kcal/mol) for transition metal chemistry and captures the distributions of the DFAs that are most likely to be accurate.",https://api.openreview.net/pdf/ac008f1ab6d19ff401199133fb59f952e56a841b.pdf,graph;zero_few-shot;meta-learning;transfer learning;llm,https://scholar.google.com/scholar?q=A+Density+Functional+Recommendation+Approach+for+Accurate+Predictions+of+Vertical+Spin+Splitting+of+Transition+Metal+Complexes
Building a Subspace of Policies for Scalable Continual Learning,2022,ICML,"['Jean-Baptiste Gaya', 'Thang Doan', 'Lucas Caccia', 'Laure Soulier', 'Ludovic Denoyer', 'Roberta Raileanu']",poster,"['continual learning', 'deep reinforcement learning']","The ability to continuously acquire new knowledge and skills is crucial for autonomous agents. However, existing methods are typically based on either fixed-size models that cannot capture many diverse behaviors, or growing-size models that scale poorly with the number of tasks. In this paper, we introduce Continual Subspace of Policies (CSP), a method that iteratively learns a subspace of policies in the continual reinforcement learning setting where tasks are presented sequentially. The subspace's high expressivity allows our method to strike a good balance between stability (i.e. not forgetting prior tasks) and plasticity (i.e. learning new tasks), while the number of parameters grows sublinearly with the number of tasks. In addition, CSP displays good transfer, being able to quickly adapt to new tasks including combinations of previously seen ones without additional training. Finally, CSP outperforms state-of-the-art methods on a wide range of scenarios in two different domains. An interactive visualization of the subspace can be found at https://continual-subspace-policies-streamlit-app-gofujp.streamlitapp.com/.",https://api.openreview.net/pdf/42a4f44d4c6e176218bfede6407fdccffde87f01.pdf,reinforcement learning;zero_few-shot;transfer learning;active learning;llm,https://scholar.google.com/scholar?q=Building+a+Subspace+of+Policies+for+Scalable+Continual+Learning
Representation Gap in Deep Reinforcement Learning,2022,ICML,"['Qiang He', 'Huangyuan Su', 'Jieyu Zhang', 'Xinwen Hou']",poster,"['deep reinforcement learning', 'representation learning', 'policy evaluation']","Deep reinforcement learning gives the promise that an agent learns good policy from high-dimensional information. Whereas representation learning removes irrelevant and redundant information and retains pertinent information. We consider the representation capacity of action value function and theoretically reveal its inherent property, representation gap with its target action value function. This representation gap is favorable. However, through illustrative experiments, we show that the representation of action value function grows similarly compared with its target value function, i.e. the undesirable inactivity of the representation gap (representation overlap). Representation overlap results in a loss of representation capacity, which further leads to sub-optimal learning performance. To activate the representation gap, we propose a simple but effective framework Policy Optimization from Preventing Representation Overlaps (POPRO), which regularizes the policy evaluation phase through differing the representation of action value function from its target. We also provide the convergence rate guarantee of POPRO. We evaluate POPRO on gym continuous control suites. The empirical results show that POPRO using pixel inputs outperforms or parallels the sample-efficiency of methods that use state-based features. ",https://api.openreview.net/pdf/1121ce32c14cc428e23407d74c0e13524533488a.pdf,reinforcement learning;optimization;representation;llm,https://scholar.google.com/scholar?q=Representation+Gap+in+Deep+Reinforcement+Learning
PGT: a prompt based generative transformer for the patent domain,2022,ICML,"['Dimitrios Christofidellis', 'Antonio Berrios Torres', 'Ashish Dave', 'Manuel Roveri', 'Kristin Schmidt', 'Sarath Swaminathan', 'Hans Vandierendonck', 'Dmitry Zubarev', 'Matteo Manica']",poster,"['patent generation', 'nlp', 'transformers']","Patents are a valuable source of knowledge, but drafting them is a time-consuming and expensive task.
Methods that assist patent generation can provide a two-fold improvement as they can speed up the generation process and
suggest to the inventor ideas and claims.
Herein, influenced by recent advances in language modeling via multitask learning and prompt engineering, we present Patent Generative Transformer (PGT), a transformer-based language model trained to facilitate patent drafting.
Specifically, the model supports three tasks: part-of-patent generation, text infilling, and patent coherence evaluation.
PGT complements inventors and assures the fast and successful transition from their input to a coherent patent disclosure taking advantage of its multitasking nature.
We show how the model outperforms a collection of task-specific baselines on relevant metrics. We further test the quality of the generated text via blind testing by subject matter experts.
Finally, we explore a zero-shot extension of the model showing how to use PGT for generating domain-specific abstracts.",https://api.openreview.net/pdf/1e3e4b672739f588e5e22fdf72b3bf4929b4ac20.pdf,graph;zero_few-shot;transformer;generative model;metric;llm,https://scholar.google.com/scholar?q=PGT:+a+prompt+based+generative+transformer+for+the+patent+domain
Repository-Level Prompt Generation for Large Language Models of Code,2022,ICML,"['Disha Shrivastava', 'Hugo Larochelle', 'Daniel Tarlow']",poster,"['codex', 'large langauge models for source code', 'code-autocompletion', 'information retrieval', 'domain-knowledge']","With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex (Chen et al., 2021) used in GitHub Copilot), development of techniques where we can have the capability to introduce domain-specific knowledge in the prompt design process becomes important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using a set of rules. These rules allow us to take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn’t require any access to the weights of the LLM, making it applicable in cases where we only have a black- box access to the LLM. We conduct experiments on the task of single line code-autocompletion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our proposed rules gives up to 36% relative improvement over Codex, showing the quality of our proposed rules. Further, we show that when we train a model to select the best rule, we can achieve significant performance gains over Codex.",https://api.openreview.net/pdf/f9ebaebc24b79746fdc96a4c15cf9629ef68a352.pdf,zero_few-shot;generative model;llm,https://scholar.google.com/scholar?q=Repository-Level+Prompt+Generation+for+Large+Language+Models+of+Code
Are Large Pre-Trained Language Models Leaking Your Personal Information?,2022,ICML,"['Jie Huang', 'Hanyin Shao', 'Kevin Chang']",poster,"['Analysis of Language Models', 'Privacy', 'Memorization', 'Association']","Are Large Pre-Trained Language Models Leaking Your Personal Information? In this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to leaking personal information. Specifically, we query PLMs for email addresses with contexts of the email address or prompts containing the owner’s name. We find that PLMs do leak personal information due to memorization. However, since the models are weak at association, the risk of specific personal information being extracted by attackers is low. We hope this work could help the community to better understand the privacy risk of PLMs and bring new insights to make PLMs safe.",https://api.openreview.net/pdf/8a2aa6b979f1db6dbaee023550573a28275d8535.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Are+Large+Pre-Trained+Language+Models+Leaking+Your+Personal+Information?
Large Language Models are Zero-Shot Reasoners,2022,ICML,"['Takeshi Kojima', 'Shixiang Shane Gu', 'Machel Reid', 'Yutaka Matsuo', 'Yusuke Iwasawa']",poster,"['chain of thought (CoT)', 'zero-shot learning', 'multi-step reasoning', 'arithmetic', 'commonsense reasoning', 'prompting', 'large language models (LLMs)']","Chain of thought (CoT) prompting, a recent technique for eliciting multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning.While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding ``Let's think step by step'' before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks(Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with an 175B parameter Instruct-GPT, as well as similar magnitudes of improvements with  540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted through simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs.",https://api.openreview.net/pdf/e177eb6b1efa56359e6fa462481bb6a7a36c0642.pdf,graph;zero_few-shot;multi-task;llm,https://scholar.google.com/scholar?q=Large+Language+Models+are+Zero-Shot+Reasoners
Enhancing Multi-hop Connectivity for Graph Convolutional Networks,2022,ICML,"['Songtao Liu', 'Shixiong Jing', 'Tong Zhao', 'Zengfeng Huang', 'Dinghao Wu']",poster,"['Homophily', 'High-order Neihgbors']","Graph Convolutional Network and many of its variants are known to suffer from the dilemma between model depth and over-smoothing issues. Stacking layers of GCN usually lead to the exponential expansion of the receptive field (i.e., high-order neighbors). In order to incorporate the information from high-order neighbors to learn node representations without drastically increasing the number of graph convolution layers, we propose a simple and effective pre-processing technique to increase graph connectivity. Our approach selectively inserts connections between center nodes and informative high-order neighbors, with learnable weights to control the information flow through the connection. Experiments show that our approach improves the performance of GCN, and reduce the depth of GCNII without sacrificing its performance. Besides, our proposed homophily-based weight assignment can mitigate the effect of graph structural attacks.",https://api.openreview.net/pdf/8ef942ec850d38218f012e0f4da3d7ae7353d595.pdf,graph;zero_few-shot;representation;flow;llm,https://scholar.google.com/scholar?q=Enhancing+Multi-hop+Connectivity+for+Graph+Convolutional+Networks
Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming,2022,ICML,"['Hanlin Zhang', 'Ziyang Li', 'Jiani Huang', 'Mayur Naik', 'Eric Xing']",poster,[],"Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module equipped with provenance generates top-k proofs by deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning architecture efficiently learns weighted rules to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. Our experiments show that DSR-LM leads to improved logical reasoning of pre-trained LMs and outperforms a spectrum of competitive baselines even under systematic distribution shifts on sequence lengths.",https://api.openreview.net/pdf/9c6e266e2035d92d8759f6a972c766894aabe781.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Improved+Logical+Reasoning+of+Language+Models+via+Differentiable+Symbolic+Programming
Memorization in NLP Fine-tuning Methods,2022,ICML,"['Fatemehsadat Mireshghallah', 'Archit Uniyal', 'Tianhao Wang', 'David Evans', 'Taylor Berg-Kirkpatrick']",poster,"['Fine-tuning', 'Memorization', 'Privacy']","Large language models are shown to present privacy risks through memorization of training data, and several recent works have studied such risks for the pre-training phase. Little attention, however, has been given to the fine-tuning phase and it is not well understood how different fine-tuning  methods (such as fine-tuning the full model, the model head, and adapter) compare in terms of memorization risk. This presents increasing concern as the ``pre-train and fine-tune'' paradigm proliferates. In this paper, we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different. We observe that fine-tuning the head of the model has the highest susceptibility to attacks, whereas fine-tuning smaller adapters appears to be less vulnerable to known extraction attacks.",https://api.openreview.net/pdf/3f59c6505c2ebecf7bd6231cada6707a0eff3f2a.pdf,transformer;inference;llm,https://scholar.google.com/scholar?q=Memorization+in+NLP+Fine-tuning+Methods
Predicting Human Similarity Judgments Using Large Language Models,2022,ICML,"['Raja Marjieh', 'Ilia Sucholutsky', 'Theodore Sumers', 'Nori Jacoby', 'Thomas L. Griffiths']",poster,"['language', 'representational similarity', 'cognitive science', 'perception', 'computer vision', 'machine learning', 'NLP', 'pre-training']","Similarity judgments provide a well-established method for accessing mental representations, with applications in psychology, neuroscience and machine learning. However, collecting similarity judgments can be prohibitively expensive for naturalistic datasets as the number of comparisons grows quadratically in the number of stimuli. We leverage recent advances in language models and online recruitment, proposing an efficient domain-general procedure for predicting human similarity judgments based on text descriptions. Crucially, the number of descriptions required grows only linearly with the number of stimuli, drastically reducing the amount of data required. We test this procedure on six datasets of naturalistic images and show that our models outperform previous approaches based on visual information.",https://api.openreview.net/pdf/f4b3551c3ba8921e09968ae64a0f016a0896ae6b.pdf,representation;online learning;llm,https://scholar.google.com/scholar?q=Predicting+Human+Similarity+Judgments+Using+Large+Language+Models
"Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision",2022,ICML,"['Yufeng Cui', 'Lichen Zhao', 'Feng Liang', 'Yangguang Li', 'Jing Shao']",poster,"['Contrastive Language-Image Pre-training', 'Benchmark', 'Zero-shot Learning']","Contrastive Language-Image Pretraining (CLIP) has emerged as a novel paradigm to learn visual models from language supervision.  While researchers continue to push the frontier of CLIP, reproducing these works remains challenging. This is because researchers do not choose consistent training recipes and even use different data, hampering the fair comparison between different methods. In this work, we propose CLIP-benchmark, a first attempt to evaluate, analyze, and benchmark CLIP and its variants.  We conduct a comprehensive analysis of three key factors: data, supervision, and model architecture. We find considerable intuitive or counter-intuitive insights:  (1). Data quality has a significant impact on performance.  (2). Certain supervision has different effects for Convolutional Networks (ConvNets) and Vision Transformers (ViT).  Applying more proper supervision can effectively improve the performance of CLIP. (3). Curtailing the text encoder reduces the training cost but not much affect the final performance. Moreover, we further combine DeCLIP with FILIP, bringing us the strongest variant DeFILIP. The CLIPbenchmark is released at: https://github.com/Sense-GVT/DeCLIP for future CLIP research.",https://api.openreview.net/pdf/9b5067a9cb479cc1c8459d2410d932abdb80e24e.pdf,graph;transformer;contrastive learning;llm,"https://scholar.google.com/scholar?q=Democratizing+Contrastive+Language-Image+Pre-training:+A+CLIP+Benchmark+of+Data,+Model,+and+Supervision"
PowerGraph: Using neural networks and principal components to determine multivariate statistical power trade-offs,2022,ICML,"['Ajinkya K Mulay', 'Sean P Lane', 'Erin Hennes']",poster,"['statistical power', 'transfer learning']","Statistical power estimation for studies with multiple model parameters is inherently a multivariate problem. Power for individual parameters of interest cannot be reliably estimated univariately since correlation and variance explained relative to one parameter will impact the power for another parameter, all usual univariate considerations being equal. Explicit solutions in such cases, especially for models with many parameters, are either impractical or impossible to solve, leaving researchers to the prevailing method of simulating power. However, the point estimates for a vector of model parameters are uncertain, and the impact of inaccuracy is unknown. In such cases, sensitivity analysis is recommended such that multiple combinations of possible observable parameter vectors are simulated to understand power trade-offs. A limitation to this approach is that it is computationally expensive to generate sufficient sensitivity combinations to accurately map the power trade-off function in increasingly high-dimensional spaces for the models that social scientists estimate. This paper explores the efficient estimation and graphing of statistical power for a study over varying model parameter combinations. We propose a simple and generalizable machine learning inspired solution to cut the computational cost to less than 10% of the brute force method while providing F1 scores above 90%. We further motivate the impact of transfer learning in learning power manifolds across varying distributions.",https://api.openreview.net/pdf/984265a4cfafcfe5a5a2f97ccdb365e5e5eb5016.pdf,graph;transfer learning;llm,https://scholar.google.com/scholar?q=PowerGraph:+Using+neural+networks+and+principal+components+to+determine+multivariate+statistical+power+trade-offs
From Kepler to Newton: Explainable AI for Science Discovery,2022,ICML,"['Zelong Li', 'Jianchao Ji', 'Yongfeng Zhang']",poster,"['Explainable AI', 'Science Discovery', 'Scientific Method', 'Technological Singularity', 'Human and Nature']","The Observation --- Hypothesis --- Prediction --- Experimentation loop paradigm for scientific research has been practiced by researchers for years towards scientific discoveries. However, with data explosion in both mega-scale and milli-scale research, it has been sometimes very difficult to manually analyze the data and propose new hypotheses to drive the cycle for scientific discovery.
In this paper, we discuss the role of Explainable AI in scientific discovery process by demonstrating an Explainable AI-based paradigm for science discovery. The key is to use Explainable AI to help derive data or model interpretations, hypotheses, as well as scientific discoveries or insights. We show how computational and data-intensive methodology---together with experimental and theoretical methodology---can be seamlessly integrated for scientific research. To demonstrate the AI-based science discovery process, and to pay our respect to some of the greatest minds in human history, we show how Kepler's laws of planetary motion and Newton's law of universal gravitation can be rediscovered by (Explainable) AI based on Tycho Brahe's astronomical observation data, whose works were leading the scientific revolution in the 16-17th century. This work also highlights the important role of Explainable AI (as compared to Blackbox AI) in science discovery to help humans prevent or better prepare for the possible technological singularity that may happen in the future, since science is not only about the know how, but also the know why.",https://api.openreview.net/pdf/ab0ae14763754c5ae34a50a68f1303185d8ed8a9.pdf,llm,https://scholar.google.com/scholar?q=From+Kepler+to+Newton:+Explainable+AI+for+Science+Discovery
Weakly Supervised Inversion of Multi-physics Data for Geophysical Properties,2022,ICML,"['Shihang Feng', 'Peng Jin', 'Yinpeng Chen', 'Xitong Zhang', 'Zicheng Liu', 'David Alumbaugh', 'Michael Commer', 'Youzuo Lin']",poster,"['Multi-physics Inversion', 'Weakly Supervised', 'Pseudo Labels']","Multi-physics inversion plays a critical role in geophysics. It has been widely used to simultaneously infer various geophysical properties~(such as velocity and conductivity). Among those inversion problems, some are explicitly governed by partial differential equations~(PDEs), while others are not. Without explicit governing equations, conventional physical-based inversion techniques are not feasible and data-driven inversion requires expensive full labels. To overcome this issue, we proposed a new data-driven multi-physics inversion technique with extremely weak supervision. Our key finding is that the pseudo labels can be constructed by learning the local relationship among geophysical properties at very sparse locations. We explore the multi-physics inversion problem from two distinct measurements~(seismic and electromagnetic data) to three geophysical properties~(velocity, conductivity, and $\mathrm{CO_2}$ saturation) with synthetic data based on the Kimberlina storage reservoir in California.  Our results show that we are able to invert for properties without explicit governing equations. Moreover, the labeled data on three geophysical properties can be significantly reduced by 50 times~(from 100 down to only 2 locations).",https://api.openreview.net/pdf/cda488a8bd7734adfe75d066f569b88ab1903f16.pdf,zero_few-shot;sparse;llm,https://scholar.google.com/scholar?q=Weakly+Supervised+Inversion+of+Multi-physics+Data+for+Geophysical+Properties
Featurizations Matter: A Multiview Contrastive Learning Approach to Molecular Pretraining,2022,ICML,"['Yanqiao Zhu', 'Dingshuo Chen', 'Yuanqi Du', 'Yingze Wang', 'Qiang Liu', 'Shu Wu']",poster,[],"Molecular representation learning, which aims to automate feature learning for molecules, is a vital task in computational chemistry and drug discovery. Despite rapid advances in molecular pretraining models with various types of featurizations, from SMILES strings, 2D graphs to 3D geometry, there is a paucity of research on how to utilize different molecular featurization techniques to obtain better representations. To bridge that gap, we present a novel multiview contrastive learning approach dubbed MEMO in this paper.
Our pretraining framework, in particular, is capable of learning from four basic but nontrivial featurizations of molecules and adaptively learning to optimize the combinations of featurization techniques for different downstream tasks. Extensive experiments on a broad range of molecular property prediction benchmarks show that our MEMO outperforms state-of-the-art baselines and also yields reasonable an interpretation of molecular featurizations weights in accordance with chemical knowledge.",https://api.openreview.net/pdf/34b5b56d1a12e0014725942393eb80c896dd68ba.pdf,graph;representation;adaptive;contrastive learning;3d;llm,https://scholar.google.com/scholar?q=Featurizations+Matter:+A+Multiview+Contrastive+Learning+Approach+to+Molecular+Pretraining
DEQGAN: Learning the Loss Function for PINNs with Generative Adversarial Networks,2022,ICML,"['Blake Bullwinkel', 'Dylan Randle', 'Pavlos Protopapas', 'David Sondak']",poster,"['differential equations', 'generative adversarial networks', 'GANs', 'physics-informed neural networks', 'deep learning']","Solutions to differential equations are of significant scientific and engineering relevance. Physics-Informed Neural Networks (PINNs) have emerged as a promising method for solving differential equations, but they lack a theoretical justification for the use of any particular loss function. This work presents Differential Equation GAN (DEQGAN), a novel method for solving differential equations using generative adversarial networks to ""learn the loss function"" for optimizing the neural network. Presenting results on a suite of twelve ordinary and partial differential equations, including the nonlinear Burgers', Allen-Cahn, Hamilton, and modified Einstein's gravity equations, we show that DEQGAN can obtain multiple orders of magnitude lower mean squared errors than PINNs that use $L_2$, $L_1$, and Huber loss functions. We also show that DEQGAN achieves solution accuracies that are competitive with popular numerical methods. Finally, we present two methods to improve the robustness of DEQGAN to different hyperparameter settings.",https://api.openreview.net/pdf/b3de9caf10b853a2960729a5ed8f742394900e20.pdf,graph;zero_few-shot;generative model;online learning;llm,https://scholar.google.com/scholar?q=DEQGAN:+Learning+the+Loss+Function+for+PINNs+with+Generative+Adversarial+Networks
Target-aware Molecular Graph Generation,2022,ICML,"['Cheng Tan', 'Zhangyang Gao', 'Stan Z. Li']",poster,"['drug discovery', 'molecular generation']","Generating molecules with desired biological activities has attracted growing attention in drug discovery. Previous molecular generation models are designed as chemocentric methods that hardly consider the drug-target interaction, limiting their practical applications. In this paper, we aim to generate molecular drugs in a target-aware manner that bridges biological activity and molecular design. To solve this problem, we compile a benchmark dataset from several publicly available datasets and build baselines in a unified framework. Building on the recent advantages of flow-based molecular generation models, we propose SiamFlow, which forces the flow to fit the distribution of target sequence embeddings in latent space. Specifically, we employ an alignment loss and a uniform loss to bring target sequence embeddings and drug graph embeddings into agreements while avoiding collapse. Furthermore, we formulate the alignment into a one-to-many problem by learning spaces of target sequence embeddings. Experiments quantitatively show that our proposed method learns meaningful representations in the latent space toward the target-aware molecular graph generation and provides an alternative approach to bridge biology and chemistry in drug discovery.",https://api.openreview.net/pdf/567131bab8db193eedbb8d443fb8a8db43ab930b.pdf,graph;zero_few-shot;transformer;representation;generative model;flow;llm,https://scholar.google.com/scholar?q=Target-aware+Molecular+Graph+Generation
Bias in the Benchmark: Systematic experimental errors in bioactivity databases confound multi-task and meta-learning algorithms,2022,ICML,"['Leo Klarner', 'Michael Reutlinger', 'Torsten Schindler', 'Charlotte Deane', 'Garrett Morris']",poster,[],"There is considerable interest in employing deep learning algorithms to predict pharmaceutically relevant properties of small molecules. To overcome the issues inherent in this low-data regime, researchers are increasingly exploring multi-task and meta-learning algorithms that leverage sets of related biochemical and toxicological assays to learn robust and generalisable representations.
However, we show that the data from which commonly used multi-task benchmarks are derived often exhibits systematic experimental errors that lead to confounding statistical dependencies across tasks. Representation learning models that aim to acquire an inductive bias in this domain risk compounding these biases and may overfit to patterns that are counterproductive to many downstream applications of interest. We investigate to what extent these issues are reflected in the molecular embeddings learned by multi-task graph neural networks and discuss methods to address this pathology.",https://api.openreview.net/pdf/0b5556e97220011e4ae29b5de1308b76c0689c85.pdf,graph;zero_few-shot;representation;meta-learning;multi-task;llm,https://scholar.google.com/scholar?q=Bias+in+the+Benchmark:+Systematic+experimental+errors+in+bioactivity+databases+confound+multi-task+and+meta-learning+algorithms
Plex: Towards Reliability using Pretrained Large Model Extensions,2022,ICML,"['Dustin Tran', 'Jeremiah Zhe Liu', 'Michael W Dusenberry', 'Du Phan', 'Mark Collier', 'Jie Ren', 'Kehang Han', 'Zi Wang', 'Zelda E Mariet', 'Huiyi Hu', 'Neil Band', 'Tim G. J. Rudner', 'Karan Singhal', 'Zachary Nado', 'Joost van Amersfoort', 'Andreas Kirsch', 'Rodolphe Jenatton', 'Nithum Thain', 'Honglin Yuan', 'E. Kelly Buchanan', 'Kevin Patrick Murphy', 'D. Sculley', 'Yarin Gal', 'Zoubin Ghahramani', 'Jasper Snoek', 'Balaji Lakshminarayanan']",oral,"['reliability', 'large models', 'uncertainty', 'robustness', 'adaptation']","A recent trend in artificial intelligence is the use of pretrained models for language and vision tasks, which have achieved extraordinary performance but also puzzling failures. Probing these models' abilities in diverse ways is therefore critical to the field. In this paper, we explore the reliability of models, where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision-making tasks involving uncertainty (e.g., selective prediction, open set recognition), robust generalization (e.g., accuracy and proper scoring rules such as log-likelihood on in- and out-of-distribution datasets), and adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of tasks over 38 datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability, we developed ViT-Plex and T5-Plex, pretrained large model extensions (plex) for vision and language modalities, respectively. Plex greatly improves the state-of-the-art across reliability tasks, and simplifies the traditional protocol as it does not require designing scores or tuning the model for each individual task. We demonstrate scaling effects over model sizes up to 1B parameters and pretraining dataset sizes up to 4B examples. We also demonstrate Plex's capabilities on challenging tasks including zero-shot open set recognition, active learning, and uncertainty in conversational language understanding.",https://api.openreview.net/pdf/99b15deb9a66c6af60390b9fe676c27e5f368160.pdf,graph;zero_few-shot;transformer;active learning;llm,https://scholar.google.com/scholar?q=Plex:+Towards+Reliability+using+Pretrained+Large+Model+Extensions
Evaluating Model Robustness to Patch Perturbations,2022,ICML,"['Jindong Gu', 'Volker Tresp', 'Yao Qin']",poster,"['Natural Patch Corruption', 'Adversarial Patch', 'Vision Transformer']","Recent advances in Vision Transformer (ViT) have demonstrated its impressive performance in image classification, which makes it a promising alternative to Convolutional Neural Network (CNN). Unlike CNNs, ViT represents an input image as a sequence of image patches. The patch-based input image representation makes the following question interesting: How does ViT perform when individual input image patches are perturbed with natural corruptions or adversarial perturbations, compared to CNNs? In this submission, we propose to evaluate model robustness to patch-wise perturbations. Two types of patch perturbations are considered to model robustness. One is natural corruptions, which is to test models' robustness under distributional shifts. The other is adversarial perturbations, which are created by an adversary to specifically fool a model to make a wrong prediction. The experimental results on the popular CNNs and ViTs are surprising. We find that ViTs are more robust to naturally corrupted patches than CNNs, whereas they are more vulnerable to adversarial patches. Given the architectural traits of state-of-the-art ViTs and the interesting results above, we propose to add the robustness to natural patch corruption and adversarial patch attack into the robustness benchmark.",https://api.openreview.net/pdf/d1beaaf2ecfedd7f613e348b86d58b54fda3af18.pdf,zero_few-shot;transformer;representation;llm,https://scholar.google.com/scholar?q=Evaluating+Model+Robustness+to+Patch+Perturbations
Statistical Learning and Inverse Problems: A Stochastic Gradient Approach,2022,NIPS,"['Yuri Fonseca', 'Yuri Saporito']",poster,"['Statistical Learning', 'Inverse Problems', 'Stochastic Gradient Descent']","Inverse problems are paramount in Science and Engineering. In this paper, we consider the setup of Statistical Inverse Problem (SIP) and demonstrate how Stochastic Gradient Descent (SGD) algorithms can be used to solve linear SIP. We provide consistency and finite sample bounds for the excess risk. We also propose a modification for the SGD algorithm where we leverage machine learning methods to smooth the stochastic gradients and improve empirical performance. We exemplify the algorithm in a setting of great interest nowadays: the Functional Linear Regression model. In this case we consider a synthetic data example and a classification problem for predicting the main activity of bitcoin addresses based on their balances. ",https://api.openreview.net/pdf/3d17507f77d113bf831e8f163a55a9eb448a02fc.pdf,graph;llm,https://scholar.google.com/scholar?q=Statistical+Learning+and+Inverse+Problems:+A+Stochastic+Gradient+Approach
Modeling Transitivity and Cyclicity in Directed Graphs via Binary Code Box Embeddings,2022,NIPS,"['Dongxu Zhang', 'Michael Boratko', 'Cameron N Musco', 'Andrew McCallum']",poster,"['graph representation learning', 'geometric representation learning', 'directed graphs', 'cyclic graphs', 'transitivity']","Modeling directed graphs with differentiable representations is a fundamental requirement for performing machine learning on graph-structured data. Geometric embedding models (e.g. hyperbolic, cone, and box embeddings) excel at this task, exhibiting useful inductive biases for directed graphs. However, modeling directed graphs that both contain cycles and some element of transitivity, two properties common in real-world settings, is challenging. Box embeddings, which can be thought of as representing the graph as an intersection over some learned super-graphs, have a natural inductive bias toward modeling transitivity, but (as we prove) cannot model cycles. To this end, we propose binary code box embeddings, where a learned binary code selects a subset of graphs for intersection. We explore several variants, including global binary codes (amounting to a union over intersections) and per-vertex binary codes (allowing greater flexibility) as well as methods of regularization. Theoretical and empirical results show that the proposed models not only preserve a useful inductive bias of transitivity but also have sufficient representational capacity to model arbitrary graphs, including graphs with cycles.",https://api.openreview.net/pdf/e304979bc345e5d73c4d569abe2a8ec9c9f2994a.pdf,graph;zero_few-shot;representation;metric;llm,https://scholar.google.com/scholar?q=Modeling+Transitivity+and+Cyclicity+in+Directed+Graphs+via+Binary+Code+Box+Embeddings
"A Practical, Progressively-Expressive GNN",2022,NIPS,"['Lingxiao Zhao', 'Neil Shah', 'Leman Akoglu']",poster,"['GNN', 'k-WL', 'expressiveness']","Message passing neural networks (MPNNs) have become a dominant flavor of graph neural networks (GNNs) in recent years. Yet, MPNNs come with notable limitations; namely, they are at most as powerful as the 1-dimensional Weisfeiler-Leman (1-WL) test in distinguishing graphs in a graph isomorphism testing frame-work. To this end, researchers have drawn inspiration from the k-WL hierarchy to develop more expressive GNNs. However, current k-WL-equivalent GNNs are not practical for even small values of k, as k-WL becomes combinatorially more complex as k grows. At the same time, several works have found great empirical success in graph learning tasks without highly expressive models, implying that chasing expressiveness with a “coarse-grained ruler” of expressivity like k-WL is often unneeded in practical tasks. To truly understand the expressiveness-complexity tradeoff, one desires a more “fine-grained ruler,” which can more gradually increase expressiveness. Our work puts forth such a proposal: Namely, we first propose the (k, c)(≤)-SETWL hierarchy with greatly reduced complexity from k-WL, achieved by moving from k-tuples of nodes to sets with ≤k nodes defined over ≤c connected components in the induced original graph. We show favorable theoretical results for this model in relation to k-WL, and concretize it via (k, c)(≤)-SETGNN, which is as expressive as (k, c)(≤)-SETWL. Our model is practical and progressively-expressive, increasing in power with k and c. We demonstrate effectiveness on several benchmark datasets, achieving several state-of-the-art results with runtime and memory usage applicable to practical graphs. We open source our implementation at https://github.com/LingxiaoShawn/KCSetGNN.
",https://api.openreview.net/pdf/e7ba2a91c4cbd3e13136135b80b4be2d84c2ca31.pdf,graph;llm,"https://scholar.google.com/scholar?q=A+Practical,+Progressively-Expressive+GNN"
Learning the Structure of Large Networked Systems Obeying Conservation Laws,2022,NIPS,"['Anirudh Rayas', 'Rajasekhar Anguluri', 'Gautam Dasarathy']",poster,"['Structure Learning', 'Networked Systems', 'Conservation Laws', 'Gaussian Graphical models', 'Sparsistency', 'High dimensional regime']","Many networked systems such as electric networks, the brain, and social networks of opinion dynamics are known to obey conservation laws. Examples of this phenomenon include the Kirchoff laws in electric networks and opinion consensus in social networks. Conservation laws in networked systems are modeled as balance equations of the form $X = B^\ast Y$, where the sparsity pattern of $B^\ast \in \mathbb{R}^{p\times p}$ captures the connectivity of the network on $p$ nodes, and  $Y, X \in \mathbb{R}^p$ are vectors of ''potentials'' and ''injected flows'' at the nodes respectively. The node potentials $Y$ cause flows across edges which aim to balance out the potential difference, and the flows $X$ injected at the nodes are extraneous to the network dynamics. In several practical systems, the network structure is often unknown and needs to be estimated from data to facilitate modeling, management, and control. To this end, one has access to samples of the node potentials $Y$, but only the statistics of the node injections $X$. Motivated by this important problem, we study the estimation of the sparsity structure of the matrix $B^\ast$ from $n$ samples of $Y$ under the assumption that the node injections $X$ follow a Gaussian distribution with a known covariance $\Sigma_X$. We propose a new $\ell_{1}$-regularized maximum likelihood estimator for tackling this problem in the high-dimensional regime where the size of the network may be vastly larger than the number of samples $n$. We show that this optimization problem is convex in the objective and admits a unique solution. Under a new mutual incoherence condition, we establish sufficient conditions on the triple $(n,p,d)$ for which exact sparsity recovery of $B^\ast$ is possible with high probability; $d$ is the degree of the underlying graph. We also establish guarantees for the recovery of $B^\ast$ in the element-wise maximum, Frobenius, and operator norms. Finally, we complement these theoretical results with experimental validation of the performance of the proposed estimator on synthetic and real-world data.",https://api.openreview.net/pdf/42fed3f6fd5d6f1c1d556fd96b3b26bc843d2e6a.pdf,graph;optimization;zero_few-shot;flow;llm,https://scholar.google.com/scholar?q=Learning+the+Structure+of+Large+Networked+Systems+Obeying+Conservation+Laws
S4ND: Modeling Images and Videos as Multidimensional Signals with State Spaces,2022,NIPS,"['Eric Nguyen', 'Karan Goel', 'Albert Gu', 'Gordon Downs', 'Preey Shah', 'Tri Dao', 'Stephen Baccus', 'Christopher Ré']",poster,"['Deep learning', 'computer vision', 'state space model', 'S4']","Visual data such as images and videos are typically modeled as discretizations of inherently continuous, multidimensional signals.  Existing continuous-signal models attempt to exploit this fact by modeling the underlying signals of visual (e.g., image) data directly. However, these models have not yet been able to achieve competitive performance on practical vision tasks such as large-scale image and video classification. Building on a recent line of work on deep state space models (SSMs), we propose \method, a new multidimensional SSM layer that extends the continuous-signal modeling ability of SSMs to multidimensional data including images and videos. We show that S4ND can model large-scale visual data in $1$D, $2$D, and $3$D as continuous multidimensional signals and demonstrates strong performance by simply swapping Conv2D and self-attention layers with \method\ layers in existing state-of-the-art models. On ImageNet-1k, \method\ exceeds the performance of a Vision Transformer baseline by $1.5\%$ when training with a $1$D sequence of patches, and matches ConvNeXt when modeling images in $2$D. For videos, S4ND improves on an inflated $3$D ConvNeXt in activity classification on HMDB-51 by $4\%$. S4ND implicitly learns global, continuous convolutional kernels that are resolution invariant by construction, providing an inductive bias that enables generalization across multiple resolutions. By developing a simple bandlimiting modification to S4 to overcome aliasing, S4ND achieves strong zero-shot (unseen at training time) resolution performance, outperforming a baseline Conv2D by $40\%$ on CIFAR-10 when trained on $8 \times 8$ and tested on $32 \times 32$ images. When trained with progressive resizing, S4ND comes within $\sim 1\%$ of a high-resolution model while training $22\%$ faster.
",https://api.openreview.net/pdf/da9890fe32c938d002b93c65c5c382a737e1752f.pdf,zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=S4ND:+Modeling+Images+and+Videos+as+Multidimensional+Signals+with+State+Spaces
Training language models to follow instructions with human feedback,2022,NIPS,"['Long Ouyang', 'Jeffrey Wu', 'Xu Jiang', 'Diogo Almeida', 'Carroll Wainwright', 'Pamela Mishkin', 'Chong Zhang', 'Sandhini Agarwal', 'Katarina Slama', 'Alex Gray', 'John Schulman', 'Jacob Hilton', 'Fraser Kelton', 'Luke Miller', 'Maddie Simens', 'Amanda Askell', 'Peter Welinder', 'Paul Christiano', 'Jan Leike', 'Ryan Lowe']",poster,[],"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",https://api.openreview.net/pdf/12f896590d59191cc66b798c10ffaba467bf8af6.pdf,reinforcement learning;zero_few-shot;generative model;multimodal;llm,https://scholar.google.com/scholar?q=Training+language+models+to+follow+instructions+with+human+feedback
Self-Supervised Contrastive Pre-Training For Time Series via Time-Frequency Consistency,2022,NIPS,"['Xiang Zhang', 'Ziyuan Zhao', 'Theodoros Tsiligkaridis', 'Marinka Zitnik']",poster,"['time series', 'pre-training', 'contrastive learning', 'transfer learning', 'self-supervised learning']","Pre-training on time series poses a unique challenge due to the potential mismatch between pre-training and target domains, such as shifts in temporal dynamics, fast-evolving trends, and long-range and short-cyclic effects, which can lead to poor downstream performance. While domain adaptation methods can mitigate these shifts, most methods need examples directly from the target domain, making them suboptimal for pre-training. To address this challenge, methods need to accommodate target domains with different temporal dynamics and be capable of doing so without seeing any target examples during pre-training. Relative to other modalities, in time series, we expect that time-based and frequency-based representations of the same example are located close together in the time-frequency space. To this end, we posit that time-frequency consistency (TF-C) --- embedding a time-based neighborhood of an example close to its frequency-based neighborhood --- is desirable for pre-training. Motivated by TF-C, we define a decomposable pre-training model, where the self-supervised signal is provided by the distance between time and frequency components, each individually trained by contrastive estimation. We evaluate the new method on eight datasets, including electrodiagnostic testing, human activity recognition, mechanical fault detection, and physical status monitoring.  Experiments against eight state-of-the-art methods show that TF-C outperforms baselines by 15.4% (F1 score) on average in one-to-one settings (e.g., fine-tuning an EEG-pretrained model on EMG data) and by 8.4% (precision) in challenging one-to-many settings (e.g., fine-tuning an EEG-pretrained model for either hand-gesture recognition or mechanical fault prediction), reflecting the breadth of scenarios that arise in real-world applications. The source code and datasets are available at https://github.com/mims-harvard/TFC-pretraining.",https://api.openreview.net/pdf/9e766b5d14ed5b918e22dd4970e55ee719bbcaa3.pdf,graph;zero_few-shot;representation;contrastive learning;llm,https://scholar.google.com/scholar?q=Self-Supervised+Contrastive+Pre-Training+For+Time+Series+via+Time-Frequency+Consistency
Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization,2022,NIPS,"['Zijie Zhang', 'Yang Zhou', 'Xin Zhao', 'Tianshi Che', 'Lingjuan Lyu']",poster,"['Certified machine unlearning', 'randomized gradient smoothing', 'gradient quantization', 'theoretical guarantee', 'prompt unlearning']","The right to be forgotten calls for efficient machine unlearning techniques that make trained machine learning models forget a cohort of data. The combination of training and unlearning operations in traditional machine unlearning methods often leads to the expensive computational cost on large-scale data. This paper presents a prompt certified machine unlearning algorithm, PCMU, which executes one-time operation of simultaneous training and unlearning in advance for a series of machine unlearning requests, without the knowledge of the removed/forgotten data. First, we establish a connection between randomized smoothing for certified robustness on classification and randomized smoothing for certified machine unlearning on gradient quantization. Second, we propose a prompt certified machine unlearning model based on randomized data smoothing and gradient quantization. We theoretically derive the certified radius R regarding the data change before and after data removals and the certified budget of data removals about R. Last but not least, we present another practical framework of randomized gradient smoothing and quantization, due to the dilemma of producing high confidence certificates in the first framework. We theoretically demonstrate the certified radius R' regarding the gradient change, the correlation between two types of certified radii, and the certified budget of data removals about R'. ",https://api.openreview.net/pdf/1bb465dd4bd4e92ca20c6267bf92d11fa9b835aa.pdf,llm,https://scholar.google.com/scholar?q=Prompt+Certified+Machine+Unlearning+with+Randomized+Gradient+Smoothing+and+Quantization
Hyperparameter Sensitivity in Deep Outlier Detection: Analysis and a Scalable Hyper-Ensemble Solution,2022,NIPS,"['Xueying Ding', 'Lingxiao Zhao', 'Leman Akoglu']",poster,"['deep outlier detection', 'unsupervised model selection', 'ensemble learning']","Outlier detection (OD) literature exhibits numerous algorithms as it applies to diverse domains. However, given a new detection task, it is unclear how to choose an algorithm to use, nor how to set its hyperparameter(s) (HPs) in unsupervised settings. HP tuning is an ever-growing problem with the arrival of many new detectors based on deep learning, which usually come with a long list of HPs. Surprisingly, the issue of model selection in the outlier mining literature has been “the elephant in the room”; a significant factor in unlocking the utmost potential of deep methods, yet little said or done to systematically tackle the issue. In the first part of this paper, we conduct the first large-scale analysis on the HP sensitivity of deep OD methods, and through more than 35,000 trained models, quantitatively demonstrate that model selection is inevitable. Next, we design a HP-robust and scalable deep hyper-ensemble model called ROBOD that assembles models with varying HP configurations, bypassing the choice paralysis. Importantly, we introduce novel strategies to speed up ensemble training, such as parameter sharing, batch/simultaneous training, and data subsampling, that allow us to train fewer models with fewer parameters. Extensive experiments on both image and tabular datasets show that ROBOD achieves and retains robust, state-of-the-art detection performance as compared to its modern counterparts, while taking only 2-10% of the time by the naïve hyper-ensemble with independent training.",https://api.openreview.net/pdf/84c39ff0dacb8a5bc6588ac005867ef28d491356.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Hyperparameter+Sensitivity+in+Deep+Outlier+Detection:+Analysis+and+a+Scalable+Hyper-Ensemble+Solution
Autoformalization with Large Language Models,2022,NIPS,"['Yuhuai Wu', 'Albert Qiaochu Jiang', 'Wenda Li', 'Markus Norman Rabe', 'Charles E Staats', 'Mateja Jamnik', 'Christian Szegedy']",poster,"['Large language models', 'Autoformalization', 'Formal Math', 'miniF2F.']","Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence.
While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from~$29.6\%$ to~$35.2\%$.",https://api.openreview.net/pdf/51c4ad17de2b76b8e59b8d486a53165e6c650a4e.pdf,llm,https://scholar.google.com/scholar?q=Autoformalization+with+Large+Language+Models
Phase transitions in when feedback is useful,2022,NIPS,"['Lokesh Boominathan', 'Xaq Pitkow']",poster,"['Bayesian Inference', 'Predictive Coding', 'Efficient Coding', 'Linear Quadratic Gaussian']","Sensory observations about the world are invariably ambiguous. Inference about the world's latent variables is thus an important computation for the brain. However, computational constraints limit the performance of these computations. These constraints include energetic costs for neural activity and noise on every channel. Efficient coding is one prominent theory that describes how such limited resources can best be used. In one incarnation, this leads to a theory of predictive coding, where predictions are subtracted from signals, reducing the cost of sending something that is already known. This theory does not, however, account for the costs or noise associated with those predictions. Here we offer a theory that accounts for both feedforward and feedback costs, and noise in all computations. We formulate this inference problem as message-passing on a graph whereby feedback serves as an internal control signal aiming to maximize how well an inference tracks a target state while minimizing the costs of computation. We apply this novel formulation of inference as control to the canonical problem of inferring the hidden scalar state of a linear dynamical system with Gaussian variability. The best solution depends on architectural constraints, such as Dale's law, the ubiquitous law that each neuron makes solely excitatory or inhibitory postsynaptic connections. This biological structure can create asymmetric costs for feedforward and feedback channels. Under such conditions, our theory predicts the gain of optimal predictive feedback and how it is incorporated into the inference computation. We show that there is a non-monotonic dependence of optimal feedback gain as a function of both the computational parameters and the world dynamics, leading to phase transitions in whether feedback provides any utility in optimal inference under computational constraints.",https://api.openreview.net/pdf/977905110e60ab8814855e3f3bfac31475e23466.pdf,graph;optimization;inference;metric;llm,https://scholar.google.com/scholar?q=Phase+transitions+in+when+feedback+is+useful
A Fast Scale-Invariant Algorithm for Non-negative Least Squares with Non-negative Data,2022,NIPS,"['Jelena Diakonikolas', 'Chenghui Li', 'Swati Padmanabhan', 'Chaobing Song']",poster,"['acceleration', 'non-negative least squares', 'scale invariance']"," Nonnegative (linear) least square problems are a fundamental class of problems that is well-studied in statistical learning and for which solvers have been implemented in many of the standard programming languages used within the machine learning community. The existing off-the-shelf solvers view the non-negativity constraint in these problems as an obstacle and, compared to unconstrained least squares, perform additional effort to address it. However, in many of the typical applications, the data itself is nonnegative as well, and we show that the nonnegativity in this case makes the problem easier. In particular, while the worst-case dimension-independent oracle complexity of unconstrained least squares problems necessarily scales with one of the data matrix constants (typically the spectral norm) and these problems are solved to additive error, we show that nonnegative least squares problems with nonnegative data are solvable to  multiplicative error and with complexity that is independent of any matrix constants. The algorithm we introduce is accelerated and based on a primal-dual perspective. We further show how to provably obtain linear convergence using adaptive restart coupled with our method and demonstrate its effectiveness on large-scale data via numerical experiments. ",https://api.openreview.net/pdf/5905ec851bd8e3e288f574d73747b5eb6252b9f2.pdf,optimization;adaptive;llm,https://scholar.google.com/scholar?q=A+Fast+Scale-Invariant+Algorithm+for+Non-negative+Least+Squares+with+Non-negative+Data
When Does Differentially Private Learning Not Suffer in High Dimensions?,2022,NIPS,"['Xuechen Li', 'Daogao Liu', 'Tatsunori Hashimoto', 'Huseyin A Inan', 'Janardhan Kulkarni', 'YinTat Lee', 'Abhradeep Guha Thakurta']",poster,"['Differential Privacy', 'fine-tuning', 'DP convex optimization', 'pretrained models']","Large pretrained models can be fine-tuned with differential privacy to achieve performance approaching that of non-private models. A common theme in these results is the surprising observation that high-dimensional models can achieve favorable privacy-utility trade-offs. This seemingly contradicts known results on the model-size dependence of differentially private convex learning and raises the following research question: When does the performance of differentially private learning not degrade with increasing model size? We identify that the magnitudes of gradients projected onto subspaces is a key factor that determines performance. To precisely characterize this for private convex learning, we introduce a condition on the objective that we term restricted Lipschitz continuity and derive improved bounds for the excess empirical and population risks that are dimension- independent under additional conditions. We empirically show that in private fine-tuning of large language models, gradients obtained during fine-tuning are mostly controlled by a few principal components. This behavior is similar to conditions under which we obtain dimension-independent bounds in convex settings. Our theoretical and empirical results together provide a possible explanation for the recent success of large-scale private fine-tuning. Code to reproduce our results can be found at https://github.com/lxuechen/private-transformers/tree/main/examples/classification/spectral_analysis. ",https://api.openreview.net/pdf/2f3937729f5e56e328788ae652b67a4052e22982.pdf,zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=When+Does+Differentially+Private+Learning+Not+Suffer+in+High+Dimensions?
Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models,2022,NIPS,"['Manli Shu', 'Weili Nie', 'De-An Huang', 'Zhiding Yu', 'Tom Goldstein', 'Anima Anandkumar', 'Chaowei Xiao']",poster,"['Foundation Model', 'Prompt Tuning', 'Generalization', 'Out-of-Distribution Robustness']","Pre-trained vision-language models (e.g., CLIP) have shown promising zero-shot generalization in many downstream tasks with properly designed text prompts. Instead of relying on hand-engineered prompts, recent works learn prompts using the training data from downstream tasks. While effective, training on domain-specific data reduces a model's generalization capability to unseen new domains. In this work, we propose test-time prompt tuning (TPT), a method that can learn adaptive prompts on the fly with a single test sample. TPT optimizes the prompt by minimizing the entropy with confidence selection so that the model has consistent predictions across different augmented views of each test sample. In evaluating generalization to natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP by 3.6\% on average, surpassing previous prompt tuning approaches that require additional task-specific training data. In evaluating cross-dataset generalization with unseen categories, TPTperforms on par with the state-of-the-art approaches that use additional training data.",https://api.openreview.net/pdf/5c1757f721f0e7d61d6ec4e17c0ea05a2e70aedd.pdf,graph;zero_few-shot;adaptive;augmentation;llm,https://scholar.google.com/scholar?q=Test-Time+Prompt+Tuning+for+Zero-Shot+Generalization+in+Vision-Language+Models
Staircase Attention for Recurrent Processing of Sequences,2022,NIPS,"['Da JU', 'Stephen Roller', 'Sainbayar Sukhbaatar', 'Jason E Weston']",poster,[],"Attention mechanisms have become a standard tool for sequence modeling tasks, in particular by stacking self-attention layers over the entire input sequence as in the Transformer architecture. In this work we introduce a novel attention procedure called staircase attention that, unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence). Thus our model can trade off performance and compute, by increasing the amount of recurrence through time and depth. Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence. Further, it is shown to provide improved modeling power for the same size model (number of parameters) compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.",https://api.openreview.net/pdf/3e6423abfdf0023f9e6a41e2ca39a497cec42af4.pdf,transformer;llm,https://scholar.google.com/scholar?q=Staircase+Attention+for+Recurrent+Processing+of+Sequences
Exploring Length Generalization in Large Language Models,2022,NIPS,"['Cem Anil', 'Yuhuai Wu', 'Anders Johan Andreassen', 'Aitor Lewkowycz', 'Vedant Misra', 'Vinay Venkatesh Ramasesh', 'Ambrose Slone', 'Guy Gur-Ari', 'Ethan Dyer', 'Behnam Neyshabur']",poster,"['length generalization', 'multi-step reasoning', 'large language models', 'out-of-distribution generalization']","The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.",https://api.openreview.net/pdf/931be5faef555d0bfb975e64c857a49909a2727c.pdf,transformer;llm,https://scholar.google.com/scholar?q=Exploring+Length+Generalization+in+Large+Language+Models
Randomized Channel Shuffling: Minimal-Overhead Backdoor Attack Detection without Clean Datasets,2022,NIPS,"['Ruisi Cai', 'Zhenyu Zhang', 'Tianlong Chen', 'Xiaohan Chen', 'Zhangyang Wang']",poster,[],"Deep neural networks (DNNs) typically require massive data to train on, which is a hurdle for numerous practical domains. Facing the data shortfall, one viable option is to acquire domain-specific training data from external uncensored sources, such as open webs or third-party data collectors. However, the quality of such acquired data is often not rigorously scrutinized, and one cannot easily rule out the risk of `""poisoned"" examples being included in such unreliable datasets, resulting in unreliable trained models which pose potential risks to many high-stake applications. While existing options usually suffer from high computational costs or assumptions on clean data access, this paper attempts to detect backdoors for potential victim models with minimal prior knowledge. In particular, provided with a trained model, users are assumed to (1) have no prior knowledge of whether it is already poisoned, or what the target class/percentage of samples is poisoned, and (2) have no access to a clean sample set from the same training distribution, nor any trusted model trained on such clean data. To tackle this challenging scenario, we first observe the contrasting channel-level statistics between the backdoor trigger and clean image features, and consequently, how they can be differentiated by progressive channel shuffling. We then propose the randomized channel shuffling method for backdoor-targeted class detection, which requires only a few feed-forward passes. It thus incurs minimal overheads and demands no clean sample nor prior knowledge. We further explore a “full” clean data-free setting, where neither the target class detection nor the trigger recovery can access the clean data. Extensive experiments are conducted with three datasets (CIFAR-10,  GTSRB, Tiny ImageNet), three architectures (AlexNet, ResNet-20, SENet-18), and three attacks (BadNets, clean label attack, and WaNet). Results consistently endorse the effectiveness of our proposed technique in backdoor model detection,  with margins of 0.291 ～ 0.640 AUROC over the current state-of-the-arts. Codes are available at https://github.com/VITA-Group/Random-Shuffling-BackdoorDetect.",https://api.openreview.net/pdf/8a0c5fb7d856a9ca21433b90a5249ad9814429c8.pdf,graph;llm,https://scholar.google.com/scholar?q=Randomized+Channel+Shuffling:+Minimal-Overhead+Backdoor+Attack+Detection+without+Clean+Datasets
Differentially Private Graph Learning via Sensitivity-Bounded Personalized PageRank,2022,NIPS,"['Alessandro Epasto', 'Vahab Mirrokni', 'Bryan Perozzi', 'Anton Tsitsulin', 'Peilin Zhong']",poster,"['Differential privacy', 'Graph algorithms', 'Ranking', 'Personalized Page Rank']","Personalized PageRank (PPR) is a fundamental tool in unsupervised learning of graph representations such as node ranking, labeling, and graph embedding. However, while data privacy is one of the most important recent concerns, existing PPR algorithms are not designed to protect user privacy. PPR is highly sensitive to the input graph edges: the difference of only one edge may cause a big change in the PPR vector, potentially leaking private user data.

In this work, we propose an algorithm which outputs an approximate PPR and has provably bounded sensitivity to input edges. In addition, we prove that our algorithm achieves  similar accuracy to non-private algorithms when the input graph has large degrees. Our sensitivity-bounded PPR directly implies private algorithms for several tools of graph learning, such as, differentially private (DP) PPR ranking, DP node classification, and DP node embedding. To complement our theoretical analysis, we also empirically verify the practical performances of our algorithms.
",https://api.openreview.net/pdf/24282dd896c7889b3f5826d46c42bc145884b570.pdf,graph;representation;llm,https://scholar.google.com/scholar?q=Differentially+Private+Graph+Learning+via+Sensitivity-Bounded+Personalized+PageRank
Generating Training Data with Language Models: Towards Zero-Shot Language Understanding,2022,NIPS,"['Yu Meng', 'Jiaxin Huang', 'Yu Zhang', 'Jiawei Han']",poster,"['Zero-Shot Learning', 'Natural Language Understanding', 'Pretrained Language Models']","Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class.",https://api.openreview.net/pdf/2416ed5206a5bfe214d73b1a069cb219d8857eee.pdf,zero_few-shot;transformer;generative model;llm,https://scholar.google.com/scholar?q=Generating+Training+Data+with+Language+Models:+Towards+Zero-Shot+Language+Understanding
"Statistical, Robustness, and Computational Guarantees for Sliced Wasserstein Distances",2022,NIPS,"['Sloan Nietert', 'Ziv Goldfeld', 'Ritwik Sadhu', 'Kengo Kato']",poster,"['sliced optimal transport', 'Wasserstein distances', 'empirical convergence', 'robust statistics']","Sliced Wasserstein distances preserve properties of classic Wasserstein distances while being more scalable for computation and estimation in high dimensions. The goal of this work is to quantify this scalability from three key aspects: (i) empirical convergence rates; (ii) robustness to data contamination; and (iii) efficient computational methods. For empirical convergence, we derive fast rates with explicit dependence of constants on dimension, subject to log-concavity of the population distributions. For robustness, we characterize minimax optimal, dimension-free robust estimation risks, and show an equivalence between robust sliced 1-Wasserstein estimation and robust mean estimation. This enables lifting statistical and algorithmic guarantees available for the latter to the sliced 1-Wasserstein setting. Moving on to computational aspects, we analyze the Monte Carlo estimator for the average-sliced distance, demonstrating that larger dimension can result in faster convergence of the numerical integration error. For the max-sliced distance, we focus on a subgradient-based local optimization algorithm that is frequently used in practice, albeit without formal guarantees, and establish an $O(\epsilon^{-4})$ computational complexity bound for it. Our theory is validated by numerical experiments, which altogether provide a comprehensive quantitative account of the scalability question.",https://api.openreview.net/pdf/27335912ad69da32c2cebc47c8369ade66d4fce5.pdf,optimization;llm,"https://scholar.google.com/scholar?q=Statistical,+Robustness,+and+Computational+Guarantees+for+Sliced+Wasserstein+Distances"
Operative dimensions in unconstrained connectivity of recurrent neural networks,2022,NIPS,"['Renate Barbara Krause', 'Matthew Cook', 'Sepp Kollmorgen', 'Valerio Mante', 'Giacomo Indiveri']",poster,"['recurrent neural networks', 'computation through dynamics', 'dimensionality']","Recurrent Neural Networks (RNN) are commonly used models to study neural computation. However, a comprehensive understanding of how dynamics in RNN emerge from the underlying connectivity is largely lacking. Previous work derived such an understanding for RNN fulfilling very specific constraints on their connectivity, but it is unclear whether the resulting insights apply more generally. Here we study how network dynamics are related to network connectivity in RNN trained without any specific constraints on several tasks previously employed in neuroscience. Despite the apparent high-dimensional connectivity of these RNN, we show that a low-dimensional, functionally relevant subspace of the weight matrix can be found through the identification of \textit{operative} dimensions, which we define as components of the connectivity whose removal has a large influence on local RNN dynamics. We find that a weight matrix built from only a few operative dimensions is sufficient for the RNN to operate with the original performance, implying that much of the high-dimensional structure of the trained connectivity is functionally irrelevant. The existence of a low-dimensional, operative subspace in the weight matrix simplifies the challenge of linking connectivity to network dynamics and suggests that independent network functions may be placed in specific, separate subspaces of the weight matrix to avoid catastrophic forgetting in continual learning.",https://api.openreview.net/pdf/3be069e5a711eaa9cc3591946dd3c8aaec445e4e.pdf,graph;optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Operative+dimensions+in+unconstrained+connectivity+of+recurrent+neural+networks
Biologically plausible solutions for spiking networks with efficient coding,2022,NIPS,"['Veronika Koren', 'Stefano Panzeri']",poster,"['spiking neural networks', 'optimization', 'loss function', 'information', 'population code', 'E-I network', 'Generalized leaky integrate-and-fire (LIF) neuron', 'biological constraints']","Understanding how the dynamics of neural networks is shaped by the computations they perform is a fundamental question in neuroscience. 
Recently, the framework of efficient coding proposed a theory of how spiking neural networks can compute low-dimensional stimulus signals with high efficiency. Efficient spiking networks are based on time-dependent minimization of a loss function related to information coding with spikes. To inform the understanding of the function and dynamics of biological networks in the brain, however, the mathematical models have to be informed by biology and obey the same constraints as biological networks. Currently, spiking network models of efficient coding have been extended to include some features of biological plausibility, such as architectures with excitatory and inhibitory neurons. However, biological realism of efficient coding theories is still limited to simple cases and does not include  single neuron and network properties that are known to be key in biological circuits. Here, we revisit the theory of efficient coding with spikes to  develop spiking neural networks that are closer to biological circuits. Namely, we find a biologically plausible spiking model realizing efficient coding in the case of a generalized leaky integrate-and-fire network with excitatory and inhibitory units, equipped with fast and slow synaptic currents, local homeostatic currents such as spike-triggered adaptation, hyperpolarization-activated rebound current, heterogeneous firing thresholds and resets, heterogeneous postsynaptic potentials, and structured, low-rank connectivity. We show how the rank of E-E connectivity matrix shapes network responses.",https://api.openreview.net/pdf/7d8a3d747be5687ec8037bc57fc81b22163a7e53.pdf,optimization;zero_few-shot;low-rank;llm,https://scholar.google.com/scholar?q=Biologically+plausible+solutions+for+spiking+networks+with+efficient+coding
Fine-tuning language models to find agreement among humans with diverse preferences,2022,NIPS,"['Michiel A. Bakker', 'Martin J Chadwick', 'Hannah Sheahan', 'Michael Henry Tessler', 'Lucy Campbell-Gillingham', 'Jan Balaguer', 'Nat McAleese', 'Amelia Glaese', 'John Aslanides', 'Matthew Botvinick', 'Christopher Summerfield']",poster,"['large language models', 'LLMs', 'alignment', 'NLP', 'fine-tuning', 'reward modelling', 'preference modelling', 'human-centered AI']","Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a single ""generic"" user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g., ""should we raise taxes on the rich?""), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs ($>70\%$) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions ($>65\%$). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another.",https://api.openreview.net/pdf/fde9d1ed13707b7018f8ffa9dcf20ea356556b21.pdf,llm,https://scholar.google.com/scholar?q=Fine-tuning+language+models+to+find+agreement+among+humans+with+diverse+preferences
Deep Compression of Pre-trained Transformer Models,2022,NIPS,"['Naigang Wang', 'Chi-Chun Liu', 'Swagath Venkataramani', 'Sanchari Sen', 'Chia-Yu Chen', 'Kaoutar El Maghraoui', 'Viji Srinivasan', 'Leland Chang']",poster,"['Quantization', 'Sparsity', 'Pruning', 'Pre-trained', 'Transformer', 'Foundation Model', 'Inference', 'NLP', 'vision', 'speech', 'BERT', 'Wav2vec', 'ViT']","Pre-trained transformer models have achieved remarkable success in natural language processing (NLP) and have recently become competitive alternatives to Convolution Neural Networks (CNN) and Recurrent Neural Networks (RNN) in vision and speech tasks, respectively. Due to excellent computational efficiency and scalability, transformer models can be trained on exceedingly large amounts of data; however, model sizes can grow tremendously. As high performance, large-scale, and pre-trained transformer models become available for users to download and fine-tune for customized downstream tasks, the deployment of these models becomes challenging due to the vast amount of operations and large memory footprint. To address this challenge, we introduce methods to deeply compress pre-trained transformer models across three major application domains: NLP, speech, and vision. Specifically, we quantize transformer backbones down to 4-bit and further achieve 50% fine-grained structural sparsity on pre-trained BERT, Wav2vec2.0 and Vision Transformer (ViT) models to achieve 16x compression while maintaining model accuracy. This is achieved by identifying the critical initialization for quantization/sparsity aware fine-tuning, as well as novel techniques including quantizers with zero-preserving format and scheduled dropout. These hardware-friendly techniques need only to be applied in the fine-tuning phase for downstream tasks; hence, are especially suitable for acceleration and deployment of pre-trained transformer models.",https://api.openreview.net/pdf/66220839e1e2f836a25c7b5b3161d1b2d151beab.pdf,graph;transformer;llm,https://scholar.google.com/scholar?q=Deep+Compression+of+Pre-trained+Transformer+Models
Unsupervised Learning for Combinatorial Optimization with Principled Objective Relaxation,2022,NIPS,"['Haoyu Peter Wang', 'Nan Wu', 'Hang Yang', 'Cong Hao', 'Pan Li']",poster,"['learning for combinatorial optimization', 'model-based optimization', 'graph neural networks']","Using machine learning to solve combinatorial optimization (CO) problems is challenging, especially when the data is unlabeled. This work proposes an unsupervised learning framework for CO problems. Our framework follows the standard relaxation-plus-rounding approach and adopts neural networks to parameterize the relaxed solutions so that simple back-propagation can train them end-to-end. Our key contribution is the observation that if the relaxed objective satisfies entry-wise concavity, a low optimization loss guarantees the quality of the obtained integral solutions. This observation significantly generalizes the applicability of the previous framework inspired by Erdos' probabilistic method (Karalias & Loukas, 2020). Our framework is particularly suitable to guide the design of objective models in the applications where the objectives are not given explicitly while requiring being modeled and learned first. We evaluate our framework by solving a synthetic graph optimization problem, and two real-world applications including resource allocation in circuit design and approximate computing. Our framework largely outperforms the baselines based on reinforcement learning and Gumbel-softmax tricks. ",https://api.openreview.net/pdf/8152957f60d7736ad99a022d3f2de128aa5793de.pdf,reinforcement learning;graph;optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Unsupervised+Learning+for+Combinatorial+Optimization+with+Principled+Objective+Relaxation
GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale,2022,NIPS,"['Tim Dettmers', 'Mike Lewis', 'Younes Belkada', 'Luke Zettlemoyer']",poster,"['quantization', '8-bit', 'transformers', 'inference']","Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, {\bf LLM.int8()}. We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open source our software.",https://api.openreview.net/pdf/f693d62a49abe6edc4716f2a2f269502817d7f69.pdf,transformer;inference;llm,https://scholar.google.com/scholar?q=GPT3.int8():+8-bit+Matrix+Multiplication+for+Transformers+at+Scale
Learning low-dimensional generalizable natural features from retina using a U-net,2022,NIPS,"['Siwei Wang', 'Benjamin Hoshal', 'Elizabeth A de Laittre', 'Olivier Marre', 'Michael Berry', 'Stephanie Palmer']",poster,"['retina', 'efficient coding', 'convolutional neural network', 'synergy', 'natural scene']","Much of sensory neuroscience focuses on sensory features that are chosen by the experimenter because they are thought to be behaviorally relevant to the organism. However, it is not generally known what these features are in complex, natural scenes. This work focuses on using the retinal encoding of natural movies to determine the presumably behaviorally-relevant features that the brain represents. It is prohibitive to parameterize a natural movie and its respective retinal encoding fully. We use time within a natural movie as a proxy for the whole suite of features evolving across the scene. We then use a task-agnostic deep architecture, an encoder-decoder, to model the retinal encoding process and characterize its representation of ``time in the natural scene'' in a compressed latent space. In our end-to-end training, an encoder learns a compressed latent representation from a large population of salamander retinal ganglion cells responding to natural movies, while a decoder samples from this compressed latent space to generate the appropriate movie frame. By comparing latent representations of retinal activity from three movies, we find that the retina performs transfer learning to encode time: the precise, low-dimensional representation of time learned from one movie can be used to represent time in a different movie, with up to 17ms resolution. We then show that static textures and velocity features of a natural movie are synergistic. The retina simultaneously encodes both to establishes a generalizable, low-dimensional representation of time in the natural scene.",https://api.openreview.net/pdf/13fdb2fb245dee258e1e41f800a6ec0799c250c2.pdf,zero_few-shot;representation;transfer learning;llm,https://scholar.google.com/scholar?q=Learning+low-dimensional+generalizable+natural+features+from+retina+using+a+U-net
Active Ranking without Strong Stochastic Transitivity,2022,NIPS,"['Hao Lou', 'Tao Jin', 'Yue Wu', 'Pan Xu', 'Quanquan Gu', 'Farzad Farnoud']",poster,"['ranking', 'noisy comparison', 'WST', 'SST', 'sample complexity']","Ranking from noisy comparisons is of great practical interest in machine learning. In this paper, we consider the problem of recovering the exact full ranking for a list of items under ranking models that do *not* assume the Strong Stochastic Transitivity property. We propose a $$\delta$$-correct algorithm, Probe-Rank, that actively learns the ranking of the items from noisy pairwise comparisons. We prove a sample complexity upper bound for Probe-Rank, which only depends on the preference probabilities between items that are adjacent in the true ranking. This improves upon existing sample complexity results that depend on the preference probabilities for all pairs of items. Probe-Rank thus outperforms existing methods over a large collection of instances that do not satisfy Strong Stochastic Transitivity. 
Thorough numerical experiments in various settings are conducted, demonstrating that Probe-Rank is significantly more sample-efficient than the state-of-the-art active ranking method.",https://api.openreview.net/pdf/5031b75746699fbe7b1219a0e3338f9f1783dab0.pdf,active learning;llm,https://scholar.google.com/scholar?q=Active+Ranking+without+Strong+Stochastic+Transitivity
Subgroup Robustness Grows On Trees: An Empirical Baseline Investigation,2022,NIPS,"['Joshua P Gardner', 'Zoran Popovi', 'Ludwig Schmidt']",poster,"['robustness', 'fairness', 'tabular data', 'gradient boosting']","Researchers have proposed many methods for fair and robust machine learning, but comprehensive empirical evaluation of their subgroup robustness is lacking. In this work, we address this gap in the context of tabular data, where sensitive subgroups are clearly-defined, real-world fairness problems abound, and prior works often do not compare to state-of-the-art tree-based models as baselines. We conduct an empirical comparison of several previously-proposed methods for fair and robust learning  alongside state-of-the-art tree-based methods  and other baselines. Via experiments with more than $340{,}000$ model configurations on eight datasets, we show that tree-based methods have strong subgroup robustness, even when compared to robustness- and fairness-enhancing methods. Moreover, the best tree-based models tend to show good performance over a range of metrics, while robust or group-fair models can show brittleness, with significant performance differences across different metrics for a fixed model. We also demonstrate that tree-based models show less sensitivity to hyperparameter configurations, and are less costly to train. Our work suggests that tree-based ensemble models make an effective baseline for tabular data, and are a sensible default when subgroup robustness is desired. See https://github.com/jpgard/subgroup-robustness-grows-on-trees for code to reproduce our experiments and detailed experimental results.",https://api.openreview.net/pdf/1be82cd369255c4544ee588811d8df7336e9371f.pdf,metric;llm,https://scholar.google.com/scholar?q=Subgroup+Robustness+Grows+On+Trees:+An+Empirical+Baseline+Investigation
An empirical analysis of compute-optimal large language model training,2022,NIPS,"['Jordan Hoffmann', 'Sebastian Borgeaud', 'Arthur Mensch', 'Elena Buchatskaya', 'Trevor Cai', 'Eliza Rutherford', 'Diego de las Casas', 'Lisa Anne Hendricks', 'Johannes Welbl', 'Aidan Clark', 'Tom Hennigan', 'Eric Noland', 'Katherine Millican', 'George van den Driessche', 'Bogdan Damoc', 'Aurelia Guy', 'Simon Osindero', 'Karen Simonyan', 'Erich Elsen', 'Oriol Vinyals', 'Jack William Rae', 'Laurent Sifre']",poster,"['NLP', 'Deep Learning', 'Large Language Models']","We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more data. Chinchilla uniformly and significantly outperformsGopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, a 7% improvement over Gopher. ",https://api.openreview.net/pdf/bcac8e9cd3bca5485939a352332aead550719481.pdf,graph;transformer;inference;llm,https://scholar.google.com/scholar?q=An+empirical+analysis+of+compute-optimal+large+language+model+training
On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning,2022,NIPS,"['Lorenzo Bonicelli', 'Matteo Boschini', 'Angelo Porrello', 'Concetto Spampinato', 'Simone Calderara']",poster,"['continual learning', 'lifelong learning', 'incremental learning', 'rehearsal', 'lipschitz continuity']","Rehearsal approaches enjoy immense popularity with Continual Learning (CL) practitioners. These methods collect samples from previously encountered data distributions in a small memory buffer; subsequently, they repeatedly optimize on the latter to prevent catastrophic forgetting. This work draws attention to a hidden pitfall of this widespread practice: repeated optimization on a small pool of data inevitably leads to tight and unstable decision boundaries, which are a major hindrance to generalization. To address this issue, we propose Lipschitz-DrivEn Rehearsal (LiDER), a surrogate objective that induces smoothness in the backbone network by constraining its layer-wise Lipschitz constants w.r.t. replay examples. By means of extensive experiments, we show that applying LiDER delivers a stable performance gain to several state-of-the-art rehearsal CL methods across multiple datasets, both in the presence and absence of pre-training. Through additional ablative experiments, we highlight peculiar aspects of buffer overfitting in CL and better characterize the effect produced by LiDER. Code is available at https://github.com/aimagelab/LiDER.",https://api.openreview.net/pdf/e6ce11b7e20d1cb24078310e94fa076362de9b5d.pdf,optimization;transformer;llm,https://scholar.google.com/scholar?q=On+the+Effectiveness+of+Lipschitz-Driven+Rehearsal+in+Continual+Learning
Shape And Structure Preserving Differential Privacy,2022,NIPS,"['Carlos J Soto', 'Karthik Bharath', 'Matthew Reimherr', 'Aleksandra Slavkovic']",poster,"['differential privacy', 'shape analysis', 'manifolds']","It is common for data structures such as images and shapes of 2D objects to be represented as points on a manifold. The utility of a mechanism to produce sanitized differentially private estimates from such data is intimately linked to how compatible it is with the underlying structure and geometry of the space. In particular, as recently shown, utility of the Laplace mechanism on a positively curved manifold, such as Kendall’s 2D shape space, is significantly influenced by the curvature. Focusing on the problem of sanitizing the Fr\'echet mean of a sample of points on a manifold, we exploit the characterization of the mean as the minimizer of an objective function comprised of the sum of squared distances and develop a K-norm gradient mechanism on Riemannian manifolds that favors values that produce gradients close to the the zero of the objective function. For the case of positively curved manifolds, we describe how using the gradient of the squared distance function offers better control over sensitivity than the Laplace mechanism, and demonstrate this numerically on a dataset of shapes of corpus callosa. Further illustrations of the mechanism’s utility on a sphere and the manifold of symmetric positive definite matrices are also presented.",https://api.openreview.net/pdf/ee68d0f09ddfc27c30c99389071f00efa399474b.pdf,metric;llm,https://scholar.google.com/scholar?q=Shape+And+Structure+Preserving+Differential+Privacy
Forward-Backward Latent State Inference for Hidden Continuous-Time semi-Markov Chains,2022,NIPS,"['Nicolai Engelmann', 'Heinz Koeppl']",poster,"['forward-backward', 'continuous time', 'hsmm', 'ctsmc', 'semi-Markov', 'latent state inference']","Hidden semi-Markov Models (HSMM's) - while broadly in use - are restricted to a discrete and uniform time grid. They are thus not well suited to explain often irregularly spaced discrete event data from continuous-time phenomena. We show that non-sampling-based latent state inference used in HSMM's can be generalized to latent Continuous-Time semi-Markov Chains (CTSMC's). We formulate integro-differential forward and backward equations adjusted to the observation likelihood and introduce an exact integral equation for the Bayesian posterior marginals and a scalable Viterbi-type algorithm for posterior path estimates. The presented equations can be efficiently solved using well-known numerical methods. As a practical tool, variable-step HSMM's are introduced. We evaluate our approaches in latent state inference scenarios in comparison to classical HSMM's.",https://api.openreview.net/pdf/0b4bac935a8790667f5f7c82a1fe56923a3faa64.pdf,graph;bayesian;inference;llm,https://scholar.google.com/scholar?q=Forward-Backward+Latent+State+Inference+for+Hidden+Continuous-Time+semi-Markov+Chains
Characterizing the Ventral Visual Stream with Response-Optimized Neural Encoding Models,2022,NIPS,"['Meenakshi Khosla', 'Keith Jamison', 'Amy Kuceyeski', 'Mert R. Sabuncu']",poster,"['deep neural networks', 'computational neuroscience', 'ventral visual stream', 'fMRI']","Decades of experimental research based on simple, abstract stimuli has revealed the coding principles of the ventral visual processing hierarchy, from the presence of edge detectors in the primary visual cortex to the selectivity for complex visual categories in the anterior ventral stream. However, these studies are, by construction, constrained by their $\textit{a priori}$ hypotheses. Furthermore, beyond the early stages, precise neuronal tuning properties and representational transformations along the ventral visual pathway remain poorly understood. In this work, we propose to employ response-optimized encoding models trained solely to predict the functional MRI activation, in order to gain insights into the tuning properties and representational transformations in the series of areas along the ventral visual pathway. We demonstrate the strong generalization abilities of these models on artificial stimuli and novel datasets. Intriguingly, we find that response-optimized models trained towards the ventral-occipital and lateral-occipital areas, but not early visual areas, can recapitulate complex visual behaviors like object categorization and perceived image-similarity in humans. We further probe the trained networks to reveal representational biases in different visual areas and generate experimentally testable hypotheses. Our analyses suggest a shape-based processing along the ventral visual stream and provide a unified picture of multiple neural phenomena characterized over the last decades with controlled fMRI studies. ",https://api.openreview.net/pdf/fe41a340f467cdcdbce6ebb3a3a5c48af6f30352.pdf,optimization;representation;llm,https://scholar.google.com/scholar?q=Characterizing+the+Ventral+Visual+Stream+with+Response-Optimized+Neural+Encoding+Models
On global convergence of ResNets: From finite to infinite width using linear parameterization,2022,NIPS,"['Raphaël Barboni', 'Gabriel Peyré', 'François-Xavier Vialard']",poster,"['Residual Neural Networks', 'Neural ODEs', 'Deep Learning']","Overparameterization is a key factor in the absence of convexity to explain global convergence of gradient descent (GD) for neural networks. Beside the well studied lazy regime, infinite width (mean field) analysis has been developed for shallow networks, using on convex optimization technics. To bridge the gap between the lazy and mean field regimes, we study Residual Networks (ResNets) in which the residual block has linear parameterization while still being nonlinear. Such ResNets admit both infinite depth and width limits, encoding residual blocks in a Reproducing Kernel Hilbert Space (RKHS). In this limit, we prove a local Polyak-Lojasiewicz inequality. Thus, every critical point is a global minimizer and a local convergence result of GD holds, retrieving the lazy regime. In contrast with other mean-field studies, it applies to both parametric and non-parametric cases under an expressivity condition on the residuals. Our analysis leads to a practical and quantified recipe: starting from a universal RKHS, Random Fourier Features are applied to obtain a finite dimensional parameterization satisfying with high-probability our expressivity condition.",https://api.openreview.net/pdf/9eedde854b2c62b57768d7a3d5cefd0fe6a1cd21.pdf,optimization;zero_few-shot;transformer;online learning;metric;llm,https://scholar.google.com/scholar?q=On+global+convergence+of+ResNets:+From+finite+to+infinite+width+using+linear+parameterization
Template based Graph Neural Network with Optimal Transport Distances,2022,NIPS,"['Cédric Vincent-Cuaz', 'Rémi Flamary', 'Marco Corneli', 'Titouan Vayer', 'Nicolas Courty']",poster,"['Optimal Transport', 'Graph Neural Networks', 'Graph classification', 'Graph Representation Learning']","Current Graph Neural Networks (GNN) architectures generally rely on two important components: node features embedding through message passing, and aggregation with a specialized form of pooling. The structural (or topological) information is implicitly taken into account in these two steps. We propose in this work a novel point of view, which places distances to some learnable graph templates at the core of the graph representation. This distance embedding is constructed thanks to an optimal transport distance: the Fused Gromov-Wasserstein (FGW) distance, which encodes simultaneously feature and structure dissimilarities by solving a soft graph-matching problem. We postulate that the vector of FGW distances to a set of template graphs has a strong discriminative power, which is then fed to a non-linear classifier for final predictions. Distance embedding can be seen as a new layer, and can leverage on existing message passing techniques to promote sensible feature representations. Interestingly enough, in our work the optimal set of template graphs is also learnt in  an end-to-end fashion by differentiating through this layer. After describing the corresponding learning procedure, we empirically validate our claim on several synthetic and real life graph classification datasets, where our method is competitive or surpasses kernel and GNN state-of-the-art approaches. We complete our experiments by an ablation study and a sensitivity analysis to parameters.",https://api.openreview.net/pdf/ff1756593c3dad6e262148f01c8915e416a6fd88.pdf,graph;representation;llm,https://scholar.google.com/scholar?q=Template+based+Graph+Neural+Network+with+Optimal+Transport+Distances
Data augmentation for efficient learning from parametric experts,2022,NIPS,"['Alexandre Galashov', 'Josh Merel', 'Nicolas Heess']",poster,"['behavior cloning', 'expert-driven learning']","We present a simple, yet powerful data-augmentation technique to enable data-efficient learning from parametric experts for reinforcement and imitation learning. We focus on what we call the policy cloning setting, in which we use online or offline queries of an expert or expert policy to inform the behavior of a student policy. This setting arises naturally in a number of problems, for instance as variants of behavior cloning, or as a component of other algorithms such as DAGGER, policy distillation or KL-regularized RL. Our approach, augmented policy cloning (APC), uses synthetic states to induce feedback-sensitivity in a region around sampled trajectories, thus dramatically reducing the environment interactions required for successful cloning of the expert. We achieve highly data-efficient transfer of behavior from an expert to a student policy for high-degrees-of-freedom control problems. We demonstrate the benefit of our method in the context of several existing and widely used algorithms that include policy cloning as a constituent part. Moreover, we highlight the benefits of our approach in two practically relevant settings (a) expert compression, i.e. transfer to a student with fewer parameters; and (b) transfer from privileged experts, i.e. where the expert has a different observation space than the student, usually including access to privileged information.",https://api.openreview.net/pdf/ba7b00c8733f703a7c358c9cd9721a7ff8d8e0cc.pdf,reinforcement learning;offline reinforcement learning;online learning;metric;transfer learning;augmentation;distillation;imitation learning;llm,https://scholar.google.com/scholar?q=Data+augmentation+for+efficient+learning+from+parametric+experts
Average Sensitivity of Euclidean k-Clustering,2022,NIPS,"['Yuichi Yoshida', 'Shinji Ito']",poster,"['k-means', 'clustering', 'average sensitivity', 'consistent algorithms', 'dynamic algorithms']","Given a set of $n$ points in $\mathbb{R}^d$, the goal of Euclidean $(k,\ell)$-clustering is to find $k$ centers that minimize the sum of the $\ell$-th powers of the Euclidean distance of each point to the closest center. In practical situations, the clustering result must be stable against points missing in the input data so that we can make trustworthy and consistent decisions. To address this issue, we consider the average sensitivity of Euclidean $(k,\ell)$-clustering, which measures the stability of the output in total variation distance against deleting a random point from the input data. We first show that a popular algorithm \textsc{$k$-means++} and its variant called \textsc{$D^\ell$-sampling} have low average sensitivity. Next, we show that any approximation algorithm for Euclidean $(k,\ell)$-clustering can be transformed to an algorithm with low average sensitivity while almost preserving the approximation guarantee. As byproducts of our results, we provide several algorithms for consistent $(k,\ell)$-clustering and dynamic $(k,\ell)$-clustering in the random-order model, where the input points are randomly permuted and given in an online manner. The goal of the consistent setting is to maintain a good solution while minimizing the number of changes to the solution during the process, and that of the dynamic setting is to maintain a good solution while minimizing the  (amortized) update time.",https://api.openreview.net/pdf/3800359e9b87e7240978f2819d29fd38dc766cb9.pdf,zero_few-shot;online learning;llm,https://scholar.google.com/scholar?q=Average+Sensitivity+of+Euclidean+k-Clustering
RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent Neural Networks,2022,NIPS,"['Leo Kozachkov', 'Michaela M Ennis', 'Jean-Jacques Slotine']",poster,"['Neuroscience', 'Recurrent Neural Networks', 'Control Theory', 'Machine Learning', 'Dynamical Systems']","Recurrent neural networks (RNNs) are widely used throughout neuroscience as models of local neural activity. Many properties of single RNNs are well characterized theoretically, but experimental neuroscience has moved in the direction of studying multiple interacting areas, and RNN theory needs to be likewise extended. We take a constructive approach towards this problem, leveraging tools from nonlinear control theory and machine learning to characterize when combinations of stable RNNs will themselves be stable. Importantly, we derive conditions which allow for massive feedback connections between interacting RNNs. We parameterize these conditions for easy optimization using gradient-based techniques, and show that stability-constrained ""networks of networks"" can perform well on challenging sequential-processing benchmark tasks. Altogether, our results provide a principled approach towards understanding distributed, modular function in the brain.",https://api.openreview.net/pdf/fbee8d317b633ef7b7a424bc6f6e0d89cdee7604.pdf,graph;optimization;zero_few-shot;online learning;llm,https://scholar.google.com/scholar?q=RNNs+of+RNNs:+Recursive+Construction+of+Stable+Assemblies+of+Recurrent+Neural+Networks
Bridging Central and Local Differential Privacy in Data Acquisition Mechanisms,2022,NIPS,"['Alireza Fallah', 'Ali Makhdoumi', 'Azarakhsh Malekian', 'Asuman E. Ozdaglar']",poster,"['Mechanism design', 'algorithmic game theory', 'optimal data acquisition', 'differential privacy']","We study the design of optimal Bayesian data acquisition mechanisms for a platform interested in estimating the mean of a distribution by collecting data from privacy-conscious users. In our setting, users have heterogeneous sensitivities for two types of privacy losses corresponding to local and central differential privacy measures. The local privacy loss is due to the leakage of a user's information when she shares her data with the platform, and the central privacy loss is due to the released estimate by the platform to the public. The users share their data in exchange for a payment (e.g., through monetary transfers or services) that compensates for their privacy losses. The platform does not know the privacy sensitivity of users and must design a mechanism to solicit their preferences and then deliver both local and central privacy guarantees while minimizing the estimation error plus the expected payment to users. We first establish minimax lower bounds for the estimation error, given a vector of privacy guarantees, and show that a linear estimator is (near) optimal. We then turn to our main goal: designing an optimal data acquisition mechanism. We establish that the design of such mechanisms in a Bayesian setting (where the platform knows the distribution of users' sensitivities and not their realizations) can be cast as a nonconvex optimization problem. Additionally, for the class of linear estimators, we prove that finding the optimal mechanism admits a Polynomial Time Approximation Scheme.",https://api.openreview.net/pdf/f5cf2d68a36d8b7b27a98f7c645a6b30b75a106e.pdf,graph;optimization;zero_few-shot;bayesian;transfer learning;llm,https://scholar.google.com/scholar?q=Bridging+Central+and+Local+Differential+Privacy+in+Data+Acquisition+Mechanisms
Mesoscopic modeling of hidden spiking neurons,2022,NIPS,"['Shuqi Wang', 'Valentin Schmutz', 'Guillaume Bellec', 'Wulfram Gerstner']",poster,"['recurrent spiking neural network', 'hidden spiking neurons', 'maximum likelihood', 'mean-field approximation', 'finite-size fluctuations', 'neural population dynamics', 'latent variable model', 'expectation-maximization algorithm', 'metastable dynamics']","Can we use spiking neural networks (SNN) as generative models of multi-neuronal recordings, while taking into account that most neurons are unobserved? Modeling the unobserved neurons with large pools of hidden spiking neurons leads to severely underconstrained problems that are hard to tackle with maximum likelihood estimation. In this work, we use coarse-graining and mean-field approximations to derive a bottom-up, neuronally-grounded latent variable model (neuLVM), where the activity of the unobserved neurons is reduced to a low-dimensional mesoscopic description. In contrast to previous latent variable models, neuLVM can be explicitly mapped to a recurrent, multi-population SNN, giving it a transparent biological interpretation. We show, on synthetic spike trains, that a few observed neurons are sufficient for neuLVM to perform efficient model inversion of large SNNs, in the sense that it can recover connectivity parameters, infer single-trial latent population activity, reproduce ongoing metastable dynamics, and generalize when subjected to perturbations mimicking optogenetic stimulation.",https://api.openreview.net/pdf/8b7c30ac54884248eb835952cfc64e01467450d7.pdf,optimization;zero_few-shot;generative model;meta-learning;llm,https://scholar.google.com/scholar?q=Mesoscopic+modeling+of+hidden+spiking+neurons
EZNAS: Evolving Zero-Cost Proxies For Neural Architecture Scoring,2022,NIPS,"['Yash Akhauri', 'Juan Pablo Munoz', 'Nilesh Jain', 'Ravishankar Iyer']",poster,"['Zero Shot Neural Architecture Search', 'Neural Architecture Search', 'Program Synthesis', 'Evolutionary Search', 'Genetic Programming']","Neural Architecture Search (NAS) has significantly improved productivity in the design and deployment of neural networks (NN). As NAS typically evaluates multiple models by training them partially or completely, the improved productivity comes at the cost of significant carbon footprint. To alleviate this expensive training routine, zero-shot/cost proxies analyze an NN at initialization to generate a score, which correlates highly with its true accuracy. Zero-cost proxies are currently designed by experts conducting multiple cycles of empirical testing on possible algorithms, datasets, and neural architecture design spaces. This experimentation lowers productivity and is an unsustainable approach towards zero-cost proxy design as deep learning use-cases diversify in nature. Additionally, existing zero-cost proxies fail to generalize across neural architecture design spaces. In this paper, we propose a genetic programming framework to automate the discovery of zero-cost proxies for neural architecture scoring. Our methodology efficiently discovers an interpretable and generalizable zero-cost proxy that gives state of the art score-accuracy correlation on all datasets and search spaces of NASBench-201 and Network Design Spaces (NDS). We believe that this research indicates a promising direction towards automatically discovering zero-cost proxies that can work across network architecture design spaces, datasets, and tasks.",https://api.openreview.net/pdf/f0c9f581f40477e850c3c2527efaf70e6303dc47.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=EZNAS:+Evolving+Zero-Cost+Proxies+For+Neural+Architecture+Scoring
On the Stability and Scalability of Node Perturbation Learning,2022,NIPS,"['Naoki Hiratani', 'Yash Mehta', 'Timothy P Lillicrap', 'Peter E. Latham']",poster,"['Biologically Plausible Deep Networks', 'Credit Assignment', 'Neuroscience', 'Dynamical Systems']","To survive, animals must adapt synaptic weights based on external stimuli and rewards. And they must do so using local, biologically plausible, learning rules -- a highly nontrivial constraint. One possible approach is to perturb neural activity (or use intrinsic, ongoing noise to perturb it), determine whether performance increases or decreases, and use that information to adjust the weights. This algorithm -- known as node perturbation -- has been shown to work on simple problems, but little is known about either its stability or its scalability with respect to network size. We investigate these issues both analytically, in deep linear networks, and numerically, in deep nonlinear ones.
We show analytically that in deep linear networks with one hidden layer, both learning time and performance depend very weakly on hidden layer size. However, unlike stochastic gradient descent, when there is model mismatch between the student and teacher networks, node perturbation is always unstable. The instability is triggered by weight diffusion, which eventually leads to very large weights. This instability can be suppressed by weight normalization, at the cost of bias in the learning rule. We confirm numerically that a similar instability, and to a lesser extent scalability, exist in deep nonlinear networks trained on both a motor control task and image classification tasks. Our study highlights the limitations and potential of node perturbation as a biologically plausible learning rule in the brain.",https://api.openreview.net/pdf/26aa6d79eb5328e7c4eb9030e16a3c628d8bdbcc.pdf,optimization;zero_few-shot;online learning;llm,https://scholar.google.com/scholar?q=On+the+Stability+and+Scalability+of+Node+Perturbation+Learning
Neural Attentive Circuits,2022,NIPS,"['Martin Weiss', 'Nasim Rahaman', 'Francesco Locatello', 'Christopher Pal', 'Yoshua Bengio', 'Bernhard Schölkopf', 'Li Erran Li', 'Nicolas Ballas']",poster,"['Deep Learning', 'Low-Shot Adaptation', 'Attention Mechanisms', 'General Purpose Neural Architectures']","Recent work has seen the development of general purpose neural architectures that can be trained to perform tasks across diverse data modalities. General purpose models typically make few assumptions about the underlying data-structure and are known to perform well in the large-data regime. At the same time, there has been growing interest in modular neural architectures that represent the data using sparsely interacting modules. These models can be more robust out-of-distribution, computationally efficient, and capable of sample-efficient adaptation to new data. However, they tend to make domain-specific assumptions about the data, and present challenges in how module behavior (i.e., parameterization) and connectivity (i.e., their layout) can be jointly learned. In this work, we introduce a general purpose, yet modular neural architecture called Neural Attentive Circuits (NACs) that jointly learns the parameterization and a sparse connectivity of neural modules without using domain knowledge. NACs are best understood as the combination of two systems that are jointly trained end-to-end: one that determines the module configuration and the other that executes it on an input.  We demonstrate qualitatively that NACs learn diverse and meaningful module configurations on the Natural Language and Visual Reasoning for Real (NLVR2) dataset without additional supervision. Quantitatively, we show that by incorporating modularity in this way, NACs improve upon a strong non-modular baseline in terms of low-shot adaptation on CIFAR and Caltech-UCSD Birds dataset (CUB) by about 10 percent, and OOD robustness on Tiny ImageNet-R by about 2.5 percent. Further, we find that NACs can achieve an 8x speedup at inference time while losing less than 3 percent performance. Finally, we find NACs to yield competitive results on diverse data modalities spanning point-cloud classification, symbolic processing and text-classification from ASCII bytes, thereby confirming its general purpose nature. ",https://api.openreview.net/pdf/df025fd0c81f77d1bd1066d684f79a87b0b3468d.pdf,zero_few-shot;inference;sparse;llm,https://scholar.google.com/scholar?q=Neural+Attentive+Circuits
Holomorphic Equilibrium Propagation Computes Exact Gradients Through Finite Size Oscillations,2022,NIPS,"['Axel Laborieux', 'Friedemann Zenke']",poster,"['Equilibrium propagation', 'credit assignment', 'bio-plausible deep learning', 'implicit differentiation']","Equilibrium propagation (EP) is an alternative to backpropagation (BP) that allows the training of deep neural networks with local learning rules. It thus provides a compelling framework for training neuromorphic systems and understanding learning in neurobiology. However, EP requires infinitesimal teaching signals, thereby limiting its applicability to noisy physical systems. Moreover, the algorithm requires separate temporal phases and has not been applied to large-scale problems. Here we address these issues by extending EP to holomorphic networks. We show analytically that this extension naturally leads to exact gradients for finite-amplitude teaching signals. Importantly, the gradient can be computed as the first Fourier coefficient from finite neuronal activity oscillations in continuous time without requiring separate phases. Further, we demonstrate in numerical simulations that our approach permits robust estimation of gradients in the presence of noise and that deeper models benefit from the finite teaching signals. Finally, we establish the first benchmark for EP on the ImageNet $32 \times 32$ dataset and show that it matches the performance of an equivalent network trained with BP. Our work provides analytical insights that enable scaling EP to large-scale problems and establishes a formal framework for how oscillations could support learning in biological and neuromorphic systems.",https://api.openreview.net/pdf/41e65a6caed0579e2f0a5a8f420a5eea69dbacdb.pdf,zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Holomorphic+Equilibrium+Propagation+Computes+Exact+Gradients+Through+Finite+Size+Oscillations
DevFly: Bio-Inspired Development of Binary Connections for Locality Preserving Sparse Codes,2022,NIPS,"['Tianqi Wei', 'Rana Alkhoury Maroun', 'Qinghai Guo', 'Barbara Webb']",poster,"['bioinspired', 'neurodevelopment', 'mushroom body', 'locality-sensitive hash', 'sparse coding']","Neural circuits undergo developmental processes which can be influenced by experience. Here we explore a bio-inspired development process to form the connections in a network used for locality sensitive hashing. The network is a simplified model of the insect mushroom body, which has sparse connections from the input layer to a second layer of higher dimension, forming a sparse code. In previous versions of this model, connectivity between the layers is random. We investigate whether the performance of the hash, evaluated in nearest neighbour query tasks, can be improved by process of developing the connections, in which the strongest input dimensions in successive samples are wired to each successive coding dimension. Experiments show that the accuracy of searching for nearest neighbours is improved, although performance is dependent on the parameter values and datasets used. Our approach is also much faster than alternative methods that have been proposed for training the connections in this model. Importantly, the development process does not impact connections built at an earlier stage, which should provide stable coding results for simultaneous learning in a downstream network. ",https://api.openreview.net/pdf/91c9f36bc00f6be691608ebde7256ef9c32d2ee4.pdf,sparse;llm,https://scholar.google.com/scholar?q=DevFly:+Bio-Inspired+Development+of+Binary+Connections+for+Locality+Preserving+Sparse+Codes
PhysGNN: A Physics--Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image--Guided Neurosurgery,2022,NIPS,"['Yasmin Salehi', 'Dennis Giannacopoulos']",poster,"['PhysGNN', 'Physics-Driven', 'Graph Neural Network', 'Tissue Deformation', 'Image-Guided Systems', 'GraphSAGE', 'GraphConv', 'Jumping Knowledge', 'Physical Simulation', 'Mechanical Simulation', 'Finite Element Method', 'Finite Element Analysis']","Correctly capturing intraoperative brain shift in image-guided neurosurgical procedures is a critical task for aligning preoperative data with intraoperative geometry for ensuring accurate surgical navigation. While the finite element method (FEM) is a proven technique to effectively approximate soft tissue deformation through biomechanical formulations, their degree of success boils down to a trade-off between accuracy and speed. To circumvent this problem, the most recent works in this domain have proposed leveraging data-driven models obtained by training various machine learning algorithms---e.g., random forests, artificial neural networks (ANNs)---with the results of finite element analysis (FEA) to speed up tissue deformation approximations by prediction. These methods, however, do not account for the structure of the finite element (FE) mesh during training that provides information on node connectivities as well as the distance between them, which can aid with approximating tissue deformation based on the proximity of force load points with the rest of the mesh nodes. Therefore, this work proposes a novel framework, PhysGNN, a data-driven model that approximates the solution of the FEM by leveraging graph neural networks (GNNs), which are capable of accounting for the mesh structural information and inductive learning over unstructured grids and complex topological structures. Empirically, we demonstrate that the proposed architecture, PhysGNN, promises accurate and fast soft tissue deformation approximations, and is competitive with the state-of-the-art (SOTA) algorithms while promising enhanced computational feasibility, therefore suitable for neurosurgical settings.",https://api.openreview.net/pdf/bbf9f0f3d2af714ebb173161b890a56d675bcf18.pdf,graph;llm,https://scholar.google.com/scholar?q=PhysGNN:+A+Physics--Driven+Graph+Neural+Network+Based+Model+for+Predicting+Soft+Tissue+Deformation+in+Image--Guided+Neurosurgery
Data-Efficient Pipeline for Offline Reinforcement Learning with Limited Data,2022,NIPS,"['Allen Nie', 'Yannis Flet-Berliac', 'Deon Richmond Jordan', 'William Steenbergen', 'Emma Brunskill']",poster,"['offline reinforcement learning', 'hyperparameter selection', 'policy deployment', 'small data regime', 'batch value function tournament (BVFT)']","Offline reinforcement learning (RL) can be used to improve future performance by leveraging historical data. There exist many different algorithms for offline RL, and it is well recognized that these algorithms, and their hyperparameter settings, can lead to decision policies with substantially differing performance. This prompts the need for pipelines that allow practitioners to systematically perform algorithm-hyperparameter selection for their setting. Critically, in most real-world settings, this pipeline must only involve the use of historical data. 
Inspired by statistical model selection methods for supervised learning, we introduce a task- and method-agnostic pipeline for automatically training, comparing, selecting, and deploying the best policy when the provided dataset is limited in size. In particular, our work highlights the importance of performing multiple data splits to produce more reliable algorithm-hyperparameter selection. While this is a common approach in supervised learning, to our knowledge, this has not been discussed in detail in the offline RL setting. We show it can have substantial impacts when the dataset is small. Compared to alternate approaches, our proposed pipeline outputs higher-performing deployed policies from a broad range of offline policy learning algorithms and across various simulation domains in healthcare, education, and robotics. This work contributes toward the development of a general-purpose meta-algorithm for automatic algorithm-hyperparameter selection for offline RL.",https://api.openreview.net/pdf/b49a42f4a6efa82ecb6000fe3168209830360bd5.pdf,reinforcement learning;offline reinforcement learning;graph;zero_few-shot;meta-learning;llm,https://scholar.google.com/scholar?q=Data-Efficient+Pipeline+for+Offline+Reinforcement+Learning+with+Limited+Data
Where2comm: Communication-Efficient Collaborative Perception via Spatial Confidence Maps,2022,NIPS,"['Yue Hu', 'Shaoheng Fang', 'Zixing Lei', 'Yiqi Zhong', 'Siheng Chen']",poster,"['Collaborative perception', '3D object detection']","Multi-agent collaborative perception could significantly upgrade the perception performance by enabling agents to share complementary information with each other through communication. It inevitably results in a fundamental trade-off between perception performance and communication bandwidth. To tackle this bottleneck issue, we propose a spatial confidence map, which reflects the spatial heterogeneity of perceptual information. It empowers agents to only share spatially sparse, yet perceptually critical information, contributing to where to communicate. Based on this novel spatial confidence map, we propose Where2comm, a communication-efficient collaborative perception framework. Where2comm has two distinct advantages: i) it considers pragmatic compression and uses less communication to achieve higher perception performance by focusing on perceptually critical areas; and ii) it can handle varying communication bandwidth by dynamically adjusting spatial areas involved in communication. To evaluate Where2comm, we consider 3D object detection in both real-world and simulation scenarios with two modalities (camera/LiDAR) and two agent types (cars/drones) on four datasets: OPV2V, V2X-Sim, DAIR-V2X, and our original CoPerception-UAVs. Where2comm consistently outperforms previous methods; for example, it achieves more than $100,000 \times$ lower communication volume and still outperforms DiscoNet and V2X-ViT on OPV2V. Our code is available at~\url{https://github.com/MediaBrain-SJTU/where2comm}.",https://api.openreview.net/pdf/78a06fc92dbd04d692fbcee236af4d1f64745a12.pdf,reinforcement learning;graph;zero_few-shot;sparse;multi-agent;3d;llm,https://scholar.google.com/scholar?q=Where2comm:+Communication-Efficient+Collaborative+Perception+via+Spatial+Confidence+Maps
The least-control principle for local learning at equilibrium,2022,NIPS,"['Alexander Meulemans', 'Nicolas Zucchet', 'Seijin Kobayashi', 'Johannes Von Oswald', 'Joao Sacramento']",poster,"['Biologically-plausible learning', 'local learning rules', 'predictive coding', 'neuroscience', 'equilibrium recurrent neural networks', 'meta learning', 'deep equilibrium models', 'implicit models', 'optimal control']","Equilibrium systems are a powerful way to express neural computations. As special cases, they include models of great current interest in both neuroscience and machine learning, such as deep neural networks, equilibrium recurrent neural networks, deep equilibrium models, or meta-learning. Here, we present a new principle for learning such systems with a temporally- and spatially-local rule. Our principle casts learning as a \emph{least-control} problem, where we first introduce an optimal controller to lead the system towards a solution state, and then define learning as reducing the amount of control needed to reach such a state. We show that incorporating learning signals within a dynamics as an optimal control enables transmitting activity-dependent credit assignment information, avoids storing intermediate states in memory, and does not rely on infinitesimal learning signals. In practice, our principle leads to strong performance matching that of leading gradient-based learning methods when applied to an array of problems involving recurrent neural networks and meta-learning. Our results shed light on how the brain might learn and offer new ways of approaching a broad class of machine learning problems.",https://api.openreview.net/pdf/0dddba62e8b9d949c65ccfdc5da168ec80f8eb5f.pdf,meta-learning;llm,https://scholar.google.com/scholar?q=The+least-control+principle+for+local+learning+at+equilibrium
Scalable Multi-agent Covering Option Discovery based on Kronecker Graphs,2022,NIPS,"['Jiayu Chen', 'Jingdi Chen', 'Tian Lan', 'Vaneet Aggarwal']",poster,"['Multi-agent Reinforcement Learning', 'Option Discovery', 'Kronecker Product']","Covering option discovery has been developed to improve the exploration of RL in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. Given that joint state space grows exponentially with the number of agents in multi-agent systems, existing researches still relying on single-agent option discovery either become prohibitive or fail to directly discover joint options that improve the connectivity of the joint state space. In this paper, we show how to directly compute multi-agent options with collaborative exploratory behaviors while still enjoying the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph, based on which we can directly estimate its Fiedler vector using the Laplacian spectrum of individual agents' transition graphs. Further, considering that directly computing the Laplacian spectrum is intractable for tasks with infinite-scale state spaces, we further propose a deep learning extension of our method by estimating eigenfunctions through NN-based representation learning techniques. The evaluation on multi-agent tasks built with simulators like Mujoco, shows that the proposed algorithm can successfully identify multi-agent options, and significantly outperforms the state-of-the-art. Codes are available at: https://github.itap.purdue.edu/Clan-labs/Scalable_MAOD_via_KP.",https://api.openreview.net/pdf/cec4d8c227820594ba7a1b7a88b73f8569d16b58.pdf,reinforcement learning;graph;representation;sparse;multi-agent;llm,https://scholar.google.com/scholar?q=Scalable+Multi-agent+Covering+Option+Discovery+based+on+Kronecker+Graphs
The Gyro-Structure of Some Matrix Manifolds,2022,NIPS,['Xuan Son Nguyen'],poster,"['manifold learning', 'representation learning', 'deep learning', 'gyrovector spaces']","In this paper, we study the gyrovector space structure (gyro-structure) of matrix manifolds. Our work is motivated by the success of hyperbolic neural networks (HNNs) that have demonstrated impressive performance in a variety of applications. At the heart of HNNs is the theory of gyrovector spaces that provides a powerful tool for studying hyperbolic geometry. Here we focus on two matrix manifolds, i.e., Symmetric Positive Definite (SPD) and Grassmann manifolds, and consider connecting the Riemannian geometry of these manifolds with the basic operations, i.e., the binary operation and scalar multiplication on gyrovector spaces. Our work reveals some interesting facts about SPD and Grassmann manifolds. First, SPD matrices with an Affine-Invariant (AI) or a Log-Euclidean (LE) geometry have rich structure with strong connection to hyperbolic geometry. Second, linear subspaces, when equipped with our proposed basic operations, form what we call gyrocommutative and gyrononreductive gyrogroups. Furthermore, they share remarkable analogies with gyrovector spaces. We demonstrate the applicability of our approach for human activity understanding and question answering.",https://api.openreview.net/pdf/a24f842deeb2a1ec5e0f6b9633350bfe66253a0d.pdf,metric;llm,https://scholar.google.com/scholar?q=The+Gyro-Structure+of+Some+Matrix+Manifolds
Confident Adaptive Language Modeling,2022,NIPS,"['Tal Schuster', 'Adam Fisch', 'Jai Gupta', 'Mostafa Dehghani', 'Dara Bahri', 'Vinh Q. Tran', 'Yi Tay', 'Donald Metzler']",poster,"['adaptive compute', 'early exit', 'language model', 'transformer', 'nlp']","Recent advances in Transformer-based large language models (LLMs) have led to significant performance improvements across many tasks. These gains come with a drastic increase in the models' size, potentially leading to slow and costly use at inference time. In practice, however, the series of generations made by LLMs is composed of varying levels of difficulty. While certain predictions truly benefit from the models' full capacity, other continuations are more trivial and can be solved with reduced compute. In this work, we introduce Confident Adaptive Language Modeling (CALM), a framework for dynamically allocating different amounts of compute per input and generation timestep. Early exit decoding involves several challenges that we address here, such as: (1) what confidence measure to use; (2) connecting sequence-level constraints to local per-token exit decisions; and (3) attending back to missing hidden representations due to early exits in previous tokens. Through theoretical analysis and empirical experiments on three diverse text generation tasks, we demonstrate the efficacy of our framework in reducing compute---potential speedup of up to $\times 3$---while provably maintaining high performance. ",https://api.openreview.net/pdf/c44dd792801d3ab77f33f9cf9e2d4df316ab52d9.pdf,optimization;zero_few-shot;transformer;representation;generative model;adaptive;inference;llm,https://scholar.google.com/scholar?q=Confident+Adaptive+Language+Modeling
A Unified Sequence Interface for Vision Tasks,2022,NIPS,"['Ting Chen', 'Saurabh Saxena', 'Lala Li', 'Tsung-Yi Lin', 'David J. Fleet', 'Geoffrey Hinton']",poster,"['generalist model', 'computer vision multi-task', 'sequence modeling']","While language tasks are naturally expressed in a single, unified, modeling framework, i.e., generating sequences of tokens, this has not been the case in computer vision. As a result, there is a proliferation of distinct architectures and loss functions for different vision tasks. In this work we show that a diverse set of ""core"" computer vision tasks can also be unified if formulated in terms of a shared pixel-to-sequence interface. We focus on four tasks, namely, object detection, instance segmentation, keypoint detection, and image captioning, all with diverse types of outputs, e.g., bounding boxes or dense masks. Despite that, by formulating the output of each task as a sequence of discrete tokens with a unified interface, we show that one can train a neural network with a single model architecture and loss function on all these tasks, with no task-specific customization. To solve a specific task, we use a short prompt as task description, and the sequence output adapts to the prompt so it can produce task-specific output. We show that such a model can achieve competitive performance compared to well-established task-specific models.",https://api.openreview.net/pdf/9a5b6c91f994aa810fcf0831d0c3082ca4cc5fad.pdf,segmentation;llm,https://scholar.google.com/scholar?q=A+Unified+Sequence+Interface+for+Vision+Tasks
Bayesian Optimistic Optimization: Optimistic Exploration for Model-based Reinforcement Learning,2022,NIPS,"['Chenyang Wu', 'Tianci Li', 'Zongzhang Zhang', 'Yang Yu']",poster,"['Model-based Reinforcement Learning', 'Exploration and Exploitation', 'Optimism in the Face of Uncertainty']","Reinforcement learning (RL) is a general framework for modeling sequential decision making problems, at the core of which lies the dilemma of exploitation and exploration. An agent failing to explore systematically will inevitably fail to learn efficiently. Optimism in the face of uncertainty (OFU) is a conventionally successful strategy for efficient exploration. An agent following the OFU principle explores actively and efficiently. However, when applied to model-based RL, it involves specifying a confidence set of the underlying model and solving a series of nonlinear constrained optimization, which can be computationally intractable. This paper proposes an algorithm, Bayesian optimistic optimization (BOO), which adopts a dynamic weighting technique for enforcing the constraint rather than explicitly solving a constrained optimization problem. BOO is a general algorithm proved to be sample-efficient for models in a finite-dimensional reproducing kernel Hilbert space. We also develop techniques for effective optimization and show through some simulation experiments that BOO is competitive with the existing algorithms.",https://api.openreview.net/pdf/62d8264669d3e0e12608765a4664e1be6b883c44.pdf,reinforcement learning;optimization;zero_few-shot;transformer;online learning;bayesian;active learning;llm,https://scholar.google.com/scholar?q=Bayesian+Optimistic+Optimization:+Optimistic+Exploration+for+Model-based+Reinforcement+Learning
Vision Transformers provably learn spatial structure,2022,NIPS,"['Samy Jelassi', 'Michael Eli Sander', 'Yuanzhi Li']",poster,"['deep learning theory', 'vision transformers', 'implicit bias']","Vision Transformers (ViTs) have recently achieved comparable or superior performance to Convolutional neural networks (CNNs) in computer vision. This empirical breakthrough is even more remarkable since ViTs discards spatial information by mixing patch embeddings and positional encodings and do not embed any visual inductive bias (e.g.\ spatial locality). Yet, recent work showed that while minimizing their training loss, ViTs specifically learn spatially delocalized patterns. This raises a central question: how do ViTs learn this pattern by solely minimizing their training loss using gradient-based methods from \emph{random initialization}? We propose a structured classification dataset and a simplified ViT model to provide preliminary theoretical justification of this phenomenon. Our model relies on a simplified attention mechanism --the positional attention mechanism-- where the attention matrix solely depends on the positional encodings. While the problem admits multiple solutions that generalize, we show that our model implicitly learns the spatial structure of the dataset while generalizing. 
We finally prove that learning the structure helps to  sample-efficiently transfer to downstream datasets that share the same structure as the pre-training one but with different  features. We empirically verify that ViTs using only the positional attention mechanism perform similarly to the original one on CIFAR-10/100, SVHN and ImageNet.",https://api.openreview.net/pdf/ea6f16a7e1453da7b730b14e5d4e2bdd1d790297.pdf,graph;transformer;transfer learning;llm,https://scholar.google.com/scholar?q=Vision+Transformers+provably+learn+spatial+structure
On the difficulty of learning chaotic dynamics with RNNs,2022,NIPS,"['Jonas Magdy Mikhaeil', 'Zahra Monfared', 'Daniel Durstewitz']",poster,"['Recurrent neural networks', 'Dynamical systems', 'Attractors', 'Time series analysis', 'Chaos', 'Exploding and vanishing gradient problem', 'Teacher forcing']","Recurrent neural networks (RNNs) are wide-spread machine learning tools for modeling sequential and time series data. They are notoriously hard to train because their loss gradients backpropagated in time tend to saturate or diverge during training. This is known as the exploding and vanishing gradient problem. Previous solutions to this issue either built on rather complicated, purpose-engineered architectures with gated memory buffers, or - more recently - imposed constraints that ensure convergence to a fixed point or restrict (the eigenspectrum of) the recurrence matrix. Such constraints, however, convey severe limitations on the expressivity of the RNN. Essential intrinsic dynamics such as multistability or chaos are disabled. This is inherently at disaccord with the chaotic nature of many, if not most, time series encountered in nature and society. It is particularly problematic in scientific applications where one aims to reconstruct the underlying dynamical system. 
Here we offer a comprehensive theoretical treatment of this problem by relating the loss gradients during RNN training to the Lyapunov spectrum of RNN-generated orbits. We mathematically prove that RNNs producing stable equilibrium or cyclic behavior have bounded gradients, whereas the gradients of RNNs with chaotic dynamics always diverge. 
Based on these analyses and insights we suggest ways of how to optimize the training process on chaotic data according to the system's Lyapunov spectrum, regardless of the employed RNN architecture. ",https://api.openreview.net/pdf/c84a9a3e435a7f3cccd132d77118cc3d3b4cb806.pdf,graph;optimization;llm,https://scholar.google.com/scholar?q=On+the+difficulty+of+learning+chaotic+dynamics+with+RNNs
Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling,2022,NIPS,"['Dmitry Kovalev', 'Alexander Gasnikov', 'Peter Richtárik']",poster,"['convex optimization', 'saddle-point problems', 'minimax optimization']","In this paper we study the convex-concave saddle-point problem $\min_x \max_y f(x) + y^\top\mathbf{A}x - g(y)$, where $f(x)$ and $g(y)$ are smooth and convex functions. We propose an Accelerated Primal-Dual Gradient Method (APDG) for solving this problem, achieving (i) an optimal linear convergence rate in the strongly-convex-strongly-concave regime, matching the lower complexity bound (Zhang et al., 2021), and (ii) an accelerated linear convergence rate in the case when only one of the functions $f(x)$ and $g(y)$ is strongly convex or even none of them are. Finally, we obtain a linearly convergent algorithm for the general smooth and convex-concave saddle point problem $\min_x \max_y F(x,y)$ without the requirement of strong convexity or strong concavity.",https://api.openreview.net/pdf/00ad932d326f56844a5c666cc086d87bed43b44f.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Accelerated+Primal-Dual+Gradient+Method+for+Smooth+and+Convex-Concave+Saddle-Point+Problems+with+Bilinear+Coupling
ShuffleMixer: An Efficient ConvNet for Image Super-Resolution,2022,NIPS,"['Long Sun', 'Jinshan Pan', 'Jinhui Tang']",poster,[],"Lightweight and efficiency are critical drivers for the practical application of image super-resolution (SR) algorithms. We propose a simple and effective approach, ShuffleMixer, for lightweight image super-resolution that explores large convolution and channel split-shuffle operation. In contrast to previous SR models that simply stack multiple small kernel convolutions or complex operators to learn representations, we explore a large kernel ConvNet for mobile-friendly SR design. Specifically, we develop a large depth-wise convolution and two projection layers based on channel splitting and shuffling as the basic component to mix features efficiently. Since the contexts of natural images are strongly locally correlated, using large depth-wise convolutions only is insufficient to reconstruct fine details. To overcome this problem while maintaining the efficiency of the proposed module, we introduce Fused-MBConvs into the proposed network to model the local connectivity of different features. Experimental results demonstrate that the proposed ShuffleMixer is about $3 \times$ smaller than the state-of-the-art efficient SR methods, e.g. CARN, in terms of model parameters and FLOPs while achieving competitive performance. ",https://api.openreview.net/pdf/8004e06b3dccce0e148e6e7c2211289153df5e11.pdf,representation;llm,https://scholar.google.com/scholar?q=ShuffleMixer:+An+Efficient+ConvNet+for+Image+Super-Resolution
BadPrompt: Backdoor Attacks on Continuous Prompts,2022,NIPS,"['Xiangrui Cai', 'haidong xu', 'Sihan Xu', 'Ying Zhang', 'Xiaojie Yuan']",poster,"['Continuous prompt', 'backdoor', 'few-shot learning']","The prompt-based learning paradigm has gained much research attention recently. It has achieved state-of-the-art performance on several NLP tasks, especially in the few-shot scenarios. While steering the downstream tasks, few works have been reported to investigate the security problems of the prompt-based models. In this paper, we conduct the first study on the vulnerability of the continuous prompt learning algorithm to backdoor attacks. We observe that the few-shot scenarios have posed a great challenge to backdoor attacks on the prompt-based models, limiting the usability of existing NLP backdoor methods. To address this challenge, we propose BadPrompt, a lightweight and task-adaptive algorithm, to backdoor attack continuous prompts. Specially, BadPrompt first generates candidate triggers which are indicative for predicting the targeted label and dissimilar to the samples of the non-targeted labels. Then, it automatically selects the most effective and invisible trigger for each sample with an adaptive trigger optimization algorithm. We evaluate the performance of BadPrompt on five datasets and two continuous prompt models. The results exhibit the abilities of BadPrompt to effectively attack continuous prompts while maintaining high performance on the clean test sets, outperforming the baseline models by a large margin. The source code of BadPrompt is publicly available.",https://api.openreview.net/pdf/5c4cf5b33c7da305a496573c8b6b363f56143e32.pdf,graph;optimization;transformer;adaptive;llm,https://scholar.google.com/scholar?q=BadPrompt:+Backdoor+Attacks+on+Continuous+Prompts
Power and limitations of single-qubit native quantum neural networks,2022,NIPS,"['Zhan Yu', 'Hongshun Yao', 'Mujin Li', 'Xin Wang']",poster,"['quantum machine learning', 'quantum neural networks', 'expressivity', 'function approximation', 'universal approximation', 'Fourier series']","Quantum neural networks (QNNs) have emerged as a leading strategy to establish applications in machine learning, chemistry, and optimization. While the applications of QNN have been widely investigated, its theoretical foundation remains less understood. In this paper, we formulate a theoretical framework for the expressive ability of data re-uploading quantum neural networks that consist of interleaved encoding circuit blocks and trainable circuit blocks. First, we prove that single-qubit quantum neural networks can approximate any univariate function by mapping the model to a partial Fourier series. We in particular establish the exact correlations between the parameters of the trainable gates and the Fourier coefficients, resolving an open problem on the universal approximation property of QNN. Second, we discuss the limitations of single-qubit native QNNs on approximating multivariate functions by analyzing the frequency spectrum and the flexibility of Fourier coefficients. We further demonstrate the expressivity and limitations of single-qubit native QNNs via numerical experiments. We believe these results would improve our understanding of QNNs and provide a helpful guideline for designing powerful QNNs for machine learning tasks.",https://api.openreview.net/pdf/c3cc2c0f1963006ceee006911403e12a5af2cf02.pdf,optimization;llm,https://scholar.google.com/scholar?q=Power+and+limitations+of+single-qubit+native+quantum+neural+networks
The First Optimal Algorithm for Smooth and Strongly-Convex-Strongly-Concave Minimax Optimization,2022,NIPS,"['Dmitry Kovalev', 'Alexander Gasnikov']",poster,"['convex optimization', 'minimax optimization', 'saddle point problems', 'optimal algorithms']","In this paper, we revisit the smooth and strongly-convex-strongly-concave minimax optimization problem. Zhang et al. (2021) and Ibrahim et al. (2020) established the lower bound $\Omega\left(\sqrt{\kappa_x\kappa_y} \log \frac{1}{\epsilon}\right)$ on the number of gradient evaluations required to find an ϵ-accurate solution, where κx and κy are condition numbers for the strong convexity and strong concavity assumptions. However, the existing state-of-the-art methods do not match this lower bound: algorithms of Lin et al. (2020) and Wang and Li (2020) have gradient evaluation complexity $\mathcal{O}\left(\sqrt{\kappa_x\kappa_y} \log^3 \frac{1}{\epsilon}\right)$ and $\mathcal{O}\left( \sqrt{\kappa_x\kappa_y}\log^3 (\kappa_x\kappa_y)\log\frac{1}{\epsilon}\right)$, respectively. We fix this fundamental issue by providing the first algorithm with $\mathcal{O}\left(\sqrt{\kappa_x\kappa_y} \log \frac{1}{\epsilon}\right)$ gradient evaluation complexity. We design our algorithm in three steps: (i) we reformulate the original problem as a minimization problem via the pointwise conjugate function; (ii) we apply a specific variant of the proximal point algorithm to the reformulated problem; (iii) we compute the proximal operator inexactly using the optimal algorithm for operator norm reduction in monotone inclusions.",https://api.openreview.net/pdf/1088a6e8695001d25c065dd1f286b8bde4a85363.pdf,graph;optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=The+First+Optimal+Algorithm+for+Smooth+and+Strongly-Convex-Strongly-Concave+Minimax+Optimization
On the Learning Mechanisms in Physical Reasoning,2022,NIPS,"['Shiqian Li', 'Kewen Wu', 'Chi Zhang', 'Yixin Zhu']",poster,"['Intuitive physics', 'physical reasoning', 'dynamics prediction']","Is dynamics prediction indispensable for physical reasoning? If so, what kind of roles do the dynamics prediction modules play during the physical reasoning process? Most studies focus on designing dynamics prediction networks and treating physical reasoning as a downstream task without investigating the questions above, taking for granted that the designed dynamics prediction would undoubtedly help the reasoning process. In this work, we take a closer look at this assumption, exploring this fundamental hypothesis by comparing two learning mechanisms: Learning from Dynamics (LfD) and Learning from Intuition (LfI). In the first experiment, we directly examine and compare these two mechanisms. Results show a surprising finding: Simple LfI is better than or on par with state-of-the-art LfD. This observation leads to the second experiment with Ground-truth Dynamics (GD), the ideal case of LfD wherein dynamics are obtained directly from a simulator. Results show that dynamics, if directly given instead of approximated, would achieve much higher performance than LfI alone on physical reasoning; this essentially serves as the performance upper bound. Yet practically, LfD mechanism can only predict Approximate Dynamics (AD) using dynamics learning modules that mimic the physical laws, making the following downstream physical reasoning modules degenerate into the LfI paradigm; see the third experiment. We note that this issue is hard to mitigate, as dynamics prediction errors inevitably accumulate in the long horizon. Finally, in the fourth experiment, we note that LfI, the extremely simpler strategy when done right, is more effective in learning to solve physical reasoning problems. Taken together, the results on the challenging benchmark of PHYRE show that LfI is, if not better, as good as LfD with bells and whistles for dynamics prediction. However, the potential improvement from LfD, though challenging, remains lucrative.",https://api.openreview.net/pdf/918c7cea477a210ee5f880ad34337a6582c23e64.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=On+the+Learning+Mechanisms+in+Physical+Reasoning
Ordered Subgraph Aggregation Networks,2022,NIPS,"['Chendi Qian', 'Gaurav Rattan', 'Floris Geerts', 'Mathias Niepert', 'Christopher Morris']",poster,"['GNNs', 'expressivity', 'subgraphs', 'differentiating through discrete structures']","Numerous subgraph-enhanced graph neural networks (GNNs) have emerged recently, provably boosting the expressive power of standard (message-passing) GNNs. However, there is a limited understanding of how these approaches relate to each other and to the Weisfeiler-Leman hierarchy. Moreover, current approaches either use all subgraphs of a given size, sample them uniformly at random, or use hand-crafted heuristics instead of learning to select subgraphs in a data-driven manner. Here, we offer a unified way to study such architectures by introducing a theoretical framework and extending the known expressivity results of subgraph-enhanced GNNs. Concretely, we show that increasing subgraph size always increases the expressive power and develop a better understanding of their limitations by relating them to the established $k\mathsf{\text{-}WL}$ hierarchy. In addition, we explore different approaches for learning to sample subgraphs using recent methods for backpropagating through complex discrete probability distributions. Empirically, we study the predictive performance of different subgraph-enhanced GNNs, showing that our data-driven architectures increase prediction accuracy on standard benchmark datasets compared to non-data-driven subgraph-enhanced graph neural networks while reducing computation time. ",https://api.openreview.net/pdf/3225d12e8932a32ed530882b5377dffa09314db9.pdf,graph;llm,https://scholar.google.com/scholar?q=Ordered+Subgraph+Aggregation+Networks
Bring Your Own Algorithm for Optimal Differentially Private Stochastic Minimax Optimization,2022,NIPS,"['Liang Zhang', 'Kiran Koshy Thekumparampil', 'Sewoong Oh', 'Niao He']",poster,[],"We study differentially private (DP) algorithms for smooth stochastic minimax optimization, with stochastic minimization as a byproduct. The holy grail of these settings is to guarantee the optimal trade-off between the privacy and the excess population loss, using an algorithm with a linear time-complexity in the number of training samples. We provide a general framework for solving differentially private stochastic minimax optimization (DP-SMO) problems, which enables the practitioners to bring their own base optimization algorithm and use it as a black-box to obtain the near-optimal privacy-loss trade-off. Our framework is inspired from the recently proposed Phased-ERM method [22] for nonsmooth differentially private stochastic convex optimization (DP-SCO), which exploits the stability of the empirical risk minimization (ERM) for the privacy guarantee. The flexibility of our approach enables us to sidestep the requirement that the base algorithm needs to have bounded sensitivity, and allows the use of sophisticated variance-reduced accelerated methods to achieve near-linear time-complexity. To the best of our knowledge, these are the first near-linear time algorithms with near-optimal guarantees on the population duality gap for smooth DP-SMO, when the objective is (strongly-)convex--(strongly-)concave. Additionally, based on our flexible framework, we enrich the family of near-linear time algorithms for smooth DP-SCO with the near-optimal privacy-loss trade-off.",https://api.openreview.net/pdf/3f2a7ba4818209ba10f31e71ac2740e09187f09a.pdf,optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Bring+Your+Own+Algorithm+for+Optimal+Differentially+Private+Stochastic+Minimax+Optimization
Assistive Teaching of Motor Control Tasks to Humans,2022,NIPS,"['Megha Srivastava', 'Erdem Biyik', 'Suvir Mirchandani', 'Noah Goodman', 'Dorsa Sadigh']",poster,"['teaching', 'shared autonomy', 'human-AI interaction', 'education', 'reinforcement learning']","Recent works on shared autonomy and assistive-AI technologies, such as assistive robotic teleoperation, seek to model and help human users with limited ability in a fixed task. However, these approaches often fail to account for humans' ability to adapt and eventually learn how to execute a control task themselves. Furthermore, in applications where it may be desirable for a human to intervene, these methods may have inhibited their ability to learn how to succeed with full self-control. In this paper, we focus on the problem of assistive teaching of motor control tasks such as parking a car or landing an aircraft. Despite their ubiquitous role in humans' daily activities and occupations, motor tasks are rarely taught in a uniform way due to their high complexity and variance. We propose an AI-assisted teaching algorithm that leverages skill discovery methods from reinforcement learning (RL) literature to (i) break down any motor control task into teachable skills, (ii) construct novel drill sequences, and (iii) individualize curricula to students with different capabilities. Through an extensive mix of synthetic and user studies on two motor control tasks - parking a car with a joystick and writing  characters from the Balinese alphabet - we show that assisted teaching with skills improve student performance by around 40% compared to practicing full trajectories without skills, and practicing with individualized drills can result in up to 25% further improvement.",https://api.openreview.net/pdf/51a2be6de2450f53005ee378771497421e2c8de9.pdf,reinforcement learning;llm,https://scholar.google.com/scholar?q=Assistive+Teaching+of+Motor+Control+Tasks+to+Humans
Multi-agent Performative Prediction with Greedy Deployment and Consensus Seeking Agents,2022,NIPS,"['Qiang LI', 'Chung-Yiu Yau', 'Hoi To Wai']",poster,"['Performative prediction', 'Multi-agent', 'Decentralized stochastic algorithm.']","We consider a scenario where multiple agents are learning a common decision vector from data which can be influenced by the agents’ decisions. This leads to the problem of multi-agent performative prediction (Multi-PfD). In this paper, we formulate Multi-PfD as a decentralized optimization problem that minimizes a sum of loss functions, where each loss function is based on a distribution influenced by the local decision vector. We first prove the necessary and sufficient condition for the Multi-PfD problem to admit a unique multi-agent performative stable (Multi-PS) solution. We show that enforcing consensus leads to a laxer condition for existence of Multi-PS solution with respect to the distributions’ sensitivities, compared to the single agent case. Then, we study a decentralized extension to  the greedy deployment scheme [Mendler-Dünner et al., 2020], called the DSGD-GD   scheme. We show that DSGD-GD converges to the Multi-PS solution and analyze its non asymptotic convergence rate. Numerical results validate our analysis. ",https://api.openreview.net/pdf/c0a0d7ef2de020ae7a052311da61c5f46cf69a65.pdf,reinforcement learning;optimization;multi-agent;llm,https://scholar.google.com/scholar?q=Multi-agent+Performative+Prediction+with+Greedy+Deployment+and+Consensus+Seeking+Agents
Prune and distill: similar reformatting of image information along rat visual cortex and deep neural networks,2022,NIPS,"['Paolo Muratore', 'Sina Tafazoli', 'Eugenio Piasini', 'Alessandro Laio', 'Davide Zoccolan']",poster,"['convolutional neural networks', 'computational neuroscience', 'rat', 'visual cortex', 'ventral stream', 'intrinsic dimensionality', 'vision', 'representation analysis']","Visual object recognition has been extensively studied in both neuroscience and computer vision. Recently, the most popular class of artificial systems for this task, deep convolutional neural networks (CNNs), has been shown to provide excellent models for its functional analogue in the brain, the ventral stream in visual cortex. This has prompted questions on what, if any, are the common principles underlying the reformatting of visual information as it flows through a CNN or the ventral stream. Here we consider some prominent statistical patterns that are known to exist in the internal representations of either CNNs or the visual cortex and look for them in the other system. We show that intrinsic dimensionality (ID) of object representations along the rat homologue of the ventral stream presents two distinct expansion-contraction phases, as previously shown for CNNs. Conversely, in CNNs, we show that training results in both distillation and active pruning (mirroring the increase in ID) of low- to middle-level image information in single units, as representations gain the ability to support invariant discrimination, in agreement with previous observations in rat visual cortex. Taken together, our findings suggest that CNNs and visual cortex share a similarly tight relationship between dimensionality expansion/reduction of object representations and reformatting of image information.",https://api.openreview.net/pdf/088d8061bc0fa4f6a0afd36a18c5b18a29e5bde6.pdf,zero_few-shot;representation;active learning;flow;distillation;llm,https://scholar.google.com/scholar?q=Prune+and+distill:+similar+reformatting+of+image+information+along+rat+visual+cortex+and+deep+neural+networks
Meta-Complementing the Semantics of Short Texts in Neural Topic Models,2022,NIPS,"['Delvin Ce Zhang', 'Hady W. Lauw']",poster,"['neural topic model', 'short text', 'graph neural networks', 'semantic complement']","Topic models infer latent topic distributions based on observed word co-occurrences in a text corpus. While typically a corpus contains documents of variable lengths, most previous topic models treat documents of different lengths uniformly, assuming that each document is sufficiently informative. However, shorter documents may have only a few word co-occurrences, resulting in inferior topic quality.  Some other previous works assume that all documents are short, and leverage external auxiliary data, e.g., pretrained word embeddings and document connectivity. Orthogonal to existing works, we remedy this problem within the corpus itself by proposing a Meta-Complement Topic Model, which improves topic quality of short texts by transferring the semantic knowledge learned on long documents to complement semantically limited short texts. As a self-contained module, our framework is agnostic to auxiliary data and can be further improved by flexibly integrating them into our framework. Specifically, when incorporating document connectivity, we further extend our framework to complement documents with limited edges. Experiments demonstrate the advantage of our framework.
",https://api.openreview.net/pdf/f530ea5385a4f8eac1cfbdce244dd0e6bc963618.pdf,meta-learning;transfer learning;llm,https://scholar.google.com/scholar?q=Meta-Complementing+the+Semantics+of+Short+Texts+in+Neural+Topic+Models
Capturing Failures of Large Language Models via Human Cognitive Biases,2022,NIPS,"['Erik Jones', 'Jacob Steinhardt']",poster,"['open-ended generation', 'ai safety', 'codex', 'robustness', 'cognitive biases']","Large language models generate complex, open-ended outputs: instead of outputting a class label they write summaries, generate dialogue, or produce working code. In order to asses the reliability of these open-ended generation systems, we aim to identify qualitative categories of erroneous behavior, beyond identifying individual errors. To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biases---systematic patterns of deviation from rational judgement. Specifically, we use cognitive biases as motivation to (i) generate hypotheses for problems that models may have, and (ii) develop experiments that elicit these problems. Using code generation as a case study, we find that OpenAI’s Codex errs predictably based on how the input prompt is framed, adjusts outputs towards anchors, and is biased towards outputs that mimic frequent training examples. We then use our framework to elicit high-impact errors such as incorrectly deleting files. Our results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave.",https://api.openreview.net/pdf/add89b88c20ad9f18abfa44f2eb70ff2ef8f85ea.pdf,generative model;llm,https://scholar.google.com/scholar?q=Capturing+Failures+of+Large+Language+Models+via+Human+Cognitive+Biases
Meta-Query-Net: Resolving Purity-Informativeness Dilemma in Open-set Active Learning,2022,NIPS,"['Dongmin Park', 'Yooju Shin', 'Jihwan Bang', 'Youngjun Lee', 'Hwanjun Song', 'Jae-Gil Lee']",poster,"['Active Learning', 'Open-set Noise', 'Out-of-distribution Data']","Unlabeled data examples awaiting annotations contain open-set noise inevitably. A few active learning studies have attempted to deal with this open-set noise for sample selection by filtering out the noisy examples. However, because focusing on the purity of examples in a query set leads to overlooking the informativeness of the examples, the best balancing of purity and informativeness remains an important question. In this paper, to solve this purity-informativeness dilemma in open-set active learning, we propose a novel Meta-Query-Net (MQ-Net) that adaptively finds the best balancing between the two factors. Specifically, by leveraging the multi-round property of active learning, we train MQ-Net using a query set without an additional validation set. Furthermore, a clear dominance relationship between unlabeled examples is effectively captured by MQ-Net through a novel skyline regularization. Extensive experiments on multiple open-set active learning scenarios demonstrate that the proposed MQ-Net achieves 20.14% improvement in terms of accuracy, compared with the state-of-the-art methods.",https://api.openreview.net/pdf/fc7e220c55ca8a7984e439aea2f2b8b204832856.pdf,graph;adaptive;meta-learning;active learning;llm,https://scholar.google.com/scholar?q=Meta-Query-Net:+Resolving+Purity-Informativeness+Dilemma+in+Open-set+Active+Learning
Mind Reader: Reconstructing complex images from brain activities,2022,NIPS,"['Sikun Lin', 'Thomas Christopher Sprague', 'Ambuj Singh']",poster,[],"Understanding how the brain encodes external stimuli and how these stimuli can be decoded from the measured brain activities are long-standing and challenging questions in neuroscience. In this paper, we focus on reconstructing the complex image stimuli from fMRI (functional magnetic resonance imaging) signals. Unlike previous works that reconstruct images with single objects or simple shapes, our work aims to reconstruct image stimuli that are rich in semantics, closer to everyday scenes, and can reveal more perspectives. However, data scarcity of fMRI datasets is the main obstacle to applying state-of-the-art deep learning models to this problem. We find that incorporating an additional text modality is beneficial for the reconstruction problem compared to directly translating brain signals to images. Therefore, the modalities involved in our method are: (i) voxel-level fMRI signals, (ii) observed images that trigger the brain signals, and (iii) textual description of the images. To further address data scarcity, we leverage an aligned vision-language latent space pre-trained on massive datasets. Instead of training models from scratch to find a latent space shared by the three modalities, we encode fMRI signals into this pre-aligned latent space. Then, conditioned on embeddings in this space, we reconstruct images with a generative model. The reconstructed images from our pipeline balance both naturalness and fidelity: they are photo-realistic and capture the ground truth image contents well.",https://api.openreview.net/pdf/edf0233d2639014c2f39880a88db1b34db8d5a1a.pdf,graph;generative model;multimodal;llm,https://scholar.google.com/scholar?q=Mind+Reader:+Reconstructing+complex+images+from+brain+activities
Adapting to Online Label Shift with Provable Guarantees,2022,NIPS,"['Yong Bai', 'Yu-Jie Zhang', 'Peng Zhao', 'Masashi Sugiyama', 'Zhi-Hua Zhou']",poster,"['online label shift', 'dynamic regret']","The standard supervised learning paradigm works effectively when training data shares the same distribution as the upcoming testing samples. However, this stationary assumption is often violated in real-world applications, especially when testing data appear in an online fashion. In this paper, we formulate and investigate the problem of \emph{online label shift} (OLaS): the learner trains an initial model from the labeled offline data and then deploys it to an unlabeled online environment where the underlying label distribution changes over time but the label-conditional density does not. The non-stationarity nature and the lack of supervision make the problem challenging to be tackled. To address the difficulty, we construct a new unbiased risk estimator that utilizes the unlabeled data, which exhibits many benign properties albeit with potential non-convexity. Building upon that, we propose novel online ensemble algorithms to deal with the non-stationarity of the environments. Our approach enjoys optimal \emph{dynamic regret}, indicating that the performance is competitive with a clairvoyant who knows the online environments in hindsight and then chooses the best decision for each round. The obtained dynamic regret bound scales with the intensity and pattern of label distribution shift, hence exhibiting the adaptivity in the OLaS problem. Extensive experiments are conducted to validate the effectiveness and support our theoretical findings.
",https://api.openreview.net/pdf/1ffeddf4e99fddd986d175e423d1d79723eb3f10.pdf,offline reinforcement learning;graph;online learning;llm,https://scholar.google.com/scholar?q=Adapting+to+Online+Label+Shift+with+Provable+Guarantees
What Can Transformers Learn In-Context? A Case Study of Simple Function Classes,2022,NIPS,"['Shivam Garg', 'Dimitris Tsipras', 'Percy Liang', 'Gregory Valiant']",poster,"['in-context learning', 'transformers', 'linear regression']","In-context learning is the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To investigate this, we consider the problem of training a model to in-context learn a function class (e.g., linear functions): given data derived from some functions in the class, can we train a model (e.g., a Transformer) to in-context learn most functions from that class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions---that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the Transformer and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes: sparse linear functions where the model outperforms least squares and nearly matches the performance of Lasso, and two-layer neural networks where the model performs comparably to neural networks trained on in-context examples using gradient descent.",https://api.openreview.net/pdf/5383ce90e93988b8648c5df61c5ea1c9a7dcc96a.pdf,transformer;inference;sparse;llm,https://scholar.google.com/scholar?q=What+Can+Transformers+Learn+In-Context?+A+Case+Study+of+Simple+Function+Classes
Learning with little mixing,2022,NIPS,"['Ingvar Ziemann', 'Stephen Tu']",poster,"['Learning Theory', 'Nonlinear Dynamical Systems', 'Learning with dependent data']","We study square loss in a realizable time-series framework with martingale difference noise. Our main result is a fast rate excess risk bound which shows that whenever a trajectory hypercontractivity condition holds, the risk of the least-squares estimator on dependent data matches the iid rate order-wise after a burn-in time. In comparison, many existing results in learning from dependent data have rates where the effective sample size is deflated by a factor of the mixing-time of the underlying process, even after the burn-in time. Furthermore, our results allow the covariate process to exhibit long range correlations which are substantially weaker than geometric ergodicity. We call this phenomenon learning with little mixing, and present several examples for when it occurs: bounded function classes for which the $L^2$ and $L^{2+\epsilon}$ norms are equivalent, finite state irreducible and aperiodic Markov chains, various parametric models, and a broad family of infinite dimensional $\ell^2(\mathbb{N})$ ellipsoids. By instantiating our main result to system identification of nonlinear dynamics with generalized linear model  transitions, we obtain a nearly minimax optimal  excess risk bound after only a polynomial burn-in time.
",https://api.openreview.net/pdf/60e1fc2f48df2c2b1a3367b8e1d5c7a22f5a8e16.pdf,zero_few-shot;online learning;metric;llm,https://scholar.google.com/scholar?q=Learning+with+little+mixing
DiSC: Differential Spectral Clustering of Features,2022,NIPS,"['Ram Dyuthi Sristi', 'Gal Mishne', 'Ariel Jaffe']",poster,"['differential features', 'spectral clustering', 'feature selection', 'manifold learning', 'graph theory']","Selecting subsets of features that differentiate between two conditions is a key task in a broad range of scientific domains. In many applications, the features of interest form clusters with similar effects on the data at hand. To recover such clusters we develop DiSC, a data-driven approach for detecting groups of features that differentiate between conditions. For each condition, we construct a graph whose nodes correspond to the features and whose weights are functions of the similarity between them for that condition. We then apply a spectral approach to compute subsets of nodes whose connectivity pattern differs significantly between the condition-specific feature graphs. On the theoretical front, we analyze our approach with a toy example based on the stochastic block model. We evaluate DiSC on a variety of datasets, including MNIST, hyperspectral imaging, simulated scRNA-seq and task fMRI, and demonstrate that DiSC uncovers features that better differentiate between conditions compared to competing methods.",https://api.openreview.net/pdf/2f32a75bc3d7342ce670ea443dbe4295b9018a43.pdf,graph;llm,https://scholar.google.com/scholar?q=DiSC:+Differential+Spectral+Clustering+of+Features
Online Decision Mediation,2022,NIPS,"['Daniel Jarrett', 'Alihan Hüyük', 'Mihaela van der Schaar']",poster,"['Decision System', 'Decision Mediation', 'Decision Support']","Consider learning a decision support assistant to serve as an intermediary between (oracle) expert behavior and (imperfect) human behavior: At each time, the algorithm observes an action chosen by a fallible agent, and decides whether to *accept* that agent's decision, *intervene* with an alternative, or *request* the expert's opinion. For instance, in clinical diagnosis, fully-autonomous machine behavior is often beyond ethical affordances, thus real-world decision support is often limited to monitoring and forecasting. Instead, such an intermediary would strike a prudent balance between the former (purely prescriptive) and latter (purely descriptive) approaches, while providing an efficient interface between human mistakes and expert feedback. In this work, we first formalize the sequential problem of *online decision mediation*---that is, of simultaneously learning and evaluating mediator policies from scratch with *abstentive feedback*: In each round, deferring to the oracle obviates the risk of error, but incurs an upfront penalty, and reveals the otherwise hidden expert action as a new training data point. Second, we motivate and propose a solution that seeks to trade off (immediate) loss terms against (future) improvements in generalization error; in doing so, we identify why conventional bandit algorithms may fail. Finally, through experiments and sensitivities on a variety of datasets, we illustrate consistent gains over applicable benchmarks on performance measures with respect to the mediator policy, the learned model, and the decision-making system as a whole.",https://api.openreview.net/pdf/69b36a79280ad85becce4aa44aa7684f44e4c4a1.pdf,reinforcement learning;online learning;llm,https://scholar.google.com/scholar?q=Online+Decision+Mediation
Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning,2022,NIPS,"['Yuchong Sun', 'Hongwei Xue', 'Ruihua Song', 'Bei Liu', 'Huan Yang', 'Jianlong Fu']",poster,['video-language pre-training'],"Large-scale video-language pre-training has shown significant improvement in video-language understanding tasks. Previous studies of video-language pretraining mainly focus on short-form videos (i.e., within 30 seconds) and sentences, leaving long-form video-language pre-training rarely explored. Directly learning representation from long-form videos and language may benefit many long-form
video-language understanding tasks. However, it is challenging due to the difficulty of modeling long-range relationships and the heavy computational burden caused by more frames. In this paper, we introduce a Long-Form VIdeo-LAnguage pre-training model (LF-VILA) and train it on a large-scale long-form video and paragraph dataset constructed from an existing public dataset. To effectively capture
the rich temporal dynamics and to better align video and language in an efficient end-to-end manner, we introduce two novel designs in our LF-VILA model. We first propose a Multimodal Temporal Contrastive (MTC) loss to learn the temporal relation across different modalities by encouraging fine-grained alignment between long-form videos and paragraphs. Second, we propose a Hierarchical Temporal Window Attention (HTWA) mechanism to effectively capture long-range dependency while reducing computational cost in Transformer. We fine-tune the pre-trained LF-VILA model on seven downstream long-form video-language understanding tasks of paragraph-to-video retrieval and long-form video question-answering, and achieve new state-of-the-art performances. Specifically, our model achieves 16.1% relative improvement on ActivityNet paragraph-to-video retrieval task and 2.4% on How2QA task, respectively. We release our code, dataset, and pre-trained models at https://github.com/microsoft/XPretrain.
",https://api.openreview.net/pdf/023253eaf12b20cd205bc854b4f8c504a41ca47a.pdf,graph;transformer;representation;contrastive learning;multimodal;llm,https://scholar.google.com/scholar?q=Long-Form+Video-Language+Pre-Training+with+Multimodal+Temporal+Contrastive+Learning
Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation,2022,NIPS,"['Yao Qin', 'Chiyuan Zhang', 'Ting Chen', 'Balaji Lakshminarayanan', 'Alex Beutel', 'Xuezhi Wang']",poster,"['Robustness', 'Distributional shift', 'Vision transformers']","We investigate the robustness of vision transformers (ViTs) through the lens of their special patch-based architectural structure, i.e., they process an image as a sequence of image patches. We find that ViTs are surprisingly insensitive to patch-based transformations, even when the transformation largely destroys the original semantics and makes the image unrecognizable by humans. This indicates that ViTs heavily use features that survived such transformations but are generally not indicative of the semantic class to humans. Further investigations show that these features are useful but non-robust, as ViTs trained on them can achieve high in-distribution accuracy, but break down under distribution shifts. From this understanding, we ask: can training the model to rely less on these features improve ViT robustness and out-of-distribution performance? We use the images transformed with our patch-based operations as negatively augmented views and offer losses to regularize the training away from using non-robust features. This is a complementary view to existing research that mostly focuses on augmenting inputs with semantic-preserving transformations to enforce models' invariance. We show that patch-based negative augmentation consistently improves robustness of ViTs on ImageNet based robustness benchmarks across 20+ different experimental settings. Furthermore, we find our patch-based negative augmentation are complementary to traditional (positive) data augmentation techniques and batch-based negative examples in contrastive learning. ",https://api.openreview.net/pdf/886562bb40c81b563a3a721dacd2aa3970e42db8.pdf,graph;transformer;contrastive learning;augmentation;llm,https://scholar.google.com/scholar?q=Understanding+and+Improving+Robustness+of+Vision+Transformers+through+Patch-based+Negative+Augmentation
Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,2022,NIPS,"['Byoungjip Kim', 'Sungik Choi', 'Dasol Hwang', 'Moontae Lee', 'Honglak Lee']",poster,"['Multimodal Representation Learning', 'Transfer Learning', 'CLIP']","Despite surprising performance on zero-shot transfer, pre-training a large-scale multimodal model is often prohibitive as it requires a huge amount of data and computing resources. In this paper, we propose a method (BeamCLIP) that can effectively transfer the representations of a large pre-trained multimodal model (CLIP-ViT) into a small target model (e.g., ResNet-18). For unsupervised transfer, we introduce cross-modal similarity matching (CSM) that enables a student model to learn the representations of a teacher model by matching the relative similarity distribution across text prompt embeddings. To better encode the text prompts, we design context-based prompt augmentation (CPA) that can alleviate the lexical ambiguity of input text prompts. Our experiments show that unsupervised representation transfer of a pre-trained vision-language model enables a small ResNet-18 to achieve a better ImageNet-1K top-1 linear probe accuracy (66.2%) than vision-only self-supervised learning (SSL) methods (e.g., SimCLR: 51.8%, SwAV: 63.7%), while closing the gap with supervised learning (69.8%).",https://api.openreview.net/pdf/c18eed48d618926e942b2df6833e215809d19706.pdf,zero_few-shot;representation;transfer learning;augmentation;multimodal;llm,https://scholar.google.com/scholar?q=Transferring+Pre-trained+Multimodal+Representations+with+Cross-modal+Similarity+Matching
STNDT: Modeling Neural Population Activity with Spatiotemporal Transformers,2022,NIPS,"['Trung Le', 'Eli Shlizerman']",poster,"['neuroscience', 'systems neuroscience', 'computational neuroscience', 'neural population dynamics', 'brain-computer interfaces', 'neuroprosthetics', 'electrophysiology', 'neural coding', 'transformers']","Modeling neural population dynamics underlying noisy single-trial spiking activities is essential for relating neural observation and behavior. A recent non-recurrent method - Neural Data Transformers (NDT) - has shown great success in capturing neural dynamics with low inference latency without an explicit dynamical model. However, NDT focuses on modeling the temporal evolution of the population activity while neglecting the rich covariation between individual neurons. In this paper we introduce SpatioTemporal Neural Data Transformer (STNDT), an NDT-based architecture that explicitly models responses of individual neurons in the population across time and space to uncover their underlying firing rates. In addition, we propose a contrastive learning loss that works in accordance with mask modeling objective to further improve the predictive performance. We show that our model achieves state-of-the-art performance on ensemble level in estimating neural activities across four neural datasets, demonstrating its capability to capture autonomous and non-autonomous dynamics spanning different cortical regions while being completely agnostic to the specific behaviors at hand. Furthermore, STNDT spatial attention mechanism reveals consistently important subsets of neurons that play a vital role in driving the response of the entire population, providing interpretability and key insights into how the population of neurons performs computation.",https://api.openreview.net/pdf/31af966c427775716d6c7724156947089c6c1d78.pdf,zero_few-shot;transformer;contrastive learning;inference;llm,https://scholar.google.com/scholar?q=STNDT:+Modeling+Neural+Population+Activity+with+Spatiotemporal+Transformers
"Double Bubble, Toil and Trouble: Enhancing Certified Robustness through Transitivity",2022,NIPS,"['Andrew Craig Cullen', 'Paul Montague', 'Shijie Liu', 'Sarah Monazam Erfani', 'Benjamin I. P. Rubinstein']",poster,"['certified robustness', 'adversarial', 'guarantees', 'adversarial defence', 'adversarial attack']","In response to subtle adversarial examples flipping classifications of neural network models, recent research has promoted certified robustness as a solution. There, invariance of predictions to all norm-bounded attacks is achieved through randomised smoothing of network inputs. Today's state-of-the-art certifications make optimal use of the class output scores at the input instance under test: no better radius of certification (under the $L_2$ norm) is possible given only these score. However, it is an open question as to whether such lower bounds can be improved using local information around the instance under test.  In this work, we demonstrate how today's ``optimal'' certificates can be improved by exploiting both the transitivity of certifications, and the geometry of the input space, giving rise to what we term Geometrically-Informed Certified Robustness. By considering the smallest distance to points on the boundary of a set of certifications this approach improves certifications for more than $80 \%$ of Tiny-Imagenet instances, yielding an on average $5\%$ increase in the associated certification. When incorporating training time processes that enhance the certified radius, our technique shows even more promising results, with a uniform $4$ percentage point increase in the achieved certified radius.",https://api.openreview.net/pdf/0e75a89bef7b6898dabe2323ed20581f89c47111.pdf,zero_few-shot;metric;llm,"https://scholar.google.com/scholar?q=Double+Bubble,+Toil+and+Trouble:+Enhancing+Certified+Robustness+through+Transitivity"
Why Robust Generalization in Deep Learning is Difficult: Perspective of Expressive Power,2022,NIPS,"['Binghui Li', 'Jikai Jin', 'Han Zhong', 'John E. Hopcroft', 'Liwei Wang']",poster,"['deep learning theory', 'adversarial robustness', 'robust generalization gap', 'expressive power']","It is well-known that modern neural networks are vulnerable to adversarial examples. To mitigate this problem, a series of robust learning algorithms have been proposed. However, although the robust training error can be near zero via some methods, all existing algorithms lead to a high robust generalization error. In this paper, we provide a theoretical understanding of this puzzling phenomenon from the perspective of expressive power for deep neural networks. Specifically, for binary classification problems with well-separated data, we show that, for ReLU networks, while mild over-parameterization is sufficient for high robust training accuracy, there exists a constant robust generalization gap unless the size of the neural network is exponential in the data dimension $d$. This result holds even if the data is linear separable (which means achieving standard generalization is easy), and more generally for any parameterized function classes as long as their VC dimension is at most polynomial in the number of parameters. Moreover, we establish an improved upper bound of $\exp({\mathcal{O}}(k))$ for the network size to achieve low robust generalization error when the data lies on a manifold with intrinsic dimension $k$ ($k \ll d$). Nonetheless, we also have a lower bound that grows exponentially with respect to $k$ --- the curse of dimensionality is inevitable. By demonstrating an exponential separation between the network size for achieving low robust training and generalization error, our results reveal that the hardness of robust generalization may stem from the expressive power of practical models.",https://api.openreview.net/pdf/4ae16a2fdec35a0cba6047acc313e00dca58e092.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Why+Robust+Generalization+in+Deep+Learning+is+Difficult:+Perspective+of+Expressive+Power
Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models,2022,NIPS,"['Dongkuan Xu', 'Subhabrata Mukherjee', 'Xiaodong Liu', 'Debadeepta Dey', 'Wenhui Wang', 'Xiang Zhang', 'Ahmed Hassan Awadallah', 'Jianfeng Gao']",poster,"['Pre-trained Language Models', 'Knowledge Distillation', 'Neural Architecture Search']","Traditional knowledge distillation (KD) methods manually design student architectures to compress large models given pre-specified computational cost. This requires several trials to find viable students, and repeating the process with change in computational budget. We use Neural Architecture Search (NAS) to automatically distill several compressed students with variable cost from a large model. Existing NAS methods train a single SuperLM consisting of millions of subnetworks with weight-sharing, resulting in interference between subnetworks of different sizes. Additionally, many of these works are task-specific requiring task labels for SuperLM training. Our framework AutoDistil addresses above challenges with the following steps: (a) Incorporates inductive bias and heuristics to partition Transformer search space into K compact sub-spaces (e.g., K=3 can generate typical student sizes of base, small and tiny); (b) Trains one SuperLM for each sub-space using task-agnostic objective (e.g., self-attention distillation) with weight-sharing of students; (c) Lightweight search for the optimal student without re-training. Task-agnostic training and search allow students to be reused for fine-tuning on any downstream task. Experiments on GLUE benchmark demonstrate AutoDistil to outperform state-of-the-art KD and NAS methods with upto 3x reduction in computational cost and negligible loss in task performance. Code and model checkpoints are available at https://github.com/microsoft/autodistil.",https://api.openreview.net/pdf/582befa5086ba5b0019e157eea639fa81c3461b0.pdf,zero_few-shot;transformer;distillation;llm,https://scholar.google.com/scholar?q=Few-shot+Task-agnostic+Neural+Architecture+Search+for+Distilling+Large+Language+Models
Fault-Aware Neural Code Rankers,2022,NIPS,"['Jeevana Priya Inala', 'Chenglong Wang', 'Mei Yang', 'Andres Codas', 'Mark Encarnación', 'Shuvendu K Lahiri', 'Madanlal Musuvathi', 'Jianfeng Gao']",poster,"['program synthesis', 'code generation', 'AI for code']","Large language models (LLMs) have demonstrated an impressive ability to generate code for various programming tasks. In many instances, LLMs can generate a correct program for a task when given numerous trials. Consequently, a recent trend is to do large scale sampling of programs using a model and then filtering/ranking the programs based on the program execution on a small number of known unit tests to select one candidate solution. However, these approaches assume that the unit tests are given and assume the ability to safely execute the generated programs (which can do arbitrary dangerous operations such as file manipulations). Both of the above assumptions are impractical in real-world software development. In this paper, we propose CodeRanker, a neural ranker that can predict the correctness of a sampled program without executing it. Our CodeRanker is fault-aware i.e., it is trained to predict different kinds of execution information such as predicting the exact compile/runtime error type (e.g., an IndexError or a TypeError). We show that CodeRanker can significantly increase the pass@1 accuracy of various code generation models (including Codex, GPT-Neo, GPT-J) on APPS, HumanEval and MBPP datasets. 
",https://api.openreview.net/pdf/f752177500dfddf4b8c6ec697b1656bf977e8fb6.pdf,generative model;llm,https://scholar.google.com/scholar?q=Fault-Aware+Neural+Code+Rankers
SoftPatch: Unsupervised Anomaly Detection with Noisy Data,2022,NIPS,"['Jiang Xi', 'Jianlin Liu', 'Jinbao Wang', 'Qiang Nie', 'Kai WU', 'Yong Liu', 'Chengjie Wang', 'Feng Zheng']",poster,"['Anomaly Detection', 'Noisy Label', 'Outlier Detection']","Although mainstream unsupervised anomaly detection (AD) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data. Training with noisy data is an inevitable problem in real-world anomaly detection but is seldom discussed. This paper considers label-level noise in image sensory anomaly detection for the first time. To solve this problem, we proposed a memory-based unsupervised AD method, SoftPatch, which efficiently denoises the data at the patch level. Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction. The scores are then stored in the memory bank to soften the anomaly detection boundary. Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset. Comprehensive experiments in various noise scenes demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the MVTecAD and BTAD benchmarks and is comparable to those methods under the setting without noise.",https://api.openreview.net/pdf/c9ac87f209016e331b8a7096676670bc741354b3.pdf,llm,https://scholar.google.com/scholar?q=SoftPatch:+Unsupervised+Anomaly+Detection+with+Noisy+Data
SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders,2022,NIPS,"['Gang Li', 'Heliang Zheng', 'Daqing Liu', 'Chaoyue Wang', 'Bing Su', 'Changwen Zheng']",poster,"['Semantic-Guided Masking', 'Masked Autoencoders', 'Self-Supervised Learning', 'Semantic part learning']","Recently, significant progress has been made in masked image modeling to catch up to masked language modeling. However, unlike words in NLP, the lack of semantic decomposition of images still makes masked autoencoding (MAE) different between vision and language. In this paper, we explore a potential visual analogue of words, i.e., semantic parts, and we integrate semantic information into the training process of MAE by proposing a Semantic-Guided Masking strategy. Compared to widely adopted random masking, our masking strategy can gradually guide the network to learn various information, i.e., from intra-part patterns to inter-part relations. In particular, we achieve this in two steps. 1) Semantic part learning: we design a self-supervised part learning method to obtain semantic parts by leveraging and refining the multi-head attention of a ViT-based encoder. 2) Semantic-guided MAE (SemMAE) training: we design a masking strategy that varies from masking a portion of patches in each part to masking a portion of (whole) parts in an image. Extensive experiments on various vision tasks show that SemMAE can learn better image representation by integrating semantic information. In particular, SemMAE achieves 84.5% fine-tuning accuracy on ImageNet-1k, which outperforms the vanilla MAE by 1.4%. In the semantic segmentation and fine-grained recognition tasks, SemMAE also brings significant improvements and yields the state-of-the-art performance.",https://api.openreview.net/pdf/dbacc582dff0d6c6d8bbd638d407752eb337bf56.pdf,graph;zero_few-shot;transformer;representation;segmentation;llm,https://scholar.google.com/scholar?q=SemMAE:+Semantic-Guided+Masking+for+Learning+Masked+Autoencoders
Counterfactual Neural Temporal Point Process for Estimating Causal Influence of Misinformation on Social Media,2022,NIPS,"['Yizhou Zhang', 'Defu Cao', 'Yan Liu']",poster,"['Causal Inference', 'Temporal Point Process', 'Misinformation Influence', 'Counterfactual Analysis', 'Fake News', 'Social Media', 'Deep Learning']","Recent years have witnessed the rise of misinformation campaigns that spread specific narratives on social media to manipulate public opinions on different areas, such as politics and healthcare. Consequently, an effective and efficient automatic methodology to estimate the influence of the misinformation on user beliefs and activities is needed. However, existing works on misinformation impact estimation either rely on small-scale psychological experiments or can only discover the correlation between user behaviour and misinformation. To address these issues, in this paper, we build up a causal framework that model the causal effect of misinformation from the perspective of temporal point process. To adapt the large-scale data, we design an efficient yet precise way to estimate the \textbf{Individual Treatment Effect} (ITE) via neural temporal point process and gaussian mixture models. Extensive experiments on synthetic dataset verify the effectiveness and efficiency of our model. We further apply our model on a real-world dataset of social media posts and engagements about COVID-19 vaccines. The experimental results indicate that our model recognized identifiable causal effect of misinformation that hurts people's subjective emotions toward the vaccines.",https://api.openreview.net/pdf/e5410e2b83f80dd34e01e8d79d6deeafeff11af4.pdf,llm,https://scholar.google.com/scholar?q=Counterfactual+Neural+Temporal+Point+Process+for+Estimating+Causal+Influence+of+Misinformation+on+Social+Media
Beyond Not-Forgetting: Continual Learning with Backward Knowledge Transfer,2022,NIPS,"['Sen Lin', 'Li Yang', 'Deliang Fan', 'Junshan Zhang']",poster,[],"By learning a sequence of tasks continually, an agent in continual learning (CL) can improve the learning performance of both a new task and `old' tasks by leveraging the forward knowledge transfer and the backward knowledge transfer, respectively. However, most existing CL methods focus on addressing catastrophic forgetting in neural networks by minimizing the modification of the learnt model for old tasks. This inevitably limits the backward knowledge transfer from the new task to the old tasks, because judicious model updates could possibly improve the learning performance of the old tasks as well. To tackle this problem, we first theoretically analyze the conditions under which updating the learnt model of old tasks could be beneficial for CL and also lead to backward knowledge transfer, based on the gradient projection onto the input subspaces of old tasks. Building on the theoretical analysis, we next develop a ContinUal learning method with Backward knowlEdge tRansfer (CUBER), for a fixed capacity neural network without data replay. In particular, CUBER first characterizes the task correlation to identify the positively correlated old tasks in a layer-wise manner, and then selectively modifies the learnt model of the old tasks when learning the new task. Experimental studies show that CUBER can even achieve positive backward knowledge transfer on several existing CL benchmarks for the first time without data replay, where the related baselines still suffer from catastrophic forgetting (negative backward knowledge transfer). The superior performance of CUBER on the backward knowledge transfer also leads to higher accuracy accordingly.",https://api.openreview.net/pdf/939506c373d9455be9cf5958eb4012c89be5ef55.pdf,reinforcement learning;graph;transfer learning;llm,https://scholar.google.com/scholar?q=Beyond+Not-Forgetting:+Continual+Learning+with+Backward+Knowledge+Transfer
Old can be Gold: Better Gradient Flow can Make Vanilla-GCNs Great Again,2022,NIPS,"['AJAY KUMAR JAISWAL', 'Peihao Wang', 'Tianlong Chen', 'Justin F Rousseau', 'Ying Ding', 'Zhangyang Wang']",poster,"['Deep Graph Neural Networks', 'Gradient Flow', 'Initialization', 'Isometric Learning']","Despite the enormous success of Graph Convolutional Networks (GCNs) in modeling graph-structured data, most of the current GCNs are shallow due to the notoriously challenging problems of over-smoothening and information squashing along with conventional difficulty caused by vanishing gradients and over-fitting. Previous works have been primarily focused on the study of over-smoothening and over-squashing phenomena in training deep GCNs. Surprisingly, in comparison with CNNs/RNNs, very limited attention has been given to understanding how healthy gradient flow can benefit the trainability of deep GCNs. In this paper, firstly, we provide a new perspective of gradient flow to understand the substandard performance of deep GCNs and hypothesize that by facilitating healthy gradient flow, we can significantly improve their trainability, as well as achieve state-of-the-art (SOTA) level performance from vanilla-GCNs. Next, we argue that blindly adopting the Glorot initialization for GCNs is not optimal, and derive a topology-aware isometric initialization scheme for vanilla-GCNs based on the principles of isometry. Additionally, contrary to ad-hoc addition of skip-connections, we propose to use gradient-guided dynamic rewiring of vanilla-GCNs with skip connections. Our dynamic rewiring method uses the gradient flow within each layer during training to introduce on-demand skip-connections adaptively. We provide extensive empirical evidence across multiple datasets that our methods improve gradient flow in deep vanilla-GCNs and significantly boost their performance to comfortably compete and outperform many fancy state-of-the-art methods. Codes are available at:  https://github.com/VITA-Group/GradientGCN.",https://api.openreview.net/pdf/062fdaeee27cf01cac33d8c36312086a9eb68b7f.pdf,graph;zero_few-shot;transformer;adaptive;metric;flow;llm,https://scholar.google.com/scholar?q=Old+can+be+Gold:+Better+Gradient+Flow+can+Make+Vanilla-GCNs+Great+Again
Effects of Data Geometry in Early Deep Learning,2022,NIPS,"['Saket Tiwari', 'George Konidaris']",poster,"['Deep learning', 'geometry', 'manifolds', 'deep learning theory']","Deep neural networks can approximate functions on different types of data, from images to graphs, with varied underlying structure. This underlying structure can be viewed as the geometry of the data manifold. By extending recent advances in the theoretical understanding of neural networks, we study how a randomly initialized neural network with piecewise linear activation splits the data manifold into regions where the neural network behaves as a linear function.   We derive bounds on the density of boundary of linear regions and the distance to these boundaries on the data manifold. This leads to insights into the expressivity of randomly initialized deep neural networks on non-Euclidean data sets. We empirically corroborate our theoretical results using a toy supervised learning problem. Our experiments demonstrate that number of linear regions varies across manifolds and the results hold with changing neural network architectures. We further demonstrate how the complexity of linear regions is different on the low dimensional manifold of images as compared to the Euclidean space, using the MetFaces dataset.",https://api.openreview.net/pdf/2cc3470e2114b7feffc8c41f7f46be06d5adc2b3.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Effects+of+Data+Geometry+in+Early+Deep+Learning
Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal,2022,NIPS,"['Yucheng Shi', 'Yahong Han', 'Yu-an Tan', 'Xiaohui Kuang']",poster,"['Adversarial attack', 'Black-box attack', 'Decision-based attack', 'Vision transformer']","Vision transformers (ViTs) have demonstrated impressive performance and stronger adversarial robustness compared to Convolutional Neural Networks (CNNs). On the one hand, ViTs' focus on global interaction between individual patches reduces the local noise sensitivity of images. On the other hand, the neglect of noise sensitivity differences between image regions by existing decision-based attacks further compromises the efficiency of noise compression, especially for ViTs. Therefore, validating the black-box adversarial robustness of ViTs when the target model can only be queried still remains a challenging problem. In this paper, we theoretically analyze the limitations of existing decision-based attacks from the perspective of noise sensitivity difference between regions of the image, and propose a new decision-based black-box attack against ViTs, termed Patch-wise Adversarial Removal (PAR). PAR divides images into patches through a coarse-to-fine search process and compresses the noise on each patch separately. PAR records the noise magnitude and noise sensitivity of each patch and selects the patch with the highest query value for noise compression. In addition, PAR can be used as a noise initialization method for other decision-based attacks to improve the noise compression efficiency on both ViTs and CNNs without introducing additional calculations. Extensive experiments on three datasets demonstrate that PAR achieves a much lower noise magnitude with the same number of queries.",https://api.openreview.net/pdf/89c2342a50ca0873643e74491699e482f8ccaa79.pdf,graph;zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Decision-based+Black-box+Attack+Against+Vision+Transformers+via+Patch-wise+Adversarial+Removal
Non-identifiability and the Blessings of Misspecification in Models of Molecular Fitness,2022,NIPS,"['Eli N Weinstein', 'Alan Nawzad Amin', 'Jonathan Frazer', 'Debora Susan Marks']",poster,"['identifiability', 'misspecification', 'robustness', 'proteins', 'phylogenetics']","Understanding the consequences of mutation for molecular fitness and function is a fundamental problem in biology. Recently, generative probabilistic models have emerged as a powerful tool for estimating fitness from evolutionary sequence data, with accuracy sufficient to predict both laboratory measurements of function and disease risk in humans, and to design novel functional proteins. Existing techniques rest on an assumed relationship between density estimation and fitness estimation, a relationship that we interrogate in this article. We prove that fitness is not identifiable from observational sequence data alone, placing fundamental limits on our ability to disentangle fitness landscapes from phylogenetic history. We show on real datasets that perfect density estimation in the limit of infinite data would, with high confidence, result in poor fitness estimation; current models perform accurate fitness estimation because of, not despite, misspecification. Our results challenge the conventional wisdom that bigger models trained on bigger datasets will inevitably lead to better fitness estimation, and suggest novel estimation strategies going forward.",https://api.openreview.net/pdf/2691616464cd112056c895d4a7fef9ecc3595946.pdf,generative model;llm,https://scholar.google.com/scholar?q=Non-identifiability+and+the+Blessings+of+Misspecification+in+Models+of+Molecular+Fitness
A Combinatorial Perspective on the Optimization of Shallow ReLU Networks,2022,NIPS,"['Michael S Matena', 'Colin Raffel']",poster,"['ReLU', 'zonotope', 'combinatorial', 'optimization']",The NP-hard problem of optimizing a shallow ReLU network can be characterized as a combinatorial search over each training example’s activation pattern followed by a constrained convex problem given a fixed set of activation patterns. We explore the implications of this combinatorial aspect of ReLU optimization in this work. We show that it can be naturally modeled via a geometric and combinatoric object known as a zonotope with its vertex set isomorphic to the set of feasible activation patterns. This assists in analysis and provides a foundation for further research. We demonstrate its usefulness when we explore the sensitivity of the optimal loss to perturbations of the training data. Later we discuss methods of zonotope vertex selection and its relevance to optimization. Overparameterization assists in training by making a randomly chosen vertex more likely to contain a good solution. We then introduce a novel polynomial-time vertex selection procedure that provably picks a vertex containing the global optimum using only double the minimum number of parameters required to fit the data. We further introduce a local greedy search heuristic over zonotope vertices and demonstrate that it outperforms gradient descent on underparameterized problems. ,https://api.openreview.net/pdf/980922c923d99f0b31c1764974bfd37111319609.pdf,optimization;zero_few-shot;metric;llm,https://scholar.google.com/scholar?q=A+Combinatorial+Perspective+on+the+Optimization+of+Shallow+ReLU+Networks
Revisiting Optimal Convergence Rate for Smooth and Non-convex Stochastic Decentralized Optimization,2022,NIPS,"['Kun Yuan', 'Xinmeng Huang', 'Yiming Chen', 'Xiaohan Zhang', 'Yingya Zhang', 'Pan Pan']",poster,"['decentralized optimization', 'stochastic optimization', 'non-convex optimization', 'optimal complexity']","While numerous effective decentralized algorithms have been proposed with theoretical guarantees and empirical successes, the performance limits in decentralized optimization, especially the influence of network topology and its associated weight matrix on the optimal convergence rate, have not been fully understood. While Lu and Sa have recently provided an optimal rate for non-convex stochastic decentralized optimization using weight matrices associated with linear graphs, the optimal rate with general weight matrices remains unclear. 

This paper revisits non-convex stochastic decentralized optimization and establishes an optimal convergence rate with general weight matrices. In addition, we also establish the first optimal rate when non-convex loss functions further satisfy the Polyak-Lojasiewicz (PL) condition. Following existing lines of analysis in literature cannot achieve these results. Instead, we leverage the Ring-Lattice graph to admit general weight matrices while maintaining the optimal relation between the graph diameter and weight matrix connectivity. Lastly, we develop a new decentralized algorithm to attain the above two optimal rates up to logarithm factors. ",https://api.openreview.net/pdf/7aec409fd397afbb95a27944cb092a2af671c537.pdf,graph;optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Revisiting+Optimal+Convergence+Rate+for+Smooth+and+Non-convex+Stochastic+Decentralized+Optimization
Nest Your Adaptive Algorithm for Parameter-Agnostic Nonconvex Minimax Optimization,2022,NIPS,"['Junchi YANG', 'Xiang Li', 'Niao He']",poster,"['Nonconvex optimization', 'minimax optimization', 'adaptive algorithm']","Adaptive algorithms like AdaGrad and AMSGrad are successful in nonconvex optimization owing to their parameter-agnostic ability – requiring no a priori knowledge about problem-specific parameters nor tuning of learning rates. However, when it comes to nonconvex minimax optimization, direct extensions of such adaptive optimizers without proper time-scale separation may fail to work in practice. We provide such an example proving that the simple combination of Gradient Descent Ascent (GDA) with adaptive stepsizes can diverge if the primal-dual stepsize ratio is not carefully chosen; hence, a fortiori, such adaptive extensions are not parameter-agnostic. To address the issue, we formally introduce a Nested Adaptive framework, NeAda for short, that carries an inner loop for adaptively maximizing the dual variable with controllable stopping criteria and an outer loop for adaptively minimizing the primal variable. Such mechanism can be equipped with off-the-shelf adaptive optimizers and automatically balance the progress in the primal and dual variables. Theoretically, for nonconvex-strongly-concave minimax problems, we show that NeAda with AdaGrad stepsizes can achieve the near-optimal $\widetilde{O}(\epsilon^{-2})$ and $\widetilde{O}(\epsilon^{-4})$ gradient complexities respectively in the deterministic and stochastic settings, without prior information on the problem's smoothness and strong concavity parameters. To the best of our knowledge, this is the first algorithm that simultaneously achieves near-optimal convergence rates and parameter-agnostic adaptation in the nonconvex minimax setting. Numerically, we further illustrate the robustness of the NeAda family with experiments on simple test functions and a real-world application.
",https://api.openreview.net/pdf/34a0e2991f4b1ee9c50c47d8689e90a65a563ca2.pdf,optimization;adaptive;llm,https://scholar.google.com/scholar?q=Nest+Your+Adaptive+Algorithm+for+Parameter-Agnostic+Nonconvex+Minimax+Optimization
Convergent Representations of Computer Programs in Human and Artificial Neural Networks,2022,NIPS,"['Shashank Srikant', 'Ben Lipkin', 'Anna A Ivanova', 'Evelina Fedorenko', ""Una-May O'Reilly""]",poster,"['Code representations', 'Brain representations', 'Cognitive neuroscience', 'Neuroimaging', 'Multivoxel pattern analysis', 'Representation decoding analysis', 'Representation similarity analysis', 'fMRI analysis', 'ML for PL/SE', 'ML4code']","What aspects of computer programs are represented by the human brain during comprehension? We leverage brain recordings derived from functional magnetic resonance imaging (fMRI) studies of programmers comprehending Python code to evaluate the properties and code-related information encoded in the neural signal. We first evaluate a selection of static and dynamic code properties, such as abstract syntax tree (AST)-related and runtime-related metrics. Then, to learn whether brain representations encode fine-grained information about computer programs, we train a probe to align brain recordings with representations learned by a suite of ML models. We find that both the Multiple Demand and Language systems--brain systems which are responsible for very different cognitive tasks, encode specific code properties and uniquely align with machine learned representations of code. These findings suggest at least two distinct neural mechanisms mediating computer program comprehension and evaluation, prompting the design of code model objectives that go beyond static language modeling.
We make all the corresponding code, data, and analysis publicly available at https://github.com/ALFA-group/code-representations-ml-brain",https://api.openreview.net/pdf/a903288eefa965a4c757d67361a5233dc95fb434.pdf,graph;representation;metric;llm,https://scholar.google.com/scholar?q=Convergent+Representations+of+Computer+Programs+in+Human+and+Artificial+Neural+Networks
Active Learning Polynomial Threshold Functions,2022,NIPS,"['Omri Ben-Eliezer', 'Max Hopkins', 'Chutong Yang', 'Hantao Yu']",poster,"['Statistical Learning Theory', 'Active Learning', 'Polynomial Threshold Functions', 'Enriched Queries']","We initiate the study of active learning polynomial threshold functions (PTFs). While traditional lower bounds imply that even univariate quadratics cannot be non-trivially actively learned, we show that allowing the learner basic access to the derivatives of the underlying classifier circumvents this issue and leads to a computationally efficient algorithm for active learning degree-$d$ univariate PTFs in $\tilde{O}(d^3\log(1/\varepsilon\delta))$ queries. We extend this result to the batch active setting, providing a smooth transition between query complexity and rounds of adaptivity, and also provide near-optimal algorithms for active learning PTFs in several average case settings. Finally, we prove that access to derivatives is insufficient for active learning multivariate PTFs, even those of just two variables.",https://api.openreview.net/pdf/bb3a7c85868d3643a3462b6ded7374a42df645ce.pdf,zero_few-shot;active learning;llm,https://scholar.google.com/scholar?q=Active+Learning+Polynomial+Threshold+Functions
Conformalized Fairness via Quantile Regression,2022,NIPS,"['Meichen Liu', 'Lei Ding', 'Dengdeng Yu', 'Wulong Liu', 'Linglong Kong', 'Bei Jiang']",poster,"['Fairness', 'Quantile regression', 'Conformal prediction', 'Optimal transport', 'Functional synchronization']","Algorithmic fairness has received increased attention in socially sensitive domains. While rich literature on mean fairness has been established, research on quantile fairness remains sparse but vital. To fulfill great needs and advocate the significance of quantile fairness, we propose a novel framework to learn a real-valued quantile function under the fairness requirement of Demographic Parity with respect to sensitive attributes, such as race or gender, and thereby derive a reliable fair prediction interval. Using optimal transport and functional synchronization techniques, we establish theoretical guarantees of distribution-free coverage and exact fairness for the induced prediction interval constructed by fair quantiles. A hands-on pipeline is provided to incorporate flexible quantile regressions with an efficient fairness adjustment post-processing algorithm. We demonstrate the superior empirical performance of this approach on several benchmark datasets. Our results show the model’s ability to uncover the mechanism underlying the fairness-accuracy trade-off in a wide range of societal and medical applications.",https://api.openreview.net/pdf/451fe3012194de76331ef775f6e2504aef9cf12a.pdf,graph;transformer;sparse;llm,https://scholar.google.com/scholar?q=Conformalized+Fairness+via+Quantile+Regression
Hedging as Reward Augmentation in Probabilistic Graphical Models,2022,NIPS,"['Debarun Bhattacharjya', 'Radu Marinescu']",poster,"['hedging', 'influence diagram', 'bayesian network', 'reward augmentation']","Most people associate the term `hedging' exclusively with financial applications, particularly the use of financial derivatives. We argue that hedging is an activity that human and machine agents should engage in more broadly, even when the agent's value is not necessarily in monetary units. In this paper, we propose a decision-theoretic view of hedging based on augmenting a probabilistic graphical model -- specifically a Bayesian network or an influence diagram -- with a reward. Hedging is therefore posed as a particular kind of graph manipulation, and can be viewed as analogous to control/intervention and information gathering related analysis. Effective hedging occurs when a risk-averse agent finds opportunity to balance uncertain rewards in their current situation. We illustrate the concepts with examples and counter-examples, and conduct experiments to demonstrate the properties and applicability of the proposed computational tools that enable agents to proactively identify potential hedging opportunities in real-world situations.",https://api.openreview.net/pdf/95645f1538281d36a395d5e73522ba0267f20305.pdf,reinforcement learning;graph;bayesian;active learning;augmentation;llm,https://scholar.google.com/scholar?q=Hedging+as+Reward+Augmentation+in+Probabilistic+Graphical+Models
Layer Freezing & Data Sieving: Missing Pieces of a Generic Framework for Sparse Training,2022,NIPS,"['Geng Yuan', 'Yanyu Li', 'Sheng Li', 'Zhenglun Kong', 'Sergey Tulyakov', 'Xulong Tang', 'Yanzhi Wang', 'Jian Ren']",poster,"['Sparse training', 'model compression', 'efficient training']","Recently, sparse training has emerged as a promising paradigm for efficient deep learning on edge devices. The current research mainly devotes the efforts to reducing training costs by further increasing model sparsity. However, increasing sparsity is not always ideal since it will inevitably introduce severe accuracy degradation at an extremely high sparsity level. This paper intends to explore other possible directions to effectively and efficiently reduce sparse training costs while preserving accuracy. To this end, we investigate two techniques, namely, layer freezing and data sieving. First, the layer freezing approach has shown its success in dense model training and fine-tuning, yet it has never been adopted in the sparse training domain. Nevertheless, the unique characteristics of sparse training may hinder the incorporation of layer freezing techniques. Therefore, we analyze the feasibility and potentiality of using the layer freezing technique in sparse training and find it has the potential to save considerable training costs. Second, we propose a data sieving method for dataset-efficient training, which further reduces training costs by ensuring only a partial dataset is used throughout the entire training process. We show that both techniques can be well incorporated into the sparse training algorithm to form a generic framework, which we dub SpFDE. Our extensive experiments demonstrate that SpFDE can significantly reduce training costs while preserving accuracy from three dimensions: weight sparsity, layer freezing, and dataset sieving. Our code and models will be released.",https://api.openreview.net/pdf/511a9e7d325707cd3eecb1949140d8155b7b94e0.pdf,sparse;llm,https://scholar.google.com/scholar?q=Layer+Freezing+&+Data+Sieving:+Missing+Pieces+of+a+Generic+Framework+for+Sparse+Training
Maximum a posteriori natural scene reconstruction from retinal ganglion cells with deep denoiser priors,2022,NIPS,"['Eric Gene Wu', 'Nora Brackbill', 'Alexander Sher', 'Alan Litke', 'Eero P Simoncelli', 'EJ Chichilnisky']",poster,"['retina', 'ganglion cell', 'natural scenes', 'image reconstruction', 'image prior', 'Plug and Play', 'encoding model', 'neural coding', 'neuroscience', 'neural decoding']","Visual information arriving at the retina is transmitted to the brain by signals in the optic nerve, and the brain must rely solely on these signals to make inferences about the visual world. Previous work has probed the content of these signals by directly reconstructing images from retinal activity using linear regression or nonlinear regression with neural networks. Maximum a posteriori (MAP) reconstruction using retinal encoding models and separately-trained natural image priors offers a more general and principled approach. We develop a novel method for approximate MAP reconstruction that combines a generalized linear model for retinal responses to light, including their dependence on spike history and spikes of neighboring cells, with the image prior implicitly embedded in a deep convolutional neural network trained for image denoising. We use this method to reconstruct natural images from ex vivo simultaneously-recorded spikes of hundreds of retinal ganglion cells uniformly sampling a region of the retina. The method produces reconstructions that match or exceed the state-of-the-art in perceptual similarity and exhibit additional fine detail, while using substantially fewer model parameters than previous approaches. The use of more rudimentary encoding models (a linear-nonlinear-Poisson cascade) or image priors (a 1/f spectral model) significantly reduces reconstruction performance, indicating the essential role of both components in achieving high-quality reconstructed images from the retinal signal.",https://api.openreview.net/pdf/6761d2d94cb06d0651fc1f1f950e03f42d49807e.pdf,online learning;inference;llm,https://scholar.google.com/scholar?q=Maximum+a+posteriori+natural+scene+reconstruction+from+retinal+ganglion+cells+with+deep+denoiser+priors
ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model,2022,NIPS,"['Rao Fu', 'Xiao Zhan', 'Yiwen Chen', 'Daniel Ritchie', 'Srinath Sridhar']",poster,"['language', 'shape generation', '3D representation', 'text-shape dataset']","We present ShapeCrafter, a neural network for recursive text-conditioned 3D shape generation. Existing methods to generate text-conditioned 3D shapes consume an entire text prompt to generate a 3D shape in a single step. However, humans tend to describe shapes recursively---we may start with an initial description and progressively add details based on intermediate results. To capture this recursive process, we introduce a method to generate a 3D shape distribution, conditioned on an initial phrase, that gradually evolves as more phrases are added. Since existing datasets are insufficient for training this approach, we present Text2Shape++, a large dataset of 369K shape--text pairs that supports recursive shape generation. To capture local details that are often used to refine shape descriptions, we build on top of vector-quantized deep implicit functions that generate a distribution of high-quality shapes. Results show that our method can generate shapes consistent with text descriptions, and shapes evolve gradually as more phrases are added. Our method supports shape editing, extrapolation, and can enable new applications in human--machine collaboration for creative design.",https://api.openreview.net/pdf/a45f6e653f190cc4e1391e5da7625142931f36ea.pdf,generative model;3d;llm,https://scholar.google.com/scholar?q=ShapeCrafter:+A+Recursive+Text-Conditioned+3D+Shape+Generation+Model
Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners,2022,NIPS,"['Zhenhailong Wang', 'Manling Li', 'Ruochen Xu', 'Luowei Zhou', 'Jie Lei', 'Xudong Lin', 'Shuohang Wang', 'Ziyi Yang', 'Chenguang Zhu', 'Derek Hoiem', 'Shih-Fu Chang', 'Mohit Bansal', 'Heng Ji']",poster,"['video-language', 'few-shot', 'language model', 'in-context learning']","The goal of this work is to build flexible video-language models that can generalize to various video-to-text tasks from few examples. Existing few-shot video-language learners focus exclusively on the encoder, resulting in the absence of a video-to-text decoder to handle generative tasks. Video captioners have been pretrained on large-scale video-language datasets, but they rely heavily on finetuning and lack the ability to generate text for unseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language Learner via Image and Language models, which demonstrates strong performance on few-shot video-to-text tasks without the necessity of pretraining or finetuning on any video datasets. We use image-language models to translate the video content into frame captions, object, attribute, and event phrases, and compose them into a temporal-aware template.  We then instruct a language model, with a prompt containing a few in-context examples, to generate a target output from the composed content. The flexibility of prompting allows the model to capture any form of text input, such as automatic speech recognition (ASR) transcripts. Our experiments demonstrate the power of language models in understanding videos on a wide variety of video-language tasks, including video captioning, video question answering, video caption retrieval, and video future event prediction. Especially, on video future event prediction, our few-shot model significantly outperforms state-of-the-art supervised models trained on large-scale video datasets.
Code and processed data are publicly available for research purposes at https://github.com/MikeWangWZHL/VidIL. ",https://api.openreview.net/pdf/f01ff2025c23ad34b8e263c30f711d8b64d3e50e.pdf,zero_few-shot;generative model;llm,https://scholar.google.com/scholar?q=Language+Models+with+Image+Descriptors+are+Strong+Few-Shot+Video-Language+Learners
On Scrambling Phenomena for Randomly Initialized Recurrent Networks ,2022,NIPS,"['Vaggos Chatziafratis', 'Ioannis Panageas', 'Clayton Sanford', 'Stelios Andrew Stavroulakis']",poster,"['RNNs', 'Scrambling', 'Jacobian', 'dynamical systems', 'trajectories', 'chaos', 'initialization', 'recurrent networks']","Recurrent Neural Networks (RNNs) frequently exhibit complicated dynamics, and their sensitivity to the initialization process often renders them notoriously hard to train. Recent works have shed light on such phenomena analyzing when exploding or vanishing gradients may occur, either of which is detrimental for training dynamics. In this paper, we point to a formal connection between RNNs and chaotic dynamical systems and prove a qualitatively stronger phenomenon about RNNs than what exploding gradients seem to suggest. Our main result proves that under standard initialization (e.g., He, Xavier etc.), RNNs will exhibit \textit{Li-Yorke chaos} with \textit{constant} probability \textit{independent} of the network's width. This explains the experimentally observed phenomenon of \textit{scrambling}, under which trajectories of nearby points may appear to be arbitrarily close during some timesteps, yet will be far away in future timesteps. In stark contrast to their feedforward counterparts, we show that chaotic behavior in RNNs is preserved under small perturbations and that their expressive power remains exponential in the number of feedback iterations. Our technical arguments rely on viewing RNNs as random walks under non-linear activations, and studying the existence of certain types of higher-order fixed points called \textit{periodic points} in order to establish phase transitions from order to chaos.",https://api.openreview.net/pdf/5bb1c9f65a31c1e072674bdc912e15b18b40b686.pdf,llm,https://scholar.google.com/scholar?q=On+Scrambling+Phenomena+for+Randomly+Initialized+Recurrent+Networks+
MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields,2022,NIPS,"['Ilyes Batatia', 'David Peter Kovacs', 'Gregor N. C. Simm', 'Christoph Ortner', 'Gabor Csanyi']",poster,"['GNN', 'graph neural network', 'equivariance', 'higher order', 'message passing neural network', 'point clouds', 'molecules']","Creating fast and accurate force fields is a long-standing challenge in computational chemistry and materials science. Recently, Equivariant Message Passing Neural Networks (MPNNs) have emerged as a powerful tool for building machine learning interatomic potentials, outperforming other approaches in terms of accuracy. However, they suffer from high computational cost and poor scalability. Moreover, most MPNNs only pass two-body messages leading to an intricate relationship between the number of layers and the expressivity of the features. This work introduces MACE, a new equivariant MPNN model that uses higher order messages, and demonstrates that this leads to an improved learning law. We show that by using four-body messages, the required number of message passing iterations reduces to just one, resulting in a fast and highly parallelizable model, reaching or exceeding state of the art accuracy on the rMD17 and 3BPA benchmark tasks. Our implementation is available at https://github.com/ACEsuit/mace.",https://api.openreview.net/pdf/a1ade50912bdaaf92de0d04429aead357c05975c.pdf,llm,https://scholar.google.com/scholar?q=MACE:+Higher+Order+Equivariant+Message+Passing+Neural+Networks+for+Fast+and+Accurate+Force+Fields
Single-phase deep learning in cortico-cortical networks,2022,NIPS,"['Will Greedy', 'Heng Wei Zhu', 'Joseph Oliver Pemberton', 'Jack Mellor', 'Rui Ponte Costa']",poster,"['cortical microcircuits', 'deep learning', 'synaptic plasticity', 'biologically plausible learning', 'neuroscience']","The error-backpropagation (backprop) algorithm remains the most common solution to the credit assignment problem in artificial neural networks. In neuroscience, it is unclear whether the brain could adopt a similar strategy to correctly modify its synapses. Recent models have attempted to bridge this gap while being consistent with a range of experimental observations. However, these models are either unable to effectively backpropagate error signals across multiple layers or require a multi-phase learning process, neither of which are reminiscent of learning in the brain. Here, we introduce a new model, Bursting Cortico-Cortical Networks (BurstCCN), which solves these issues by integrating known properties of cortical networks namely bursting activity, short-term plasticity (STP) and dendrite-targeting interneurons. BurstCCN relies on burst multiplexing via connection-type-specific STP to propagate backprop-like error signals within deep cortical networks. These error signals are encoded at distal dendrites and induce burst-dependent plasticity as a result of excitatory-inhibitory top-down inputs. First, we demonstrate that our model can effectively backpropagate errors through multiple layers using a single-phase learning process. Next, we show both empirically and analytically that learning in our model approximates backprop-derived gradients. Finally, we demonstrate that our model is capable of learning complex image classification tasks (MNIST and CIFAR-10). Overall, our results suggest that cortical features across sub-cellular, cellular, microcircuit and systems levels jointly underlie single-phase efficient deep learning in the brain.",https://api.openreview.net/pdf/38d05ea1f460e815411925394853516bb2feed62.pdf,llm,https://scholar.google.com/scholar?q=Single-phase+deep+learning+in+cortico-cortical+networks
Composition Theorems for Interactive Differential Privacy,2022,NIPS,['Xin Lyu'],poster,"['Differential Privacy', 'Composition Theorems', 'Interactive mechanism']","An interactive mechanism is an algorithm that stores a data set and answers adaptively chosen queries to it. The mechanism is called differentially private, if any adversary cannot distinguish whether a specific individual is in the data set by interacting with the mechanism. We study composition properties of differential privacy in concurrent compositions. In this setting, an adversary interacts with $k$ interactive mechanisms in parallel and can interleave its queries to the mechanisms arbitrarily. Previously, Vadhan and Wang [2021] proved an optimal concurrent composition theorem for pure-differential privacy. We significantly generalize and extend their results. Namely, we prove optimal parallel composition properties for several major notions of differential privacy in the literature, including approximate DP, Renyi DP, and zero-concentrated DP. Our results demonstrate that the adversary gains no advantage by interleaving its queries to independently running mechanisms. Hence, interactivity is a feature that differential privacy grants us for free.
Concurrently and independently of our work, Vadhan and Zhang [2022] proved an optimal concurrent composition theorem for f-DP [Dong et al., 2022], which implies our result for the approximate DP case.",https://api.openreview.net/pdf/49f24a01fb50ac936ee42d6bfc3014867c67491c.pdf,adaptive;active learning;llm,https://scholar.google.com/scholar?q=Composition+Theorems+for+Interactive+Differential+Privacy
Acceleration in Distributed Sparse Regression,2022,NIPS,"['Marie Maros', 'Gesualdo Scutari']",poster,"['distributed optimization', 'acceleration', 'high-dimensional statistics', 'linear convergence']","We study acceleration for distributed sparse regression in   {\it  high-dimensions},  which allows the parameter size  to exceed and grow faster than the sample size. When applicable, existing  distributed algorithms employing acceleration perform poorly  in this setting, theoretically and numerically.  We  propose a new accelerated distributed algorithm suitable for high-dimensions. The method couples  a suitable instance of accelerated Nesterov's proximal gradient  with consensus and gradient-tracking mechanisms, aiming at estimating locally the gradient of the empirical loss while enforcing agreement on the local estimates.  Under standard assumptions on the statistical model and tuning parameters, the proposed method is proved to  globally converge   at {\it linear} rate  to an estimate that is within the {\it statistical precision} of the model. The iteration  complexity scales as $\mathcal{O}(\sqrt{\kappa})$, while the communications per iteration are at most $\widetilde{\mathcal{O}}(\log m/(1-\rho))$, 
 where $\kappa$ is the restricted condition number of the empirical loss, $m$ is the number of agents, and $\rho\in (0,1)$ measures the network connectivity. As by-product of our design, we also report    an accelerated method for high-dimensional estimations over  master-worker architectures, which is of independent interest and  compares favorably with existing works.",https://api.openreview.net/pdf/b02b7c25a1679df958f0d4f3f4592fd6a6289b7c.pdf,reinforcement learning;zero_few-shot;sparse;llm,https://scholar.google.com/scholar?q=Acceleration+in+Distributed+Sparse+Regression
DGD^2: A Linearly Convergent Distributed Algorithm For High-dimensional Statistical Recovery,2022,NIPS,"['Marie Maros', 'Gesualdo Scutari']",poster,"['Distributed optimization', 'convex optimization', 'high-dimensional statistics', 'linear convergence']","We study linear regression from data distributed over a network of agents (with no master node) under high-dimensional scaling, which allows the ambient dimension to grow faster than the sample size. We propose a novel decentralization of the projected gradient algorithm whereby agents iteratively update their local estimates by a “double-mixing” mechanism, which suitably combines averages of iterates and gradients of neighbouring nodes. Under standard assumptions on the statistical model and network connectivity, the proposed method enjoys global linear convergence up to the statistical precision of the model. This improves on guarantees of (plain) DGD algorithms, whose iteration complexity grows undesirably with the ambient dimension. Our technical contribution is a novel convergence analysis that resembles (albeit different) algorithmic stability arguments extended to high-dimensions and distributed setting, which is of independent interest.",https://api.openreview.net/pdf/c9e6683eae9722077a0422e3e0bb40612bb94076.pdf,reinforcement learning;zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=DGD^2:+A+Linearly+Convergent+Distributed+Algorithm+For+High-dimensional+Statistical+Recovery
Deep Architecture Connectivity Matters for Its Convergence: A Fine-Grained Analysis,2022,NIPS,"['Wuyang Chen', 'Wei Huang', 'Xinyu Gong', 'Boris Hanin', 'Zhangyang Wang']",poster,"['Neural Network Gaussian Process', 'Convergence', 'Neural Network Architecture']","Advanced deep neural networks (DNNs), designed by either human or AutoML algorithms, are growing increasingly complex. Diverse operations are connected by complicated connectivity patterns, e.g., various types of skip connections. Those topological compositions are empirically effective and observed to smooth the loss landscape and facilitate the gradient flow in general. However, it remains elusive to derive any principled understanding of their effects on the DNN capacity or trainability, and to understand why or in which aspect one specific connectivity pattern is better than another. In this work, we theoretically characterize the impact of connectivity patterns on the convergence of DNNs under gradient descent training in fine granularity. By analyzing a wide network's Neural Network Gaussian Process (NNGP), we are able to depict how the spectrum of an NNGP kernel propagates through a particular connectivity pattern, and how that affects the bound of convergence rates. As one practical implication of our results, we show that by a simple filtration of ""unpromising"" connectivity patterns, we can trim down the number of models to evaluate, and significantly accelerate the large-scale neural architecture search without any overhead.",https://api.openreview.net/pdf/fe67a6a8064a1f540520bbee278fefb509b477c7.pdf,zero_few-shot;flow;llm,https://scholar.google.com/scholar?q=Deep+Architecture+Connectivity+Matters+for+Its+Convergence:+A+Fine-Grained+Analysis
Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers,2022,NIPS,"['Ran Liu', 'Mehdi Azabou', 'Max Dabagia', 'Jingyun Xiao', 'Eva L Dyer']",poster,"['Transformers', 'population dynamics', 'multi-variate time-series', 'neural activity', 'many-body systems']","Complex time-varying systems are often studied by abstracting away from the dynamics of individual components to build a model of the population-level dynamics from the start. However, when building a population-level description, it can be easy to lose sight of each individual and how they contribute to the larger picture. In this paper, we present a novel transformer architecture for learning from time-varying data that builds descriptions of both the individual as well as the collective population dynamics. Rather than combining all of our data into our model at the onset, we develop a separable architecture that operates on individual time-series first before passing them forward; this induces a permutation-invariance property and can be used to transfer across systems of different size and order. After demonstrating that our model can be applied to successfully recover complex interactions and dynamics in many-body systems, we apply our approach to populations of neurons in the nervous system. On neural activity datasets, we show that our model not only yields robust decoding performance, but also provides impressive performance in transfer across recordings of different animals without any neuron-level correspondence. By enabling flexible pre-training that can be transferred to neural recordings of different size and order, our work provides a first step towards creating a foundation model for neural decoding.",https://api.openreview.net/pdf/0314b5a5b5b91c4b457a60e15c1484d4e595eb03.pdf,transformer;representation;transfer learning;llm,https://scholar.google.com/scholar?q=Seeing+the+forest+and+the+tree:+Building+representations+of+both+individual+and+collective+dynamics+with+transformers
KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,2022,NIPS,"['Ta-Chung Chi', 'Ting-Han Fan', 'Peter Ramadge', 'Alexander Rudnicky']",poster,"['Transformer Language Modeling', 'Length Extrapolation', 'Kernel Method']","Relative positional embeddings (RPE) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation. We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences. We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention. The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at~\url{https://github.com/chijames/KERPLE.git}.",https://api.openreview.net/pdf/8454975f969fcabd1ebee718881010a21c621f8a.pdf,zero_few-shot;transformer;metric;llm,https://scholar.google.com/scholar?q=KERPLE:+Kernelized+Relative+Positional+Embedding+for+Length+Extrapolation
STaR: Bootstrapping Reasoning With Reasoning,2022,NIPS,"['Eric Zelikman', 'Yuhuai Wu', 'Jesse Mu', 'Noah Goodman']",poster,"['chain-of-thought', 'reasoning', 'language model', 'bootstrapping']","Generating step-by-step ""chain-of-thought"" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the ""Self-Taught Reasoner"" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.",https://api.openreview.net/pdf/9976d4d8534659792a1c95426e3f1a62dbee90b8.pdf,generative model;inference;llm,https://scholar.google.com/scholar?q=STaR:+Bootstrapping+Reasoning+With+Reasoning
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,2022,NIPS,"['Jason Wei', 'Xuezhi Wang', 'Dale Schuurmans', 'Maarten Bosma', 'brian ichter', 'Fei Xia', 'Ed H. Chi', 'Quoc V Le', 'Denny Zhou']",poster,"['Language models', 'natural language processing', 'reasoning']","We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",https://api.openreview.net/pdf/69e0946271fd64932a087332874bd380fd2152a3.pdf,llm,https://scholar.google.com/scholar?q=Chain-of-Thought+Prompting+Elicits+Reasoning+in+Large+Language+Models
Iterative Structural Inference of Directed Graphs,2022,NIPS,"['Aoran Wang', 'Jun Pang']",poster,"['Structural Inference', 'Graph Neural Networks', 'Information Bottleneck', 'Deep Learning']","In this paper, we propose a variational model, iterative Structural Inference of Directed Graphs (iSIDG), to infer the existence of directed interactions from observational agents’ features over a time period in a dynamical system. First, the iterative process in our model feeds the learned interactions back to encourage our model to eliminate indirect interactions and to emphasize directional representation during learning. Second, we show that extra regularization terms in the objective function for smoothness, connectiveness, and sparsity prompt our model to infer a more realistic structure and to further eliminate indirect interactions. We evaluate iSIDG on various datasets including biological networks, simulated fMRI data, and physical simulations to demonstrate that our model is able to precisely infer the existence of interactions, and is significantly superior to baseline models.",https://api.openreview.net/pdf/39062db6eac5b376b47a4eaea35ca751f10e13a0.pdf,reinforcement learning;graph;representation;inference;llm,https://scholar.google.com/scholar?q=Iterative+Structural+Inference+of+Directed+Graphs
Scale-invariant Learning by Physics Inversion,2022,NIPS,"['Philipp Holl', 'Vladlen Koltun', 'Nils Thuerey']",poster,"['Higher-order Optimization', 'Neural Networks', 'Inverse Problems', 'Physical Simulations']","Solving inverse problems, such as parameter estimation and optimal control, is a vital part of science. Many experiments repeatedly collect data and rely on machine learning algorithms to quickly infer solutions to the associated inverse problems. We find that state-of-the-art training techniques are not well-suited to many problems that involve physical processes. The highly nonlinear behavior, common in physical processes, results in strongly varying gradients that lead first-order optimizers like SGD or Adam to compute suboptimal optimization directions.
We propose a novel hybrid training approach that combines higher-order optimization methods with machine learning techniques. We take updates from a scale-invariant inverse problem solver and embed them into the gradient-descent-based learning pipeline, replacing the regular gradient of the physical process.
We demonstrate the capabilities of our method on a variety of canonical physical systems, showing that it yields significant improvements on a wide range of optimization and learning problems.",https://api.openreview.net/pdf/119af8c2abd543304c905798fe713b3845ba53b9.pdf,optimization;online learning;llm,https://scholar.google.com/scholar?q=Scale-invariant+Learning+by+Physics+Inversion
Reinforcement Learning with Logarithmic Regret and Policy Switches,2022,NIPS,"['Grigoris Velegkas', 'Zhuoran Yang', 'Amin Karbasi']",poster,"['reinforcement learning theory', 'function approximation', 'instance-dependent regret']","In this paper, we study the problem of regret minimization for episodic Reinforcement Learning (RL) both in the model-free and the model-based setting. We focus on learning with general function classes and general model classes, and we derive results that scale with the eluder dimension of these classes. In contrast to the existing body of work that mainly establishes instance-independent regret guarantees, we focus on the instance-dependent setting and show that the regret scales logarithmically with the horizon $T$, provided that there is a gap between the best and the second best action in every state. In addition, we show that such a logarithmic regret bound is realizable by algorithms with $O(\log T)$ switching cost (also known as adaptivity complexity). In other words, these algorithms rarely switch their policy during the course of their execution. Finally, we complement our results with lower bounds which show that even in the tabular setting, we cannot hope for regret guarantees lower than $O(\log T)$.",https://api.openreview.net/pdf/2862b0eabf42b767998ecf6588cb01608e46a4a3.pdf,reinforcement learning;zero_few-shot;llm,https://scholar.google.com/scholar?q=Reinforcement+Learning+with+Logarithmic+Regret+and+Policy+Switches
Self-Supervised Learning of Brain Dynamics from Broad Neuroimaging Data,2022,NIPS,"['Armin W Thomas', 'Christopher Ré', 'Russell A. Poldrack']",poster,"['pre-training', 'self-supervised learning', 'neuroimaging', 'mental state decoding', 'natural language processing', 'language modelling']","Self-supervised learning techniques are celebrating immense success in natural language processing (NLP) by enabling models to learn from broad language data at unprecedented scales. Here, we aim to leverage the success of these techniques for mental state decoding, where researchers aim to identify specific mental states (e.g., the experience of anger or joy) from brain activity. To this end, we devise a set of novel self-supervised learning frameworks for neuroimaging data inspired by prominent learning frameworks in NLP. At their core, these frameworks learn the dynamics of brain activity by modeling sequences of activity akin to how sequences of text are modeled in NLP. We evaluate the frameworks by pre-training models on a broad neuroimaging dataset spanning functional Magnetic Resonance Imaging data from 11,980 experimental runs of 1,726 individuals across 34 datasets, and subsequently adapting the pre-trained models to benchmark mental state decoding datasets. The pre-trained models transfer well, generally outperforming baseline models trained from scratch, while models trained in a learning framework based on causal language modeling clearly outperform the others.",https://api.openreview.net/pdf/7cd581346e5aabd0de5c02967280238c234645e5.pdf,graph;zero_few-shot;transfer learning;llm,https://scholar.google.com/scholar?q=Self-Supervised+Learning+of+Brain+Dynamics+from+Broad+Neuroimaging+Data
GLIF: A Unified Gated Leaky Integrate-and-Fire Neuron for Spiking Neural Networks,2022,NIPS,"['Xingting Yao', 'Fanrong Li', 'Zitao Mo', 'Jian Cheng']",poster,"['Spiking Neural Networks', 'Leaky Integrate-and-Fire', 'Unified Spiking Neuron', 'Biological Features']","Spiking Neural Networks (SNNs) have been studied over decades to incorporate their biological plausibility and leverage their promising energy efficiency. Throughout existing SNNs, the leaky integrate-and-fire (LIF) model is commonly adopted to formulate the spiking neuron and evolves into numerous variants with different biological features. However, most LIF-based neurons support only single biological feature in different neuronal behaviors, limiting their expressiveness and neuronal dynamic diversity. In this paper, we propose GLIF, a unified spiking neuron, to fuse different bio-features in different neuronal behaviors, enlarging the representation space of spiking neurons. In GLIF, gating factors, which are exploited to determine the proportion of the fused bio-features, are learnable during training. Combining all learnable membrane-related parameters, our method can make spiking neurons different and constantly changing, thus increasing the heterogeneity and adaptivity of spiking neurons. Extensive experiments on a variety of datasets demonstrate that our method obtains superior performance compared with other SNNs by simply changing their neuronal formulations to GLIF. In particular, we train a spiking ResNet-19 with GLIF and achieve $77.35\%$ top-1 accuracy with six time steps on CIFAR-100, which has advanced the state-of-the-art. Codes are available at https://github.com/Ikarosy/Gated-LIF.",https://api.openreview.net/pdf/34135e8fce49c55a46afb89e33f0785f75f6ae5a.pdf,graph;representation;llm,https://scholar.google.com/scholar?q=GLIF:+A+Unified+Gated+Leaky+Integrate-and-Fire+Neuron+for+Spiking+Neural+Networks
Flamingo: a Visual Language Model for Few-Shot Learning,2022,NIPS,"['Jean-Baptiste Alayrac', 'Jeff Donahue', 'Pauline Luc', 'Antoine Miech', 'Iain Barr', 'Yana Hasson', 'Karel Lenc', 'Arthur Mensch', 'Katherine Millican', 'Malcolm Reynolds', 'Roman Ring', 'Eliza Rutherford', 'Serkan Cabi', 'Tengda Han', 'Zhitao Gong', 'Sina Samangooei', 'Marianne Monteiro', 'Jacob Menick', 'Sebastian Borgeaud', 'Andrew Brock', 'Aida Nematzadeh', 'Sahand Sharifzadeh', 'Mikolaj Binkowski', 'Ricardo Barreira', 'Oriol Vinyals', 'Andrew Zisserman', 'Karen Simonyan']",poster,[],"Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer, captioning tasks, which evaluate the ability to describe a scene or an event, and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",https://api.openreview.net/pdf/4e2c7c5fc1974f94f48ead8ee5c9b6f51323e912.pdf,zero_few-shot;multimodal;llm,https://scholar.google.com/scholar?q=Flamingo:+a+Visual+Language+Model+for+Few-Shot+Learning
Tracking Functional Changes in Nonstationary Signals with Evolutionary Ensemble Bayesian Model for Robust Neural Decoding,2022,NIPS,"['Xinyun Zhu', 'Yu Qi', 'Gang Pan', 'Yueming Wang']",poster,"['Brain-machine-interface', 'Bayesian filter', 'state-space model', 'evolutionary computation']","Neural signals are typical nonstationary data where the functional mapping between neural activities and the intentions (such as the velocity of movements) can occasionally change. Existing studies mostly use a fixed neural decoder, thus suffering from an unstable performance given neural functional changes. We propose a novel evolutionary ensemble framework (EvoEnsemble) to dynamically cope with changes in neural signals by evolving the decoder model accordingly. EvoEnsemble integrates evolutionary computation algorithms in a Bayesian framework where the fitness of models can be sequentially computed with their likelihoods according to the incoming data at each time slot, which enables online tracking of time-varying functions. Two strategies of evolve-at-changes and history-model-archive are designed to further improve efficiency and stability. Experiments with simulations and neural signals demonstrate that EvoEnsemble can track the changes in functions effectively thus improving the accuracy and robustness of neural decoding. The improvement is most significant in neural signals with functional changes.",https://api.openreview.net/pdf/f6b6e3dbcd34b2a81d36405e4cc3931101f3ca0c.pdf,online learning;bayesian;llm,https://scholar.google.com/scholar?q=Tracking+Functional+Changes+in+Nonstationary+Signals+with+Evolutionary+Ensemble+Bayesian+Model+for+Robust+Neural+Decoding
Sparse Winning Tickets are Data-Efficient Image Recognizers,2022,NIPS,"['Mukund Varma T', 'Xuxi Chen', 'Zhenyu Zhang', 'Tianlong Chen', 'Subhashini Venugopalan', 'Zhangyang Wang']",poster,[],"Improving the performance of deep networks in data-limited regimes has warranted much attention. In this work, we empirically show that “winning tickets” (small sub-networks) obtained via magnitude pruning based on the lottery ticket hypothesis, apart from being sparse are also effective recognizers in data-limited regimes. Based on extensive experiments, we find that in low data regimes (datasets of 50-100 examples per class), sparse winning tickets substantially outperform the original dense networks. This approach, when combined with augmentations or fine-tuning from a self-supervised backbone network, shows further improvements in performance by as much as 16% (absolute) on low-sample datasets and long-tailed classification. Further, sparse winning tickets are more robust to synthetic noise and distribution shifts compared to their dense counterparts. Our analysis of winning tickets on small datasets indicates that, though sparse, the networks retain density in the initial layers and their representations are more generalizable. Code is available at https://github.com/VITA-Group/DataEfficientLTH.",https://api.openreview.net/pdf/b91fe4f7f598c160e36fe903b629b6005696ab94.pdf,graph;zero_few-shot;transformer;representation;sparse;augmentation;llm,https://scholar.google.com/scholar?q=Sparse+Winning+Tickets+are+Data-Efficient+Image+Recognizers
Improving GANs with A Dynamic Discriminator,2022,NIPS,"['Ceyuan Yang', 'Yujun Shen', 'Yinghao Xu', 'Deli Zhao', 'Bo Dai', 'Bolei Zhou']",poster,"['generative adversarial network', 'image synthesis']","Discriminator plays a vital role in training generative adversarial networks (GANs) via distinguishing real and synthesized samples. While the real data distribution remains the same, the synthesis distribution keeps varying because of the evolving generator, and thus effects a corresponding change of the bi-classification task assigned to the discriminator. We argue that a discriminator with an on-the-fly adjustment on its capacity can better accommodate such a time-varying task. A comprehensive empirical study confirms that the proposed training strategy, termed as DynamicD, improves the synthesis performance without incurring any additional computation cost or training objectives. Two capacity adjusting schemes are developed for training GANs under different data regimes: i) given a sufficient amount of training data, the discriminator benefits from a progressively increased learning capacity, and ii) when the training data is limited, gradually decreasing the layer width mitigates the over-fitting issue of the discriminator. Experiments on both 2D and 3D-aware image synthesis tasks conducted on a range of datasets substantiate the generalizability of our DynamicD as well as its substantial improvement over the baselines. Furthermore, DynamicD is synergistic to other discriminator-improving approaches (including data augmentation, regularizers, and pre-training), and brings continuous performance gain when combined with them for learning GANs. Code will be made publicly available.",https://api.openreview.net/pdf/46d882366183d341a93435d508b64a776e3fa377.pdf,generative model;augmentation;3d;llm,https://scholar.google.com/scholar?q=Improving+GANs+with+A+Dynamic+Discriminator
Aligning individual brains with fused unbalanced Gromov Wasserstein,2022,NIPS,"['Alexis Thual', 'Quang Huy TRAN', 'Tatiana Zemskova', 'Nicolas Courty', 'Rémi Flamary', 'Stanislas Dehaene', 'Bertrand Thirion']",poster,"['Brain imaging', 'Optimal transport', 'fMRI', 'registration']","Individual brains vary in both anatomy and functional organization, even within a given species. Inter-individual variability is a major impediment when trying to draw generalizable conclusions from neuroimaging data collected on groups of subjects. Current co-registration procedures rely on limited data, and thus lead to very coarse inter-subject alignments. 
In this work, we present a novel method for inter-subject alignment based on Optimal Transport, denoted as Fused Unbalanced Gromov Wasserstein (FUGW). The method aligns two cortical surfaces based on the similarity of their functional signatures in response to a variety of stimuli, while penalizing large deformations of individual topographic organization.
We demonstrate that FUGW is suited for whole-brain landmark-free alignment. The unbalanced feature allows to deal with the fact that functional areas vary in size across subjects. Results show that FUGW alignment significantly increases between-subject correlation of activity during new independent fMRI tasks and runs, and leads to more precise maps of fMRI results at the group level.",https://api.openreview.net/pdf/d5b8fa4ed97c8ac7db65f564d290b5ae7abfd6a1.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Aligning+individual+brains+with+fused+unbalanced+Gromov+Wasserstein
Structured Energy Network As a Loss,2022,NIPS,"['Jay-Yoon Lee', 'Dhruvesh Patel', 'Purujit Goyal', 'Wenlong Zhao', 'Zhiyang Xu', 'Andrew McCallum']",poster,"['Structured Prediction', 'Structured Energy network', 'Energy-based models', 'Trainable Loss-function', 'Dynamic loss function', 'Noise-Contrastive Estimation']","Belanger & McCallum (2016) and Gygli et al. (2017) have shown that an energy network can capture arbitrary dependencies amongst the output variables in structured prediction; however, their reliance on gradient-based inference (GBI) makes the inference slow and unstable. In this work, we propose Structured Energy As Loss (SEAL) to take advantage of the expressivity of energy networks without incurring the high inference cost. This is a novel learning framework that uses an energy network as a trainable loss function (loss-net) to train a separate neural network (task-net), which is then used to perform the inference through a forward pass. We establish SEAL as a general framework wherein various learning strategies like margin-based, regression, and noise-contrastive, could be employed to learn the parameters of loss-net.  Through extensive evaluation on multi-label classification, semantic role labeling, and image segmentation, we demonstrate that SEAL provides various useful design choices, is faster at inference than GBI, and leads to significant performance gains over the baselines.
",https://api.openreview.net/pdf/369cb9986fcca1c06a21b9f41d86129a7a845482.pdf,graph;zero_few-shot;contrastive learning;inference;segmentation;llm,https://scholar.google.com/scholar?q=Structured+Energy+Network+As+a+Loss
Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs,2022,NIPS,"['Jinguo Zhu', 'Xizhou Zhu', 'Wenhai Wang', 'Xiaohua Wang', 'Hongsheng Li', 'Xiaogang Wang', 'Jifeng Dai']",poster,"['Generalist Models', 'Task Interference', 'Mixture of Experts']","To build an artificial neural network like the biological intelligence system, recent works have unified numerous tasks into a generalist model, which can process various tasks with shared parameters and do not have any task-specific modules. While generalist models achieve promising results on various benchmarks, they have performance degradation on some tasks compared with task-specialized models. In this work, we find that interference among different tasks and modalities is the main factor to this phenomenon. To mitigate such interference, we introduce the Conditional Mixture-of-Experts (Conditional MoEs) to generalist models. Routing strategies under different levels of conditions are proposed to take both the training/inference cost and generalization ability into account. By incorporating the proposed Conditional MoEs, the recently proposed generalist model Uni-Perceiver can effectively mitigate the interference across tasks and modalities, and achieves state-of-the-art results on a series of downstream tasks via prompt tuning on 1% of downstream data. Moreover, the introduction of Conditional MoEs still holds the generalization ability of generalist models to conduct zero-shot inference on new tasks, e.g., videotext retrieval and video caption. Code and pre-trained generalist models are publicly released at https://github.com/fundamentalvision/Uni-Perceiver.",https://api.openreview.net/pdf/769266169aeda576e24247d1ea3c18d10efa8ca8.pdf,zero_few-shot;inference;sparse;llm,https://scholar.google.com/scholar?q=Uni-Perceiver-MoE:+Learning+Sparse+Generalist+Models+with+Conditional+MoEs
Robust Graph Structure Learning via Multiple Statistical Tests,2022,NIPS,"['Yaohua Wang', 'Fangyi Zhang', 'Ming Lin', 'Senzhang Wang', 'Xiuyu Sun', 'Rong Jin']",poster,"['Graph Structure Learning', 'Graph Convolutional Networks (GCNs)', 'Computer Vision']","Graph structure learning aims to learn connectivity in a graph from data. It is particularly important for many computer vision related tasks since no explicit graph structure is available for images for most cases. A natural way to construct a graph among images is to treat each image as a node and assign pairwise image similarities as weights to corresponding edges. It is well known that pairwise similarities between images are sensitive to the noise in feature representations, leading to unreliable graph structures. We address this problem from the viewpoint of statistical tests. By viewing the feature vector of each node as an independent sample, the decision of whether creating an edge between two nodes based on their similarity in feature representation can be thought as a ${\it single}$ statistical test. To improve the robustness in the decision of creating an edge, multiple samples are drawn and integrated by ${\it multiple}$ statistical tests to generate a more reliable similarity measure, consequentially more reliable graph structure. The corresponding elegant matrix form named $\mathcal{B}$$\textbf{-Attention}$ is designed for efficiency. The effectiveness of multiple tests for graph structure learning is verified both theoretically and empirically on multiple clustering and ReID benchmark datasets. Source codes are available at https://github.com/Thomas-wyh/B-Attention.",https://api.openreview.net/pdf/5661ee262c2da8b0ce7aad63e02ca0ed9177fc36.pdf,graph;transformer;representation;llm,https://scholar.google.com/scholar?q=Robust+Graph+Structure+Learning+via+Multiple+Statistical+Tests
Effective Adaptation in Multi-Task Co-Training for Unified Autonomous Driving,2022,NIPS,"['Xiwen Liang', 'Yangxin Wu', 'Jianhua Han', 'Hang Xu', 'Chunjing Xu', 'Xiaodan Liang']",poster,[],"Aiming towards a holistic understanding of multiple downstream tasks simultaneously, there is a need for extracting features with better transferability. Though many latest self-supervised pre-training methods have achieved impressive performance on various vision tasks under the prevailing pretrain-finetune paradigm, their generalization capacity to multi-task learning scenarios is yet to be explored. In this paper, we extensively investigate the transfer performance of various types of self-supervised methods, e.g., MoCo and SimCLR, on three downstream tasks, including semantic segmentation, drivable area segmentation, and traffic object detection, on the large-scale driving dataset BDD100K. We surprisingly find that their performances are sub-optimal or even lag far behind the single-task baseline, which may be due to the distinctions of training objectives and architectural design lied in the pretrain-finetune paradigm. To overcome this dilemma as well as avoid redesigning the resource-intensive pre-training stage, we propose a simple yet effective pretrain-adapt-finetune paradigm for general multi-task training, where the off-the-shelf pretrained models can be effectively adapted without increasing the training overhead. During the adapt stage, we utilize learnable multi-scale adapters to dynamically adjust the pretrained model weights supervised by multi-task objectives while leaving the pretrained knowledge untouched. Furthermore, we regard the vision-language pre-training model CLIP as a strong complement to the pretrain-adapt-finetune paradigm and propose a novel adapter named LV-Adapter, which incorporates language priors in the multi-task model via task-specific prompting and alignment between visual and textual features. Our experiments demonstrate that the adapt stage significantly improves the overall performance of those off-the-shelf pretrained models and the contextual features generated by LV-Adapter are of general benefits for downstream tasks.",https://api.openreview.net/pdf/446f1c4f90eda1d7cd0b2114c8d674dbcdb6a246.pdf,zero_few-shot;transfer learning;segmentation;multi-task;llm,https://scholar.google.com/scholar?q=Effective+Adaptation+in+Multi-Task+Co-Training+for+Unified+Autonomous+Driving
AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition,2022,NIPS,"['Shoufa Chen', 'Chongjian GE', 'Zhan Tong', 'Jiangliu Wang', 'Yibing Song', 'Jue Wang', 'Ping Luo']",poster,"['Efficient Finetuning', 'Visual Adapter']","Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains.
To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently.
It possesses several benefits more appealing than prior arts.
Firstly, AdaptFormer introduces lightweight modules that only add less than 2% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\% fully fine-tuned models on action recognition benchmarks.
Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks.
Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5% extra parameters, it achieves about 10% and 19% relative improvement compared to the fully fine-tuned models on Something-Something~v2 and HMDB51, respectively. 
Code is available at https://github.com/ShoufaChen/AdaptFormer.",https://api.openreview.net/pdf/ca9c0cbc01e5f4225618132541e1435b56e649c3.pdf,graph;zero_few-shot;transformer;transfer learning;llm,https://scholar.google.com/scholar?q=AdaptFormer:+Adapting+Vision+Transformers+for+Scalable+Visual+Recognition
Stimulative Training of Residual Networks: A Social Psychology Perspective of Loafing,2022,NIPS,"['Peng Ye', 'Shengji Tang', 'Baopu Li', 'Tao Chen', 'Wanli Ouyang']",poster,"['Residual Networks', 'Network Loafing', 'Stimulative Training']","Residual networks have shown great success and become indispensable in today’s deep models. In this work, we aim to re-investigate the training process of residual networks from a novel social psychology perspective of loafing, and further propose a new training strategy to strengthen the performance of residual networks. As residual networks can be viewed as ensembles of relatively shallow networks (i.e., unraveled view) in prior works, we also start from such view and consider that the final performance of a residual network is co-determined by a group of sub-networks. Inspired by the social loafing problem of social psychology, we find that residual networks invariably suffer from similar problem, where sub-networks in a residual network are prone to exert less effort when working as part of the group compared to working alone. We define this previously overlooked problem as network loafing. As social loafing will ultimately cause the low individual productivity and the reduced overall performance, network loafing will also hinder the performance of a given residual network and its sub-networks. Referring to the solutions of social psychology, we propose stimulative training, which randomly samples a residual sub-network and calculates the KL-divergence loss between the sampled sub-network and the given residual network, to act as extra supervision for sub-networks and make the overall goal consistent. Comprehensive empirical results and theoretical analyses verify that stimulative training can well handle the loafing problem, and improve the performance of a residual network by improving the performance of its sub-networks. The code is available at https://github.com/Sunshine-Ye/NIPS22-ST. ",https://api.openreview.net/pdf/76f9545860b7776e62e492ba5c0b497d9a1fa0fa.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Stimulative+Training+of+Residual+Networks:+A+Social+Psychology+Perspective+of+Loafing
Retrospective Adversarial Replay for Continual Learning,2022,NIPS,"['Lilly Kumari', 'Shengjie Wang', 'Tianyi Zhou', 'Jeff Bilmes']",poster,"['continual learning', 'adversarial perturbations', 'class incremental learning', 'boundary samples', 'catastrophic forgetting']","Continual learning is an emerging research challenge in machine learning that addresses the problem where models quickly fit the most recently trained-on data but suffer from catastrophic forgetting of previous data due to distribution shifts --- it does this by maintaining a small historical replay buffer in replay-based methods. To avoid these problems, this paper proposes a method, ``Retrospective Adversarial Replay (RAR)'', that synthesizes adversarial samples near the forgetting boundary. RAR perturbs a buffered sample towards its nearest neighbor drawn from the current task in a latent representation space. By replaying such samples, we are able to refine the boundary between previous and current tasks, hence combating forgetting and reducing bias towards the current task. To mitigate the severity of a small replay buffer, we develop a novel MixUp-based strategy to increase replay variation by replaying mixed augmentations. Combined with RAR, this achieves a holistic framework that helps to alleviate catastrophic forgetting. We show that this excels on broadly-used benchmarks and outperforms other continual learning baselines especially when only a small buffer is available. We conduct a thorough ablation study over each key component as well as a hyperparameter sensitivity analysis to demonstrate the effectiveness and robustness of RAR.",https://api.openreview.net/pdf/5a4ca8b2444794c185c5007a2d11b715ad85440e.pdf,graph;representation;augmentation;llm,https://scholar.google.com/scholar?q=Retrospective+Adversarial+Replay+for+Continual+Learning
Equivariant Graph Hierarchy-Based Neural Networks,2022,NIPS,"['Jiaqi Han', 'Wenbing Huang', 'Tingyang Xu', 'Yu Rong']",poster,['equivariant graph neural network'],"Equivariant Graph neural Networks (EGNs) are powerful in characterizing the dynamics of multi-body physical systems. Existing EGNs conduct flat message passing, which, yet, is unable to capture the spatial/dynamical hierarchy for complex systems particularly, limiting substructure discovery and global information fusion. In this paper, we propose Equivariant Hierarchy-based Graph Networks (EGHNs) which consist of the three key components: generalized Equivariant Matrix Message Passing (EMMP) , E-Pool and E-UnPool. In particular, EMMP is able to improve the expressivity of conventional equivariant message passing, E-Pool assigns the quantities of the low-level nodes into high-level clusters, while E-UnPool leverages the high-level information to update the dynamics of the low-level nodes. As their names imply, both E-Pool and E-UnPool are guaranteed to be equivariant to meet physic symmetry. Considerable experimental evaluations verify the effectiveness of our EGHN on several applications including multi-object dynamics simulation, motion capture, and protein dynamics modeling.",https://api.openreview.net/pdf/80466cd1da287ed33be08f11251817754aeeacd4.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Equivariant+Graph+Hierarchy-Based+Neural+Networks
FIRE: Semantic Field of Words Represented as Non-Linear Functions,2022,NIPS,"['Xin Du', 'Kumiko Tanaka-Ishii']",poster,"['natural language', 'nonlinear word representation', 'field representation', 'word polysemy', 'semantic compositionality']","State-of-the-art word embeddings presume a linear vector space, but this approach does not easily incorporate the nonlinearity that is necessary to represent polysemy. We thus propose a novel semantic FIeld REepresentation, called FIRE, which is a $D$-dimensional field in which every word is represented as a set of its locations and a nonlinear function covering the field. The strength of a word's relation to another word at a certain location is measured as the function value at that location. With FIRE, compositionality is represented via functional additivity, whereas polysemy is represented via the set of points and the function's multimodality. By implementing FIRE for English and comparing it with previous representation methods via word and sentence similarity tasks, we show that FIRE produces comparable or even better results. In an evaluation of polysemy to predict the number of word senses, FIRE greatly outperformed BERT and Word2vec, providing evidence of how FIRE represents polysemy. The code is available at https://github.com/kduxin/firelang.",https://api.openreview.net/pdf/8c6670b75022b9d867b437a95087445951211a86.pdf,transformer;representation;online learning;multimodal;llm,https://scholar.google.com/scholar?q=FIRE:+Semantic+Field+of+Words+Represented+as+Non-Linear+Functions
VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training,2022,NIPS,"['Zhan Tong', 'Yibing Song', 'Jue Wang', 'Limin Wang']",poster,"['video representation learning', 'action recognition', 'self-supervised learning']","Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging and meaningful self-supervision task, thus encouraging extracting more effective video representations during the pre-training process. We obtain three important findings with VideoMAE: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance for VideoMAE. The temporally redundant video content enables higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. This is partially ascribed to the challenging task of video reconstruction to enforce high-level structure learning. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important factor. Notably, our VideoMAE with the vanilla ViT backbone can achieve 87.4% on Kinects-400, 75.4% on Something-Something V2, 91.3% on UCF101, and 62.6% on HMDB51, without using any extra data. Code is available at https://github.com/MCG-NJU/VideoMAE.",https://api.openreview.net/pdf/51fad132b21efae47114e62a6d485ff82d54da5f.pdf,graph;zero_few-shot;transformer;representation;self-supervision;llm,https://scholar.google.com/scholar?q=VideoMAE:+Masked+Autoencoders+are+Data-Efficient+Learners+for+Self-Supervised+Video+Pre-Training
Action-modulated midbrain dopamine activity arises from distributed control policies,2022,NIPS,"['Jack Lindsey', 'Ashok Litwin-Kumar']",poster,"['neuroscience', 'reinforcement learning', 'dopamine', 'basal ganglia', 'off-policy learning']","Animal behavior is driven by multiple brain regions working in parallel with distinct control policies. We present a biologically plausible model of off-policy reinforcement learning in the basal ganglia, which enables learning in such an architecture. The model accounts for action-related modulation of dopamine activity that is not captured by previous models that implement on-policy algorithms.  In particular, the model predicts that dopamine activity signals a combination of reward prediction error (as in classic models) and ""action surprise,"" a measure of how unexpected an action is relative to the basal ganglia's current policy. In the presence of the action surprise term, the model implements an approximate form of $Q$-learning.  On benchmark navigation and reaching tasks, we show empirically that this model is capable of learning from data driven completely or in part by other policies (e.g. from other brain regions).  By contrast, models without the action surprise term suffer in the presence of additional policies, and are incapable of learning at all from behavior that is completely externally driven.  The model provides a computational account for numerous experimental findings about dopamine activity that cannot be explained by classic models of reinforcement learning in the basal ganglia.  These include differing levels of action surprise signals in dorsal and ventral striatum, decreasing amounts movement-modulated dopamine activity with practice, and representations of action initiation and kinematics in dopamine activity. It also provides further predictions that can be tested with recordings of striatal dopamine activity.",https://api.openreview.net/pdf/2d8baa09ec620fd3cd9f018f7358c40cc7922896.pdf,reinforcement learning;representation;llm,https://scholar.google.com/scholar?q=Action-modulated+midbrain+dopamine+activity+arises+from+distributed+control+policies
On the Importance of Gradient Norm in PAC-Bayesian Bounds,2022,NIPS,"['Itai Gat', 'Yossi Adi', 'Alex Schwing', 'Tamir Hazan']",poster,"['generalization bound', 'pac-bayes']","Generalization bounds which assess the difference between the true risk and the empirical risk have been studied extensively. However, to obtain bounds, current techniques use strict assumptions such as a uniformly bounded or a Lipschitz loss function. To avoid these assumptions, in this paper, we follow an alternative approach: we relax uniform bounds assumptions by using on-average bounded loss and on-average bounded gradient norm assumptions. Following this relaxation, we propose a new generalization bound that exploits the contractivity of the log-Sobolev inequalities. These inequalities add an additional loss-gradient norm term to the generalization bound, which is intuitively a surrogate of the model complexity. We apply the proposed bound on Bayesian deep nets and empirically analyze the effect of this new loss-gradient norm term on different neural architectures.",https://api.openreview.net/pdf/90d5535f79dbd304e196b8f15cf22980ef1f184a.pdf,zero_few-shot;bayesian;llm,https://scholar.google.com/scholar?q=On+the+Importance+of+Gradient+Norm+in+PAC-Bayesian+Bounds
Learning a Condensed Frame for Memory-Efficient Video Class-Incremental Learning,2022,NIPS,"['Yixuan Pei', 'Zhiwu Qing', 'Jun CEN', 'Xiang Wang', 'Shiwei Zhang', 'Yaxiong Wang', 'Mingqian Tang', 'Nong Sang', 'Xueming Qian']",poster,[],"Recent incremental learning for action recognition usually  stores representative videos to mitigate catastrophic forgetting. However, only a few bulky videos can be stored due to the limited memory. To address this problem, we propose FrameMaker, a memory-efficient video class-incremental learning approach that learns to produce a condensed frame for each selected video. Specifically, FrameMaker is mainly composed of two crucial components: Frame Condensing and Instance-Specific Prompt. The former is to reduce the memory cost by preserving only one condensed frame instead of the whole video, while the latter aims to compensate the lost spatio-temporal details in the Frame Condensing stage. By this means, FrameMaker enables a remarkable reduction in memory but keep enough information that can be applied to following incremental tasks. Experimental results on multiple challenging benchmarks, i.e., HMDB51, UCF101 and Something-Something V2, demonstrate that FrameMaker can achieve better performance to recent advanced methods while consuming only 20% memory. Additionally, under the same memory consumption conditions, FrameMaker significantly outperforms existing state-of-the-arts by a convincing margin. ",https://api.openreview.net/pdf/49142b7d143a4d071e9702d0c08a70d0a40544bd.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Learning+a+Condensed+Frame+for+Memory-Efficient+Video+Class-Incremental+Learning
Factuality Enhanced Language Models for Open-Ended Text Generation,2022,NIPS,"['Nayeon Lee', 'Wei Ping', 'Peng Xu', 'Mostofa Patwary', 'Pascale Fung', 'Mohammad Shoeybi', 'Bryan Catanzaro']",poster,[],"Pretrained language models (LMs) are susceptible to generate text with nonfactual information.  In this work, we measure and improve the factual accuracy of large-scale LMs for open-ended text generation.  We design the FactualityPrompts test set and metrics to measure the factuality of LM generations.  Based on that, we study the factual accuracy of LMs with parameter sizes ranging from 126M to 530B.   Interestingly, we find that larger LMs are more factual than smaller ones, although a previous study suggests that larger LMs can be less truthful in terms of misconceptions.  In addition, popular sampling algorithms (e.g., top-p) in open-ended text generation can harm the factuality due to the ``uniform randomness'' introduced at every sampling step.  We propose the factual-nucleus sampling algorithm that dynamically adapts the randomness to improve the factuality of generation while maintaining quality.  Furthermore, we analyze the inefficiencies of the standard training method in learning correct associations between entities from factual text corpus (e.g., Wikipedia).   We propose a factuality-enhanced training method that uses TopicPrefix for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors.",https://api.openreview.net/pdf/58d7ebef32d6f579a1eec00f4454303cdb08f3bc.pdf,graph;generative model;metric;llm,https://scholar.google.com/scholar?q=Factuality+Enhanced+Language+Models+for+Open-Ended+Text+Generation
Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity,2022,NIPS,"['William de Vazelhes', 'Hualin Zhang', 'Huimin Wu', 'Xiaotong Yuan', 'Bin Gu']",poster,"['Sparse learning', 'Hard-thresholding', 'Zeroth-order', 'Stochastic optimization']","$\ell_0$ constrained optimization is prevalent in machine learning, particularly for high-dimensional problems, because it is a fundamental approach to achieve sparse learning. Hard-thresholding gradient descent is a dominant technique to solve this problem. However, first-order gradients of the objective function may be either unavailable or expensive to calculate in a lot of real-world problems, where zeroth-order (ZO) gradients could be a good surrogate. Unfortunately, whether ZO gradients can work with the hard-thresholding operator is still an unsolved problem.
To solve this puzzle, in this paper, we focus on the $\ell_0$ constrained black-box stochastic optimization problems, and propose a new stochastic zeroth-order gradient hard-thresholding (SZOHT) algorithm with  a general ZO gradient estimator powered by a novel random support sampling. We provide the convergence analysis of SZOHT under standard assumptions.   Importantly, we   reveal a conflict between  the deviation of  ZO estimators and  the expansivity of the hard-thresholding operator,  and provide a theoretical   minimal value of the number of random directions in ZO gradients. In addition,  we find that the query complexity of SZOHT is independent or weakly dependent on the dimensionality under different settings.  Finally, we illustrate the utility of our method on a portfolio optimization problem as well as black-box adversarial attacks.",https://api.openreview.net/pdf/d0653bef5853e7f1324b7216833c739c55a95b22.pdf,optimization;zero_few-shot;sparse;llm,https://scholar.google.com/scholar?q=Zeroth-Order+Hard-Thresholding:+Gradient+Error+vs.+Expansivity
Multi-view Subspace Clustering on Topological Manifold,2022,NIPS,"['Shudong Huang', 'Hongjie Wu', 'Yazhou Ren', 'Ivor Tsang', 'Zenglin Xu', 'Wentao Feng', 'Jiancheng Lv']",poster,"['Subspace Clustering', 'Multi-view Learning', 'Topological Manifold Learning']","Multi-view subspace clustering aims to exploit a common affinity representation by means of self-expression. Plenty of works have been presented to boost the clustering performance, yet seldom considering the topological structure in data, which is crucial for clustering data on manifold. Orthogonal to existing works, in this paper, we argue that it is beneficial to explore the implied data manifold by learning the topological relationship between data points. Our model seamlessly integrates multiple affinity graphs into a consensus one with the topological relevance considered. Meanwhile, we manipulate the consensus graph by a connectivity constraint such that the connected components precisely indicate different clusters. Hence our model is able to directly obtain the final clustering result without reliance on any label discretization strategy as previous methods do. Experimental results on several benchmark datasets illustrate the effectiveness of the proposed model, compared to the state-of-the-art competitors over the clustering performance.",https://api.openreview.net/pdf/b94f106fb1cec530ce760decb5ffda6450fd737f.pdf,graph;optimization;representation;multi-view;llm,https://scholar.google.com/scholar?q=Multi-view+Subspace+Clustering+on+Topological+Manifold
Globally Convergent Policy Search for Output Estimation,2022,NIPS,"['Jack Umenberger', 'Max Simchowitz', 'Juan Carlos Perdomo', 'Kaiqing Zhang', 'Russ Tedrake']",poster,"['Model-free reinforcement learning', 'policy optimization', 'global convergence', 'partially observable systems']","We introduce the first direct policy search algorithm which provably converges to the globally optimal dynamic filter for the classical problem of predicting the outputs of a linear dynamical system, given noisy, partial observations. Despite the ubiquity of partial observability in practice, theoretical guarantees for direct policy search algorithms, one of the backbones of modern reinforcement learning, have proven difficult to achieve. This is primarily due to the degeneracies which arise when optimizing over filters that maintain an internal state. In this paper, we provide a new perspective on this challenging problem based on the notion of informativity, which intuitively requires that all components of a filter’s internal state are representative of the true state of the underlying dynamical system. We show that informativity overcomes the aforementioned degeneracy. Specifically, we propose a regularizer which explicitly enforces informativity, and establish that gradient descent on this regularized objective - combined with a “reconditioning step” – converges to the globally optimal cost at a $O(1/T)$ rate.",https://api.openreview.net/pdf/6e0a31dc95b7a49fee46ac38aeae3aa4d0e12148.pdf,reinforcement learning;graph;llm,https://scholar.google.com/scholar?q=Globally+Convergent+Policy+Search+for+Output+Estimation
Alleviating the Sample Selection Bias in Few-shot Learning by Removing Projection to the Centroid,2022,NIPS,"['Jing Xu', 'Xu Luo', 'Xinglin Pan', 'Yanan Li', 'Wenjie Pei', 'Zenglin Xu']",poster,"['Few-shot learning', 'metric learning', 'image classification']","Few-shot learning (FSL) targets at generalization of vision models towards unseen tasks without sufficient annotations. Despite the emergence of a number of few-shot learning methods, the sample selection bias problem, i.e., the sensitivity to the limited amount of support data, has not been well understood. In this paper, we find that this problem usually occurs when the positions of support samples are in the vicinity of task centroid—the mean of all class centroids in the task. This motivates us to propose an extremely simple feature transformation to alleviate this problem, dubbed Task Centroid Projection Removing (TCPR). TCPR is applied directly to all image features in a given task, aiming at removing the dimension of features along the direction of the task centroid. While the exact task centoid cannot be accurately obtained from limited data, we estimate it using base features that are each similar to one of the support features. Our method effectively prevents features from being too close to the task centroid. Extensive experiments over ten datasets from different domains show that TCPR can reliably improve classification accuracy across various feature extractors, training algorithms and datasets. The code has been made available at https://github.com/KikimorMay/FSL-TCBR.",https://api.openreview.net/pdf/fcc2f44f26c32f006bdaad7eea9a1906173686e1.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Alleviating+the+Sample+Selection+Bias+in+Few-shot+Learning+by+Removing+Projection+to+the+Centroid
Symbolic Distillation for Learned TCP Congestion Control,2022,NIPS,"['S P Sharan', 'Wenqing Zheng', 'Kuo-Feng Hsu', 'Jiarong Xing', 'Ang Chen', 'Zhangyang Wang']",poster,"['symbolic regression', 'TCP congestion control', 'efficiency', 'interpretability']","Recent advances in TCP congestion control (CC) have achieved tremendous success with deep reinforcement learning (RL) approaches, which use feedforward neural networks (NN) to learn complex environment conditions and make better decisions. However, such ``black-box'' policies lack interpretability and reliability, and often, they need to operate outside the traditional TCP datapath due to the use of complex NNs. This paper proposes a novel two-stage solution to achieve the best of both worlds: first to train a deep RL agent, then distill its (over-)parameterized NN policy into white-box, light-weight rules in the form of symbolic expressions that are much easier to understand and to implement in constrained environments. At the core of our proposal is a novel symbolic branching algorithm that enables the rule to be aware of the context in terms of various network conditions, eventually converting the NN policy into a symbolic tree. The distilled symbolic rules preserve and often improve performance over state-of-the-art NN policies while being faster and simpler than a standard neural network. We validate the performance of our distilled symbolic rules on both simulation and emulation environments. Our code is available at https://github.com/VITA-Group/SymbolicPCC.",https://api.openreview.net/pdf/829f22af0fa32b03ba213654761d8e8d2785c622.pdf,reinforcement learning;optimization;distillation;llm,https://scholar.google.com/scholar?q=Symbolic+Distillation+for+Learned+TCP+Congestion+Control
When to Trust Your Simulator: Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning,2022,NIPS,"['Haoyi Niu', 'Shubham Sharma', 'Yiwen Qiu', 'Ming Li', 'Guyue Zhou', 'Jianming HU', 'Xianyuan Zhan']",poster,[],"Learning effective reinforcement learning (RL) policies to solve real-world complex tasks can be quite challenging without a high-fidelity simulation environment. In most cases, we are only given imperfect simulators with simplified dynamics, which inevitably lead to severe sim-to-real gaps in RL policy learning. The recently emerged field of offline RL provides another possibility to learn policies directly from pre-collected historical data. However, to achieve reasonable performance, existing offline RL algorithms need impractically large offline data with sufficient state-action space coverage for training. This brings up a new question: is it possible to combine learning from limited real data in offline RL and unrestricted exploration through imperfect simulators in online RL to address the drawbacks of both approaches? In this study, we propose the Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning (H2O) framework to provide an affirmative answer to this question. H2O introduces a dynamics-aware policy evaluation scheme, which adaptively penalizes the Q function learning on simulated state-action pairs with large dynamics gaps, while also simultaneously allowing learning from a fixed real-world dataset. Through extensive simulation and real-world tasks, as well as theoretical analysis, we demonstrate the superior performance of H2O against other cross-domain online and offline RL algorithms. H2O provides a brand new hybrid offline-and-online RL paradigm, which can potentially shed light on future RL algorithm design for solving practical real-world tasks.",https://api.openreview.net/pdf/58135669d26ec94b6efa9f2cbe6a7e4e8c9f3e34.pdf,reinforcement learning;offline reinforcement learning;graph;zero_few-shot;online learning;adaptive;multimodal;llm,https://scholar.google.com/scholar?q=When+to+Trust+Your+Simulator:+Dynamics-Aware+Hybrid+Offline-and-Online+Reinforcement+Learning
Brownian Noise Reduction: Maximizing Privacy Subject to Accuracy Constraints,2022,NIPS,"['Justin Whitehouse', 'Aaditya Ramdas', 'Steven Wu', 'Ryan Rogers']",poster,"['Differential privacy', 'Brownian motion', 'Laplace mechanism', 'Gaussian mechanism', 'Confidence sequences', 'Empirical risk minimization']","There is a disconnect between how researchers and practitioners handle privacy-utility tradeoffs. Researchers primarily operate from a privacy first perspective, setting strict privacy requirements and minimizing risk subject to these constraints. Practitioners often desire an accuracy first perspective, possibly satisfied with the greatest privacy they can get subject to obtaining sufficiently small error. Ligett et al. have introduced a `""noise reduction"" algorithm to address the latter perspective. The authors show that by adding correlated Laplace noise and progressively reducing it on demand, it is possible to produce a sequence of increasingly accurate estimates of a private parameter and only pay a privacy cost for the least noisy iterate released. In this work, we generalize noise reduction to the setting of Gaussian noise, introducing the Brownian mechanism. The Brownian mechanism works by first adding Gaussian noise of high variance corresponding to the final point of a simulated Brownian motion. Then, at the practitioner's discretion, noise is gradually decreased by tracing back along the Brownian path to an earlier time. Our mechanism is more naturally applicable to the common setting of bounded $\ell_2$-sensitivity, empirically outperforms existing work on common statistical tasks, and provides customizable control of privacy loss over the entire interaction with the practitioner. We complement our Brownian mechanism with ReducedAboveThreshold, a generalization of the classical AboveThreshold algorithm that provides adaptive privacy guarantees. Overall, our results demonstrate that one can meet utility constraints while still maintaining strong levels of privacy.",https://api.openreview.net/pdf/42cf98e608b65ca3f654af179f63b5af66cf3f87.pdf,optimization;adaptive;llm,https://scholar.google.com/scholar?q=Brownian+Noise+Reduction:+Maximizing+Privacy+Subject+to+Accuracy+Constraints
Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning,2022,NIPS,"['Haokun Liu', 'Derek Tam', 'Muqeeth Mohammed', 'Jay Mohta', 'Tenghao Huang', 'Mohit Bansal', 'Colin Raffel']",poster,"['few-shot learning', 'in-context learning', 'parameter-efficient training']","Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)^3 that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6% absolute. All of the code used in our experiments will be publicly available.",https://api.openreview.net/pdf/951bf07ebade58be0b50a9681737cf5aacfc06f5.pdf,zero_few-shot;sparse;llm,https://scholar.google.com/scholar?q=Few-Shot+Parameter-Efficient+Fine-Tuning+is+Better+and+Cheaper+than+In-Context+Learning
No-regret learning in games with noisy feedback: Faster rates and adaptivity via learning rate separation,2022,NIPS,"['Yu-Guan Hsieh', 'Kimon Antonakopoulos', 'Volkan Cevher', 'Panayotis Mertikopoulos']",poster,"['Learning in game', 'Regret', 'Noise', 'Optimism', 'Adaptivity', 'Nash equilibrium']","We examine the problem of regret minimization when the learner is involved in a continuous game with other optimizing agents: in this case, if all players follow a no-regret algorithm, it is possible to achieve significantly lower regret relative to fully adversarial environments. We study this problem in the context of variationally stable games (a class of continuous games which includes all convex-concave and monotone games), and when the players only have access to noisy estimates of their individual payoff gradients. If the noise is additive, the game-theoretic and purely adversarial settings enjoy similar regret guarantees; however, if the noise is \emph{multiplicative}, we show that the learners can, in fact, achieve \emph{constant} regret. We achieve this faster rate via an optimistic gradient scheme with \emph{learning rate separation} \textendash\ that is, the method's extrapolation and update steps are tuned to different schedules, depending on the noise profile. Subsequently, to eliminate the need for delicate hyperparameter tuning, we propose a fully adaptive method that smoothly interpolates between worst- and best-case regret guarantees.",https://api.openreview.net/pdf/70d19708628286384e841ed68a08be08f1100269.pdf,reinforcement learning;zero_few-shot;adaptive;llm,https://scholar.google.com/scholar?q=No-regret+learning+in+games+with+noisy+feedback:+Faster+rates+and+adaptivity+via+learning+rate+separation
Scalable Sensitivity and Uncertainty Analyses for Causal-Effect Estimates of Continuous-Valued Interventions,2022,NIPS,"['Andrew Jesson', 'Alyson Rose Douglas', 'Peter Manshausen', 'Maëlys Solal', 'Nicolai Meinshausen', 'Philip Stier', 'Yarin Gal', 'Uri Shalit']",poster,"['causal effect inference', 'hidden confounding', 'continuous intervention', 'climate', 'uncertainty', 'dose response', 'deep learning']","Estimating the effects of continuous-valued interventions from observational data is a critically important task for climate science, healthcare, and economics. Recent work focuses on designing neural network architectures and regularization functions to allow for scalable estimation of average and individual-level dose-response curves from high-dimensional, large-sample data. Such methodologies assume ignorability (observation of all confounding variables) and positivity (observation of all treatment levels for every covariate value describing a set of units), assumptions problematic in the continuous treatment regime. Scalable sensitivity and uncertainty analyses to understand the ignorance induced in causal estimates when these assumptions are relaxed are less studied. Here, we develop a continuous treatment-effect marginal sensitivity model (CMSM) and derive bounds that agree with the observed data and a researcher-defined level of hidden confounding. We introduce a scalable algorithm and uncertainty-aware deep models to derive and estimate these bounds for high-dimensional, large-sample observational data. We work in concert with climate scientists interested in the climatological impacts of human emissions on cloud properties using satellite observations from the past 15 years. This problem is known to be complicated by many unobserved confounders.",https://api.openreview.net/pdf/85df2df3e315d0bb4f8e7dd0d4adacda6969fa79.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Scalable+Sensitivity+and+Uncertainty+Analyses+for+Causal-Effect+Estimates+of+Continuous-Valued+Interventions
Neural Circuit Architectural Priors for Embodied Control,2022,NIPS,"['Nikhil X. Bhattasali', 'Anthony M. Zador', 'Tatiana A Engel']",poster,"['neuroscience', 'neural circuits', 'motor control']","Artificial neural networks for motor control usually adopt generic architectures like fully connected MLPs. While general, these tabula rasa architectures rely on large amounts of experience to learn, are not easily transferable to new bodies, and have internal dynamics that are difficult to interpret. In nature, animals are born with highly structured connectivity in their nervous systems shaped by evolution; this innate circuitry acts synergistically with learning mechanisms to provide inductive biases that enable most animals to function well soon after birth and learn efficiently. Convolutional networks inspired by visual circuitry have encoded useful biases for vision. However, it is unknown the extent to which ANN architectures inspired by neural circuitry can yield useful biases for other AI domains. In this work, we ask what advantages biologically inspired ANN architecture can provide in the domain of motor control. Specifically, we translate C. elegans locomotion circuits into an ANN model controlling a simulated Swimmer agent. On a locomotion task, our architecture achieves good initial performance and asymptotic performance comparable with MLPs, while dramatically improving data efficiency and requiring orders of magnitude fewer parameters. Our architecture is interpretable and transfers to new body designs. An ablation analysis shows that constrained excitation/inhibition is crucial for learning, while weight initialization contributes to good initial performance. Our work demonstrates several advantages of biologically inspired ANN architecture and encourages future work in more complex embodied control.",https://api.openreview.net/pdf/026519dbd8ae7b8c1b89e09ab5a3c3faa2022bf7.pdf,reinforcement learning;optimization;transfer learning;llm,https://scholar.google.com/scholar?q=Neural+Circuit+Architectural+Priors+for+Embodied+Control
LIFT: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks,2022,NIPS,"['Tuan Dinh', 'Yuchen Zeng', 'Ruisu Zhang', 'Ziqian Lin', 'Michael Gira', 'Shashank Rajput', 'Jy-yong Sohn', 'Dimitris Papailiopoulos', 'Kangwook Lee']",poster,"['language interface', 'language-interfaced learning', 'fine-tuning', 'language models', 'non-language tasks', 'regression', 'classification.']","Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling ""no-code machine learning with LMs.""  We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g., context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. Our code is available at https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.",https://api.openreview.net/pdf/57a41761c63acf08f72d783d2b529cebebbdd8f1.pdf,graph;zero_few-shot;generative model;llm,https://scholar.google.com/scholar?q=LIFT:+Language-Interfaced+Fine-Tuning+for+Non-language+Machine+Learning+Tasks
Momentum Aggregation for Private Non-convex ERM,2022,NIPS,"['Hoang Tran', 'Ashok Cutkosky']",poster,"['differential privacy', 'momentum', 'non-convex optimization', 'ERM', 'tree-aggregation']","We introduce new algorithms and convergence guarantees for privacy-preserving non-convex Empirical Risk Minimization (ERM) on smooth $d$-dimensional objectives. We develop an improved sensitivity analysis of stochastic gradient descent on smooth objectives that exploits the recurrence of examples in different epochs. By combining this new approach with recent analysis of momentum with private aggregation techniques, we provide an $(\epsilon,\delta)$-differential private algorithm that finds a gradient of norm $O\left(\frac{d^{1/3}}{(\epsilon N)^{2/3}}\right)$ in $O\left(\frac{N^{7/3}\epsilon^{4/3}}{d^{2/3}}\right)$ gradient evaluations, improving the previous best gradient bound of $\tilde O\left(\frac{d^{1/4}}{\sqrt{\epsilon N}}\right)$.",https://api.openreview.net/pdf/cdecfff58c3dd8e81d2b8f6abcb014b9f90df323.pdf,llm,https://scholar.google.com/scholar?q=Momentum+Aggregation+for+Private+Non-convex+ERM
Uncertainty-Aware Reinforcement Learning for Risk-Sensitive Player Evaluation in Sports Game,2022,NIPS,"['Guiliang Liu', 'Yudong Luo', 'Oliver Schulte', 'Pascal Poupart']",poster,"['Reinforcement Learning', 'Uncertainty Estimation', 'Sports Analytic', 'Agent Evaluation']","A major task of sports analytics is player evaluation. Previous methods commonly measured the impact of players' actions on desirable outcomes (e.g., goals or winning) without considering the risk induced by stochastic game dynamics.  In this paper, we design an uncertainty-aware Reinforcement Learning (RL) framework to learn a risk-sensitive player evaluation metric from stochastic game dynamics. To embed the risk of a player’s movements into the distribution of action-values, we model their 1) aleatoric uncertainty, which represents the intrinsic stochasticity in a sports game, and 2) epistemic uncertainty, which is due to a model's insufficient knowledge regarding Out-of-Distribution (OoD) samples. We demonstrate how a distributional Bellman operator and a feature-space density model can capture these uncertainties. Based on such uncertainty estimation, we propose a Risk-sensitive Game Impact Metric (RiGIM) that measures players' performance over a season by conditioning on a specific confidence level. Empirical evaluation, based on over 9M play-by-play ice hockey and soccer events, shows that RiGIM correlates highly with standard success measures and has a consistent risk sensitivity.",https://api.openreview.net/pdf/396871bca163706d59b0ccea227fb3673b1dcc99.pdf,reinforcement learning;metric;llm,https://scholar.google.com/scholar?q=Uncertainty-Aware+Reinforcement+Learning+for+Risk-Sensitive+Player+Evaluation+in+Sports+Game
Local-Global MCMC kernels: the best of both worlds,2022,NIPS,"['Sergey Samsonov', 'Evgeny Lagutin', 'Marylou Gabrié', 'Alain Durmus', 'Alexey Naumov', 'Eric Moulines']",poster,"['MCMC', 'Markov chains', 'adaptive MCMC']","Recent works leveraging learning to enhance sampling have shown promising results, in particular by designing effective non-local moves and global proposals. However, learning accuracy is inevitably limited in regions where little data is available such as in the tails of distributions as well as in high-dimensional problems. In the present paper we study an Explore-Exploit Markov chain Monte Carlo strategy ($\operatorname{Ex^2MCMC}$) that combines local and global samplers showing that it enjoys the advantages of both approaches. We prove $V$-uniform geometric ergodicity of $\operatorname{Ex^2MCMC}$ without requiring a uniform adaptation of the global sampler to the target distribution. We also compute explicit bounds on the mixing rate of the Explore-Exploit strategy under realistic conditions. Moreover, we propose an adaptive version of the strategy ($\operatorname{FlEx^2MCMC}$) where a normalizing flow is trained while sampling to serve as a proposal for global moves. We illustrate the efficiency of $\operatorname{Ex^2MCMC}$ and its adaptive version on classical sampling benchmarks as well as in sampling high-dimensional distributions defined by Generative Adversarial Networks seen as Energy Based Models.",https://api.openreview.net/pdf/9ebf0d9855981c38da50027a19658d097ea89d55.pdf,graph;zero_few-shot;generative model;adaptive;metric;flow;llm,https://scholar.google.com/scholar?q=Local-Global+MCMC+kernels:+the+best+of+both+worlds
Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy,2022,NIPS,"['Zhiqi Bu', 'Jialin Mao', 'Shiyun Xu']",poster,"['deep learning', 'differential privacy', 'complexity', 'convolutional neural network', 'vision transformer']","Large convolutional neural networks (CNN) can be difficult to train in the differentially private (DP) regime, since the optimization algorithms require a computationally expensive operation, known as the per-sample gradient clipping. We propose an efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy. The improvement in efficiency is rigorously studied through the first complexity analysis for the mixed ghost clipping and existing DP training algorithms.

Extensive experiments on vision classification tasks, with large ResNet, VGG, and Vision Transformers (ViT), demonstrate that DP training with mixed ghost clipping adds $1\sim 10\%$ memory overhead and $<2\times$ slowdown to the standard non-private training. Specifically, when training VGG19 on CIFAR10, the mixed ghost clipping is $3\times$ faster than state-of-the-art Opacus library with $18\times$ larger maximum batch size. To emphasize the significance of efficient DP training on convolutional layers, we achieve 96.7\% accuracy on CIFAR10 and 83.0\% on CIFAR100 at $\epsilon=1$ using BEiT, while the previous best results are 94.8\% and 67.4\%, respectively. We open-source a privacy engine (\url{https://github.com/woodyx218/private_vision}) that implements DP training of CNN (including convolutional ViT) with a few lines of code.",https://api.openreview.net/pdf/8c331752641e3a2471c7e5e7b1fce8ee25bddeee.pdf,graph;optimization;zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Scalable+and+Efficient+Training+of+Large+Convolutional+Neural+Networks+with+Differential+Privacy
Constrained Predictive Coding as a Biologically Plausible Model of the Cortical Hierarchy,2022,NIPS,"['Siavash Golkar', 'Tiberiu Tesileanu', 'Yanis Bahroun', 'Anirvan M. Sengupta', 'Dmitri Chklovskii']",poster,"['predictive coding', 'biologically plausible', 'calcium plateau', 'cortical hierarchy', 'biologically realistic']","Predictive coding (PC) has emerged as an influential normative model of neural computation with numerous extensions and applications. As such, much effort has been put into mapping PC faithfully onto the cortex, but there are issues that remain unresolved or controversial. In particular, current implementations often involve separate value and error neurons and require symmetric forward and backward weights across different brain regions. These features have not been experimentally confirmed. In this work, we show that the PC framework in the linear regime can be modified to map faithfully onto the cortical hierarchy in a manner compatible with empirical observations. By employing a disentangling-inspired constraint on hidden-layer neural activities, we derive an upper bound for the PC objective. Optimization of this upper bound leads to an algorithm that shows the same performance as the original objective and maps onto a biologically plausible network. The units of this network can be interpreted as multi-compartmental neurons with non-Hebbian learning rules, with a remarkable resemblance to recent experimental findings. There exist prior models which also capture these features, but they are phenomenological, while our work is a normative derivation. This allows us to determine which features are necessary for the functioning of the model. For instance, the network we derive does not involve one-to-one connectivity or signal multiplexing, which the phenomenological models require, indicating that these features are not necessary for learning in the cortex. The normative nature of our algorithm in the simplified linear case also allows us to prove interesting properties of the framework and analytically understand the computational role of our network's components. The parameters of our network have natural interpretations as physiological quantities in a multi-compartmental model of pyramidal neurons, providing a concrete link between PC and experimental measurements carried out in the cortex.",https://api.openreview.net/pdf/29f13096b30da15ac4d4419b0f64add39e92038b.pdf,graph;optimization;zero_few-shot;metric;llm,https://scholar.google.com/scholar?q=Constrained+Predictive+Coding+as+a+Biologically+Plausible+Model+of+the+Cortical+Hierarchy
Renyi Differential Privacy of Propose-Test-Release and Applications to Private and Robust Machine Learning,2022,NIPS,"['Tianhao Wang', 'Saeed Mahloujifar', 'Shouda Wang', 'Ruoxi Jia', 'Prateek Mittal']",poster,"['Renyi Differential Privacy', 'Propose Test Release']","Propose-Test-Release (PTR) is a differential privacy framework that works with local sensitivity of functions, instead of their global sensitivity. This framework is typically used for releasing robust statistics such as median or trimmed mean in a differentially private manner. While PTR is a common framework introduced over a decade ago, using it in applications such as robust SGD where we need many adaptive robust queries is challenging. This is mainly due to the lack of \Renyi Differential Privacy (RDP) analysis, an essential ingredient underlying the moments accountant approach for differentially private deep learning. In this work, we generalize the standard PTR and derive the first RDP bound for it. We show that our RDP bound for PTR yields tighter DP guarantees than the directly analyzed $(\varepsilon, \delta)$-DP. We also derive the algorithm-specific privacy amplification bound of PTR under subsampling. We show that our bound is much tighter than the general upper bound and close to the lower bound. Our RDP bounds enable tighter privacy loss calculation for the composition of many adaptive runs of PTR. As an application of our analysis, we show that PTR and our theoretical results can be used to design differentially private variants for byzantine robust training algorithms that use robust statistics for gradients aggregation. We conduct experiments on the settings of label, feature, and gradient corruption across different datasets and architectures. We show that PTR-based private and robust training algorithm significantly improves the utility compared with the baseline. ",https://api.openreview.net/pdf/5fa2cc35352accf5fc85e34ea5147086ee0bf0ed.pdf,graph;zero_few-shot;adaptive;llm,https://scholar.google.com/scholar?q=Renyi+Differential+Privacy+of+Propose-Test-Release+and+Applications+to+Private+and+Robust+Machine+Learning
Can Push-forward Generative Models Fit Multimodal Distributions?,2022,NIPS,"['Antoine Salmona', 'Valentin De Bortoli', 'Julie Delon', 'Agnès Desolneux']",poster,"['Generative Models', 'GAN', 'VAE', 'Diffusion Models', 'Score-based Models', 'Expressivity', 'Lipschitz Mappings']","Many generative models synthesize data by transforming a standard Gaussian random variable using a deterministic neural network. Among these models are the Variational Autoencoders and the Generative Adversarial Networks. In this work, we call them ""push-forward"" models and study their expressivity. We formally demonstrate that the Lipschitz constant of these generative networks has to be large in order to fit multimodal distributions. More precisely, we show that the total variation distance and the Kullback-Leibler divergence between the generated 
and the data distribution are bounded from below by a constant depending on the mode separation and the Lipschitz constant. Since constraining the Lipschitz constants of neural networks is a common way to stabilize generative models, there is a provable trade-off between the ability of push-forward models to approximate multimodal distributions and the stability of their training. We validate our findings on one-dimensional and image datasets and empirically show that the recently introduced diffusion models do not suffer of such limitation.",https://api.openreview.net/pdf/a1a6f024413fa9d8afd17e996b306215f206e7cb.pdf,optimization;zero_few-shot;vae;generative model;multimodal;diffusion models;llm,https://scholar.google.com/scholar?q=Can+Push-forward+Generative+Models+Fit+Multimodal+Distributions?
DualCoOp: Fast Adaptation to Multi-Label Recognition with Limited Annotations,2022,NIPS,"['Ximeng Sun', 'Ping Hu', 'Kate Saenko']",poster,"['Multi-Label Recognition', 'Prompt Learning']","Solving multi-label recognition (MLR) for images in the low-label regime is a challenging task with many real-world applications. Recent work learns an alignment between textual and visual spaces to compensate for insufficient image labels, but loses accuracy because of the limited amount of available MLR annotations. In this work, we utilize the strong alignment of textual and visual features pretrained with millions of auxiliary image-text pairs and propose \textit{Dual Context Optimization} (DualCoOp)  as a unified framework for partial-label MLR and zero-shot MLR. \ours encodes positive and negative contexts with class names as part of the linguistic input (i.e. prompts). Since \ours only introduces a very light learnable overhead upon the pretrained vision-language framework, it can quickly adapt to multi-label recognition tasks that have limited annotations and even unseen classes.  Experiments on standard multi-label recognition benchmarks across two challenging low-label settings demonstrate the advantages of our approach over state-of-the-art methods. Our code will be publicly available.Project page: https://cs-people.bu.edu/sunxm/DualCoOp/project.html",https://api.openreview.net/pdf/0185705925d2e3a16742682d9a0d09d4dbeb864a.pdf,graph;optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=DualCoOp:+Fast+Adaptation+to+Multi-Label+Recognition+with+Limited+Annotations
When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture,2022,NIPS,"['Yichuan Mo', 'Dongxian Wu', 'Yifei Wang', 'Yiwen Guo', 'Yisen Wang']",poster,"['Vision Transformer', 'Adversarial Training', 'Robustness']","Vision Transformers (ViTs) have recently achieved competitive performance in broad vision tasks. Unfortunately, on popular threat models, naturally trained ViTs are shown to provide no more adversarial robustness than convolutional neural networks (CNNs). Adversarial training is still required for ViTs to defend against such adversarial attacks. In this paper, we provide the first and comprehensive study on the adversarial training recipe of ViTs via extensive evaluation of various training techniques across benchmark datasets. We find that pre-training and SGD optimizer are necessary for ViTs' adversarial training. Further considering ViT as a new type of model architecture, we investigate its adversarial robustness from the perspective of its unique architectural components. We find, when randomly masking gradients from some attention blocks or masking perturbations on some patches during adversarial training, the adversarial robustness of ViTs can be remarkably improved, which may potentially open up a line of work to explore the architectural information inside the newly designed models like ViTs. Our code is available at https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers.",https://api.openreview.net/pdf/0c041c0ac0c76ed6e33a41fadbc13d1da0c3c4e9.pdf,transformer;llm,https://scholar.google.com/scholar?q=When+Adversarial+Training+Meets+Vision+Transformers:+Recipes+from+Training+to+Architecture
Egocentric Video-Language Pretraining,2022,NIPS,"['Kevin Qinghong Lin', 'Jinpeng Wang', 'Mattia Soldan', 'Michael Wray', 'Rui Yan', 'Eric Zhongcong Xu', 'Denial Gao', 'Rong-Cheng Tu', 'Wenzhe Zhao', 'Weijie Kong', 'Chengfei Cai', 'WANG HongFa', 'Dima Damen', 'Bernard Ghanem', 'Wei Liu', 'Mike Zheng Shou']",poster,"['Video-Language Pretraining', 'Egocentric Video Datasets']","Video-Language Pretraining (VLP), which aims to learn transferable representation to advance a wide range of video-text downstream tasks, has recently received increasing attention. Best performing works rely on large-scale, 3rd-person video-text datasets, such as HowTo100M. In this work, we exploit the recently released Ego4D dataset to pioneer Egocentric VLP along three directions. (i) We create EgoClip, a 1st-person video-text pretraining dataset comprising 3.8M clip-text pairs well-chosen from Ego4D, covering a large variety of human daily activities. (ii) We propose a novel pretraining objective, dubbed EgoNCE, which adapts video-text contrastive learning to the egocentric domain by mining egocentric-aware positive and negative samples. (iii) We introduce EgoMCQ, a development benchmark that is close to EgoClip and hence can support effective validation and fast exploration of our design decisions in EgoClip and EgoNCE. Furthermore, we demonstrate strong performance on five egocentric downstream tasks across three datasets: video-text retrieval on EPIC-KITCHENS-100; action recognition on Charades-Ego; natural language query, moment query, and object state change classification on Ego4D challenge benchmarks. The dataset and code are available at https://github.com/showlab/EgoVLP.",https://api.openreview.net/pdf/e85f0a65b708d7e699b7a98e58e4ec2e529a7733.pdf,transformer;representation;contrastive learning;transfer learning;llm,https://scholar.google.com/scholar?q=Egocentric+Video-Language+Pretraining
Incorporating Bias-aware Margins into Contrastive Loss for Collaborative Filtering,2022,NIPS,"['An Zhang', 'Wenchang Ma', 'Xiang Wang', 'Tat-Seng Chua']",poster,"['Recommendation', 'Collaborative Filtering', 'Popularity Bias', 'Contrastive Loss', 'Popularity Debiasing']","Collaborative ﬁltering (CF) models easily suffer from popularity bias, which makes recommendation deviate from users’ actual preferences. However, most current debiasing strategies are prone to playing a trade-off game between head and tail performance, thus inevitably degrading the overall recommendation accuracy. To reduce the negative impact of popularity bias on CF models, we incorporate Bias-aware margins into Contrastive loss and propose a simple yet effective BC Loss, where the margin tailors quantitatively to the bias degree of each user-item interaction. We investigate the geometric interpretation of BC loss, then further visualize and theoretically prove that it simultaneously learns better head and tail representations by encouraging the compactness of similar users/items and enlarging the dispersion of dissimilar users/items. Over six benchmark datasets, we use BC loss to optimize two high-performing CF models. In various evaluation settings (i.e., imbalanced/balanced, temporal split, fully-observed unbiased, tail/head test evaluations), BC loss outperforms the state-of-the-art debiasing and non-debiasing methods with remarkable improvements. Considering the theoretical guarantee and empirical success of BC loss, we advocate using it not just as a debiasing strategy, but also as a standard loss in recommender models. Codes are available at https://github.com/anzhang314/BC-Loss.",https://api.openreview.net/pdf/4165fe49e91aa4259fc88d112d308fae6208237a.pdf,graph;representation;contrastive learning;metric;llm,https://scholar.google.com/scholar?q=Incorporating+Bias-aware+Margins+into+Contrastive+Loss+for+Collaborative+Filtering
Improved Utility Analysis of Private CountSketch,2022,NIPS,"['Rasmus Pagh', 'Mikkel Thorup']",poster,"['sketching', 'dimension reduction', 'sparsity', 'differential privacy', 'countsketch']","Sketching is an important tool for dealing with high-dimensional vectors that are sparse (or well-approximated by a sparse vector), especially useful in distributed, parallel, and streaming settings.
It is known that sketches can be made differentially private by adding noise according to the sensitivity of the sketch, and this has been used in private analytics and federated learning settings.
The post-processing property of differential privacy implies that \emph{all} estimates computed from the sketch can be released within the given privacy budget.

In this paper we consider the classical CountSketch, made differentially private with the Gaussian mechanism, and give an improved analysis of its estimation error.
Perhaps surprisingly, the privacy-utility trade-off is essentially the best one could hope for, independent of the number of repetitions in CountSketch:
The error is almost identical to the error from non-private CountSketch plus the noise needed to make the vector private in the original, high-dimensional domain.
",https://api.openreview.net/pdf/5d4c83d54905c5b0c9715e177469d9cf4ecbd49d.pdf,graph;sparse;federated learning;llm,https://scholar.google.com/scholar?q=Improved+Utility+Analysis+of+Private+CountSketch
IM-Loss: Information Maximization Loss for Spiking Neural Networks,2022,NIPS,"['Yufei Guo', 'Yuanpei Chen', 'Liwen Zhang', 'Xiaode Liu', 'Yinglei Wang', 'Xuhui Huang', 'Zhe Ma']",poster,[],"Spiking Neural Network (SNN), recognized as a type of biologically plausible architecture, has recently drawn much research attention. It transmits information by $0/1$ spikes. This bio-mimetic mechanism of SNN demonstrates extreme energy efficiency since it avoids any multiplications on neuromorphic hardware. However, the forward-passing $0/1$ spike quantization will cause information loss and accuracy degradation. To deal with this problem, the Information maximization loss (IM-Loss) that aims at maximizing the information flow in the SNN is proposed in the paper. The IM-Loss not only enhances the information expressiveness of an SNN directly but also plays a part of the role of normalization without introducing any additional operations (\textit{e.g.}, bias and scaling) in the inference phase. Additionally, we introduce a novel differentiable spike activity estimation, Evolutionary Surrogate Gradients (ESG) in SNNs. By appointing automatic evolvable surrogate gradients for spike activity function, ESG can ensure sufficient model updates at the beginning and accurate gradients at the end of the training, resulting in both easy convergence and high task performance. Experimental results on both popular non-spiking static and neuromorphic datasets show that the SNN models trained by our method outperform the current state-of-the-art algorithms.",https://api.openreview.net/pdf/89c5a2294cdcb736d7f3567f4d409a3ef5018704.pdf,graph;zero_few-shot;transformer;inference;flow;llm,https://scholar.google.com/scholar?q=IM-Loss:+Information+Maximization+Loss+for+Spiking+Neural+Networks
Toward a realistic model of speech processing in the brain with self-supervised learning,2022,NIPS,"['Juliette MILLET', 'Charlotte Caucheteux', 'Pierre Orhan', 'Yves Boubenec', 'Alexandre Gramfort', 'Ewan Dunbar', 'Christophe Pallier', 'Jean-Remi King']",poster,"['Neuroscience', 'Deep Learning', 'Speech Processing', 'Self-supervised learning', 'fMRI']","Several deep neural networks have recently been shown to generate activations similar to those of the brain in response to the same input. These algorithms, however, remain largely implausible: they require (1) extraordinarily large amounts of data, (2) unobtainable supervised labels, (3) textual rather than raw sensory input, and / or (4) implausibly large memory (e.g. thousands of contextual words). These elements highlight the need to identify algorithms that, under these limitations, would suffice to account for both behavioral and brain responses. Focusing on speech processing, we here hypothesize that self-supervised algorithms trained on the raw waveform constitute a promising candidate. Specifically, we compare a recent self-supervised model, wav2vec 2.0, to the brain activity of 412 English, French, and Mandarin individuals recorded with functional Magnetic Resonance Imaging (fMRI), while they listened to approximately one hour of audio books. First, we show that this algorithm learns brain-like representations with as little as 600 hours of unlabelled speech -- a quantity comparable to what infants can be exposed to during language acquisition. Second, its functional hierarchy aligns with the cortical hierarchy of speech processing. Third, different training regimes reveal a functional specialization akin to the cortex: wav2vec 2.0 learns sound-generic, speech-specific and language-specific representations similar to those of the prefrontal and temporal cortices. Fourth, we confirm the similarity of this specialization with the behavior of 386 additional participants. These elements, resulting from the largest neuroimaging benchmark to date, show how self-supervised learning can account for a rich organization of speech processing in the brain, and thus delineate a path to identify the laws of language acquisition which shape the human brain.",https://api.openreview.net/pdf/83f5fb9f6402a963f2cb13a18976c7ba7a2a5a65.pdf,graph;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Toward+a+realistic+model+of+speech+processing+in+the+brain+with+self-supervised+learning
Online Neural Sequence Detection with Hierarchical Dirichlet Point Process,2022,NIPS,"['Weihan Li', 'Yu Qi', 'Gang Pan']",poster,"['Neural sequence detection', 'Online learning', 'Dirichlet process mixture model']","Neural sequence detection plays a vital role in neuroscience research. Recent impressive works utilize convolutive nonnegative matrix factorization and Neyman-Scott process to solve this problem. However, they still face two limitations. Firstly, they accommodate the entire dataset into memory and perform iterative updates of multiple passes, which can be inefficient when the dataset is large or grows frequently. Secondly, they rely on the prior knowledge of the number of sequence types, which can be impractical with data when the future situation is unknown. To tackle these limitations, we propose a hierarchical Dirichlet point process model for efficient neural sequence detection. Instead of computing the entire data, our model can sequentially detect sequences in an online unsupervised manner with Particle filters. Besides, the Dirichlet prior enables our model to automatically introduce new sequence types on the fly as needed, thus avoiding specifying the number of types in advance. We manifest these advantages on synthetic data and neural recordings from songbird higher vocal center and rodent hippocampus.",https://api.openreview.net/pdf/cf61514a13c7d18b6b7b5b6a6026dd6d2d507746.pdf,online learning;llm,https://scholar.google.com/scholar?q=Online+Neural+Sequence+Detection+with+Hierarchical+Dirichlet+Point+Process
OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression,2022,NIPS,"['Wanhua Li', 'Xiaoke Huang', 'Zheng Zhu', 'Yansong Tang', 'Xiu Li', 'Jie Zhou', 'Jiwen Lu']",poster,"['Ordinal Regression', 'Representation Learning', 'Vision-Language', 'Prompt Learning']","This paper presents a language-powered paradigm for ordinal regression. Existing methods usually treat each rank as a category and employ a set of weights to learn these concepts. These methods are easy to overfit and usually attain unsatisfactory performance as the learned concepts are mainly derived from the training set. Recent large pre-trained vision-language models like CLIP have shown impressive performance on various visual tasks. In this paper, we propose to learn the rank concepts from the rich semantic CLIP latent space. Specifically, we reformulate this task as an image-language matching problem with a contrastive objective, which regards labels as text and obtains a language prototype from a text encoder for each rank. While prompt engineering for CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable prompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists of learnable context tokens and learnable rank embeddings. The learnable rank embeddings are constructed by explicitly modeling numerical continuity, resulting in well-ordered, compact language prototypes in the CLIP space. Once learned, we can only save the language prototypes and discard the huge language model, resulting in zero additional computational overhead compared with the linear head counterpart. Experimental results show that our paradigm achieves competitive performance in general ordinal regression tasks, and gains improvements in few-shot and distribution shift settings for age estimation. The code is available at https://github.com/xk-huang/OrdinalCLIP.
",https://api.openreview.net/pdf/69d038f445c67c87093ea023736c448af71bd921.pdf,graph;contrastive learning;llm,https://scholar.google.com/scholar?q=OrdinalCLIP:+Learning+Rank+Prompts+for+Language-Guided+Ordinal+Regression
HierSpeech: Bridging the Gap between Text and Speech by Hierarchical Variational Inference using Self-supervised Representations for Speech Synthesis,2022,NIPS,"['Sang-Hoon Lee', 'Seung-Bin Kim', 'Ji-Hyun Lee', 'Eunwoo Song', 'Min-Jae Hwang', 'Seong-Whan Lee']",poster,"['Speech Synthesis', 'Text-to-Speech', 'Self-supervised speech representation']","This paper presents HierSpeech, a high-quality end-to-end text-to-speech (TTS) system based on a hierarchical conditional variational autoencoder (VAE) utilizing self-supervised speech representations. Recently, single-stage TTS systems, which directly generate raw speech waveform from text, have been getting interest thanks to their ability in generating high-quality audio within a fully end-to-end training pipeline. However, there is still a room for improvement in the conventional TTS systems. Since it is challenging to infer both the linguistic and acoustic attributes from the text directly, missing the details of attributes, specifically linguistic information, is inevitable, which results in mispronunciation and over-smoothing problem in their synthetic speech. To address the aforementioned problem, we leverage self-supervised speech representations as additional linguistic representations to bridge an information gap between text and speech. Then, the hierarchical conditional VAE is adopted to connect these representations and to learn each attribute hierarchically by improving the linguistic capability in latent representations. Compared with the state-of-the-art TTS system, HierSpeech achieves +0.303 comparative mean opinion score, and reduces the phoneme error rate of synthesized speech from 9.16% to 5.78% on the VCTK dataset. Furthermore, we extend our model to HierSpeech-U, an untranscribed text-to-speech system. Specifically, HierSpeech-U can adapt to a novel speaker by utilizing self-supervised speech representations without text transcripts. The experimental results reveal that our method outperforms publicly available TTS models, and show the effectiveness of speaker adaptation with untranscribed speech.",https://api.openreview.net/pdf/ee7d1b4e47ec7356b0fc427d2509e6be20406e16.pdf,graph;zero_few-shot;representation;vae;inference;llm,https://scholar.google.com/scholar?q=HierSpeech:+Bridging+the+Gap+between+Text+and+Speech+by+Hierarchical+Variational+Inference+using+Self-supervised+Representations+for+Speech+Synthesis
Spartan: Differentiable Sparsity via Regularized Transportation,2022,NIPS,"['Kai Sheng Tai', 'Taipeng Tian', 'Ser-Nam Lim']",poster,"['sparsity', 'optimal transport']","We present Spartan, a method for training sparse neural network models with a predetermined level of sparsity. Spartan is based on a combination of two techniques: (1) soft top-k masking of low-magnitude parameters via a regularized optimal transportation problem and (2) dual averaging-based parameter updates with hard sparsification in the forward pass. This scheme realizes an exploration-exploitation tradeoff: early in training, the learner is able to explore various sparsity patterns, and as the soft top-k approximation is gradually sharpened over the course of training, the balance shifts towards parameter optimization with respect to a fixed sparsity mask. Spartan is sufficiently flexible to accommodate a variety of sparsity allocation policies, including both unstructured and block-structured sparsity, global and per-layer sparsity budgets, as well as general cost-sensitive sparsity allocation mediated by linear models of per-parameter costs. On ImageNet-1K classification, we demonstrate that training with Spartan yields 95% sparse ResNet-50 models and 90% block sparse ViT-B/16 models while incurring absolute top-1 accuracy losses of less than 1% compared to fully dense training.",https://api.openreview.net/pdf/7eac241bc2258d4b0f84a38c925122be9ce2787f.pdf,graph;optimization;zero_few-shot;sparse;llm,https://scholar.google.com/scholar?q=Spartan:+Differentiable+Sparsity+via+Regularized+Transportation
Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning,2022,NIPS,"['Yujia Xie', 'Luowei Zhou', 'Xiyang Dai', 'Lu Yuan', 'Nguyen Bach', 'Ce Liu', 'Michael Zeng']",poster,"['Image paragraph captioning', 'vision-language models', 'zero-shot learning']","People say, ""A picture is worth a thousand words"". Then how can we get the rich information out of the image? We argue that by using visual clues to bridge large pretrained vision foundation models and language models, we can do so without any extra cross-modal training. Thanks to the strong zero-shot capability of foundation models, we start by constructing a rich semantic representation of the image (e.g., image tags, object attributes / locations, captions) as a structured textual prompt, called visual clues, using a vision foundation model. Based on visual clues, we use large language model to produce a series of comprehensive descriptions for the visual content, which is then verified by the vision model again to select the candidate that aligns best with the image. We evaluate the quality of generated descriptions by quantitative and qualitative measurement. The results demonstrate the effectiveness of such a structured semantic representation. ",https://api.openreview.net/pdf/72869e40a968a7e085c1ac408bf472b0253740fd.pdf,graph;zero_few-shot;representation;multimodal;llm,https://scholar.google.com/scholar?q=Visual+Clues:+Bridging+Vision+and+Language+Foundations+for+Image+Paragraph+Captioning
Visual Prompting via Image Inpainting,2022,NIPS,"['Amir Bar', 'Yossi Gandelsman', 'Trevor Darrell', 'Amir Globerson', 'Alexei A Efros']",poster,"['Computer Vision', 'Self-Supervised Learning', 'Visual Prompting']","How does one adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification? Inspired by prompting in NLP, this paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples. We show that posing this problem as simple image inpainting -- literally just filling in a hole in a concatenated visual prompt image -- turns out to be surprisingly effective, provided that the inpainting algorithm has been trained on the right data. We train masked auto-encoders on a new dataset that we curated -- 88k unlabeled figures from academic papers sources on Arxiv. We apply visual prompting to these pretrained models and demonstrate results on various downstream image-to-image tasks, including foreground segmentation, single object detection, colorization, edge detection, etc. Project page: https://yossigandelsman.github.io/visual_prompt",https://api.openreview.net/pdf/572d8f95e76a71bcc40b76ab887a58fe6b715574.pdf,segmentation;llm,https://scholar.google.com/scholar?q=Visual+Prompting+via+Image+Inpainting
Memorization Without Overfitting:  Analyzing the Training Dynamics of Large Language Models,2022,NIPS,"['Kushal Tirumala', 'Aram H. Markosyan', 'Luke Zettlemoyer', 'Armen Aghajanyan']",poster,[],"Despite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples. Together, these findings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.",https://api.openreview.net/pdf/759ba7b66f25eb8aceac5ae59a4cfe2b76b2abca.pdf,llm,https://scholar.google.com/scholar?q=Memorization+Without+Overfitting:++Analyzing+the+Training+Dynamics+of+Large+Language+Models
Bayesian Clustering of Neural Spiking Activity Using a Mixture of Dynamic Poisson Factor Analyzers,2022,NIPS,"['Ganchao Wei', 'Ian Stevenson', 'Xiaojing Wang']",poster,"['neural spike data', 'clustering', 'dynamic Poisson factor model (DPFA)', 'mixture of finite mixtures model', 'Markov chain Monte Carlo(MCMC)', 'Laplace Approximation']","Modern neural recording techniques allow neuroscientists to observe the spiking activity of many neurons simultaneously. Although previous work has illustrated how activity within and between known populations of neurons can be summarized by low-dimensional latent vectors, in many cases what determines a unique population may be unclear. Neurons differ in their anatomical location, but also, in their cell types and response properties. Moreover, multiple distinct populations may not be well described by a single low-dimensional, linear representation.	To tackle these challenges, we develop a clustering method based on a mixture of dynamic Poisson factor analyzers (DPFA) model, with the number of clusters treated as an unknown parameter. To do the analysis of DPFA model, we propose a novel Markov chain Monte Carlo (MCMC) algorithm to efficiently sample its posterior distribution. Validating our proposed MCMC algorithm with simulations, we find that it can accurately recover the true clustering and latent states and is insensitive to the initial cluster assignments. We then apply the proposed mixture of DPFA model to multi-region experimental recordings, where we find that the proposed method can identify novel, reliable clusters of neurons based on their activity, and may, thus, be a useful tool for neural data analysis.",https://api.openreview.net/pdf/f2efb99c4640ab7055d93264157d1c2532bc9214.pdf,zero_few-shot;representation;bayesian;llm,https://scholar.google.com/scholar?q=Bayesian+Clustering+of+Neural+Spiking+Activity+Using+a+Mixture+of+Dynamic+Poisson+Factor+Analyzers
Solving Quantitative Reasoning Problems with Language Models,2022,NIPS,"['Aitor Lewkowycz', 'Anders Johan Andreassen', 'David Dohan', 'Ethan Dyer', 'Henryk Michalewski', 'Vinay Venkatesh Ramasesh', 'Ambrose Slone', 'Cem Anil', 'Imanol Schlag', 'Theo Gutman-Solo', 'Yuhuai Wu', 'Behnam Neyshabur', 'Guy Gur-Ari', 'Vedant Misra']",poster,"['language models', 'quantitative reasoning', 'transformers', 'math and science word problems']","Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering questions at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves strong performance in a variety of evaluations, including state-of-the-art performance on the MATH dataset. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a quarter of them.",https://api.openreview.net/pdf/cd0888d5bb1ee2a2445b7b59a6d46c863c219d23.pdf,graph;llm,https://scholar.google.com/scholar?q=Solving+Quantitative+Reasoning+Problems+with+Language+Models
Attention-based Neural Cellular Automata,2022,NIPS,"['Mattie Tesfaldet', 'Derek Nowrouzezahrai', 'Christopher Pal']",poster,"['neural cellular automata', 'cellular automata', 'vision transformer', 'transformer', 'denoising autoencoding', 'computer vision', 'deep learning']","Recent extensions of Cellular Automata (CA) have incorporated key ideas from modern deep learning, dramatically extending their capabilities and catalyzing a new family of Neural Cellular Automata (NCA) techniques. Inspired by Transformer-based architectures, our work presents a new class of _attention-based_ NCAs formed using a spatially localized—yet globally organized—self-attention scheme. We introduce an instance of this class named _Vision Transformer Cellular Automata (ViTCA)_. We present quantitative and qualitative results on denoising autoencoding across six benchmark datasets, comparing ViTCA to a U-Net, a U-Net-based CA baseline (UNetCA), and a Vision Transformer (ViT). When comparing across architectures configured to similar parameter complexity, ViTCA architectures yield superior performance across all benchmarks and for nearly every evaluation metric. We present an ablation study on various architectural configurations of ViTCA, an analysis of its effect on cell states, and an investigation on its inductive biases. Finally, we examine its learned representations via linear probes on its converged cell state hidden representations, yielding, on average, superior results when compared to our U-Net, ViT, and UNetCA baselines.",https://api.openreview.net/pdf/ff35814bcbc77de0e83a664c53cfe4eca3a74d48.pdf,transformer;representation;metric;llm,https://scholar.google.com/scholar?q=Attention-based+Neural+Cellular+Automata
The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,2022,NIPS,"['Xi Ye', 'Greg Durrett']",poster,[],"Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially.

We further show that explanations generated by the LLMs may not entail the models’ predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs’ predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good—logically consistent with the input and the prediction—more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",https://api.openreview.net/pdf/7da1b5b09f88af846c1cad704dc4cd5221640bc9.pdf,zero_few-shot;inference;active learning;llm,https://scholar.google.com/scholar?q=The+Unreliability+of+Explanations+in+Few-shot+Prompting+for+Textual+Reasoning
Regularized Gradient Descent Ascent for Two-Player Zero-Sum Markov Games,2022,NIPS,"['Sihan Zeng', 'Thinh T. Doan', 'Justin Romberg']",poster,"['Markov game', 'reinforcement learning', 'minimax optimization']","We study the problem of finding the Nash equilibrium in a two-player zero-sum Markov game. Due to its formulation as a minimax optimization program, a natural approach to solve the problem is to perform gradient descent/ascent with respect to each player in an alternating fashion. However, due to the non-convexity/non-concavity of the underlying objective function, theoretical understandings of this method are limited. In our paper, we consider solving an entropy-regularized variant of the Markov game. The regularization introduces structures into the optimization landscape that make the solutions more identifiable and allow the problem to be solved more efficiently. Our main contribution is to show that under proper choices of the regularization parameter, the gradient descent ascent algorithm converges to the Nash equilibrium of the original unregularized problem. We explicitly characterize the finite-time performance of the last iterate of our algorithm, which vastly improves over the existing convergence bound of the gradient descent ascent algorithm without regularization. Finally, we complement the analysis with numerical simulations that illustrate the accelerated convergence of the algorithm.",https://api.openreview.net/pdf/b972d15a7b024844971e1eef86e4be9b04a85c50.pdf,graph;optimization;zero_few-shot;multi-agent;llm,https://scholar.google.com/scholar?q=Regularized+Gradient+Descent+Ascent+for+Two-Player+Zero-Sum+Markov+Games
Cluster and Aggregate: Face Recognition with Large Probe Set,2022,NIPS,"['Minchul Kim', 'Feng Liu', 'Anil Jain', 'Xiaoming Liu']",poster,"['feature fusion', 'face recognition']","Feature fusion plays a crucial role in unconstrained face recognition where inputs (probes) comprise of a set of $N$ low quality images whose individual qualities vary. Advances in attention and recurrent modules have led to feature fusion that can model the relationship among the images in the input set. However, attention mechanisms cannot scale to large $N$ due to their quadratic complexity and recurrent modules suffer from input order sensitivity. We propose a two-stage feature fusion paradigm, Cluster and Aggregate, that can both scale to large $N$ and maintain the ability to perform sequential inference with order invariance. Specifically, Cluster stage is a linear assignment of $N$ inputs to $M$ global cluster centers, and Aggregation stage is a fusion over $M$ clustered features. The clustered features play an integral role when the inputs are sequential as they can serve as a summarization of past features. By leveraging the order-invariance of incremental averaging operation, we design an update rule that achieves batch-order invariance, which guarantees that the contributions of early image in the sequence do not diminish as time steps increase. Experiments on IJB-B and IJB-S benchmark datasets show the superiority of the proposed two-stage paradigm in unconstrained face recognition.",https://api.openreview.net/pdf/12df299f99077b7d80450d5cff310ce0a41f77e9.pdf,graph;optimization;zero_few-shot;transformer;inference;llm,https://scholar.google.com/scholar?q=Cluster+and+Aggregate:+Face+Recognition+with+Large+Probe+Set
VTC-LFC: Vision Transformer Compression with Low-Frequency Components,2022,NIPS,"['Zhenyu Wang', 'Hao Luo', 'Pichao WANG', 'Feng Ding', 'Fan Wang', 'Hao Li']",poster,"['ViT', 'compression', 'low-frequency']","Although Vision transformers (ViTs) have recently dominated many vision tasks, deploying ViT models on resource-limited devices remains a challenging problem. To address such a challenge, several methods have been proposed to compress ViTs. Most of them borrow experience in convolutional neural networks (CNNs) and mainly focus on the spatial domain. However, the compression only in the spatial domain suffers from a dramatic performance drop without fine-tuning and is not robust to noise, as the noise in the spatial domain can easily confuse the pruning criteria, leading to some parameters/channels being pruned incorrectly. Inspired by recent findings that self-attention is a low-pass filter and low-frequency signals/components are more informative to ViTs, this paper proposes compressing ViTs with low-frequency components. Two metrics named low-frequency sensitivity (LFS) and low-frequency energy (LFE) are proposed for better channel pruning and token pruning. Additionally, a bottom-up cascade pruning scheme is applied to compress different dimensions jointly. Extensive experiments demonstrate that the proposed method could save 40% ～ 60% of the FLOPs in ViTs, thus significantly increasing the throughput on practical devices with less than 1% performance drop on ImageNet-1K.",https://api.openreview.net/pdf/46778117d0faa4c03aae7163779f69249700010f.pdf,graph;zero_few-shot;transformer;metric;llm,https://scholar.google.com/scholar?q=VTC-LFC:+Vision+Transformer+Compression+with+Low-Frequency+Components
TransBoost: Improving the Best ImageNet Performance using Deep Transduction,2022,NIPS,"['Omer Belhasin', 'Guy Bar-Shalom', 'Ran El-Yaniv']",poster,"['Deep Transductive Learning', 'Image Classification']","This paper deals with deep transductive learning, and proposes TransBoost as a procedure for fine-tuning any deep neural model to improve its performance on any (unlabeled) test set provided at training time. TransBoost is inspired by a large margin principle and is efficient and simple to use. Our method significantly improves the ImageNet classification performance on a wide range of architectures, such as ResNets, MobileNetV3-L, EfficientNetB0, ViT-S, and ConvNext-T, leading to state-of-the-art transductive performance.
Additionally we show that TransBoost is effective on a wide variety of image classification datasets. The implementation of TransBoost is provided at: https://github.com/omerb01/TransBoost .",https://api.openreview.net/pdf/f3bb5624140aeb8932b212f5451e186c17bb763c.pdf,graph;llm,https://scholar.google.com/scholar?q=TransBoost:+Improving+the+Best+ImageNet+Performance+using+Deep+Transduction
Data Distributional Properties Drive Emergent In-Context Learning in Transformers,2022,NIPS,"['Stephanie C.Y. Chan', 'Adam Santoro', 'Andrew Kyle Lampinen', 'Jane X Wang', 'Aaditya K Singh', 'Pierre Harvey Richemond', 'James McClelland', 'Felix Hill']",poster,"['in-context learning', 'few-shot learning', 'transformers']","Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having a large number of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. Our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and indicate how future work might encourage both in-context and in-weights learning in domains beyond language. ",https://api.openreview.net/pdf/6c28a6ada5d40bb53a21c5e76b1baf1efe7c3991.pdf,zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Data+Distributional+Properties+Drive+Emergent+In-Context+Learning+in+Transformers
Automatic differentiation of nonsmooth iterative algorithms,2022,NIPS,"['Jerome Bolte', 'Edouard Pauwels', 'Samuel Vaiter']",poster,"['Fixed point solvers', 'nonsmooth algorithms', 'automatic differentiation', 'unrolling', 'piggy back differentiation', 'conservative gradients.']","Differentiation along algorithms, i.e., piggyback propagation of derivatives, is now routinely used to differentiate iterative solvers in differentiable programming. Asymptotics is well understood for many smooth problems but the nondifferentiable case is hardly considered. Is there a limiting object for nonsmooth piggyback automatic differentiation (AD)? Does it have any variational meaning and can it be used effectively in machine learning? Is there a connection with classical derivative? All these questions are addressed under appropriate contractivity conditions in the framework of conservative derivatives which has proved useful in understanding nonsmooth AD. For nonsmooth piggyback iterations, we characterize the attractor set of nonsmooth piggyback iterations as a set-valued fixed point which remains in the conservative framework. This has various consequences and in particular almost everywhere convergence of classical derivatives. Our results are illustrated on parametric convex optimization problems with forward-backward, Douglas-Rachford and Alternating Direction of Multiplier algorithms as well as the Heavy-Ball method.",https://api.openreview.net/pdf/0920ba4e3658686c02555d3c2338294ffae051d4.pdf,optimization;metric;llm,https://scholar.google.com/scholar?q=Automatic+differentiation+of+nonsmooth+iterative+algorithms
Extracting computational mechanisms from neural data using low-rank RNNs,2022,NIPS,"['Adrian Valente', 'Jonathan W. Pillow', 'Srdjan Ostojic']",poster,"['RNN', 'computational neuroscience']","An influential framework within systems neuroscience posits that neural computations can be understood in terms of low-dimensional dynamics in recurrent circuits. A number of methods have thus been developed to extract latent dynamical systems from neural recordings, but inferring models that are both predictive and interpretable remains a difficult challenge. Here we propose a new method called Low-rank Inference from Neural Trajectories (LINT), based on a class of low-rank recurrent neural networks (lrRNNs) for which a link between connectivity and dynamics has been previously demonstrated. By fitting such networks to trajectories of neural activity, LINT yields a mechanistic model of latent dynamics, as well as a set of axes for dimensionality reduction and verifiable predictions for inactivations of specific populations of neurons. Here, we first demonstrate the consistency of our method and apply it to two use cases: (i) we reverse-engineer ""black-box"" vanilla RNNs trained to perform cognitive tasks, and (ii) we infer latent dynamics and neural contributions from electrophysiological recordings of nonhuman primates performing a similar task. ",https://api.openreview.net/pdf/e67b4490261e2e9568106766e6875c5b7f0f9a54.pdf,graph;zero_few-shot;inference;low-rank;llm,https://scholar.google.com/scholar?q=Extracting+computational+mechanisms+from+neural+data+using+low-rank+RNNs
Forecasting Human Trajectory from Scene History,2022,NIPS,"['Mancheng Meng', 'Ziyan Wu', 'Terrence Chen', 'Xiran Cai', 'Xiang Sean Zhou', 'Fan Yang', 'Dinggang Shen']",poster,"['Human trajectory prediction', 'Scene history', 'Group trajectory', 'Cross-modal interaction']","Predicting the future trajectory of a person remains a challenging problem, due to randomness and subjectivity. However, the moving patterns of human in constrained scenario typically conform to a limited number of regularities to a certain extent, because of the scenario restrictions (\eg, floor plan, roads and obstacles) and person-person or person-object interactivity. Thus, an individual person in this scenario should follow one of the regularities as well. In other words, a person's subsequent trajectory has likely been traveled by others. Based on this hypothesis, we propose to forecast a person's future trajectory by learning from the implicit scene regularities. We call the regularities, inherently derived from the past dynamics of the people and the environment in the scene,  \emph{scene history}. We categorize scene history information into two types: historical group trajectories and individual-surroundings interaction. To exploit these information for trajectory prediction, we propose a novel framework Scene History Excavating Network (SHENet), where the scene history is leveraged in a simple yet effective approach. In particular, we design two components, the group trajectory bank module to extract representative group trajectories as the candidate for future path, and the cross-modal interaction module to model the interaction between individual past trajectory and its surroundings for trajectory refinement, respectively.  In addition, to mitigate the uncertainty in the evaluation, caused by the aforementioned randomness and subjectivity, we propose to include smoothness into evaluation metrics. We conduct extensive evaluations to validate the efficacy of proposed framework on ETH, UCY, as well as a new, challenging benchmark dataset PAV, demonstrating superior performance compared to state-of-the-art methods.",https://api.openreview.net/pdf/cd01204f4380910e9f932559c7e3b4f2a9c20eca.pdf,graph;optimization;zero_few-shot;metric;multimodal;llm,https://scholar.google.com/scholar?q=Forecasting+Human+Trajectory+from+Scene+History
Deep invariant networks with differentiable augmentation layers,2022,NIPS,"['Cédric Rommel', 'Thomas Moreau', 'Alexandre Gramfort']",poster,"['invariance learning', 'data augmentation', 'automatic data augmentation']","Designing learning systems which are invariant to certain data transformations is critical in machine learning. Practitioners can typically enforce a desired invariance on the trained model through the choice of a network architecture, e.g. using convolutions for translations, or using data augmentation. Yet, enforcing true invariance in the network can be difficult, and data invariances are not always known a piori. State-of-the-art methods for learning data augmentation policies require held-out data and are based on bilevel optimization problems, which are complex to solve and often computationally demanding. In this work we investigate new ways of learning invariances only from the training data. Using learnable augmentation layers built directly in the network, we demonstrate that our method is very versatile. It can incorporate any type of differentiable augmentation and be applied to a broad class of learning problems beyond computer vision. We provide empirical evidence showing that our approach is easier and faster to train than modern automatic data augmentation techniques based on bilevel optimization, while achieving comparable results. Experiments show that while the invariances transferred to a model through automatic data augmentation are limited by the model expressivity, the invariance yielded by our approach is insensitive to it by design.",https://api.openreview.net/pdf/cacd70900e34d0857723301444828fb184e48c85.pdf,optimization;transfer learning;augmentation;llm,https://scholar.google.com/scholar?q=Deep+invariant+networks+with+differentiable+augmentation+layers
Amortized Mixing Coupling Processes for Clustering,2022,NIPS,"['Huafeng Liu', 'Liping Jing']",poster,"['generative model', 'amortized clustering']","Considering the ever-increasing scale of data, which may contain tens of thousands of data points or complicated latent structures, the issue of scalability and algorithmic efficiency becomes of vital importance for clustering. In this paper, we propose cluster-wise amortized mixing coupling processes (AMCP), which is able to achieve efficient amortized clustering in a well-defined non-parametric Bayesian posterior. Specifically, AMCP learns clusters sequentially with the aid of the proposed intra-cluster mixing (IntraCM) and inter-cluster coupling (InterCC) strategies, which investigate the relationship between data points and reference distribution in a linear optimal transport mixing view, and coupling the unassigned set and assigned set to generate new cluster. IntraCM and InterCC avoid pairwise calculation of distances between clusters and reduce the computational complexity from quadratic to linear in the current number of clusters. Furthermore, cluster-wise sequential process is able to improve the quick adaptation ability for the next cluster generation. In this case, AMCP simultaneously learns what makes a cluster, how to group data points into clusters, and how to adaptively control the number of clusters. To illustrate the superiority of the proposed method, we perform experiments on both synthetic data and real-world data in terms of clustering performance and computational efficiency. The source code is available at https://github.com/HuafengHK/AMCP.",https://api.openreview.net/pdf/1b50fa2cd40f80876ac7a5b014ddb80169833f60.pdf,generative model;adaptive;bayesian;metric;llm,https://scholar.google.com/scholar?q=Amortized+Mixing+Coupling+Processes+for+Clustering
Green Hierarchical Vision Transformer for Masked Image Modeling,2022,NIPS,"['Lang Huang', 'Shan You', 'Mingkai Zheng', 'Fei Wang', 'Chen Qian', 'Toshihiko Yamasaki']",poster,"['Self-Supervised Learning', 'Masked Image Modeling', 'Vision Transformers']","We present an efficient approach for Masked Image Modeling (MIM) with hierarchical Vision Transformers (ViTs), allowing the hierarchical ViTs to discard masked patches and operate only on the visible ones. Our approach consists of three key designs. First, for window attention, we propose a Group Window Attention scheme following the Divide-and-Conquer strategy. To mitigate the quadratic complexity of the self-attention w.r.t. the number of patches, group attention encourages a uniform partition that visible patches within each local window of arbitrary size can be grouped with equal size, where masked self-attention is then performed within each group. Second, we further improve the grouping strategy via the Dynamic Programming algorithm to minimize the overall computation cost of the attention on the grouped patches. Third, as for the convolution layers, we convert them to the Sparse Convolution that works seamlessly with the sparse data, i.e., the visible patches in MIM. As a result, MIM can now work on most, if not all, hierarchical ViTs in a green and efficient way. For example, we can train the hierarchical ViTs, e.g., Swin Transformer and Twins Transformer, about 2.7$\times$ faster and reduce the GPU memory usage by 70%, while still enjoying competitive performance on ImageNet classification and the superiority on downstream COCO object detection benchmarks.",https://api.openreview.net/pdf/8a469610a08f87aeb560c0f3ecd133d8df23e2ed.pdf,zero_few-shot;transformer;sparse;llm,https://scholar.google.com/scholar?q=Green+Hierarchical+Vision+Transformer+for+Masked+Image+Modeling
Learning State-Aware Visual Representations from Audible Interactions,2022,NIPS,"['Himangi Mittal', 'Pedro Morgado', 'Unnat Jain', 'Abhinav Gupta']",poster,"['Video Representation learning', 'self-supervised learning', 'contrastive learning', 'audio-visual learning', 'egocentric videos', 'Ego4D', 'EPIC-Kitchens']","We propose a self-supervised algorithm to learn representations from egocentric video data. Recently, significant efforts have been made to capture humans interacting with their own environments as they go about their daily activities. In result, several large egocentric datasets of interaction-rich multi-modal data have emerged. However, learning representations from videos can be challenging. First, given the uncurated nature of long-form continuous videos, learning effective representations require focusing on moments in time when interactions take place. Second, visual representations of daily activities should be sensitive to changes in the state of the environment. However, current successful multi-modal learning frameworks encourage representation invariance over time. To address these challenges, we leverage audio signals to identify moments of likely interactions which are conducive to better learning. We also propose a novel self-supervised objective that learns from audible state changes caused by interactions. We validate these contributions extensively on two large-scale egocentric datasets, EPIC-Kitchens-100 and the recently released Ego4D, and show improvements on several downstream tasks, including action recognition, long-term action anticipation, and object state change classification.",https://api.openreview.net/pdf/3c00a294611b2926715d5743b6eb92bea655be53.pdf,graph;zero_few-shot;representation;multimodal;llm,https://scholar.google.com/scholar?q=Learning+State-Aware+Visual+Representations+from+Audible+Interactions
Optimal Comparator Adaptive Online Learning with Switching Cost,2022,NIPS,"['Zhiyu Zhang', 'Ashok Cutkosky', 'Ioannis Paschalidis']",poster,"['parameter-free online learning', 'adaptive online learning', 'switching cost']","Practical online learning tasks are often naturally defined on unconstrained domains, where optimal algorithms for general convex losses are characterized by the notion of comparator adaptivity. In this paper, we design such algorithms in the presence of switching cost - the latter penalizes the typical optimism in adaptive algorithms, leading to a delicate design trade-off. Based on a novel dual space scaling strategy discovered by a continuous-time analysis, we propose a simple algorithm that improves the existing comparator adaptive regret bound [ZCP22a] to the optimal rate. The obtained benefits are further extended to the expert setting, and the practicality of the proposed algorithm is demonstrated through a sequential investment task.",https://api.openreview.net/pdf/523883b00f1906e89e9ecf4202bc1e938a3c1d97.pdf,optimization;transformer;online learning;adaptive;llm,https://scholar.google.com/scholar?q=Optimal+Comparator+Adaptive+Online+Learning+with+Switching+Cost
Efficient and Modular Implicit Differentiation,2022,NIPS,"['Mathieu Blondel', 'Quentin Berthet', 'marco cuturi', 'Roy Frostig', 'Stephan Hoyer', 'Felipe Llinares-López', 'Fabian Pedregosa', 'Jean-Philippe Vert']",poster,"['implicit differentiation', 'bilevel optimization', 'autodiff', 'jax']","Automatic differentiation (autodiff) has revolutionized machine learning.  It
allows to express complex computations by composing elementary ones in creative
ways and removes the burden of computing their derivatives by hand. More
recently, differentiation of optimization problem solutions has attracted
widespread attention with applications such as optimization layers, and in
bi-level problems such as hyper-parameter optimization and meta-learning.
However, so far, implicit differentiation remained difficult to use for
practitioners, as it often required case-by-case tedious mathematical
derivations and implementations. In this paper, we propose
automatic implicit differentiation, an efficient
and modular approach for implicit differentiation of optimization problems. In
our approach, the user defines directly in Python a function $F$ capturing the
optimality conditions of the problem to be differentiated. Once this is done, we
leverage autodiff of $F$ and the implicit function theorem to automatically
differentiate the optimization problem.  Our approach thus combines the benefits
of implicit differentiation and autodiff.  It is efficient as it can be added on
top of any state-of-the-art solver and modular as the optimality condition
specification is decoupled from the implicit differentiation mechanism.  We show
that seemingly simple principles allow to recover many existing implicit
differentiation methods and create new ones easily.  We demonstrate the ease of
formulating and solving bi-level optimization problems using our framework. We
also showcase an application to the sensitivity analysis of molecular dynamics.",https://api.openreview.net/pdf/85e7ecc17b6faeff583c548c159ac8a41d00092d.pdf,optimization;zero_few-shot;transformer;meta-learning;llm,https://scholar.google.com/scholar?q=Efficient+and+Modular+Implicit+Differentiation
Efficient Risk-Averse Reinforcement Learning,2022,NIPS,"['Ido Greenberg', 'Yinlam Chow', 'Mohammad Ghavamzadeh', 'Shie Mannor']",poster,"['Reinforcement Learning', 'safe RL', 'risk averse RL', 'risk sensitive RL', 'sample efficient RL', 'coherent risk measures', 'CVaR', 'blindness to success', 'cross entropy method', 'CEM']","In risk-averse reinforcement learning (RL), the goal is to optimize some risk measure of the returns. A risk measure often focuses on the worst returns out of the agent's experience. As a result, standard methods for risk-averse RL often ignore high-return strategies. We prove that under certain conditions this inevitably leads to a local-optimum barrier, and propose a mechanism we call soft risk to bypass it. We also devise a novel cross entropy module for sampling, which (1) preserves risk aversion despite the soft risk; (2) independently improves sample efficiency. By separating the risk aversion of the sampler and the optimizer, we can sample episodes with poor conditions, yet optimize with respect to successful strategies. We combine these two concepts in CeSoR - Cross-entropy Soft-Risk optimization algorithm - which can be applied on top of any risk-averse policy gradient (PG) method. We demonstrate improved risk aversion in maze navigation, autonomous driving, and resource allocation benchmarks, including in scenarios where standard risk-averse PG completely fails.",https://api.openreview.net/pdf/8699e52949c5985c4ff3fee74aba06e11b83e036.pdf,reinforcement learning;optimization;llm,https://scholar.google.com/scholar?q=Efficient+Risk-Averse+Reinforcement+Learning
EfficientFormer: Vision Transformers at MobileNet Speed,2022,NIPS,"['Yanyu Li', 'Geng Yuan', 'Yang Wen', 'Eric Hu', 'Georgios Evangelidis', 'Sergey Tulyakov', 'Yanzhi Wang', 'Jian Ren']",poster,['Vision Transformer'],"Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks. 
However, due to the massive number of parameters and model design, e.g., attention mechanism, ViT-based models are generally times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance? To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs. Then we introduce a dimension-consistent pure transformer (without MobileNet blocks) as a design paradigm. Finally, we perform latency-driven slimming to get a series of final models dubbed EfficientFormer. Extensive experiments show the superiority of EfficientFormer in performance and speed on mobile devices. Our fastest model, EfficientFormer-L1, achieves $79.2\%$ top-1 accuracy on ImageNet-1K with only $1.6$ ms inference latency on iPhone 12 (compiled with CoreML), which runs as fast as MobileNetV2$\times 1.4$ ($1.6$ ms, $74.7\%$ top-1), and our largest model, EfficientFormer-L7, obtains $83.3\%$ accuracy with only $7.0$ ms latency. Our work proves that properly designed transformers can reach extremely low latency on mobile devices while maintaining high performance. ",https://api.openreview.net/pdf/6eaf30d6436a8c2bac1b7a2e7e63c77abf73ae78.pdf,graph;optimization;zero_few-shot;transformer;inference;llm,https://scholar.google.com/scholar?q=EfficientFormer:+Vision+Transformers+at+MobileNet+Speed
SNN-RAT: Robustness-enhanced Spiking Neural Network through Regularized Adversarial Training,2022,NIPS,"['Jianhao Ding', 'Tong Bu', 'Zhaofei Yu', 'Tiejun Huang', 'Jian K Liu']",poster,"['Spiking Neural Networks', 'Neural Coding', 'Perturbation Analysis']","Spiking neural networks (SNNs) are promising to be widely deployed in real-time and safety-critical applications with the advance of neuromorphic computing. Recent work has demonstrated the insensitivity of SNNs to small random perturbations due to the discrete internal information representation. The variety of training algorithms and the involvement of the temporal dimension pose more threats to the robustness of SNNs than that of typical neural networks. We account for the vulnerability of SNNs by constructing adversaries based on different differentiable approximation techniques. By deriving a Lipschitz constant specifically for the spike representation, we first theoretically answer the question of how much adversarial invulnerability is retained in SNNs. Hence, to defend against the broad attack methods, we propose a regularized adversarial training scheme with low computational overheads. SNNs can benefit from the constraint of the perturbed spike distance's amplification and the generalization on multiple adversarial $\epsilon$-neighbourhoods. Our experiments on the image recognition benchmarks have proven that our training scheme can defend against powerful adversarial attacks crafted from strong differentiable approximations. To be specific, our approach makes the black-box attacks of the Projected Gradient Descent attack nearly ineffective. We believe that our work will facilitate the spread of SNNs for safety-critical applications and help understand the robustness of the human brain.",https://api.openreview.net/pdf/745dce6df8f914f78b2250702969cca1124dc10a.pdf,optimization;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=SNN-RAT:+Robustness-enhanced+Spiking+Neural+Network+through+Regularized+Adversarial+Training
M³ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design,2022,NIPS,"['hanxue liang', 'Zhiwen Fan', 'Rishov Sarkar', 'Ziyu Jiang', 'Tianlong Chen', 'Kai Zou', 'Yu Cheng', 'Cong Hao', 'Zhangyang Wang']",poster,"['multi-task learning', 'mixture of experts', 'vision transformer', 'hardware co-design']","Multi-task learning (MTL) encapsulates multiple learned tasks in a single model and often lets those tasks learn better jointly. Multi-tasking models have become successful and often essential for many sophisticated systems such as autonomous driving and indoor robots. However, when deploying MTL onto those real-world systems that are often resource-constrained or latency-sensitive, two prominent challenges arise: (i) during training, simultaneously optimizing all tasks is often difficult due to gradient conflicts across tasks, and the challenge is amplified when a growing number of tasks have to be squeezed into one compact model; (ii) at inference, current MTL regimes have to activate nearly the entire model even to just execute a single task. Yet most real systems demand only one or two tasks at each moment, while flexibly switching between tasks per need: therefore such “all tasks activated” inference is also highly inefficient and non-scalable in practice. 
In this paper, we present a model-accelerator co-design framework to enable efficient on-device MTL, that tackles both training and inference bottlenecks. Our framework, dubbed M³ViT, customizes mixture-of-experts (MoE) layers into a vision transformer (ViT) backbone for MTL, and sparsely activates task-specific experts during training, which effectively disentangles the parameter spaces to avoid different tasks’ training conflicts. Then at inference with any task of interest, the same design allows for activating only the task-corresponding sparse “expert” pathway, instead of the full model. Our new model design is further enhanced by hardware-level innovations, in particular, a novel computation reordering scheme tailored for memory-constrained MTL that achieves zero-overhead switching between tasks and can scale to any number of experts. Extensive experiments on PASCAL-Context and NYUD-v2 datasets at both software and hardware levels are conducted to demonstrate the effectiveness of the proposed design. When executing the practical scenario of single-task inference, M³ViT achieves higher accuracies than encoder-focused MTL methods, while significantly reducing 88% inference FLOPs. When implemented on a hardware platform of one Xilinx ZCU104 FPGA, our co-design framework reduces the memory requirement by 2.40×, while achieving energy efficiency (as the product of latency and power) up to 9.23× times higher than a comparable FPGA baseline.",https://api.openreview.net/pdf/ca4172a26d34dafddc642742820ce5dcb3b91c34.pdf,optimization;zero_few-shot;transformer;inference;sparse;multi-task;llm,https://scholar.google.com/scholar?q=M³ViT:+Mixture-of-Experts+Vision+Transformer+for+Efficient+Multi-task+Learning+with+Model-Accelerator+Co-design
"Recipe for a General, Powerful, Scalable Graph Transformer",2022,NIPS,"['Ladislav Rampasek', 'Mikhail Galkin', 'Vijay Prakash Dwivedi', 'Anh Tuan Luu', 'Guy Wolf', 'Dominique Beaini']",poster,"['graph transformers', 'graph neural networks', 'transformers', 'learning on graphs', 'graph representation learning']","We propose a recipe on how to build a general, powerful, scalable (GPS) graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks. Graph Transformers (GTs) have gained popularity in the field of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. In this paper, we summarize the different types of encodings with a clearer definition and categorize them as being $\textit{local}$, $\textit{global}$ or $\textit{relative}$. The prior GTs are constrained to small graphs with a few hundred nodes, here we propose the first architecture with a complexity linear in the number of nodes and edges $O(N+E)$ by decoupling the local real-edge aggregation from the fully-connected Transformer. We argue that this decoupling does not negatively affect the expressivity, with our architecture being a universal function approximator on graphs. Our GPS recipe consists of choosing 3 main ingredients: (i) positional/structural encoding, (ii) local message-passing mechanism, and (iii) global attention mechanism. We provide a modular framework $\textit{GraphGPS}$ that supports multiple types of encodings and that provides efficiency and scalability both in small and large graphs. We test our architecture on 16 benchmarks and show highly competitive results in all of them, show-casing the empirical benefits gained by the modularity and the combination of different strategies.",https://api.openreview.net/pdf/90cc453a2becd62ab16648236a138ccb3601f6bf.pdf,graph;optimization;transformer;representation;llm,"https://scholar.google.com/scholar?q=Recipe+for+a+General,+Powerful,+Scalable+Graph+Transformer"
Optimizing Relevance Maps of Vision Transformers Improves Robustness,2022,NIPS,"['Hila Chefer', 'Idan Schwartz', 'Lior Wolf']",poster,"['Vision Transformers', 'Explainability', 'Robustness']","It has been observed that visual classification models often rely mostly on spurious cues such as the image background, which hurts their robustness to distribution changes.  
To alleviate this shortcoming, we propose to monitor the model's relevancy signal and direct the model to base its prediction on the foreground object.
This is done as a finetuning step, involving relatively few samples consisting of pairs of images and their associated foreground masks. Specifically, we encourage the model's relevancy map (i) to assign lower relevance to background regions, (ii) to consider as much information as possible from the foreground, and (iii) we encourage the decisions to have high confidence. When applied to Vision Transformer (ViT) models, a marked improvement in robustness to domain-shifts is observed. Moreover, the foreground masks can be obtained automatically, from a self-supervised variant of the ViT model itself; therefore no additional supervision is required. Our code is available at: https://github.com/hila-chefer/RobustViT.",https://api.openreview.net/pdf/6d70329b4d1656bfe637c26290e1dd72ed421f6a.pdf,zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Optimizing+Relevance+Maps+of+Vision+Transformers+Improves+Robustness
Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks,2022,NIPS,"['Blake Bordelon', 'Cengiz Pehlevan']",poster,"['Deep Learning Theory', 'Infinite Width', 'Kernel Methods']","We analyze feature learning in infinite-width neural networks trained with gradient flow through a self-consistent dynamical field theory. We construct a collection of deterministic dynamical order parameters which are inner-product kernels for hidden unit activations and gradients in each layer at pairs of time points, providing a reduced description of network activity through training. These kernel order parameters collectively define the hidden layer activation distribution, the evolution of the neural tangent kernel, and consequently output predictions. We show that the field theory derivation recovers the recursive stochastic process of infinite-width feature learning networks obtained from Yang & Hu with Tensor Programs. For deep linear networks, these kernels satisfy a set of algebraic matrix equations. For nonlinear networks, we provide an alternating sampling procedure to self-consistently solve for the kernel order parameters. We provide comparisons of the self-consistent solution to various approximation schemes including the static NTK approximation, gradient independence assumption, and leading order perturbation theory, showing that each of these approximations can break down in regimes where general self-consistent solutions still provide an accurate description. Lastly, we provide experiments in more realistic settings which demonstrate that the loss and kernel dynamics of CNNs at fixed feature learning strength is preserved across different widths on a CIFAR classification task.",https://api.openreview.net/pdf/959778b3742b502e4a3edcfba0e662882b43ca55.pdf,zero_few-shot;online learning;flow;llm,https://scholar.google.com/scholar?q=Self-Consistent+Dynamical+Field+Theory+of+Kernel+Evolution+in+Wide+Neural+Networks
Random Sharpness-Aware Minimization,2022,NIPS,"['Yong Liu', 'Siqi Mai', 'Minhao Cheng', 'Xiangning Chen', 'Cho-Jui Hsieh', 'Yang You']",poster,"['Sharpness-Aware Minimization', 'Generalization', 'Adversarial Training']","Currently, Sharpness-Aware Minimization (SAM) is proposed to seek the parameters that lie in a flat region to improve the generalization when training neural networks. In particular, a minimax optimization objective is defined to find the maximum loss value centered on the weight, out of the purpose of simultaneously minimizing loss value and loss sharpness. For the sake of simplicity, SAM applies one-step gradient ascent to approximate the solution of the inner maximization.  However, one-step gradient ascent may not be sufficient and multi-step gradient ascents will cause additional training costs.  Based on this observation, we propose a novel random smoothing based SAM (R-SAM) algorithm. To be specific, R-SAM essentially smooths the loss landscape, based on which we are able to apply the one-step gradient ascent on the smoothed weights to improve the approximation of the inner maximization. Further, we evaluate our proposed R-SAM on CIFAR and ImageNet datasets. The experimental results illustrate that R-SAM can consistently improve the performance on ResNet and Vision Transformer (ViT) training. ",https://api.openreview.net/pdf/6a17e3fab177bdfa7a840bb7c9c0662f8b5b985b.pdf,optimization;transformer;llm,https://scholar.google.com/scholar?q=Random+Sharpness-Aware+Minimization
Expediting Large-Scale Vision Transformer for Dense Prediction without Fine-tuning,2022,NIPS,"['Weicong Liang', 'Yuhui Yuan', 'Henghui Ding', 'Xiao Luo', 'Weihong Lin', 'Ding Jia', 'Zheng Zhang', 'Chao Zhang', 'Han Hu']",poster,"['Transformer', 'High-Resolution', 'Semantic Segmentation', 'Depth Estimation', 'Classification', 'Efficient Architecture']","Vision transformers have recently achieved competitive results across various vision tasks but still suffer from heavy computation costs when processing a large number of tokens. Many advanced approaches have been developed to reduce the total number of tokens in the large-scale vision transformers, especially for image classification tasks. Typically, they select a small group of essential tokens according to their relevance with the [\texttt{class}] token, then fine-tune the weights of the vision transformer. Such fine-tuning is less practical for dense prediction due to the much heavier computation and GPU memory cost than image classification.

In this paper, we focus on a more challenging problem, \ie, accelerating large-scale vision transformers for dense prediction without any additional re-training or fine-tuning. In response to the fact that high-resolution representations are necessary for dense prediction, we present two non-parametric operators, a \emph{token clustering layer} to decrease the number of tokens and a \emph{token reconstruction layer} to increase the number of tokens. The following steps are performed to achieve this: (i) we use the token clustering layer to cluster the neighboring tokens together, resulting in low-resolution representations that maintain the spatial structures; (ii) we apply the following transformer layers only to these low-resolution representations or clustered tokens; and (iii) we use the token reconstruction layer to re-create the high-resolution representations from the refined low-resolution representations. The results obtained by our method are promising on five dense prediction tasks including object detection, semantic segmentation, panoptic segmentation, instance segmentation, and depth estimation. Accordingly, our method accelerates $40\%\uparrow$ FPS and saves $30\%\downarrow$ GFLOPs of ``Segmenter+ViT-L/$16$'' while maintaining $99.5\%$ of the performance on ADE$20$K without fine-tuning the official weights.",https://api.openreview.net/pdf/a33da06e5b26b33db70e6b6ae4d6d4a749fbb870.pdf,graph;zero_few-shot;transformer;representation;metric;segmentation;llm,https://scholar.google.com/scholar?q=Expediting+Large-Scale+Vision+Transformer+for+Dense+Prediction+without+Fine-tuning
SAPA: Similarity-Aware Point Affiliation for Feature Upsampling,2022,NIPS,"['Hao Lu', 'Wenze Liu', 'Zixuan Ye', 'Hongtao Fu', 'Yuliang Liu', 'Zhiguo Cao']",poster,"['Feature upsampling', 'dense prediction', 'semantic segmentation', 'object detection', 'depth estimation', 'image matting']","We introduce point affiliation into feature upsampling, a notion that describes the affiliation of each upsampled point to a semantic cluster formed by local decoder feature points with semantic similarity. By rethinking point affiliation, we present a generic formulation for generating upsampling kernels. The kernels encourage not only semantic smoothness but also boundary sharpness in the upsampled feature maps. Such properties are particularly useful for some dense prediction tasks such as semantic segmentation. The key idea of our formulation is to generate similarity-aware kernels by comparing the similarity between each encoder feature point and the spatially associated local region of decoder features. In this way, the encoder feature point can function as a cue to inform the semantic cluster of upsampled feature points. To embody the formulation, we further instantiate a lightweight upsampling operator, termed Similarity-Aware Point Affiliation (SAPA), and investigate its variants. SAPA invites consistent performance improvements on a number of dense prediction tasks, including semantic segmentation, object detection, depth estimation, and image matting. Code is available at: https://github.com/poppinace/sapa",https://api.openreview.net/pdf/58e544f48355a5d20acce637f77ca14b64fdd65b.pdf,segmentation;llm,https://scholar.google.com/scholar?q=SAPA:+Similarity-Aware+Point+Affiliation+for+Feature+Upsampling
Generalized Laplacian Eigenmaps,2022,NIPS,"['Hao Zhu', 'Piotr Koniusz']",poster,"['GCL', 'graph contrastive learning', 'node embedding', 'logdet', 'rank minimization']","Graph contrastive learning attracts/disperses node representations for similar/dissimilar node pairs under some notion of similarity. It may be combined with a low-dimensional embedding of nodes to preserve intrinsic and structural properties of a graph. COLES, a recent graph contrastive method combines traditional graph embedding and negative sampling into one framework. COLES in fact minimizes the trace difference between the within-class scatter matrix encapsulating the graph connectivity and the total scatter matrix encapsulating negative sampling. In this paper, we propose a more essential framework for graph embedding, called Generalized Laplacian EigeNmaps (GLEN), which learns a graph representation by maximizing the rank difference between the  total scatter matrix and the within-class scatter matrix, resulting in the minimum class separation guarantee. However, the rank difference minimization is an NP-hard problem. Thus, we replace the trace difference that corresponds to the difference of nuclear norms by the difference of LogDet expressions, which we argue is a more accurate surrogate for the NP-hard rank difference than the trace difference.  While enjoying a lesser computational cost, the difference of LogDet terms is lower-bounded by the Affine-invariant Riemannian metric (AIRM) and  Jesen-Bregman the LogDet Divergence (JBLD), and upper-bounded by AIRM scaled by the factor of $\sqrt{m}$. We show that GLEN offers favourable accuracy/scalability compared to  state-of-the-art baselines.",https://api.openreview.net/pdf/7a5869abf2ad572f3e6eb482b270eac60fb4098b.pdf,graph;zero_few-shot;representation;contrastive learning;metric;llm,https://scholar.google.com/scholar?q=Generalized+Laplacian+Eigenmaps
Neural Matching Fields: Implicit Representation of Matching Fields for Visual Correspondence,2022,NIPS,"['Sunghwan Hong', 'Ji Su Nam', 'Seokju Cho', 'Susung Hong', 'Sangryul Jeon', 'Dongbo Min', 'Seungryong Kim']",poster,"['Implicit neural representation', 'semantic correspondence']","Existing pipelines of semantic correspondence commonly include extracting high-level semantic features for the invariance against intra-class variations and background clutters. This architecture, however, inevitably results in a low-resolution matching field that additionally requires an ad-hoc interpolation process as a post-processing for converting it into a high-resolution one, certainly limiting the overall performance of matching results. To overcome this, inspired by recent success of implicit neural representation, we present a novel method for semantic correspondence, called Neural Matching Field (NeMF). However, complicacy and high-dimensionality of a 4D matching field are the major hindrances, which we propose a cost embedding network to process a coarse cost volume to use as a guidance for establishing high-precision matching field through the following fully-connected network. Nevertheless, learning a high-dimensional matching field remains challenging mainly due to computational complexity, since a na\""ive exhaustive inference would require querying from all pixels in the 4D space to infer pixel-wise correspondences. To overcome this, we propose adequate training and inference procedures, which in the training phase, we randomly sample matching candidates and in the inference phase, we iteratively performs PatchMatch-based inference and coordinate optimization at test time. With these combined, competitive results are attained on several standard benchmarks for semantic correspondence. Code and pre-trained weights are available at~\url{https://ku-cvlab.github.io/NeMF/}.",https://api.openreview.net/pdf/dc4039ad52c849e6d1c610756b5782f283f29c18.pdf,graph;optimization;zero_few-shot;representation;inference;llm,https://scholar.google.com/scholar?q=Neural+Matching+Fields:+Implicit+Representation+of+Matching+Fields+for+Visual+Correspondence
A Unified Diversity Measure for Multiagent Reinforcement Learning,2022,NIPS,"['Zongkai Liu', 'Chao Yu', 'Yaodong Yang', 'peng sun', 'Zifan Wu', 'Yuan Li']",poster,[],"Promoting behavioural diversity is of critical importance in multi-agent reinforcement learning, since it helps the agent population maintain robust performance when encountering unfamiliar opponents at test time, or,  when the game is highly non-transitive in the strategy space (e.g., Rock-Paper-Scissor). While a myriad of diversity metrics have been proposed, there are no widely accepted or unified  definitions in the literature, making the consequent diversity-aware learning algorithms difficult to evaluate and the insights elusive. In this work, we propose a novel  metric called the Unified Diversity Measure (UDM) that offers a unified view for existing diversity metrics. Based on UDM, we design the UDM-Fictitious Play (UDM-FP) and UDM-Policy Space Response Oracle (UDM-PSRO) algorithms as efficient solvers for  normal-form games and open-ended games. In theory, we prove that UDM-based methods can enlarge the gamescape by increasing the response capacity of the strategy pool, and have convergence guarantee to two-player Nash equilibrium. We validate our  algorithms on games that show strong non-transitivity, and empirical results show that our algorithms achieve better performances than strong PSRO baselines in terms of the exploitability and population effectivity. ",https://api.openreview.net/pdf/026f13525e0b127471023bd9d13b36ca4774c5e5.pdf,reinforcement learning;metric;multi-agent;llm,https://scholar.google.com/scholar?q=A+Unified+Diversity+Measure+for+Multiagent+Reinforcement+Learning
Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,2022,NIPS,"['Chitwan Saharia', 'William Chan', 'Saurabh Saxena', 'Lala Li', 'Jay Whang', 'Emily Denton', 'Seyed Kamyar Seyed Ghasemipour', 'Raphael Gontijo-Lopes', 'Burcu Karagol Ayan\u200e', 'Tim Salimans', 'Jonathan Ho', 'David J. Fleet', 'Mohammad Norouzi']",poster,"['text-to-image', 'generative models', 'diffusion models']","We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g., T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.",https://api.openreview.net/pdf/3c56da71deacc9029fd7714d9e2ecaccc0c38a37.pdf,graph;transformer;generative model;diffusion models;llm,https://scholar.google.com/scholar?q=Photorealistic+Text-to-Image+Diffusion+Models+with+Deep+Language+Understanding
Distinguishing Learning Rules with Brain Machine Interfaces,2022,NIPS,"['Jacob Portes', 'Christian Schmid', 'James M Murray']",poster,"['brain-machine interface', 'recurrent neural network', 'reinforcement learning', 'biological learning rules']","Despite extensive theoretical work on biologically plausible learning rules, clear evidence about whether and how such rules are implemented in the brain has been difficult to obtain. We consider biologically plausible supervised- and reinforcement-learning rules and ask whether changes in network activity during learning can be used to determine which learning rule is being used. Supervised learning requires a credit-assignment model estimating the mapping from neural activity to behavior, and, in a biological organism, this model will inevitably be an imperfect approximation of the ideal mapping, leading to a bias in the direction of the weight updates relative to the true gradient. Reinforcement learning, on the other hand, requires no credit-assignment model and tends to make weight updates following the true gradient direction. We derive a metric to distinguish between learning rules by observing changes in the network activity during learning, given that the mapping from brain to behavior is known by the experimenter. Because brain-machine interface (BMI) experiments allow for precise knowledge of this mapping, we model a cursor-control BMI task using recurrent neural networks, showing that  learning rules can be distinguished in simulated experiments using only observations that a  neuroscience experimenter would plausibly have access to. ",https://api.openreview.net/pdf/4da0750d3dd30072f65ba8153679b0e0d72bd07f.pdf,reinforcement learning;zero_few-shot;metric;llm,https://scholar.google.com/scholar?q=Distinguishing+Learning+Rules+with+Brain+Machine+Interfaces
Learning Chaotic Dynamics in Dissipative Systems,2022,NIPS,"['Zongyi Li', 'Miguel Liu-Schiaffini', 'Nikola Borislavov Kovachki', 'Kamyar Azizzadenesheli', 'Burigede Liu', 'Kaushik Bhattacharya', 'Andrew Stuart', 'Anima Anandkumar']",poster,"['Dissipative Chaotic systems', 'operator learning', 'invariant statistics', 'attractor learning']","Chaotic systems are notoriously challenging to predict because of their sensitivity to perturbations and errors due to time stepping. Despite this unpredictable behavior, for many dissipative systems the statistics of the long term trajectories are governed by an invariant measure supported on a set, known as the global attractor; for many problems this set is finite dimensional, even if the state space is infinite dimensional. For Markovian systems, the statistical properties of long-term trajectories are uniquely determined by the solution operator that maps the evolution of the system over arbitrary positive time increments. In this work, we propose a machine learning framework to learn the underlying solution operator for dissipative chaotic systems, showing that the resulting learned operator accurately captures short-time trajectories and long-time statistical behavior. Using this framework, we are able to predict various statistics of the invariant measure for the turbulent Kolmogorov Flow dynamics with Reynolds numbers up to $5000$.",https://api.openreview.net/pdf/a4df2cdd1acf9563562b356b3f2f3ceea528d1f7.pdf,graph;zero_few-shot;flow;llm,https://scholar.google.com/scholar?q=Learning+Chaotic+Dynamics+in+Dissipative+Systems
Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets,2022,NIPS,"['Zhiying Lu', 'Hongtao Xie', 'Chuanbin Liu', 'Yongdong Zhang']",poster,['vision transformer'],"There still remains an extreme performance gap between Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs) when training from scratch on small datasets, which is concluded to the lack of inductive bias. In this paper, we further consider this problem and point out two weaknesses of ViTs in inductive biases, that is, the spatial relevance and diverse channel representation. First, on spatial aspect, objects are locally compact and relevant, thus fine-grained feature needs to be extracted from a token and its neighbors. While the lack of data hinders ViTs to attend the spatial relevance. Second, on channel aspect, representation exhibits diversity on different channels. But the scarce data can not enable ViTs to learn strong enough representation for accurate recognition. To this end, we propose Dynamic Hybrid Vision Transformer (DHVT) as the solution to enhance the two inductive biases. On spatial aspect, we adopt a hybrid structure, in which convolution is integrated into patch embedding and multi-layer perceptron module, forcing the model to capture the token features as well as their neighboring features. On channel aspect, we introduce a dynamic feature aggregation module in MLP and a brand new ""head token"" design in multi-head self-attention module to help re-calibrate channel representation and make different channel group representation interacts with each other. The fusion of weak channel representation forms a strong enough representation for classification. With this design, we successfully eliminate the performance gap between CNNs and ViTs, and our DHVT achieves a series of state-of-the-art performance with a lightweight model, 85.68% on CIFAR-100 with 22.8M parameters, 82.3% on ImageNet-1K with 24.0M parameters. Code is available at https://github.com/ArieSeirack/DHVT.",https://api.openreview.net/pdf/78a90466b52acb1e4296db80328d7da889d5619a.pdf,graph;zero_few-shot;transformer;representation;llm,https://scholar.google.com/scholar?q=Bridging+the+Gap+Between+Vision+Transformers+and+Convolutional+Neural+Networks+on+Small+Datasets
Adaptively Exploiting d-Separators with Causal Bandits,2022,NIPS,"['Blair Bilodeau', 'Linbo Wang', 'Daniel M. Roy']",poster,"['bandit', 'causal bandit', 'adaptive', 'd-separation', 'online']","Multi-armed bandit problems provide a framework to identify the optimal intervention over a sequence of repeated experiments. Without additional assumptions, minimax optimal performance (measured by cumulative regret) is well-understood. With access to additional observed variables that d-separate the intervention from the outcome (i.e., they are a d-separator), recent ""causal bandit"" algorithms provably incur less regret. However, in practice it is desirable to be agnostic to whether observed variables are a d-separator. Ideally, an algorithm should be adaptive; that is, perform nearly as well as an algorithm with oracle knowledge of the presence or absence of a d-separator. In this work, we formalize and study this notion of adaptivity, and provide a novel algorithm that simultaneously achieves (a) optimal regret when a d-separator is observed, improving on classical minimax algorithms, and (b) significantly smaller regret than recent causal bandit algorithms when the observed variables are not a d-separator. Crucially, our algorithm does not require any oracle knowledge of whether a d-separator is observed. We also generalize this adaptivity to other conditions, such as the front-door criterion.",https://api.openreview.net/pdf/377d66705a7cf80cc0f951939aec1650d5ce1a0b.pdf,adaptive;llm,https://scholar.google.com/scholar?q=Adaptively+Exploiting+d-Separators+with+Causal+Bandits
Rethinking Resolution in the Context of Efficient Video Recognition,2022,NIPS,"['Chuofan Ma', 'Qiushan Guo', 'Yi Jiang', 'Ping Luo', 'Zehuan Yuan', 'XIAOJUAN QI']",poster,"['Efficient Video Recognition', 'Action Recognition']","In this paper, we empirically study how to make the most of low-resolution frames for efficient video recognition. Existing methods mainly focus on developing compact networks or alleviating temporal redundancy of video inputs to increase efficiency, whereas compressing frame resolution has rarely been considered a promising solution. A major concern is the poor recognition accuracy on low-resolution frames. We thus start by analyzing the underlying causes of performance degradation on low-resolution frames. Our key finding is that the major cause of degradation is not information loss in the down-sampling process, but rather the mismatch between network architecture and input scale. Motivated by the success of knowledge distillation (KD), we propose to bridge the gap between network and input size via cross-resolution KD (ResKD). Our work shows that ResKD is a simple but effective method to boost recognition accuracy on low-resolution frames. Without bells and whistles, ResKD considerably surpasses all competitive methods in terms of efficiency and accuracy on four large-scale benchmark datasets, i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V2. In addition, we extensively demonstrate its effectiveness over state-of-the-art architectures, i.e., 3D-CNNs and Video Transformers, and scalability towards super low-resolution frames. The results suggest ResKD can serve as a general inference acceleration method for state-of-the-art video recognition. Our code will be available at https://github.com/CVMI-Lab/ResKD.",https://api.openreview.net/pdf/eb51b66865e8d79d861c705b3923aa350d8dfac0.pdf,zero_few-shot;transformer;inference;distillation;3d;llm,https://scholar.google.com/scholar?q=Rethinking+Resolution+in+the+Context+of+Efficient+Video+Recognition
Feature-Proxy Transformer for Few-Shot Segmentation,2022,NIPS,"['Jian-Wei Zhang', 'Yifan Sun', 'Yi Yang', 'Wei Chen']",poster,"['Few-shot segmentation', 'vision transformer', 'prompt learning']","Few-shot segmentation~(FSS) aims at performing semantic segmentation on novel classes given a few annotated support samples. With a rethink of recent advances, we find that the current FSS framework has deviated far from the supervised segmentation framework: Given the deep features, FSS methods typically use an intricate decoder to perform sophisticated pixel-wise matching, while the supervised segmentation methods use a simple linear classification head. Due to the intricacy of the decoder and its matching pipeline, it is not easy to follow such an FSS framework. This paper revives the straightforward framework of ``feature extractor $+$ linear classification head'' and proposes a novel Feature-Proxy Transformer (FPTrans) method, in which the ``proxy'' is the vector representing a semantic class in the linear classification head. FPTrans has two keypoints for learning discriminative features and representative proxies: 1) To better utilize the limited support samples, the feature extractor makes the query interact with the support features from bottom to top layers using a novel prompting strategy. 2) FPTrans uses multiple local background proxies (instead of a single one) because the background is not homogeneous and may contain some novel foreground regions. These two keypoints are easily integrated into the vision transformer backbone with the prompting mechanism in the transformer. Given the learned features and proxies, FPTrans directly compares their cosine similarity for segmentation. Although the framework is straightforward, we show that FPTrans achieves competitive FSS accuracy on par with state-of-the-art decoder-based methods. ",https://api.openreview.net/pdf/10de4c5fa89c158eff574dffb73397e85f8949ef.pdf,zero_few-shot;transformer;segmentation;llm,https://scholar.google.com/scholar?q=Feature-Proxy+Transformer+for+Few-Shot+Segmentation
Learning Physical Dynamics with Subequivariant Graph Neural Networks,2022,NIPS,"['Jiaqi Han', 'Wenbing Huang', 'Hengbo Ma', 'Jiachen Li', 'Joshua B. Tenenbaum', 'Chuang Gan']",poster,"['physical dynamics', 'graph neural networks', 'symmetry']","Graph Neural Networks (GNNs) have become a prevailing tool for learning physical dynamics. However, they still encounter several challenges: 1) Physical laws abide by symmetry,  which is a vital inductive bias accounting for model generalization and should be incorporated into the model design. Existing simulators either consider insufficient symmetry, or enforce excessive equivariance in practice when symmetry is partially broken by gravity. 2) Objects in the physical world possess diverse shapes, sizes, and properties, which should be appropriately processed by the model. To tackle these difficulties, we propose a novel backbone, called Subequivariant Graph Neural Network, which 1) relaxes equivariance to subequivariance by considering external fields like gravity, where the universal approximation ability holds theoretically; 2) introduces a new subequivariant object-aware message passing for learning physical interactions between multiple objects of various shapes in particle-based representation; 3) operates in a hierarchical fashion, allowing for modeling long-range and complex interactions. Our model achieves on average over 3% enhancement in contact prediction accuracy across 8 scenarios on Physion and 2$\times$ lower rollout MSE on RigidFall compared with state-of-the-art GNN simulators, while exhibiting strong generalization and data efficiency.",https://api.openreview.net/pdf/7b5d6b6ad5aa5a4b5cd07ef9cbcad044c290cf8e.pdf,graph;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Learning+Physical+Dynamics+with+Subequivariant+Graph+Neural+Networks
MCMAE: Masked Convolution Meets Masked Autoencoders,2022,NIPS,"['Peng Gao', 'Teli Ma', 'Hongsheng Li', 'Ziyi Lin', 'Jifeng Dai', 'Yu Qiao']",poster,"['Masked auto-encoders', 'Convolution Neural Networks', 'Vision Transformer']","Vision Transformers (ViT) become widely-adopted architectures for various vision tasks. Masked auto-encoding for feature pretraining and multi-scale hybrid convolution-transformer architectures can further unleash the potentials of ViT, leading to state-of-the-art performances on image classification, detection and semantic segmentation. In this paper, our MCMAE framework demonstrates that multi-scale hybrid convolution-transformer can learn more discriminative representations via the mask auto-encoding scheme. However, directly using the original masking strategy leads to the heavy computational cost and pretraining-finetuning discrepancy. To tackle the issue, we adopt the masked convolution to prevent information leakage in the convolution blocks. A simple block-wise masking strategy is proposed to ensure computational efficiency. We also propose to more directly supervise the multi-scale features of the encoder to boost multi-scale features. Based on our pretrained MCMAE models, MCMAE-Base improves ImageNet-1K finetuning accuracy by 1.4% compared with MAE-Base. On object detection, MCMAE-Base finetuned for only 25 epochs surpasses MAE-Base fined-tuned for 100 epochs by 2.9% box AP and 2.2% mask AP respectively. Code and pretrained models are available at \url{https://github.com/Alpha-VL/ConvMAE}. ",https://api.openreview.net/pdf/dba5bec03530f67ca8174a742f1ef297e3ea6b4b.pdf,graph;transformer;representation;segmentation;llm,https://scholar.google.com/scholar?q=MCMAE:+Masked+Convolution+Meets+Masked+Autoencoders
SAViT: Structure-Aware Vision Transformer Pruning via Collaborative Optimization,2022,NIPS,"['Zheng Chuanyang', 'Zheyang Li', 'Kai Zhang', 'Zhi Yang', 'Wenming Tan', 'Jun Xiao', 'Ye Ren', 'Shiliang Pu']",poster,"['Vision Transformer', 'Pruning', 'Compression']","Vision Transformers (ViTs) yield impressive performance across various vision tasks. However, heavy computation and memory footprint make them inaccessible for edge devices. Previous works apply importance criteria determined independently by each individual component to prune ViTs. Considering that heterogeneous components in ViTs play distinct roles, these approaches lead to suboptimal performance. In this paper, we introduce joint importance, which integrates essential structural-aware interactions between components for the first time, to perform collaborative pruning. Based on the theoretical analysis, we construct a Taylor-based approximation to evaluate the joint importance. This guides pruning toward a more balanced reduction across all components. To further reduce the algorithm complexity, we incorporate the interactions into the optimization function under some mild assumptions. Moreover, the proposed method can be seamlessly applied to various tasks including object detection. Extensive experiments demonstrate the effectiveness of our method. Notably, the proposed approach outperforms the existing state-of-the-art approaches on ImageNet, increasing accuracy by 0.7% over the DeiT-Base baseline while saving 50% FLOPs. On COCO, we are the first to show that 70% FLOPs of FasterRCNN with ViT backbone can be removed with only 0.3% mAP drop. The code is available at https://github.com/hikvision-research/SAViT.",https://api.openreview.net/pdf/cb5ebfa9649db31cf6a4dd957bbbc7863cd0be33.pdf,optimization;transformer;llm,https://scholar.google.com/scholar?q=SAViT:+Structure-Aware+Vision+Transformer+Pruning+via+Collaborative+Optimization
"Don't Roll the Dice, Ask Twice: The Two-Query Distortion of Matching Problems and Beyond",2022,NIPS,"['Georgios Amanatidis', 'Georgios Birmpas', 'Aris Filos-Ratsikas', 'Alexandros A. Voudouris']",poster,"['Distortion', 'Matching', 'Social Choice', 'Query']","In most social choice settings, the participating agents express their preferences over the different alternatives in the form of linear orderings. While this clearly simplifies preference elicitation, it inevitably leads to poor performance with respect to optimizing a cardinal objective, such as the social welfare, since the values of the agents remain virtually unknown. This loss in performance because of lack of information is measured by distortion. A recent array of works put forward the agenda of designing mechanisms that learn the values of the agents for a small number of alternatives via queries, and use this limited extra information to make better-informed decisions, thus improving distortion. Following this agenda, in this work we focus on a class of combinatorial problems that includes most well-known matching problems and several of their generalizations, such as One-Sided Matching, Two-Sided Matching, General Graph Matching, and k-Constrained Resource Allocation. We design two-query mechanisms that achieve the best-possible worst-case distortion in terms of social welfare, and outperform the best-possible expected distortion achieved by randomized ordinal mechanisms.",https://api.openreview.net/pdf/53eabc26fa20b0a684d0112b93c3c39d2a268840.pdf,reinforcement learning;graph;optimization;zero_few-shot;llm,"https://scholar.google.com/scholar?q=Don't+Roll+the+Dice,+Ask+Twice:+The+Two-Query+Distortion+of+Matching+Problems+and+Beyond"
Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models,2022,NIPS,"['Chen Henry Wu', 'Saman Motamed', 'Shaunak Srivastava', 'Fernando De la Torre']",poster,"['generative models', 'energy-based models', 'normalizing flows', 'generative adversarial networks', 'diffusion models', 'amortized inference']","Generative models (e.g., GANs, diffusion models) learn the underlying data distribution in an unsupervised manner. However, many applications of interest require sampling from a particular region of the output space or sampling evenly over a range of characteristics. For efficient sampling in these scenarios, we propose Generative Visual Prompt (PromptGen), a framework for distributional control over pre-trained generative models by incorporating knowledge of other off-the-shelf models. PromptGen defines control as energy-based models (EBMs) and samples images in a feed-forward manner by approximating the EBM with invertible neural networks, avoiding optimization at inference. Our experiments demonstrate how PromptGen can efficiently sample from several unconditional generative models (e.g., StyleGAN2, StyleNeRF, diffusion autoencoder, NVAE) in a controlled or/and de-biased manner using various off-the-shelf models: (1) with the CLIP model as control, PromptGen can sample images guided by text, (2) with image classifiers as control, PromptGen can de-bias generative models across a set of attributes or attribute combinations, and (3) with inverse graphics models as control, PromptGen can sample images of the same identity in different poses. (4) Finally, PromptGen reveals that the CLIP model shows a ""reporting bias"" when used as control, and PromptGen can further de-bias this controlled distribution in an iterative manner. The code is available at https://github.com/ChenWu98/Generative-Visual-Prompt.",https://api.openreview.net/pdf/a661a2f31f6dfc89508deaa6f64534cbcc46f69d.pdf,graph;optimization;vae;generative model;inference;diffusion models;llm,https://scholar.google.com/scholar?q=Generative+Visual+Prompt:+Unifying+Distributional+Control+of+Pre-Trained+Generative+Models
PointTAD: Multi-Label Temporal Action Detection with Learnable Query Points,2022,NIPS,"['Jing Tan', 'Xiaotong Zhao', 'Xintian Shi', 'Bin Kang', 'Limin Wang']",poster,"['multi-label temporal action detection', 'keyframe-based detection', 'query-based detection', 'temporal action detection']","Traditional temporal action detection (TAD) usually handles untrimmed videos with small number of action instances from a single label (e.g., ActivityNet, THUMOS). However, this setting might be unrealistic as different classes of actions often co-occur in practice. In this paper, we focus on the task of multi-label temporal action detection that aims to localize all action instances from a multi-label untrimmed video. Multi-label TAD is more challenging as it requires for fine-grained class discrimination within a single video and precise localization of the co-occurring instances. To mitigate this issue, we extend the sparse query-based detection paradigm from the traditional TAD and propose the multi-label TAD framework of PointTAD. Specifically, our PointTAD introduces a small set of learnable query points to represent the important frames of each action instance. This point-based representation provides a flexible mechanism to localize the discriminative frames at boundaries and as well the important frames inside the action. Moreover, we perform the action decoding process with the Multi-level Interactive Module to capture both point-level and instance-level action semantics. Finally, our PointTAD employs an end-to-end trainable framework simply based on RGB input for easy deployment. We evaluate our proposed method on two popular benchmarks and introduce the new metric of detection-mAP for multi-label TAD. Our model outperforms all previous methods by a large margin under the detection-mAP metric, and also achieves promising results under the segmentation-mAP metric.",https://api.openreview.net/pdf/0be1fa4968f731ce4bebbf7e505f245ccee0bc99.pdf,graph;representation;metric;sparse;active learning;segmentation;llm,https://scholar.google.com/scholar?q=PointTAD:+Multi-Label+Temporal+Action+Detection+with+Learnable+Query+Points
Theory and Approximate Solvers for Branched Optimal Transport with Multiple Sources,2022,NIPS,"['Peter Lippmann', 'Enrique Fita Sanmartín', 'Fred A Hamprecht']",poster,"['Combinatorial Optimization', 'Optimal Transport', 'Irrigation Networks', 'Structured Prediction', 'Steiner Tree Problem', 'Branched Optimal Transport', 'Transportation Networks']","Branched optimal transport (BOT) is a generalization of optimal transport in which transportation costs along an edge are subadditive. This subadditivity models an increase in transport efficiency when shipping mass along the same route, favoring branched transportation networks. We here study the NP-hard optimization of BOT networks connecting a finite number of sources and sinks in $\mathbb{R}^2$. First, we show how to efficiently find the best geometry of a BOT network for many sources and sinks, given a topology. Second, we argue that a topology with more than three edges meeting at a branching point is never optimal. Third, we show that the results obtained for the Euclidean plane generalize directly to optimal transportation networks on two-dimensional Riemannian manifolds. Finally, we present a simple but effective approximate BOT solver combining geometric optimization with a combinatorial optimization of the network topology.",https://api.openreview.net/pdf/969db01686afc3c7eafa6f8efc312b2a60bd0181.pdf,optimization;metric;llm,https://scholar.google.com/scholar?q=Theory+and+Approximate+Solvers+for+Branched+Optimal+Transport+with+Multiple+Sources
Bringing Image Scene Structure to Video via Frame-Clip Consistency of Object Tokens,2022,NIPS,"['Elad Ben Avraham', 'Roei Herzig', 'Karttikeya Mangalam', 'Amir Bar', 'Anna Rohrbach', 'Leonid Karlinsky', 'Trevor Darrell', 'Amir Globerson']",poster,"['video models', 'object centric models', 'image-video']","Recent action recognition models have achieved impressive results by integrating objects, their locations and interactions. However, obtaining dense structured annotations for each frame is tedious and time-consuming, making these methods expensive to train and less scalable. At the same time, if a small set of annotated images is available, either within or outside the domain of interest, how could we leverage these for a video downstream task? We propose a learning framework StructureViT (SViT for short), which demonstrates how utilizing the structure of a small number of images only available during training can improve a video model. SViT relies on two key insights. First, as both images and videos contain structured information, we enrich a transformer model with a set of object tokens that can be used across images and videos. Second, the scene representations of individual frames in video should ``align'' with those of still images. This is achieved via a Frame-Clip Consistency loss, which ensures the flow of structured information between images and videos. We explore a particular instantiation of scene structure, namely a Hand-Object Graph, consisting of hands and objects with their locations as nodes, and physical relations of contact/no-contact as edges. SViT shows strong performance improvements on multiple video understanding tasks and datasets, including the first place in the Ego4D CVPR'22 Point of No Return Temporal Localization Challenge. For code and pretrained models, visit the project page at https://eladb3.github.io/SViT/.",https://api.openreview.net/pdf/807b0df597eae30eaa48c14472cac201eb14bbc3.pdf,graph;zero_few-shot;transformer;representation;flow;llm,https://scholar.google.com/scholar?q=Bringing+Image+Scene+Structure+to+Video+via+Frame-Clip+Consistency+of+Object+Tokens
TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition,2022,NIPS,"['Yongwei Chen', 'Rui Chen', 'Jiabao Lei', 'Yabin Zhang', 'Kui Jia']",poster,"['3D content creation', 'CLIP', 'Text-to-3D']","Creation of 3D content by stylization is a promising yet challenging problem in computer vision and graphics research. In this work, we focus on stylizing photorealistic appearance renderings of a given surface mesh of arbitrary topology. Motivated by the recent surge of cross-modal supervision of the Contrastive Language-Image Pre-training (CLIP) model, we propose TANGO, which transfers the appearance style of a given 3D shape according to a text prompt in a photorealistic manner. Technically, we propose to disentangle the appearance style as the spatially varying bidirectional reflectance distribution function, the local geometric variation, and the lighting condition, which are jointly optimized, via supervision of the CLIP loss, by a spherical Gaussians based differentiable renderer. As such, TANGO enables photorealistic 3D style transfer by automatically predicting reflectance effects even for bare, low-quality meshes, without training on a task-specific dataset. Extensive experiments show that TANGO outperforms existing methods of text-driven 3D style transfer in terms of photorealistic quality, consistency of 3D geometry, and robustness when stylizing low-quality meshes. Our codes and results are available at our project webpage https://cyw-3d.github.io/tango/.",https://api.openreview.net/pdf/1b4bba1a76796e0b775ed188ee7b300cb980666f.pdf,graph;zero_few-shot;contrastive learning;metric;transfer learning;multimodal;3d;llm,https://scholar.google.com/scholar?q=TANGO:+Text-driven+Photorealistic+and+Robust+3D+Stylization+via+Lighting+Decomposition
Semi-supervised Vision Transformers at Scale,2022,NIPS,"['Zhaowei Cai', 'Avinash Ravichandran', 'Paolo Favaro', 'Manchen Wang', 'Davide Modolo', 'Rahul Bhotika', 'Zhuowen Tu', 'Stefano Soatto']",poster,[],"We study semi-supervised learning (SSL) for vision transformers (ViT), an under-explored topic despite the wide adoption of the ViT architectures to different tasks. To tackle this problem, we use a SSL pipeline, consisting of first un/self-supervised pre-training, followed by supervised fine-tuning, and finally semi-supervised fine-tuning. At the semi-supervised fine-tuning stage, we adopt an exponential moving average (EMA)-Teacher framework instead of the popular FixMatch, since the former is more stable and delivers higher accuracy for semi-supervised vision transformers. In addition, we propose a probabilistic pseudo mixup mechanism to interpolate unlabeled samples and their pseudo labels for improved regularization, which is important for training ViTs with weak inductive bias. Our proposed method, dubbed Semi-ViT, achieves comparable or better performance than the CNN counterparts in the semi-supervised classification setting. Semi-ViT also enjoys the scalability benefits of ViTs that can be readily scaled up to large-size models with increasing accuracy. For example, Semi-ViT-Huge achieves an impressive 80\% top-1 accuracy on ImageNet using only 1\% labels, which is comparable with Inception-v4 using 100\% ImageNet labels. The code is available at https://github.com/amazon-science/semi-vit.",https://api.openreview.net/pdf/1e049504790806bd91a01d43ab858ee892e36eb8.pdf,zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Semi-supervised+Vision+Transformers+at+Scale
Deep Model Reassembly,2022,NIPS,"['Xingyi Yang', 'Zhou Daquan', 'Songhua Liu', 'Jingwen Ye', 'Xinchao Wang']",poster,"['Transfer Learning from Model Zoo', 'Neural Network Reassembly', 'Representation Similarity']","In this paper, we explore a novel knowledge-transfer task, termed as Deep  Model Reassembly (DeRy), for general-purpose model reuse.
Given a collection of heterogeneous models pre-trained from distinct sources and with diverse architectures, the goal of DeRy, as its name implies, is to first dissect each model into distinctive building blocks, and then selectively reassemble the derived blocks to produce customized networks under both the hardware resource and performance constraints. Such ambitious nature of DeRy inevitably imposes significant challenges, including, in the first place, the feasibility of its solution. We strive to showcase that, through a dedicated paradigm proposed in this paper, DeRy can be made not only possibly but practically efficiently. Specifically, we conduct the partitions of all pre-trained networks jointly via a cover set optimization, and derive  a number of equivalence set, within each of which the network blocks are treated as functionally equivalent and hence interchangeable. The equivalence sets learned in this way, in turn, enable  picking and assembling blocks to customize networks subject to certain constraints, which is achieved via solving an integer program backed up with a training-free proxy to estimate the task performance. The reassembled models give rise to gratifying performances with the user-specified constraints satisfied. We demonstrate that on ImageNet, the best reassemble model achieves 78.6% top-1 accuracy without fine-tuning, which could be further elevated to 83.2% with end-to-end fine-tuning. Our code is available at https://github.com/Adamdad/DeRy.",https://api.openreview.net/pdf/bcb2dd45226fabd5f594c17a003e8088e75570c4.pdf,optimization;transfer learning;llm,https://scholar.google.com/scholar?q=Deep+Model+Reassembly
P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting,2022,NIPS,"['Ziyi Wang', 'Xumin Yu', 'Yongming Rao', 'Jie Zhou', 'Jiwen Lu']",poster,[],"Nowadays, pre-training big models on large-scale datasets has become a crucial topic in deep learning. The pre-trained models with high representation ability and transferability achieve a great success and dominate many downstream tasks in natural language processing and 2D vision. However, it is non-trivial to promote such a pretraining-tuning paradigm to the 3D vision, given the limited training data that are relatively inconvenient to collect. In this paper, we provide a new perspective of leveraging pre-trained 2D knowledge in 3D domain to tackle this problem, tuning pre-trained image models with the novel Point-to-Pixel prompting for point cloud analysis at a minor parameter cost. Following the principle of prompting engineering, we transform point clouds into colorful images with geometry-preserved projection and geometry-aware coloring to adapt to pre-trained image models, whose weights are kept frozen during the end-to-end optimization of point cloud analysis tasks. We conduct extensive experiments to demonstrate that cooperating with our proposed Point-to-Pixel Prompting, better pre-trained image model will lead to consistently better performance in 3D vision. Enjoying prosperous development from image pre-training field, our method attains 89.3% accuracy on the hardest setting of ScanObjectNN, surpassing conventional point cloud models with much fewer trainable parameters. Our framework also exhibits very competitive performance on ModelNet classification and ShapeNet Part Segmentation. Code is available at https://github.com/wangzy22/P2P.",https://api.openreview.net/pdf/d611a6b66c3d54a565ba6eff1547e70b19f53b3c.pdf,graph;optimization;zero_few-shot;representation;transfer learning;segmentation;3d;llm,https://scholar.google.com/scholar?q=P2P:+Tuning+Pre-trained+Image+Models+for+Point+Cloud+Analysis+with+Point-to-Pixel+Prompting
Online Training Through Time for Spiking Neural Networks,2022,NIPS,"['Mingqing Xiao', 'Qingyan Meng', 'Zongpeng Zhang', 'Di He', 'Zhouchen Lin']",poster,"['spiking neural networks', 'online training through time', 'neuromorphic computing']","Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models. Recent progress in training methods has enabled successful deep SNNs on large-scale tasks with low latency. Particularly, backpropagation through time (BPTT) with surrogate gradients (SG) is popularly used to enable models to achieve high performance in a very small number of time steps. However, it is at the cost of large memory consumption for training, lack of theoretical clarity for optimization, and inconsistency with the online property of biological learning rules and rules on neuromorphic hardware. Other works connect the spike representations of SNNs with equivalent artificial neural network formulation and train SNNs by gradients from equivalent mappings to ensure descent directions. But they fail to achieve low latency and are also not online. In this work, we propose online training through time (OTTT) for SNNs, which is derived from BPTT to enable forward-in-time learning by tracking presynaptic activities and leveraging instantaneous loss and gradients. Meanwhile, we theoretically analyze and prove that the gradients of OTTT can provide a similar descent direction for optimization as gradients from equivalent mapping between spike representations under both feedforward and recurrent conditions. OTTT only requires constant training memory costs agnostic to time steps, avoiding the significant memory costs of BPTT for GPU training. Furthermore, the update rule of OTTT is in the form of three-factor Hebbian learning, which could pave a path for online on-chip learning. With OTTT, it is the first time that the two mainstream supervised SNN training methods, BPTT with SG and spike representation-based training, are connected, and meanwhile it is in a biologically plausible form. Experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR10-DVS demonstrate the superior performance of our method on large-scale static and neuromorphic datasets in a small number of time steps. Our code is available at https://github.com/pkuxmq/OTTT-SNN.",https://api.openreview.net/pdf/be9dc1049c87cf72c8c3ab3e37731908353114f5.pdf,graph;optimization;zero_few-shot;representation;online learning;llm,https://scholar.google.com/scholar?q=Online+Training+Through+Time+for+Spiking+Neural+Networks
Peripheral Vision Transformer,2022,NIPS,"['Juhong Min', 'Yucheng Zhao', 'Chong Luo', 'Minsu Cho']",poster,"['Vision transformers', 'Peripheral vision', 'Image recognition', 'Image classification', 'Inductive bias']","Human vision possesses a special type of visual processing systems called peripheral vision. Partitioning the entire visual field into multiple contour regions based on the distance to the center of our gaze, the peripheral vision provides us the ability to perceive various visual features at different regions. In this work, we take a biologically inspired approach and explore to model peripheral vision in deep neural networks for visual recognition. We propose to incorporate peripheral position encoding to the multi-head self-attention layers to let the network learn to partition the visual field into diverse peripheral regions given training data. We evaluate the proposed network, dubbed PerViT, on ImageNet-1K and systematically investigate the inner workings of the model for machine perception, showing that the network learns to perceive visual data similarly to the way that human vision does. The performance improvements in image classification over the baselines across different model sizes demonstrate the efficacy of the proposed method.",https://api.openreview.net/pdf/236c79f4a34220db01255afe0d645b363cd6c5ce.pdf,transformer;llm,https://scholar.google.com/scholar?q=Peripheral+Vision+Transformer
Multi-dataset Training of Transformers for Robust Action Recognition,2022,NIPS,"['Junwei Liang', 'Enwei Zhang', 'Jun Zhang', 'Chunhua Shen']",poster,"['Action Recognition', 'Vision Transformers', 'Multi-task learning', 'Multi-dataset learning', 'Robust Representation']","We study the task of robust feature representations, aiming to generalize well on multiple datasets for action recognition. We build our method on Transformers for its efficacy. Although we have witnessed great progress for video action recognition in the past decade, it remains challenging yet valuable how to train a single model that can perform well across multiple datasets. Here, we propose a novel multi-dataset training paradigm, MultiTrain, with the design of two new loss terms, namely informative loss and projection loss, aiming to
learn robust representations for action recognition. In particular, the informative loss maximizes the expressiveness of the feature embedding while the projection loss for each dataset mines the intrinsic relations between classes across datasets. We verify the effectiveness of our method on five challenging datasets, Kinetics-
400, Kinetics-700, Moments-in-Time, Activitynet and Something-something-v2 datasets. Extensive experimental results show that our method can consistently improve state-of-the-art performance. Code and models are released.",https://api.openreview.net/pdf/cd8d0a789af3890fb8587a213b4e897542db1c5c.pdf,graph;transformer;representation;llm,https://scholar.google.com/scholar?q=Multi-dataset+Training+of+Transformers+for+Robust+Action+Recognition
Self-Supervised Learning via Maximum Entropy Coding,2022,NIPS,"['Xin Liu', 'Zhongdao Wang', 'Ya-Li Li', 'Shengjin Wang']",poster,[],"A mainstream type of current self-supervised learning methods pursues a general-purpose representation that can be well transferred to downstream tasks, typically by optimizing on a given pretext task such as instance discrimination. In this work, we argue that existing pretext tasks inevitably introduce biases into the learned representation, which in turn leads to biased transfer performance on various downstream tasks. To cope with this issue, we propose Maximum Entropy Coding (MEC), a more principled objective that explicitly optimizes on the structure of the representation, so that the learned representation is less biased and thus generalizes better to unseen downstream tasks. Inspired by the principle of maximum entropy in information theory, we hypothesize that a generalizable representation should be the one that admits the maximum entropy among all plausible representations. To make the objective end-to-end trainable, we propose to leverage the minimal coding length in lossy data coding as a computationally tractable surrogate for the entropy, and further derive a scalable reformulation of the objective that allows fast computation. Extensive experiments demonstrate that MEC learns a more generalizable representation than previous methods based on specific pretext tasks. It achieves state-of-the-art performance consistently on various downstream tasks, including not only ImageNet linear probe, but also semi-supervised classification, object detection, instance segmentation, and object tracking. Interestingly, we show that existing batch-wise and feature-wise self-supervised objectives could be seen equivalent to low-order approximations of MEC. Code and pre-trained models are available at https://github.com/xinliu20/MEC.",https://api.openreview.net/pdf/5f0ae88f79c1c310c294db7ee3872c27bd17367a.pdf,zero_few-shot;representation;transfer learning;segmentation;llm,https://scholar.google.com/scholar?q=Self-Supervised+Learning+via+Maximum+Entropy+Coding
Sequencer: Deep LSTM for Image Classification,2022,NIPS,"['Yuki Tatsunami', 'Masato Taki']",poster,"['computer vision', 'image classification', 'network architecture', 'long short-term memory']","In recent computer vision research, the advent of the Vision Transformer (ViT) has rapidly revolutionized various architectural design efforts: ViT achieved state-of-the-art image classification performance using self-attention found in natural language processing, and MLP-Mixer achieved competitive performance using simple multi-layer perceptrons. In contrast, several studies have also suggested that carefully redesigned convolutional neural networks (CNNs) can achieve advanced performance comparable to ViT without resorting to these new ideas. Against this background, there is growing interest in what inductive bias is suitable for computer vision. Here we propose Sequencer, a novel and competitive architecture alternative to ViT that provides a new perspective on these issues. Unlike ViTs, Sequencer models long-range dependencies using LSTMs rather than self-attention layers. We also propose a two-dimensional version of Sequencer module, where an LSTM is decomposed into vertical and horizontal LSTMs to enhance performance. Despite its simplicity, several experiments demonstrate that Sequencer performs impressively well: Sequencer2D-L, with 54M parameters, realizes 84.6% top-1 accuracy on only ImageNet-1K. Not only that, we show that it has good transferability and the robust resolution adaptability on double resolution-band. solution-band. Our source code is available at https://github.com/okojoalg/sequencer.",https://api.openreview.net/pdf/9f8b1a9004240dd04bc6217cf47a81ee04595050.pdf,transformer;transfer learning;llm,https://scholar.google.com/scholar?q=Sequencer:+Deep+LSTM+for+Image+Classification
Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning,2022,NIPS,"['Xiang Chen', 'Lei Li', 'Ningyu Zhang', 'Xiaozhuan Liang', 'Shumin Deng', 'Chuanqi Tan', 'Fei Huang', 'Luo Si', 'Huajun Chen']",poster,"['language model', 'retrieval', 'prompt-tuning', 'memorization']","Prompt learning approaches have made waves in natural language processing by inducing better few-shot performance while they still follow a parametric-based learning paradigm; the oblivion and rote memorization problems in learning may encounter unstable generalization issues. Specifically, vanilla prompt learning may struggle to utilize atypical instances by rote during fully-supervised training or overfit shallow patterns with low-shot data. To alleviate such limitations, we develop RetroPrompt with the motivation of decoupling knowledge from memorization to help the model strike a balance between generalization and memorization. In contrast with vanilla prompt learning, RetroPrompt constructs an open-book knowledge-store from training instances and implements a retrieval mechanism during the process of input, training and inference, thus equipping the model with the ability to retrieve related contexts from the training corpus as cues for enhancement. Extensive experiments demonstrate that RetroPrompt can obtain better performance in both few-shot and zero-shot settings. Besides, we further illustrate that our proposed RetroPrompt can yield better generalization abilities with new datasets. Detailed analysis of memorization indeed reveals RetroPrompt can reduce the reliance of language models on memorization; thus, improving generalization for downstream tasks. Code is available in https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.",https://api.openreview.net/pdf/51607a6935c890a37746121c01b08ed50fd7b88d.pdf,zero_few-shot;inference;metric;augmentation;llm,https://scholar.google.com/scholar?q=Decoupling+Knowledge+from+Memorization:+Retrieval-augmented+Prompt+Learning
Obj2Seq: Formatting Objects as Sequences with Class Prompt for Visual Tasks,2022,NIPS,"['Zhiyang Chen', 'Yousong Zhu', 'Zhaowen Li', 'Fan Yang', 'Wei Li', 'Haixin Wang', 'Chaoyang Zhao', 'Liwei Wu', 'Rui Zhao', 'Jinqiao Wang', 'Ming Tang']",poster,"['transformer', 'general visual framework', 'sequence prediction', 'multi-task']","Visual tasks vary a lot in their output formats and concerned contents, therefore it is hard to process them with an identical structure. One main obstacle lies in the high-dimensional outputs in object-level visual tasks. In this paper, we propose an object-centric vision framework, Obj2Seq. Obj2Seq takes objects as basic units, and regards most object-level visual tasks as sequence generation problems of objects. Therefore, these visual tasks can be decoupled into two steps. First recognize objects of given categories, and then generate a sequence for each of these objects. The definition of the output sequences varies for different tasks, and the model is supervised by matching these sequences with ground-truth targets. Obj2Seq is able to flexibly determine input categories to satisfy customized requirements, and be easily extended to different visual tasks. When experimenting on MS COCO, Obj2Seq achieves 45.7% AP on object detection, 89.0% AP on multi-label classification and 65.0% AP on human pose estimation. These results demonstrate its potential to be generally applied to different visual tasks. Code has been made available at: https://github.com/CASIA-IVA-Lab/Obj2Seq.",https://api.openreview.net/pdf/e945f44148e8c09b199fe4c1d4565e59b35e973f.pdf,generative model;llm,https://scholar.google.com/scholar?q=Obj2Seq:+Formatting+Objects+as+Sequences+with+Class+Prompt+for+Visual+Tasks
Riemannian Neural SDE: Learning Stochastic Representations on Manifolds,2022,NIPS,"['Sung Woo Park', 'Hyomin Kim', 'Kyungjae Lee', 'Junseok Kwon']",poster,"['Stochastic representation on Manifolds', 'Riemannian neural stochastic differential equation']","In recent years, the neural stochastic differential equation (NSDE) has gained attention for modeling stochastic representations with great success in various types of applications. However, it typically loses expressivity when the data representation is manifold-valued. To address this issue, we suggest a principled method for expressing the stochastic representation with the Riemannian neural SDE (RNSDE), which extends the conventional Euclidean NSDE. Empirical results for various tasks demonstrate that the proposed method significantly outperforms baseline methods.",https://api.openreview.net/pdf/50c1c58a7520fe999568bbcce8e8d12e87e53d4f.pdf,transformer;representation;llm,https://scholar.google.com/scholar?q=Riemannian+Neural+SDE:+Learning+Stochastic+Representations+on+Manifolds
Learning Latent Seasonal-Trend Representations for Time Series Forecasting,2022,NIPS,"['Zhiyuan Wang', 'Xovee Xu', 'Weifeng Zhang', 'Goce Trajcevski', 'Ting Zhong', 'Fan Zhou']",poster,[],"Forecasting complex time series is ubiquitous and vital in a range of applications but challenging. Recent advances endeavor to achieve progress by incorporating various deep learning techniques (e.g., RNN and Transformer) into sequential models. However, clear patterns are still hard to extract since time series are often composed of several intricately entangled components. Motivated by the success of disentangled variational autoencoder in computer vision and classical time series decomposition, we plan to infer a couple of representations that depict seasonal and trend components of time series. To achieve this goal, we propose LaST, which, based on variational inference, aims to disentangle the seasonal-trend representations in the latent space. Furthermore, LaST supervises and disassociates representations from the perspectives of themselves and input reconstruction, and introduces a series of auxiliary objectives. Extensive experiments prove that LaST achieves state-of-the-art performance on time series forecasting task against the most advanced representation learning and end-to-end forecasting models. For reproducibility, our implementation is publicly available on Github.",https://api.openreview.net/pdf/9729806ac641dc9f4a5de3667198b5019a797baa.pdf,graph;transformer;representation;inference;llm,https://scholar.google.com/scholar?q=Learning+Latent+Seasonal-Trend+Representations+for+Time+Series+Forecasting
DropCov: A Simple yet Effective Method for Improving Deep Architectures,2022,NIPS,"['Qilong Wang', 'Mingze Gao', 'Zhaolin Zhang', 'Jiangtao Xie', 'Peihua Li', 'Qinghua Hu']",poster,"['Global covariance pooling', 'post-normalization', 'adaptive channel dropout', 'deep convolutional neural networks', 'vision transformers']","Previous works show global covariance pooling (GCP) has great potential to improve deep architectures especially on visual recognition tasks, where post-normalization of GCP plays a very important role in final performance. Although several post-normalization strategies have been studied, these methods pay more close attention to effect of normalization on covariance representations rather than the whole GCP networks, and their effectiveness requires further understanding. Meanwhile, existing effective post-normalization strategies (e.g., matrix power normalization) usually suffer from high computational complexity (e.g., $O(d^{3})$ for $d$-dimensional inputs). To handle above issues, this work first analyzes the effect of post-normalization from the perspective of training GCP networks. Particularly, we for the first time show that \textit{effective post-normalization can make a good trade-off between representation decorrelation and information preservation for GCP, which are crucial to alleviate over-fitting and increase representation ability of deep GCP networks, respectively}. Based on this finding, we can improve existing post-normalization methods with some small modifications, providing further support to our observation. Furthermore, this finding encourages us to propose a novel pre-normalization method for GCP (namely DropCov), which develops an adaptive channel dropout on features right before GCP, aiming to reach trade-off between representation decorrelation and information preservation in a more efficient way. Our DropCov only has a linear complexity of $O(d)$, while being free for inference. Extensive experiments on various benchmarks (i.e., ImageNet-1K, ImageNet-C, ImageNet-A, Stylized-ImageNet, and iNat2017) show our DropCov is superior to the counterparts in terms of efficiency and effectiveness, and provides a simple yet effective method to improve performance of deep architectures involving both deep convolutional neural networks (CNNs) and vision transformers (ViTs).",https://api.openreview.net/pdf/531ee354893420212762618733271da3f6c0f299.pdf,transformer;representation;adaptive;inference;llm,https://scholar.google.com/scholar?q=DropCov:+A+Simple+yet+Effective+Method+for+Improving+Deep+Architectures
Posterior and Computational Uncertainty in Gaussian Processes,2022,NIPS,"['Jonathan Wenger', 'Geoff Pleiss', 'Marvin Pförtner', 'Philipp Hennig', 'John Patrick Cunningham']",poster,"['Gaussian processes', 'computational uncertainty', 'numerical methods', 'probabilistic numerics', 'probabilistic linear solvers']","Gaussian processes scale prohibitively with the size of the dataset. In response, many approximation methods have been developed, which inevitably introduce approximation error. This additional source of uncertainty, due to limited computation, is entirely ignored when using the approximate posterior. Therefore in practice, GP models are often as much about the approximation method as they are about the data. Here, we develop a new class of methods that provides consistent estimation of the combined uncertainty arising from both the finite number of data observed and the finite amount of computation expended. The most common GP approximations map to an instance in this class, such as methods based on the Cholesky factorization, conjugate gradients, and inducing points. For any method in this class, we prove (i) convergence of its posterior mean in the associated RKHS, (ii) decomposability of its combined posterior covariance into mathematical and computational covariances, and (iii) that the combined variance is a tight worst-case bound for the squared error between the method's posterior mean and the latent function. Finally, we empirically demonstrate the consequences of ignoring computational uncertainty and show how implicitly modeling it improves generalization performance on benchmark datasets.",https://api.openreview.net/pdf/8fb82faf34781812b11a6a533384c837d44552a4.pdf,llm,https://scholar.google.com/scholar?q=Posterior+and+Computational+Uncertainty+in+Gaussian+Processes
Effective Backdoor Defense by Exploiting Sensitivity of Poisoned Samples,2022,NIPS,"['Weixin Chen', 'Baoyuan Wu', 'Haoqian Wang']",poster,"['backdoor defense', 'backdoor learning', 'trustworthy AI', 'AI security']","Poisoning-based backdoor attacks are serious threat for training deep models on data from untrustworthy sources. Given a backdoored model, we observe that the feature representations of poisoned samples with trigger are more sensitive to transformations than those of clean samples. It inspires us to design a simple sensitivity metric, called feature consistency towards transformations (FCT), to distinguish poisoned samples from clean samples in the untrustworthy training set. Moreover, we propose two effective backdoor defense methods. Built upon a sample-distinguishment module utilizing the FCT metric, the first method trains a secure model from scratch using a two-stage secure training module. And the second method removes backdoor from a backdoored model with a backdoor removal module which alternatively unlearns the distinguished poisoned samples and relearns the distinguished clean samples. Extensive results on three benchmark datasets demonstrate the superior defense performance against eight types of backdoor attacks, to state-of-the-art backdoor defenses. Codes are available at: https://github.com/SCLBD/Effective_backdoor_defense.",https://api.openreview.net/pdf/82397e777241ae042276e8493ca8e5d228821582.pdf,representation;metric;llm,https://scholar.google.com/scholar?q=Effective+Backdoor+Defense+by+Exploiting+Sensitivity+of+Poisoned+Samples
Asymptotically Unbiased Instance-wise Regularized Partial AUC Optimization: Theory and Algorithm,2022,NIPS,"['HuiYang Shao', 'Qianqian Xu', 'Zhiyong Yang', 'Shilong Bao', 'Qingming Huang']",poster,"['partial AUC', 'optimization', 'minimax']","    The Partial Area Under the ROC Curve (PAUC), typically including One-way Partial AUC (OPAUC) and Two-way Partial AUC (TPAUC), measures the average performance of a binary classifier within a specific false positive rate and/or true positive rate interval, which is a widely adopted measure when decision constraints must be considered. Consequently, PAUC optimization has naturally attracted increasing attention in the machine learning community within the last few years. Nonetheless, most of the existing methods could only optimize PAUC approximately, leading to inevitable biases that are not controllable. Fortunately, a recent work presents an unbiased formulation of the PAUC optimization problem via distributional robust optimization. However, it is based on the pair-wise formulation of AUC, which suffers from the limited scalability w.r.t. sample size and a slow convergence rate, especially for TPAUC. To address this issue, we present a simpler reformulation of the problem in an asymptotically unbiased and instance-wise manner. For both OPAUC and TPAUC, we come to a nonconvex strongly concave min-max regularized problem of instance-wise functions. On top of this, we employ an efficient solver that enjoys a linear per-iteration computational complexity w.r.t. the sample size and a time-complexity of $O(\epsilon^{-1/3})$ to reach a $\epsilon$ stationary point. Furthermore, we find that the min-max reformulation also facilitates the theoretical analysis of generalization error as a byproduct. Compared with the existing results, we present new error bounds that are much easier to prove and could deal with hypotheses with real-valued outputs. Finally, extensive experiments on several benchmark datasets demonstrate the effectiveness of our method.",https://api.openreview.net/pdf/7935d2c44cd9e8a5d48a9d9669aa6e75cf040988.pdf,optimization;zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Asymptotically+Unbiased+Instance-wise+Regularized+Partial+AUC+Optimization:+Theory+and+Algorithm
Semi-Supervised Semantic Segmentation via Gentle Teaching Assistant,2022,NIPS,"['Ying Jin', 'Jiaqi Wang', 'Dahua Lin']",poster,"['gentle teaching assistant', 'semi-supervised semantic segmentation']","Semi-Supervised Semantic Segmentation aims at training the segmentation model with limited labeled data and a large amount of unlabeled data. To effectively leverage the unlabeled data, pseudo labeling, along with the teacher-student framework, is widely adopted in semi-supervised semantic segmentation. Though proved to be effective, this paradigm suffers from incorrect pseudo labels which inevitably exist and are taken as auxiliary training data. To alleviate the negative impact of incorrect pseudo labels, we delve into the current Semi-Supervised Semantic Segmentation frameworks. We argue that the unlabeled data with pseudo labels can facilitate the learning of representative features in the feature extractor, but it is unreliable to supervise the mask predictor. Motivated by this consideration, we propose a novel framework, Gentle Teaching Assistant (GTA-Seg) to disentangle the effects of pseudo labels on feature extractor and mask predictor of the student model. Specifically, in addition to the original teacher-student framework, our method introduces a teaching assistant network which directly learns from pseudo labels generated by the teacher network. The gentle teaching assistant (GTA) is coined gentle since it only transfers the beneficial feature representation knowledge in the feature extractor to the student model in an Exponential Moving Average (EMA) manner, protecting the student model from the negative influences caused by unreliable pseudo labels in the mask predictor. The student model is also supervised by reliable labeled data to train an accurate mask predictor, further facilitating feature representation. Extensive experiment results on benchmark datasets validate that our method shows competitive performance against previous methods. We promise to release our code towards reproducibility. ",https://api.openreview.net/pdf/9748aeacae2ca1f93a147f13a7427465c47100aa.pdf,graph;representation;transfer learning;segmentation;llm,https://scholar.google.com/scholar?q=Semi-Supervised+Semantic+Segmentation+via+Gentle+Teaching+Assistant
Back Razor: Memory-Efficient Transfer Learning by Self-Sparsified Backpropagation,2022,NIPS,"['Ziyu Jiang', 'Xuxi Chen', 'Xueqin Huang', 'Xianzhi Du', 'Denny Zhou', 'Zhangyang Wang']",poster,"['Memory Efficiency', 'Sparsity', 'Prune']","Transfer learning from the model trained on large datasets to customized downstream tasks has been widely used as the pre-trained model can greatly boost the generalizability. However, the increasing sizes of pre-trained models also lead to a prohibitively large memory footprints for downstream transferring, making them unaffordable for personal devices. Previous work recognizes the bottleneck of the footprint to be the activation, and hence proposes various solutions such as injecting specific lite modules. In this work, we present a novel memory-efficient transfer framework called Back Razor, that can be plug-and-play applied to any pre-trained network without changing its architecture. The key idea of Back Razor is asymmetric sparsifying: pruning the activation stored for back-propagation, while keeping the forward activation dense. It is based on the observation that the stored activation, that dominates the memory footprint, is only needed for backpropagation. Such asymmetric pruning avoids affecting the precision of forward computation, thus making more aggressive pruning possible. Furthermore, we conduct the theoretical analysis for the convergence rate of Back Razor, showing that under mild conditions, our method retains the similar convergence rate as vanilla SGD. Extensive transfer learning experiments on both Convolutional Neural Networks and Vision Transformers with classification, dense prediction, and language modeling tasks show that Back Razor could yield up to 97% sparsity, saving 9.2x memory usage, without losing accuracy. The code is available at: https://github.com/VITA-Group/BackRazor_Neurips22.",https://api.openreview.net/pdf/ef41acb3cfbad1054855b5b98341ace6a961ffb5.pdf,graph;transformer;metric;transfer learning;llm,https://scholar.google.com/scholar?q=Back+Razor:+Memory-Efficient+Transfer+Learning+by+Self-Sparsified+Backpropagation
SegViT: Semantic Segmentation with Plain Vision Transformers,2022,NIPS,"['Bowen Zhang', 'Zhi Tian', 'Quan Tang', 'Xiangxiang Chu', 'Xiaolin Wei', 'Chunhua Shen', 'Yifan liu']",poster,"['Semantic segmentation', 'Transformer', 'Efficient']","We explore the capability of plain Vision Transformers (ViTs) for semantic segmentation and propose the SegViT. Previous ViT-based segmentation networks usually learn a pixel-level representation from the output of the ViT. Differently, we make use of the fundamental component—attention mechanism, to generate masks for semantic segmentation. Specifically, we propose the Attention-to-Mask (ATM) module, in which the similarity maps between a set of learnable class tokens and the spatial feature maps are transferred to the segmentation masks. Experiments show that our proposed SegViT using the ATM module outperforms its counterparts using the plain ViT backbone on the ADE20K dataset and achieves new state-of-the-art performance on COCO-Stuff-10K and PASCAL-Context datasets. Furthermore, to reduce the computational cost of the ViT backbone, we propose query-based down-sampling (QD) and query-based up-sampling (QU) to build a Shrunk structure. With our Shrunk structure, the model can save up to 40% computations while maintaining competitive performance.",https://api.openreview.net/pdf/a9a0397ee518f27a02ce415360fe2e7c628e30e0.pdf,transformer;representation;transfer learning;segmentation;llm,https://scholar.google.com/scholar?q=SegViT:+Semantic+Segmentation+with+Plain+Vision+Transformers
VITA: Video Instance Segmentation via Object Token Association,2022,NIPS,"['Miran Heo', 'Sukjun Hwang', 'Seoung Wug Oh', 'Joon-Young Lee', 'Seon Joo Kim']",poster,"['video', 'instance segmentation', 'video instance segmentation', 'tracking', 'transformers']","We introduce a novel paradigm for offline Video Instance Segmentation (VIS), based on the hypothesis that explicit object-oriented information can be a strong clue for understanding the context of the entire sequence. To this end, we propose VITA, a simple structure built on top of an off-the-shelf Transformer-based image instance segmentation model. Specifically, we use an image object detector as a means of distilling object-specific contexts into object tokens. VITA accomplishes video-level understanding by associating frame-level object tokens without using spatio-temporal backbone features. By effectively building relationships between objects using the condensed information, VITA achieves the state-of-the-art on VIS benchmarks with a ResNet-50 backbone: 49.8 AP, 45.7 AP on YouTube-VIS 2019 & 2021, and 19.6 AP on OVIS. Moreover, thanks to its object token-based structure that is disjoint from the backbone features, VITA shows several practical advantages that previous offline VIS methods have not explored - handling long and high-resolution videos with a common GPU, and freezing a frame-level detector trained on image domain. Code is available at the link.",https://api.openreview.net/pdf/7d92464d9b22f3438efbcd790ee5c573465a13f5.pdf,offline reinforcement learning;transformer;distillation;segmentation;llm,https://scholar.google.com/scholar?q=VITA:+Video+Instance+Segmentation+via+Object+Token+Association
Parameter-Efficient Masking Networks,2022,NIPS,"['Yue Bai', 'Huan Wang', 'Xu Ma', 'Yitian Zhang', 'ZHIQIANG TAO', 'Yun Fu']",poster,"['Random weights representative capacity', 'A new network compression paradigm']","A deeper network structure generally handles more complicated non-linearity and performs more competitively. Nowadays, advanced network designs often contain a large number of repetitive structures (e.g., Transformer). They empower the network capacity to a new level but also increase the model size inevitably, which is unfriendly to either model restoring or transferring. In this study, we are the first to investigate the representative potential of fixed random weights with limited unique values by learning diverse masks and introduce the Parameter-Efficient Masking Networks (PEMN). It also naturally leads to a new paradigm for model compression to diminish the model size. Concretely, motivated by the repetitive structures in modern neural networks, we utilize one random initialized layer, accompanied with different masks, to convey different feature mappings and represent repetitive network modules. Therefore, the model can be expressed as \textit{one-layer} with a bunch of masks, which significantly reduce the model storage cost. Furthermore, we enhance our strategy by learning masks for a model filled by padding a given random weights vector. In this way, our method can further lower the space complexity, especially for models without many repetitive architectures. We validate the potential of PEMN learning masks on random weights with limited unique values and test its effectiveness for a new compression paradigm based on different network architectures.
Code is available at \href{https://github.com/yueb17/PEMN}{\textcolor{magenta}{https://github.com/yueb17/PEMN}}.",https://api.openreview.net/pdf/27aaf98d1828a1e4233a40b5b0683e753000575a.pdf,reinforcement learning;zero_few-shot;transformer;transfer learning;llm,https://scholar.google.com/scholar?q=Parameter-Efficient+Masking+Networks
Signal Recovery with Non-Expansive Generative Network Priors,2022,NIPS,['Jorio Cocola'],poster,"['inverse problems', 'generative networks', 'signal recovery', 'compressed sensing']","We study compressive sensing with a deep generative network prior. Initial theoretical guarantees for efficient recovery from compressed linear measurements have been developed for signals in the range of a ReLU network with Gaussian weights and logarithmic expansivity: that is when each layer is larger than the previous one by a logarithmic factor. It was later shown that constant expansivity is sufficient for recovery. It has remained open whether the expansivity can be relaxed, allowing for networks with contractive layers (as often the case of real generators). In this work we answer this question, proving that a signal in the range of a Gaussian generative network can be recovered from few linear measurements provided that the width of the layers is proportional to the input layer size (up to log factors). This condition allows the generative network to have contractive layers. Our result is based on showing that Gaussian matrices satisfy a matrix concentration inequality which we term Range Restricted Weight Distribution Condition (R2WDC) and which weakens the Weight Distribution Condition (WDC) upon which previous theoretical guarantees were based. The WDC has also been used to analyze other signal recovery problems with generative network priors. By replacing the WDC with the R2WDC, we are able to extend previous results for signal recovery with expansive generative network priors to non-expansive ones. We discuss these extensions for phase retrieval, denoising, and spiked matrix recovery.",https://api.openreview.net/pdf/50a581702d697eed477e97ba56d2d4398e630a63.pdf,zero_few-shot;generative model;active learning;llm,https://scholar.google.com/scholar?q=Signal+Recovery+with+Non-Expansive+Generative+Network+Priors
ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation,2022,NIPS,"['Yufei Xu', 'Jing Zhang', 'Qiming Zhang', 'Dacheng Tao']",poster,"['Vision transformer', 'Pose estimation']","Although no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for pose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision transformers as backbones to extract features for a given person instance and a lightweight decoder for pose estimation. It can be scaled up from 100M to 1B parameters by taking the advantages of the scalable model capacity and high parallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose tasks. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our basic ViTPose model outperforms representative methods on the challenging MS COCO Keypoint Detection benchmark, while the largest model sets a new state-of-the-art. The code and models are available at https://github.com/ViTAE-Transformer/ViTPose.",https://api.openreview.net/pdf/9b6bf0d3a0d9044c441efbb1169d1b82d157bf9f.pdf,graph;transformer;transfer learning;llm,https://scholar.google.com/scholar?q=ViTPose:+Simple+Vision+Transformer+Baselines+for+Human+Pose+Estimation
On the Robustness of Deep Clustering Models: Adversarial Attacks and Defenses,2022,NIPS,"['Anshuman Chhabra', 'Ashwin Sekhari', 'Prasant Mohapatra']",poster,"['Deep Clustering', 'Adversarial Attacks', 'Visual Learning', 'Robust Learning']","Clustering models constitute a class of unsupervised machine learning methods which are used in a number of application pipelines, and play a vital role in modern data science. With recent advancements in deep learning-- deep clustering models have emerged as the current state-of-the-art over traditional clustering approaches, especially for high-dimensional image datasets. While traditional clustering approaches have been analyzed from a robustness perspective, no prior work has investigated adversarial attacks and robustness for deep clustering models in a principled manner. To bridge this gap, we propose a blackbox attack using Generative Adversarial Networks (GANs) where the adversary does not know which deep clustering model is being used, but can query it for outputs. We analyze our attack against multiple state-of-the-art deep clustering models and real-world datasets, and find that it is highly successful. We then employ some natural unsupervised defense approaches, but find that these are unable to mitigate our attack. Finally, we attack Face++, a production-level face clustering API service, and find that we can significantly reduce its performance as well. Through this work, we thus aim to motivate the need for truly robust deep clustering models.",https://api.openreview.net/pdf/3e95bbedaca4bc34406b311e8ec626c4f5de2d08.pdf,generative model;llm,https://scholar.google.com/scholar?q=On+the+Robustness+of+Deep+Clustering+Models:+Adversarial+Attacks+and+Defenses
Stochastic Window Transformer for Image Restoration,2022,NIPS,"['Jie Xiao', 'Xueyang Fu', 'Feng Wu', 'Zheng-Jun Zha']",poster,"['image restoration', 'transformer', 'stochastic window strategy', 'translation invariance', 'locality']","Thanks to the powerful representation capabilities, transformers have made impressive progress in image restoration. However, existing transformers-based methods do not carefully consider the particularities of image restoration. In general, image restoration requires that an ideal approach should be translation-invariant to the degradation, i.e., the undesirable degradation should be removed irrespective of its position within the image. Furthermore, the local relationships also play a vital role, which should be faithfully exploited for recovering clean images. Nevertheless, most transformers either adopt local attention with the fixed local window strategy or global attention, which unfortunately breaks the translation invariance and causes huge loss of local relationships. To address these issues, we propose an elegant stochastic window strategy for transformers. Specifically, we first introduce the window partition with stochastic shift to replace the original fixed window partition for training. Then, we design a new layer expectation propagation algorithm to efficiently approximate the expectation of the induced stochastic transformer for testing. Our stochastic window transformer not only enjoys powerful representation but also maintains the desired property of translation invariance and locality. Experiments validate the stochastic window strategy consistently improves performance on various image restoration tasks (deraining, denoising and deblurring) by significant margins. The code is available at https://github.com/jiexiaou/Stoformer.",https://api.openreview.net/pdf/34f7290c43872e2ab600eab4a6c3a46e45dff013.pdf,graph;transformer;representation;llm,https://scholar.google.com/scholar?q=Stochastic+Window+Transformer+for+Image+Restoration
Decoupling Features in Hierarchical Propagation for Video Object Segmentation,2022,NIPS,"['Zongxin Yang', 'Yi Yang']",poster,"['Video Object Segmentation', 'Metric Learning', 'Instance Segmentation']","This paper focuses on developing a more effective method of hierarchical propagation for semi-supervised Video Object Segmentation (VOS). Based on vision transformers, the recently-developed Associating Objects with Transformers (AOT) approach introduces hierarchical propagation into VOS and has shown promising results. The hierarchical propagation can gradually propagate information from past frames to the current frame and transfer the current frame feature from object-agnostic to object-specific. However, the increase of object-specific information will inevitably lead to the loss of object-agnostic visual information in deep propagation layers. To solve such a problem and further facilitate the learning of visual embeddings, this paper proposes a Decoupling Features in Hierarchical Propagation (DeAOT) approach. Firstly, DeAOT decouples the hierarchical propagation of object-agnostic and object-specific embeddings by handling them in two independent branches. Secondly, to compensate for the additional computation from dual-branch propagation, we propose an efficient module for constructing hierarchical propagation, i.e., Gated Propagation Module, which is carefully designed with single-head attention. Extensive experiments show that DeAOT significantly outperforms AOT in both accuracy and efficiency. On YouTube-VOS, DeAOT can achieve 86.0% at 22.4fps and 82.0% at 53.4fps. Without test-time augmentations, we achieve new state-of-the-art performance on four benchmarks, i.e., YouTube-VOS (86.2%), DAVIS 2017 (86.2%), DAVIS 2016 (92.9%), and VOT 2020 (0.622 EAO).  Project page: https://github.com/z-x-yang/AOT.",https://api.openreview.net/pdf/a267fbf8ae35d678d20bef8e0589097d47358dc5.pdf,transformer;transfer learning;augmentation;segmentation;llm,https://scholar.google.com/scholar?q=Decoupling+Features+in+Hierarchical+Propagation+for+Video+Object+Segmentation
One Model to Edit Them All: Free-Form Text-Driven Image Manipulation with Semantic Modulations,2022,NIPS,"['Yiming Zhu', 'Hongyu Liu', 'Yibing Song', 'Ziyang Yuan', 'Xintong Han', 'Chun Yuan', 'Qifeng Chen', 'Jue Wang']",poster,[],"Free-form text prompts allow users to describe their intentions during image manipulation conveniently. Based on the visual latent space of StyleGAN[21] and text embedding space of CLIP[34], studies focus on how to map these two latent spaces for text-driven attribute manipulations. Currently, the latent mapping between these two spaces is empirically designed and confines that each manipulation model can only handle one fixed text prompt. In this paper, we propose a method named Free-Form CLIP (FFCLIP), aiming to  establish an automatic latent mapping so that one manipulation model handles free-form text prompts. Our FFCLIP has a cross-modality semantic modulation module containing semantic alignment and injection. The semantic alignment performs the automatic latent mapping via linear transformations with a cross attention mechanism. After alignment, we inject semantics from text prompt embeddings to the StyleGAN latent space. For one type of image (e.g., `human portrait'), one FFCLIP model can be learned to handle free-form text prompts. Meanwhile, we observe that although each training text prompt only contains a single semantic meaning, FFCLIP can leverage text prompts with multiple semantic meanings for image manipulation. In the experiments, we evaluate FFCLIP on three types of images (i.e., `human portraits', `cars', and `churches'). Both visual and numerical results show that FFCLIP effectively produces semantically accurate and visually realistic images. Project page:  https://github.com/KumapowerLIU/FFCLIP.",https://api.openreview.net/pdf/894979cb9ce6af3c7cb669007b63963895841b0c.pdf,zero_few-shot;transformer;multimodal;llm,https://scholar.google.com/scholar?q=One+Model+to+Edit+Them+All:+Free-Form+Text-Driven+Image+Manipulation+with+Semantic+Modulations
Towards Practical Control of Singular Values of Convolutional Layers,2022,NIPS,"['Alexandra Senderovich', 'Ekaterina Bulatova', 'Anton Obukhov', 'Maxim Rakhuba']",poster,[],"In general, convolutional neural networks (CNNs) are easy to train, but their essential properties, such as generalization error and adversarial robustness, are hard to control. Recent research demonstrated that singular values of convolutional layers significantly affect such elusive properties and offered several methods for controlling them. Nevertheless, these methods present an intractable computational challenge or resort to coarse approximations. In this paper, we offer a principled approach to alleviating constraints of the prior art at the expense of an insignificant reduction in layer expressivity. Our method is based on the tensor-train decomposition; it retains control over the actual singular values of convolutional mappings while providing structurally sparse and hardware-friendly representation. We demonstrate the improved properties of modern CNNs with our method and analyze its impact on the model performance, calibration, and adversarial robustness. The source code is available at: https://github.com/WhiteTeaDragon/practical_svd_conv",https://api.openreview.net/pdf/07aa27b3e7989a9eef66c5ad3acbbc96fc7b190e.pdf,optimization;representation;sparse;llm,https://scholar.google.com/scholar?q=Towards+Practical+Control+of+Singular+Values+of+Convolutional+Layers
Resource-Adaptive Federated Learning with All-In-One Neural Composition,2022,NIPS,"['Yiqun Mei', 'Pengfei Guo', 'Mo Zhou', 'Vishal Patel']",poster,"['Federated Learning', 'System Heterogeneity']","Conventional Federated Learning (FL) systems inherently assume a uniform processing capacity among clients for deployed models.  However, diverse client hardware often leads to varying computation resources in practice. Such system heterogeneity results in an inevitable trade-off between model complexity and data accessibility as a bottleneck. To avoid such a dilemma and achieve resource-adaptive federated learning, we introduce a simple yet effective mechanism, termed All-In-One Neural Composition, to systematically support training complexity-adjustable models with flexible resource adaption. It is able to efficiently construct models at various complexities using one unified neural basis shared among clients, instead of pruning the global model into local ones. The proposed mechanism endows the system with unhindered access to the full range of knowledge scattered across clients and generalizes existing pruning-based solutions by allowing soft and learnable extraction of low footprint models. Extensive experiment results on popular FL benchmarks demonstrate the effectiveness of our approach. The resulting FL system empowered by our All-In-One Neural Composition, called FLANC, manifests consistent performance gains across diverse system/data heterogeneous setups while keeping high efficiency in computation and communication. ",https://api.openreview.net/pdf/947045879a084b466ab3c2b4786ce823c14236d5.pdf,zero_few-shot;adaptive;federated learning;llm,https://scholar.google.com/scholar?q=Resource-Adaptive+Federated+Learning+with+All-In-One+Neural+Composition
When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment,2022,NIPS,"['Zhijing Jin', 'Sydney Levine', 'Fernando Gonzalez Adauto', 'Ojasv Kamal', 'Maarten Sap', 'Mrinmaya Sachan', 'Rada Mihalcea', 'Joshua B. Tenenbaum', 'Bernhard Schölkopf']",poster,"['AI safety', 'Social Aspects of Machine Learning', 'ethics', 'cognitive science', 'moral decision-making']","AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the flexibility of the human moral mind — the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of moral exception question answering (MoralExceptQA) of cases that involve potentially permissible moral exceptions – inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MoralCoT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. MoralCoT outperforms seven existing LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using MoralExceptQA. Our data is open-sourced at https://huggingface.co/datasets/feradauto/MoralExceptQA and code at https://github.com/feradauto/MoralCoT.",https://api.openreview.net/pdf/d334b60faf45e0e2c49d08b19c18df1b363e3786.pdf,graph;llm,https://scholar.google.com/scholar?q=When+to+Make+Exceptions:+Exploring+Language+Models+as+Accounts+of+Human+Moral+Judgment
Matryoshka Representation Learning,2022,NIPS,"['Aditya Kusupati', 'Gantavya Bhatt', 'Aniket Rege', 'Matthew Wallingford', 'Aditya Sinha', 'Vivek Ramanujan', 'William Howard-Snyder', 'Kaifeng Chen', 'Sham M. Kakade', 'Prateek Jain', 'Ali Farhadi']",poster,"['Representation Learning', 'Efficient Deployment', 'Large-scale Retrieval', 'Large-scale Classification', 'Deep Learning', 'Computer Vision']","Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context rigid, fixed capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka Representations offer: (a) up to $\mathbf{14}\times$ smaller embedding size for ImageNet-1K classification at the same level of accuracy; (b) up to $\mathbf{14}\times$ real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to $\mathbf{2}\%$ accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that MRL extends seamlessly to web-scale datasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and pretrained models are open-sourced at https://github.com/RAIVNLab/MRL.",https://api.openreview.net/pdf/7ba3d40dd265b1f738500f6ec43caa4dcceff4b9.pdf,graph;optimization;zero_few-shot;transformer;representation;inference;llm,https://scholar.google.com/scholar?q=Matryoshka+Representation+Learning
Discrete-Convex-Analysis-Based Framework for Warm-Starting Algorithms with Predictions,2022,NIPS,"['Shinsaku Sakaue', 'Taihei Oki']",poster,"['combinatorial optimization', 'discrete convex analysis', 'algorithms with predictions', 'time complexity']","Augmenting algorithms with learned predictions is a promising approach for going beyond worst-case bounds. Dinitz, Im, Lavastida, Moseley, and Vassilvitskii~(2021) have demonstrated that warm-starts with learned dual solutions can improve the time complexity of the Hungarian method for weighted perfect bipartite matching. We extend and improve their framework in a principled manner via \textit{discrete convex analysis} (DCA), a discrete analog of convex analysis. We show the usefulness of our DCA-based framework by applying it to weighted perfect bipartite matching, weighted matroid intersection, and discrete energy minimization for computer vision. Our DCA-based framework yields time complexity bounds that depend on the $\ell_\infty$-distance from a predicted solution to an optimal solution, which has two advantages relative to the previous $\ell_1$-distance-dependent bounds: time complexity bounds are smaller, and learning of predictions is more sample efficient. We also discuss whether to learn primal or dual solutions from the DCA perspective.",https://api.openreview.net/pdf/3a2f50d639d1448c94a2940d154c4d4e9240851e.pdf,llm,https://scholar.google.com/scholar?q=Discrete-Convex-Analysis-Based+Framework+for+Warm-Starting+Algorithms+with+Predictions
Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer,2022,NIPS,"['Yanjing Li', 'Sheng Xu', 'Baochang Zhang', 'Xianbin Cao', 'Peng Gao', 'Guodong Guo']",poster,[],"The large pre-trained vision transformers (ViTs) have demonstrated remarkable performance on various visual tasks, but suffer from expensive computational and memory cost problems when deployed on resource-constrained devices. Among the powerful compression approaches, quantization extremely reduces the computation and memory consumption by low-bit parameters and bit-wise operations. However, low-bit ViTs remain largely unexplored and usually suffer from a significant performance drop compared with the real-valued counterparts. In this work, through extensive empirical analysis, we first identify the bottleneck  for  severe performance drop comes from  the information distortion of the low-bit quantized self-attention map. We then develop an information rectification module (IRM) and a distribution guided distillation (DGD) scheme for fully quantized vision transformers (Q-ViT) to effectively eliminate such distortion, leading to a fully quantized ViTs. We evaluate our methods on popular DeiT and Swin backbones. Extensive experimental results show that our method achieves a much better performance than the prior arts. For example, our Q-ViT can theoretically accelerates the ViT-S by 6.14x and achieves about 80.9% Top-1 accuracy, even surpassing the full-precision counterpart by 1.0% on ImageNet dataset. Our codes and models are attached on https://github.com/YanjingLi0202/Q-ViT",https://api.openreview.net/pdf/4fd863a3da7d4ca0fbc20128343632e2bb98c74f.pdf,optimization;zero_few-shot;transformer;distillation;llm,https://scholar.google.com/scholar?q=Q-ViT:+Accurate+and+Fully+Quantized+Low-bit+Vision+Transformer
Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork,2022,NIPS,"['Haotao Wang', 'Junyuan Hong', 'Aston Zhang', 'Jiayu Zhou', 'Zhangyang Wang']",poster,[],"Deep neural networks (DNNs) are vulnerable to backdoor attacks. Previous works have shown it extremely challenging to unlearn the undesired backdoor behavior from the network, since the entire network can be affected by the backdoor samples. In this paper, we propose a brand-new backdoor defense strategy, which makes it much easier to remove the harmful influence of backdoor samples from the model. Our defense strategy, \emph{Trap and Replace}, consists of two stages. In the first stage, we bait and trap the backdoors in a small and easy-to-replace subnetwork. Specifically, we add an auxiliary image reconstruction head on top of the stem network shared with a light-weighted classification head. The intuition is that the auxiliary image reconstruction task encourages the stem network to keep sufficient low-level visual features that are hard to learn but semantically correct, instead of overfitting to the easy-to-learn but semantically incorrect backdoor correlations.  As a result, when trained on backdoored datasets, the backdoors are easily baited towards the unprotected classification head, since it is much more vulnerable than the shared stem, leaving the stem network hardly poisoned. In the second stage, we replace the poisoned light-weighted classification head with an untainted one, by re-training it from scratch only on a small holdout dataset with clean samples, while fixing the stem network. As a result, both the stem and the classification head in the final network are hardly affected by backdoor training samples. We evaluate our method against ten different backdoor attacks. Our method outperforms previous state-of-the-art methods by up to $20.57\%$, $9.80\%$, and $13.72\%$ attack success rate and on-average $3.14\%$, $1.80\%$, and $1.21\%$ clean classification accuracy on CIFAR10, GTSRB, and ImageNet-12, respectively. Code is available at https://github.com/VITA-Group/Trap-and-Replace-Backdoor-Defense.",https://api.openreview.net/pdf/a0b040b733099d83fd30969cd35fa8cc35c367b2.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Trap+and+Replace:+Defending+Backdoor+Attacks+by+Trapping+Them+into+an+Easy-to-Replace+Subnetwork
Anticipating Performativity by Predicting from Predictions,2022,NIPS,"['Celestine Mendler-Dünner', 'Frances Ding', 'Yixin Wang']",poster,"['performative prediction', 'performativity', 'causal identifiability', 'supervised learning', 'domain adaptation', 'concept shift', 'social impact']","Predictions about people, such as their expected educational achievement or their credit risk, can be performative and shape the outcome that they are designed to predict. Understanding the causal effect of  predictions on the eventual outcomes is crucial for foreseeing the implications of future predictive models and selecting which models to deploy. However, this causal estimation task poses unique challenges: model predictions are usually deterministic functions of input features and highly correlated with outcomes, which can make the causal effects of predictions on outcomes impossible to disentangle from the direct effect of the covariates. We study this problem through the lens of causal identifiability. Despite the hardness of this problem in full generality, we highlight three natural scenarios where the causal effect of predictions can be identified from observational data: randomization in predictions, overparameterization of the predictive model deployed during data collection, and discrete prediction outputs. Empirically we show that given our identifiability conditions hold, standard variants of supervised learning that predict from predictions by treating the prediction as an input feature can find transferable functional relationships that allow for conclusions about newly deployed predictive models. These positive results fundamentally rely on model predictions being recorded during data collection, bringing forward the importance of rethinking standard data collection practices to enable progress towards a better understanding of social outcomes and performative feedback loops.",https://api.openreview.net/pdf/1d22ed40972efdc28d0186d9fedde5f72cfa01a5.pdf,graph;zero_few-shot;transfer learning;llm,https://scholar.google.com/scholar?q=Anticipating+Performativity+by+Predicting+from+Predictions
Fast Vision Transformers with HiLo Attention,2022,NIPS,"['Zizheng Pan', 'Jianfei Cai', 'Bohan Zhuang']",poster,"['Vision Transformers', 'Image recognition']","Vision Transformers (ViTs) have triggered the most recent and significant breakthroughs in computer vision. Their efficient designs are mostly guided by the indirect metric of computational complexity, i.e., FLOPs, which however has a clear gap with the direct metric such as throughput. Thus, we propose to use the direct speed evaluation on the target platform as the design principle for efficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT which performs favourably against the existing state-of-the-art methods across a spectrum of different model sizes with faster speed. At the core of LITv2 is a novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the insight that high frequencies in an image capture local fine details and low frequencies focus on global structures, whereas a multi-head self-attention layer neglects the characteristic of different frequencies. Therefore, we propose to disentangle the high/low frequency patterns in an attention layer by separating the heads into two groups, where one group encodes high frequencies via self-attention within each local window, and another group encodes low frequencies by performing global attention between the average-pooled low-frequency keys and values from each window and each query position in the input feature map. Benefiting from the efficient design for both groups, we show that HiLo is superior to the existing attention mechanisms by comprehensively benchmarking FLOPs, speed and memory consumption on GPUs and CPUs. For example, HiLo is 1.4× faster than spatial reduction attention and 1.6× faster than local window attention on CPUs. Powered by HiLo, LITv2 serves as a strong backbone for mainstream vision tasks including image classification, dense detection and segmentation. Code is available at https://github.com/ziplab/LITv2.",https://api.openreview.net/pdf/b3fad79f81c3f58293a93757e152621eaf730a72.pdf,zero_few-shot;transformer;metric;segmentation;llm,https://scholar.google.com/scholar?q=Fast+Vision+Transformers+with+HiLo+Attention
PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining,2022,NIPS,"['Yuting Gao', 'Jinfeng Liu', 'Zihan Xu', 'Jun Zhang', 'Ke Li', 'Rongrong Ji', 'Chunhua Shen']",poster,['Vision-language Pre-training'],"Large-scale vision-language pre-training has achieved promising results on downstream tasks. Existing methods highly rely on the assumption that the image-text pairs crawled from the Internet are in perfect one-to-one correspondence. However, in real scenarios, this assumption can be difficult to hold: the text description, obtained by crawling the affiliated metadata of the image, often suffers from the semantic mismatch and the mutual compatibility. To address these issues, we introduce PyramidCLIP, which constructs an input pyramid with different semantic levels for each modality, and aligns visual elements and linguistic elements in the form of hierarchy via peer-level semantics alignment and cross-level relation alignment. Furthermore, we soften the loss of negative samples (unpaired samples) so as to weaken the strict constraint during the pre-training stage, thus mitigating the risk of forcing the model to distinguish compatible negative pairs. Experiments on five downstream tasks demonstrate the effectiveness of the proposed PyramidCLIP. In particular, with the same amount of 15 million pre-training image-text pairs, PyramidCLIP exceeds CLIP on ImageNet zero-shot classification top-1 accuracy by 10.6%/13.2%/10.0% with ResNet50/ViT-B32/ViT-B16 based image encoder respectively. When scaling to larger datasets, PyramidCLIP achieves the state-of-the-art results on several downstream tasks. In particular, the results of PyramidCLIP-ResNet50 trained on 143M image-text pairs surpass that of CLIP using 400M data on ImageNet zero-shot classification task, significantly improving the data efficiency of CLIP.",https://api.openreview.net/pdf/200fdfb59a4a6f381e23c87744ecfa1907fbf8e4.pdf,optimization;zero_few-shot;transformer;meta-learning;llm,https://scholar.google.com/scholar?q=PyramidCLIP:+Hierarchical+Feature+Alignment+for+Vision-language+Model+Pretraining
Adapting Self-Supervised Vision Transformers by Probing Attention-Conditioned Masking Consistency,2022,NIPS,"['Viraj Uday Prabhu', 'Sriram Yenamandra', 'Aaditya Singh', 'Judy Hoffman']",poster,"['domain adaptation', 'self-supervised learning', 'vision transformer', 'object recognition']","Visual domain adaptation (DA) seeks to transfer trained models to unseen, unlabeled domains across distribution shift, but approaches typically focus on adapting convolutional neural network architectures initialized with supervised ImageNet representations. In this work, we shift focus to adapting modern architectures for object recognition -- the increasingly popular Vision Transformer (ViT) -- initialized with modern pretraining based on self-supervised learning (SSL). Inspired by the design of recent SSL approaches based on learning from partial image inputs generated via masking or cropping -- either by learning to predict the missing pixels, or learning representational invariances to such augmentations -- we propose PACMAC, a two-stage adaptation algorithm for self-supervised ViTs. PACMAC first performs in-domain SSL on pooled source and target data to learn task-discriminative features, and then probes the model's predictive consistency across a set of partial target inputs generated via a novel attention-conditioned masking strategy, to identify reliable candidates for self-training. Our simple approach leads to consistent performance gains over competing methods that use ViTs and self-supervised initializations on standard object recognition benchmarks. Our code is available at https://github.com/virajprabhu/PACMAC.",https://api.openreview.net/pdf/ccdb81106e61e6210fc8d91abee720be85c881ae.pdf,zero_few-shot;transformer;representation;transfer learning;augmentation;llm,https://scholar.google.com/scholar?q=Adapting+Self-Supervised+Vision+Transformers+by+Probing+Attention-Conditioned+Masking+Consistency
OpenAUC: Towards AUC-Oriented Open-Set Recognition,2022,NIPS,"['Zitai Wang', 'Qianqian Xu', 'Zhiyong Yang', 'Yuan He', 'Xiaochun Cao', 'Qingming Huang']",poster,[],"Traditional machine learning follows a close-set assumption that the training and test set share the same label space. While in many practical scenarios, it is inevitable that some test samples belong to unknown classes (open-set). To fix this issue, Open-Set Recognition (OSR), whose goal is to make correct predictions on both close-set samples and open-set samples, has attracted rising attention. In this direction, the vast majority of literature focuses on the pattern of open-set samples. However, how to evaluate model performance in this challenging task is still unsolved. In this paper, a systematic analysis reveals that most existing metrics are essentially inconsistent with the aforementioned goal of OSR: (1) For metrics extended from close-set classification, such as Open-set F-score, Youden's index, and Normalized Accuracy, a poor open-set prediction can escape from a low performance score with a superior close-set prediction. (2) Novelty detection AUC, which measures the ranking performance between close-set and open-set samples, ignores the close-set performance. To fix these issues, we propose a novel metric named OpenAUC. Compared with existing metrics, OpenAUC enjoys a concise pairwise formulation that evaluates open-set performance and close-set performance in a coupling manner. Further analysis shows that OpenAUC is free from the aforementioned inconsistency properties. Finally, an end-to-end learning method is proposed to minimize the OpenAUC risk, and the experimental results on popular benchmark datasets speak to its effectiveness.",https://api.openreview.net/pdf/c55b18aded76d0e934b96abb2f2fc7e79043d887.pdf,graph;zero_few-shot;transformer;metric;llm,https://scholar.google.com/scholar?q=OpenAUC:+Towards+AUC-Oriented+Open-Set+Recognition
Zero-Shot Video Question Answering via Frozen Bidirectional Language Models,2022,NIPS,"['Antoine Yang', 'Antoine Miech', 'Josef Sivic', 'Ivan Laptev', 'Cordelia Schmid']",poster,"['Video Question Answering', 'Zero-Shot', 'Vision and Language', 'Computer Vision']","Video question answering (VideoQA) is a complex task that requires diverse multi-modal data for training. Manual annotation of question and answers for videos, however, is tedious and prohibits scalability. To tackle this problem, recent methods consider zero-shot settings with no manual annotation of visual question-answer. In particular, a promising approach adapts frozen autoregressive language models pretrained on Web-scale text-only data to multi-modal inputs. In contrast, we here build on frozen bidirectional language models (BiLM) and show that such an approach provides a stronger and cheaper alternative for zero-shot VideoQA. In particular, (i) we combine visual inputs with the frozen BiLM using light trainable modules, (ii) we train such modules using Web-scraped multi-modal data, and finally (iii) we perform zero-shot VideoQA inference through masked language modeling, where the masked text is the answer to a given question. Our proposed approach, FrozenBiLM, outperforms the state of the art in zero-shot VideoQA by a significant margin on a variety of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA, TGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in the few-shot and fully-supervised setting. Our code and models are publicly available at https://github.com/antoyang/FrozenBiLM.",https://api.openreview.net/pdf/77eb1542ab76214cf61612cee1b78cc12ac39763.pdf,graph;zero_few-shot;inference;multimodal;llm,https://scholar.google.com/scholar?q=Zero-Shot+Video+Question+Answering+via+Frozen+Bidirectional+Language+Models
S-Prompts Learning with Pre-trained Transformers: An Occam’s Razor for Domain Incremental Learning,2022,NIPS,"['Yabin Wang', 'Zhiwu Huang', 'Xiaopeng Hong']",poster,"['Prompts Learning', 'Pre-trained Transformers', ""Occam's Razor"", 'Domain Incremental Learning']","State-of-the-art deep neural networks are still struggling to address the catastrophic forgetting problem in continual learning. In this paper, we propose one simple paradigm (named as S-Prompting) and two concrete approaches to highly reduce the forgetting degree in one of the most typical continual learning scenarios, i.e., domain increment learning (DIL). The key idea of the paradigm is to learn prompts independently across domains with pre-trained transformers, avoiding the use of exemplars that commonly appear in conventional methods. This results in a win-win game where the prompting can achieve the best for each domain. The independent prompting across domains only requests one single cross-entropy loss for training and one simple K-NN operation as a domain identifier for inference. The learning paradigm derives an image prompt learning approach and a novel language-image prompt learning approach. Owning an excellent scalability (0.03% parameter increase per domain), the best of our approaches achieves a remarkable relative improvement (an average of about 30%) over the best of the state-of-the-art exemplar-free methods for three standard DIL tasks, and even surpasses the best of them relatively by about 6% in average when they use exemplars. Source code is available at https://github.com/iamwangyabin/S-Prompts.",https://api.openreview.net/pdf/1e1eaaf93fdd40e346c253145c68382fba2c2775.pdf,transformer;inference;llm,https://scholar.google.com/scholar?q=S-Prompts+Learning+with+Pre-trained+Transformers:+An+Occam’s+Razor+for+Domain+Incremental+Learning
Comparing Distributions by Measuring Differences that Affect Decision Making,2022,ICLR,"['Shengjia Zhao', 'Abhishek Sinha', 'Yutong He', 'Aidan Perreault', 'Jiaming Song', 'Stefano Ermon']",oral,"['probability divergence', 'two sample test', 'generative model']","Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task -- two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution. By suitably choosing the decision task, this generalizes the Jensen-Shannon divergence and the maximum mean discrepancy family. We apply our approach to two-sample tests, and on various benchmarks, we achieve superior test power compared to competing methods. In addition, a modeler can directly specify their preferences when comparing distributions through the decision loss. We apply this property to understanding the effects of climate change on different social and economic activities, evaluating sample quality, and selecting features targeting different decision tasks.",https://api.openreview.net/pdf/e99719a7a6796b569cc6afdf6f42024d0df2fbea.pdf,llm,https://scholar.google.com/scholar?q=Comparing+Distributions+by+Measuring+Differences+that+Affect+Decision+Making
Large Language Models Can Be Strong Differentially Private Learners,2022,ICLR,"['Xuechen Li', 'Florian Tramer', 'Percy Liang', 'Tatsunori Hashimoto']",oral,"['language model', 'differential privacy', 'language generation', 'fine-tuning', 'NLP']","Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead.
We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure.
With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines---by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. 
To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. 
The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. 
Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models tends to not suffer from dimension-dependent performance degradation.
Code to reproduce results can be found at https://github.com/lxuechen/private-transformers.
",https://api.openreview.net/pdf/d88e1e721c4085b8a6403837f45b8c483ad0225b.pdf,optimization;zero_few-shot;transformer;multimodal;llm,https://scholar.google.com/scholar?q=Large+Language+Models+Can+Be+Strong+Differentially+Private+Learners
Divisive Feature Normalization Improves Image Recognition Performance in AlexNet,2022,ICLR,"['Michelle Miller', 'SueYeon Chung', 'Kenneth D. Miller']",poster,"['divisive normalization', 'AlexNet', 'ImageNet', 'CIFAR-100', 'manifold capacity', 'sparsity', 'receptive fields', 'Batch Normalization', 'Group Normalization', 'Layer Normalization']","Local divisive normalization provides a phenomenological description of many nonlinear response properties of neurons across visual cortical areas. To gain insight into the utility of this operation, we studied the effects on AlexNet of a local divisive normalization between features, with learned parameters. Developing features were arranged in a line topology, with the influence between features determined by an exponential function of the distance between them. We compared an AlexNet model with no normalization or with canonical normalizations (Batch, Group, Layer) to the same models with divisive normalization added. Divisive normalization always improved performance for models with batch or group or no normalization, generally by 1-2 percentage points, on both the CIFAR-100 and ImageNet databases. To gain insight into mechanisms underlying the improved performance, we examined several aspects of network representations. In the early layers both canonical and divisive normalizations reduced manifold capacities and increased average dimension of the individual categorical manifolds. In later layers the capacity was higher and manifold dimension lower for models roughly in order of their performance improvement. Examining the sparsity of activations across a given layer, divisive normalization layers increased sparsity, while the canonical normalization layers decreased it. Nonetheless, in the final layer, the sparseness of activity increased in the order of no normalization, divisive, com- bined, and canonical. We also investigated how the receptive fields (RFs) in the first convolutional layer (where RFs are most interpretable) change with normalization. Divisive normalization enhanced RF Fourier power at low wavelengths, while divisive+canonical enhanced power at mid (batch, group) or low (layer) wavelengths, compared to canonical alone or no normalization. In conclusion, divisive normalization enhances image recognition performance, most strongly when combined with canonical normalization, and in doing so it reduces manifold capacity and sparsity in early layers while increasing them in final layers, and increases low- or mid-wavelength power in the first-layer receptive fields.",https://api.openreview.net/pdf/452011d69839dd4fa39ba4bec882b24cb5bb2649.pdf,zero_few-shot;representation;online learning;sparse;llm,https://scholar.google.com/scholar?q=Divisive+Feature+Normalization+Improves+Image+Recognition+Performance+in+AlexNet
Neural Link Prediction with Walk Pooling,2022,ICLR,"['Liming Pan', 'Cheng Shi', 'Ivan Dokmanić']",poster,"['Graph neural network', 'Link prediction', 'Random walk', 'Graph topology.']","Graph neural networks achieve high accuracy in link prediction by jointly leveraging graph topology and node attributes. Topology, however, is represented indirectly; state-of-the-art methods based on subgraph classification label nodes with distance to the target link, so that, although topological information is present, it is tempered by pooling. This makes it challenging to leverage features like loops and motifs associated with network formation mechanisms. We propose a link prediction algorithm based on a new pooling scheme called WalkPool. WalkPool combines the expressivity of topological heuristics with the feature-learning ability of neural networks. It summarizes a putative link by random walk probabilities of adjacent paths. Instead of extracting transition probabilities from the original graph, it computes the transition matrix of a ``predictive'' latent graph by applying attention to learned features; this may be interpreted as feature-sensitive topology fingerprinting. WalkPool can leverage unsupervised node features or be combined with GNNs and trained end-to-end. It outperforms state-of-the-art methods on all common link prediction benchmarks, both homophilic and heterophilic, with and without node attributes. Applying WalkPool to a set of unsupervised GNNs significantly improves prediction accuracy, suggesting that it may be used as a general-purpose graph pooling scheme.   ",https://api.openreview.net/pdf/ad031c5e836c55357e2f13cdb18fa502a7eecc80.pdf,graph;transformer;llm,https://scholar.google.com/scholar?q=Neural+Link+Prediction+with+Walk+Pooling
Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?,2022,ICLR,"['Yonggan Fu', 'Shunyao Zhang', 'Shang Wu', 'Cheng Wan', 'Yingyan Lin']",poster,"['Vision transformer', 'adversarial examples', 'robustness']","Vision transformers (ViTs) have recently set off a new wave in neural architecture design thanks to their record-breaking performance in various vision tasks. In parallel, to fulfill the goal of deploying ViTs into real-world vision applications, their robustness against potential malicious attacks has gained increasing attention. In particular, recent works show that ViTs are more robust against adversarial attacks as compared with convolutional neural networks (CNNs), and conjecture that this is because ViTs focus more on capturing global interactions among different input/feature patches, leading to their improved robustness to local perturbations imposed by adversarial attacks. In this work, we ask an intriguing question: ""Under what kinds of perturbations do ViTs become more vulnerable learners compared to CNNs?"" Driven by this question, we first conduct a comprehensive experiment regarding the robustness of both ViTs and CNNs under various existing adversarial attacks to understand the underlying reason favoring their robustness. Based on the drawn insights, we then propose a dedicated attack framework, dubbed Patch-Fool, that fools the self-attention mechanism by attacking its basic component (i.e., a single patch) with a series of attention-aware optimization techniques. Interestingly, our Patch-Fool framework shows for the first time that ViTs are not necessarily more robust than CNNs against adversarial perturbations. In particular, we find that ViTs are more vulnerable learners compared with CNNs against our Patch-Fool attack which is consistent across extensive experiments, and the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool, indicate an intriguing insight that the perturbation density and strength on each patch seem to be the key factors that influence the robustness ranking between ViTs and CNNs. It can be expected that our Patch-Fool framework will shed light on both future architecture designs and training schemes for robustifying ViTs towards their real-world deployment. Our codes are available at https://github.com/RICE-EIC/Patch-Fool.",https://api.openreview.net/pdf/4c7b8d2f80c4ea1bfe11754da2e7c69fc5183754.pdf,optimization;transformer;sparse;llm,https://scholar.google.com/scholar?q=Patch-Fool:+Are+Vision+Transformers+Always+Robust+Against+Adversarial+Perturbations?
"Audio Lottery: Speech Recognition Made Ultra-Lightweight, Noise-Robust, and Transferable",2022,ICLR,"['Shaojin Ding', 'Tianlong Chen', 'Zhangyang Wang']",poster,"['Speech Recognition', 'Lottery Ticket Hypothesis']","Lightweight speech recognition models have seen explosive demands owing to a growing amount of speech-interactive features on mobile devices. Since designing such systems from scratch is non-trivial, practitioners typically choose to compress large (pre-trained) speech models. Recently, lottery ticket hypothesis reveals the existence of highly sparse subnetworks that can be trained in isolation without sacrificing the performance of the full models. In this paper, we investigate the tantalizing possibility of using lottery ticket hypothesis to discover lightweight speech recognition models, that are (1) robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and 3) compatible with structured sparsity. We conducted extensive experiments on  CNN-LSTM, RNN-Transducer, and Transformer models, and verified the existence of highly sparse winning tickets that can match the full model performance across those backbones. We obtained winning tickets that have less than 20% of full model weights on all backbones, while the most lightweight one only keeps 4.4% weights. Those winning tickets generalize to structured sparsity with no performance loss, and transfer exceptionally from large source datasets to various target datasets. Perhaps most surprisingly, when the training utterances have high background noises, the winning tickets even substantially outperform the full models, showing the extra bonus of noise robustness by inducing sparsity. Codes are available at https://github.com/VITA-Group/Audio-Lottery.",https://api.openreview.net/pdf/3d42ff881f8ec8954935d0f8bbcb2a21d71106ea.pdf,transformer;sparse;transfer learning;active learning;llm,"https://scholar.google.com/scholar?q=Audio+Lottery:+Speech+Recognition+Made+Ultra-Lightweight,+Noise-Robust,+and+Transferable"
VC dimension of partially quantized neural networks in the overparametrized regime,2022,ICLR,"['Yutong Wang', 'Clayton Scott']",poster,"['VC dimension', 'quantized neural networks', 'classification', 'minimax theory', 'overparametrization']","Vapnik-Chervonenkis (VC) theory has so far been unable to explain the small generalization error of overparametrized neural networks. Indeed, existing applications of VC theory to large networks obtain upper bounds on VC dimension that are proportional to the number of weights, and for a large class of networks, these upper bound are known to be tight. In this work, we focus on a class of partially quantized networks that we refer to as hyperplane arrangement neural networks (HANNs). Using a sample compression analysis, we show that HANNs can have VC dimension significantly smaller than the number of weights, while being highly expressive. In particular, empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classification with Lipschitz posterior class probability. We further demonstrate the expressivity of HANNs empirically. On a panel of 121 UCI datasets, overparametrized HANNs are able to match the performance of state-of-the-art full-precision models.",https://api.openreview.net/pdf/9760187606b3496a5f4a0fe752a22416bb4a2e21.pdf,llm,https://scholar.google.com/scholar?q=VC+dimension+of+partially+quantized+neural+networks+in+the+overparametrized+regime
NETWORK INSENSITIVITY TO PARAMETER NOISE VIA PARAMETER ATTACK DURING TRAINING,2022,ICLR,"['Julian Büchel', 'Fynn Firouz Faber', 'Dylan Richard Muir']",poster,"['parameter attack', 'adversarial attack', 'neural network', 'deep learning', 'optimisation', 'neuromorphic processor']","Neuromorphic neural network processors, in the form of compute-in-memory crossbar arrays of memristors, or in the form of subthreshold analog and mixed-signal ASICs, promise enormous advantages in compute density and energy efficiency for NN-based ML tasks. However, these technologies are prone to computational non-idealities, due to process variation and intrinsic device physics. This degrades the task performance of networks deployed to the processor, by introducing parameter noise into the deployed model. While it is possible to calibrate each device, or train networks individually for each processor, these approaches are expensive and impractical for commercial deployment. Alternative methods are therefore needed to train networks that are inherently robust against parameter variation, as a consequence of network architecture and parameters. We present a new network training algorithm that attacks network parameters during training, and promotes robust performance during inference in the face of random parameter variation. Our approach introduces a loss regularization term that penalizes the susceptibility of a network to weight perturbation. We compare against previous approaches for producing parameter insensitivity such as dropout, weight smoothing and introducing parameter noise during training. We show that our approach produces models that are more robust to random mismatch-induced parameter variation as well as to targeted parameter variation. Our approach finds minima in flatter locations in the weight-loss landscape compared with other approaches, highlighting that the networks found by our technique are less sensitive to parameter perturbation. Our work provides an approach to deploy neural network architectures to inference devices that suffer from computational non-idealities, with minimal loss of performance. This method will enable deployment at scale to novel energy-efficient computational substrates, promoting cheaper and more prevalent edge inference.",https://api.openreview.net/pdf/b7b77ce8535702dba33084aa20eb08cae53193f4.pdf,inference;llm,https://scholar.google.com/scholar?q=NETWORK+INSENSITIVITY+TO+PARAMETER+NOISE+VIA+PARAMETER+ATTACK+DURING+TRAINING
Expressivity of Emergent Languages is a Trade-off between Contextual Complexity and Unpredictability,2022,ICLR,"['Shangmin Guo', 'Yi Ren', 'Kory Wallace Mathewson', 'Simon Kirby', 'Stefano V Albrecht', 'Kenny Smith']",poster,"['Emergent Language', 'Expressivity']","Researchers are using deep learning models to explore the emergence of language in various language games, where agents interact and develop an emergent language to solve tasks. We focus on the factors that determine the expressivity of emergent languages, which reflects the amount of information about input spaces those languages are capable of encoding. We measure the expressivity of emergent languages based on the generalisation performance across different games, and demonstrate that the expressivity of emergent languages is a trade-off between the complexity and unpredictability of the context those languages emerged from. Another contribution of this work is the discovery of message type collapse, i.e. the number of unique messages is lower than that of inputs. We also show that using the contrastive loss proposed by Chen et al. (2020) can alleviate this problem.",https://api.openreview.net/pdf/be46689741877d2b59dc56c09443500af7dd2941.pdf,reinforcement learning;zero_few-shot;contrastive learning;llm,https://scholar.google.com/scholar?q=Expressivity+of+Emergent+Languages+is+a+Trade-off+between+Contextual+Complexity+and+Unpredictability
Tracking the risk of a deployed model and detecting harmful distribution shifts,2022,ICLR,"['Aleksandr Podkopaev', 'Aaditya Ramdas']",poster,"['Distribution shift', 'sequential testing']","When deployed in the real world, machine learning models inevitably encounter changes in the data distribution, and certain---but not all---distribution shifts could result in significant performance degradation. In practice, it may make sense to ignore benign shifts, under which the performance of a deployed model does not degrade substantially, making interventions by a human expert (or model retraining) unnecessary.  While several works have developed tests for distribution shifts, these typically either use non-sequential methods, or detect arbitrary shifts (benign or harmful), or both. We argue that a sensible method for firing off a warning has to both (a) detect harmful shifts while ignoring benign ones, and (b) allow continuous monitoring of model performance without increasing the false alarm rate. In this work, we design simple sequential tools for testing if the difference between source (training) and target (test) distributions leads to a significant increase in a risk function of interest, like accuracy or calibration. Recent advances in constructing time-uniform confidence sequences allow efficient aggregation of statistical evidence accumulated during the tracking process. The designed framework is applicable in settings where (some) true labels are revealed after the prediction is performed, or when batches of labels become available in a delayed fashion. We demonstrate the efficacy of the proposed framework through an extensive empirical study on a collection of simulated and real datasets.",https://api.openreview.net/pdf/f763a5271b61d98bca4127ab14ce483150d152c4.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Tracking+the+risk+of+a+deployed+model+and+detecting+harmful+distribution+shifts
NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training,2022,ICLR,"['Chengyue Gong', 'Dilin Wang', 'Meng Li', 'Xinlei Chen', 'Zhicheng Yan', 'Yuandong Tian', 'qiang liu', 'Vikas Chandra']",poster,"['vision transformer', 'gradient conflict', 'neural architecture search']","Designing accurate and efficient vision transformers (ViTs) is a highly important but challenging task. Supernet-based one-shot neural architecture search (NAS) enables fast architecture optimization and has achieved state-of-the-art (SOTA) results on convolutional neural networks (CNNs). However, directly applying the supernet-based NAS to optimize ViTs leads to poor performance - even worse compared to training single ViTs. In this work, we observe that the poor performance is due to a gradient conflict issue: the gradients of different sub-networks conflict with that of the supernet more severely in ViTs than CNNs, which leads to early saturation in training and inferior convergence. To alleviate this issue, we propose a series of techniques, including a gradient projection algorithm, a switchable layer scaling design, and a simplified data augmentation and regularization training recipe. The proposed techniques significantly improve the convergence and the performance of all sub-networks. Our discovered hybrid ViT model family, dubbed NASViT, achieves top-1 accuracy from 78.2% to 81.8% on ImageNet from 200M to 800M FLOPs, and outperforms all the prior art CNNs and ViTs, including AlphaNet and LeViT, etc. When transferred to semantic segmentation tasks, NASViTs also outperform previous backbones on both Cityscape and ADE20K datasets, achieving 73.2% and 37.9% mIoU with only 5G FLOPs, respectively. Code is available at
https://github.com/facebookresearch/NASViT.
",https://api.openreview.net/pdf/a6df48abb7e0bb493e7c343c46beb7b365cdc788.pdf,graph;optimization;transformer;transfer learning;augmentation;segmentation;llm,https://scholar.google.com/scholar?q=NASViT:+Neural+Architecture+Search+for+Efficient+Vision+Transformers+with+Gradient+Conflict+aware+Supernet+Training
Efficient Self-supervised Vision Transformers for Representation Learning,2022,ICLR,"['Chunyuan Li', 'Jianwei Yang', 'Pengchuan Zhang', 'Mei Gao', 'Bin Xiao', 'Xiyang Dai', 'Lu Yuan', 'Jianfeng Gao']",poster,"['self-supervised learning', 'vision transformers', 'non-contrastive region-matching task']","This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can significantly reduce modeling complexity but with a cost of losing the ability to capture fine-grained correspondences between image regions. Second, we propose a new pre-training task, non-contrastive region-matching, which allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations. Our results show that combining the two techniques, EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput. When transferring to downstream linear classification tasks, EsViT outperforms its supervised counterpart on 17 out of 18 datasets. The code and pre-trained models are released at: https://github.com/microsoft/esvit",https://api.openreview.net/pdf/e7b63dccef8ad598db1c36a2386c8d8a63058e8e.pdf,zero_few-shot;transformer;representation;contrastive learning;sparse;transfer learning;llm,https://scholar.google.com/scholar?q=Efficient+Self-supervised+Vision+Transformers+for+Representation+Learning
A First-Occupancy Representation for Reinforcement Learning,2022,ICLR,"['Ted Moskovitz', 'Spencer R Wilson', 'Maneesh Sahani']",poster,"['successor representation', 'successor features', 'generalized policy improvement', 'GPI']","Both animals and artificial agents benefit from state representations that support rapid transfer of learning across tasks and which enable them to efficiently traverse their environments to reach rewarding states.  The successor representation (SR), which measures the expected cumulative, discounted state occupancy under a fixed policy, enables efficient transfer to different reward structures in an otherwise constant Markovian environment and has been hypothesized to underlie aspects of biological behavior and neural activity.  However, in the real world, rewards may only be available for consumption once, may shift location, or agents may simply aim to reach goal states as rapidly as possible without the constraint of artificially imposed task horizons. In such cases, the most behaviorally-relevant representation would carry information about when the agent was likely to first reach states of interest, rather than how often it should expect to visit them over a potentially infinite time span.  To reflect such demands, we introduce the first-occupancy representation (FR), which measures the expected temporal discount to the first time a state is accessed.  We demonstrate that the FR facilitates exploration, the selection of efficient paths to desired states, allows the agent, under certain conditions, to plan provably optimal trajectories defined by a sequence of subgoals, and induces similar behavior to animals avoiding threatening stimuli.",https://api.openreview.net/pdf/46abdff2d131f44012d855cdd93c0fa7034d601a.pdf,reinforcement learning;optimization;zero_few-shot;representation;transfer learning;llm,https://scholar.google.com/scholar?q=A+First-Occupancy+Representation+for+Reinforcement+Learning
The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks,2022,ICLR,"['Rahim Entezari', 'Hanie Sedghi', 'Olga Saukh', 'Behnam Neyshabur']",poster,"['Permutation', 'Invariance', 'Mode Connectivity', 'Energy Barrier', 'Loss landscape', 'Deep Learning']","In this paper, we conjecture that if the permutation invariance of neural networks is taken into account, SGD solutions will likely have no barrier in the linear interpolation between them. Although it is a bold conjecture, we show how extensive empirical attempts fall short of refuting it. We further provide a preliminary theoretical result to support our conjecture. Our conjecture has implications for the lottery ticket hypothesis, distributed training, and ensemble methods. The source code is available at \url{https://github.com/rahimentezari/PermutationInvariance}.",https://api.openreview.net/pdf/a575dfe6923ea65c17895fd63d13fc299132536f.pdf,llm,https://scholar.google.com/scholar?q=The+Role+of+Permutation+Invariance+in+Linear+Mode+Connectivity+of+Neural+Networks
HTLM: Hyper-Text Pre-Training and Prompting of Language Models,2022,ICLR,"['Armen Aghajanyan', 'Dmytro Okhonko', 'Mike Lewis', 'Mandar Joshi', 'Hu Xu', 'Gargi Ghosh', 'Luke Zettlemoyer']",poster,"['prompting', 'nlp', 'representational learning', 'priming']","We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. 'class' and 'id' attributes often encode document category information), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by infilling '<title>' tags for a webpage that contains the input text).  We show that pretraining with a BART-style denoising loss directly on simplified HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and fine-tuning for classification benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also find that hyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research. ",https://api.openreview.net/pdf/565914339be7ddb2453523852e836e1fe3ce3b8b.pdf,zero_few-shot;transfer learning;llm,https://scholar.google.com/scholar?q=HTLM:+Hyper-Text+Pre-Training+and+Prompting+of+Language+Models
Illiterate DALL-E Learns to Compose,2022,ICLR,"['Gautam Singh', 'Fei Deng', 'Sungjin Ahn']",poster,"['Zero-Shot Image Generation', 'Compositional Representation', 'Object-Centric Representation', 'Out-of-Distribution Generalization', 'Image Transformers']","Although DALL-E has shown an impressive ability of composition-based systematic generalization in image generation, it requires the dataset of text-image pairs and the compositionality is provided by the text. In contrast, object-centric representation models like the Slot Attention model learn composable representations without the text prompt. However, unlike DALL-E, its ability to systematically generalize for zero-shot generation is significantly limited. In this paper, we propose a simple but novel slot-based autoencoding architecture, called SLATE, for combining the best of both worlds: learning object-centric representations that allow systematic generalization in zero-shot image generation without text. As such, this model can also be seen as an illiterate DALL-E model. Unlike the pixel-mixture decoders of existing object-centric representation models, we propose to use the Image GPT decoder conditioned on the slots for capturing complex interactions among the slots and pixels. In experiments, we show that this simple and easy-to-implement architecture not requiring a text prompt achieves significant improvement in in-distribution and out-of-distribution (zero-shot) image generation and qualitatively comparable or better slot-attention structure than the models based on mixture decoders.",https://api.openreview.net/pdf/bcb5e847c6cefafd402be4829f49227cf6b457c7.pdf,zero_few-shot;transformer;representation;generative model;llm,https://scholar.google.com/scholar?q=Illiterate+DALL-E+Learns+to+Compose
Non-Linear Operator Approximations for Initial Value Problems,2022,ICLR,"['Gaurav Gupta', 'Xiongye Xiao', 'Radu Balan', 'Paul Bogdan']",poster,"['exponential operators', 'initial value problem', 'pade approximation', 'multiwavelets', 'partial differential equations']","Time-evolution of partial differential equations is the key to model several dynamical processes, events forecasting but the operators associated with such problems are non-linear. We propose a Padé approximation based exponential neural operator scheme for efficiently learning the map between a given initial condition and activities at a later time. The multiwavelets bases are used for space discretization. By explicitly embedding the exponential operators in the model, we reduce the training parameters and make it more data-efficient which is essential in dealing with scarce real-world datasets. The Padé exponential operator uses a $\textit{recurrent structure with shared parameters}$ to model the non-linearity compared to recent neural operators that rely on using multiple linear operator layers in succession. We show theoretically that the gradients associated with the recurrent Padé network are bounded across the recurrent horizon. We perform experiments on non-linear systems such as Korteweg-de Vries (KdV) and Kuramoto–Sivashinsky (KS) equations to show that the proposed approach achieves the best performance and at the same time is data-efficient. We also show that urgent real-world problems like Epidemic forecasting (for example, COVID-19) can be formulated as a 2D time-varying operator problem. The proposed Padé exponential operators yield better prediction results ($\textbf{53\%} (\textbf{52\%})$ better MAE than best neural operator (non-neural operator deep learning model)) compared to state-of-the-art forecasting models.",https://api.openreview.net/pdf/64034193e19bb69648d9e3ed9301e225d08d08fe.pdf,llm,https://scholar.google.com/scholar?q=Non-Linear+Operator+Approximations+for+Initial+Value+Problems
Enabling Arbitrary Translation Objectives with Adaptive Tree Search,2022,ICLR,"['Wang Ling', 'Wojciech Stokowiec', 'Domenic Donato', 'Chris Dyer', 'Lei Yu', 'Laurent Sartran', 'Austin Matthews']",poster,"['Machine Translation', 'Decoding', 'MCTS', 'Beam Search']","We introduce an adaptive tree search algorithm, which is a deterministic variant of Monte Carlo tree search, that can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. When applied to autoregressive models, our algorithm has different biases than beam search has, which enables a new analysis of the role of decoding bias in autoregressive models. Empirically, we show that our adaptive tree search algorithm finds outputs with substantially better model scores compared to beam search in autoregressive models, and compared to reranking techniques in models whose scores do not decompose additively with respect to the words in the output. We also characterise the correlation of several translation model objectives with respect to BLEU. We find that while some standard models are poorly calibrated and benefit from the beam search bias, other often more robust models (autoregressive models tuned to maximize expected automatic metric scores, the noisy channel model and a newly proposed objective) benefit from increasing amounts of search using our proposed decoder, whereas the beam search bias limits the improvements obtained from such objectives. Thus, we argue that as models improve, the improvements may be masked by over-reliance on beam search or reranking based methods.",https://api.openreview.net/pdf/db1b6e5bc8b72c86683c9aae725f36f188279740.pdf,optimization;adaptive;metric;llm,https://scholar.google.com/scholar?q=Enabling+Arbitrary+Translation+Objectives+with+Adaptive+Tree+Search
LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5,2022,ICLR,"['Chengwei Qin', 'Shafiq Joty']",poster,"['lifelong few-shot language Learning', 'prompt tuning', 'pseudo samples', 'knowledge distillation']","Existing approaches to lifelong language learning rely on plenty of labeled data for learning a new task, which is hard to obtain in most real scenarios. Considering that humans can continually learn new tasks from a handful of examples, we expect the models also to be able to generalize well on new few-shot tasks without forgetting the previous ones. In this work, we define this more challenging yet practical problem as Lifelong Few-shot Language Learning (LFLL) and propose a unified framework for it based on prompt tuning of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot learning ability, and simultaneously trains the model as a task solver and a data generator. Before learning a new domain of the same task type, LFPT5 generates pseudo (labeled) samples of previously learned domains, and later gets trained on those samples to alleviate forgetting of previous knowledge as it learns the new domain. In addition, a KL divergence loss is minimized to achieve label consistency between the previous and the current model. While adapting to a new task type, LFPT5 includes and tunes additional prompt embeddings for the new task. With extensive experiments, we demonstrate that LFPT5 can be applied to various different types of tasks and significantly outperform previous methods in different LFLL settings.",https://api.openreview.net/pdf/a61cfe769dd99962e037c723c20da960f1378949.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=LFPT5:+A+Unified+Framework+for+Lifelong+Few-shot+Language+Learning+Based+on+Prompt+Tuning+of+T5
Parallel Training of GRU Networks with a Multi-Grid Solver for Long Sequences,2022,ICLR,"['Euhyun Moon', 'Eric C Cyr']",poster,"['GRU', 'MGRIT', 'parallel-in-time', 'distributed machine learning']","Parallelizing Gated Recurrent Unit (GRU) is a challenging task, as the training procedure of GRU is inherently sequential. Prior efforts to parallelize GRU have largely focused on conventional parallelization strategies such as data-parallel and model-parallel training algorithms. However, when the given sequences are very long, existing approaches are still inevitably performance limited in terms of both training time and model accuracy. In this paper, we present a novel parallel training scheme (called  parallel-in-time) for GRU based on a multigrid reduction in time (MGRIT) solver. MGRIT partitions a sequence into multiple shorter sub-sequences and trains the sub-sequences on different processors in parallel. The key to achieving speedup is a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset, where each video is an image sequence, demonstrate that a new parallel training scheme of GRU achieves up to $6.5 \times$ speedup over a serial approach. As efficiency of our new parallelization strategy is associated with the sequence length, our parallel GRU algorithm achieves significant performance improvement as the length of sequence increases. Further, the proposed approach can be applied simultaneously with batch and other forms of model parallelism.",https://api.openreview.net/pdf/8571a323c3024175c5f0d27227e6c74f594b1290.pdf,graph;llm,https://scholar.google.com/scholar?q=Parallel+Training+of+GRU+Networks+with+a+Multi-Grid+Solver+for+Long+Sequences
Towards Continual Knowledge Learning of Language Models,2022,ICLR,"['Joel Jang', 'Seonghyeon Ye', 'Sohee Yang', 'Joongbo Shin', 'Janghoon Han', 'Gyeonghun KIM', 'Stanley Jungkyu Choi', 'Minjoon Seo']",poster,"['continual learning', 'knowledge acquisition', 'catastrophic forgetting', 'large language models', 'pretraining', 'natural language processing']","Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs.",https://api.openreview.net/pdf/873039fe2b300e8ad8b1dd3a34054bad1386f1a1.pdf,graph;metric;llm,https://scholar.google.com/scholar?q=Towards+Continual+Knowledge+Learning+of+Language+Models
Convergent Graph Solvers,2022,ICLR,"['Junyoung Park', 'Jinhyun Choo', 'Jinkyoo Park']",poster,"['Graph', 'Graph Neural Network', 'Fixed point', 'Implicit model', 'Implicit function theorem', 'Convergent']","We propose the convergent graph solver (CGS), a deep learning method that learns iterative mappings to predict the properties of a graph system at its stationary state (fixed point) with guaranteed convergence. The forward propagation of CGS proceeds in three steps: (1) constructing the input-dependent linear contracting iterative maps, (2) computing the fixed points of the iterative maps, and (3) decoding the fixed points to estimate the properties. The contractivity of the constructed linear maps guarantees the existence and uniqueness of the fixed points following the Banach fixed point theorem. To train CGS efficiently, we also derive a tractable analytical expression for its gradient by leveraging the implicit function theorem. We evaluate the performance of CGS by applying it to various network-analytic and graph benchmark problems. The results indicate that CGS has competitive capabilities for predicting the stationary properties of graph systems,  irrespective of whether the target systems are linear or non-linear. CGS also shows high performance for graph classification problems where the existence or the meaning of a fixed point is hard to be clearly defined, which highlights the potential of CGS as a general graph neural network architecture.",https://api.openreview.net/pdf/fb18728414ffbbdaa143c3e893d041eb10d254ce.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Convergent+Graph+Solvers
Dealing with Non-Stationarity in MARL via Trust-Region Decomposition,2022,ICLR,"['Wenhao Li', 'Xiangfeng Wang', 'Bo Jin', 'Junjie Sheng', 'Hongyuan Zha']",poster,"['Nonstationarity', 'Trust-Region Methods', 'Multi-Agent Reinforcement Learning']","Non-stationarity is one thorny issue in cooperative multi-agent reinforcement learning (MARL). One of the reasons is the policy changes of agents during the learning process. Some existing works have discussed various consequences caused by non-stationarity with several kinds of measurement indicators. This makes the objectives or goals of existing algorithms are inevitably inconsistent and disparate. In this paper, we introduce a novel notion, the $\delta$-$stationarity$ measurement, to explicitly measure the non-stationarity of a policy sequence, which can be further proved to be bounded by the KL-divergence of consecutive joint policies. A straightforward but highly non-trivial way is to control the joint policies' divergence, which is difficult to estimate accurately by imposing the trust-region constraint on the joint policy. Although it has lower computational complexity to decompose the joint policy and impose trust-region constraints on the factorized policies, simple policy factorization like mean-field approximation will lead to more considerable policy divergence, which can be considered as the trust-region decomposition dilemma. We model the joint policy as a pairwise Markov random field and propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established by adjusting the trust-region of the local policies adaptively in an end-to-end manner. MAMT can approximately constrain the consecutive joint policies' divergence to satisfy $\delta$-stationarity and alleviate the non-stationarity problem. Our method can bring noticeable and stable performance improvement compared with baselines in cooperative tasks of different complexity.",https://api.openreview.net/pdf/533ea53e5b32c81e14b06b4528f54a68836c63a0.pdf,reinforcement learning;optimization;zero_few-shot;adaptive;multi-agent;llm,https://scholar.google.com/scholar?q=Dealing+with+Non-Stationarity+in+MARL+via+Trust-Region+Decomposition
Auto-scaling Vision Transformers without Training,2022,ICLR,"['Wuyang Chen', 'Wei Huang', 'Xianzhi Du', 'Xiaodan Song', 'Zhangyang Wang', 'Denny Zhou']",poster,"['vision transformer', 'neural architecture search', 'training-free search', 'efficient training']","This work targets automated designing and scaling of Vision Transformers (ViTs). The motivation comes from two pain spots: 1) the lack of efficient and principled methods for designing and scaling ViTs; 2) the tremendous computational cost of training ViT that is much heavier than its convolution counterpart. To tackle these issues, we propose As-ViT, an auto-scaling framework for ViTs without training, which automatically discovers and scales up ViTs in an efficient and principled manner. Specifically, we first design a ""seed"" ViT topology by leveraging a training-free search process. This extremely fast search is fulfilled by a comprehensive study of ViT's network complexity, yielding a strong Kendall-tau correlation with ground-truth accuracies. Second, starting from the ""seed"" topology, we automate the scaling rule for ViTs by growing widths/depths to different ViT layers. This results in a series of architectures with different numbers of parameters in a single run. Finally, based on the observation that ViTs can tolerate coarse tokenization in early training stages, we propose a progressive tokenization strategy to train ViTs faster and cheaper. As a unified framework, As-ViT achieves strong performance on classification (83.5% top1 on ImageNet-1k) and detection (52.7% mAP on COCO) without any manual crafting nor scaling of ViT architectures: the end-to-end model design and scaling process costs only 12 hours on one V100 GPU. Our code is available at https://github.com/VITA-Group/AsViT.",https://api.openreview.net/pdf/ef4c46fde37c64720cfb924254715a045c828420.pdf,graph;transformer;llm,https://scholar.google.com/scholar?q=Auto-scaling+Vision+Transformers+without+Training
Meta Learning Low Rank Covariance Factors for Energy Based Deterministic Uncertainty,2022,ICLR,"['Jeffrey Ryan Willette', 'Hae Beom Lee', 'Juho Lee', 'Sung Ju Hwang']",poster,"['calibration', 'meta-learning']","Numerous recent works utilize bi-Lipschitz regularization of neural network layers to preserve relative distances between data instances in the feature spaces of each layer. This distance sensitivity with respect to the data aids in tasks such as uncertainty calibration and out-of-distribution (OOD) detection. In previous works, features extracted with a distance sensitive model are used to construct feature covariance matrices which are used in deterministic uncertainty estimation or OOD detection. However, in cases where there is a distribution over tasks, these methods result in covariances which are sub-optimal, as they may not leverage all of the meta information which can be shared among tasks. With the use of an attentive set encoder, we propose to meta learn either diagonal or diagonal plus low-rank factors to efficiently construct task specific covariance matrices. Additionally, we propose an inference procedure which utilizes scaled energy to achieve a final predictive distribution which is well calibrated under a distributional dataset shift. ",https://api.openreview.net/pdf/62e7fc9d01f6386911b5a5f46df746fe07a8c819.pdf,zero_few-shot;inference;meta-learning;low-rank;llm,https://scholar.google.com/scholar?q=Meta+Learning+Low+Rank+Covariance+Factors+for+Energy+Based+Deterministic+Uncertainty
Vitruvion: A Generative Model of Parametric CAD Sketches,2022,ICLR,"['Ari Seff', 'Wenda Zhou', 'Nick Richardson', 'Ryan P Adams']",poster,"['generative modeling', 'CAD', 'transformers', 'design', 'geometric constraints']","Parametric computer-aided design (CAD) tools are the predominant way that engineers specify physical structures, from bicycle pedals to airplanes to printed circuit boards. The key characteristic of parametric CAD is that design intent is encoded not only via geometric primitives, but also by parameterized constraints between the elements. This relational specification can be viewed as the construction of a constraint program, allowing edits to coherently propagate to other parts of the design. Machine learning offers the intriguing possibility of accelerating the design process via generative modeling of these structures, enabling new tools such as autocompletion, constraint inference, and conditional synthesis. In this work, we present such an approach to generative modeling of parametric CAD sketches, which constitute the basic computational building blocks of modern mechanical design. Our model, trained on real-world designs from the SketchGraphs dataset, autoregressively synthesizes sketches as sequences of primitives, with initial coordinates, and constraints that reference back to the sampled primitives. As samples from the model match the constraint graph representation used in standard CAD software, they may be directly imported, solved, and edited according to downstream design tasks. In addition, we condition the model on various contexts, including partial sketches (primers) and images of hand-drawn sketches. Evaluation of the proposed approach demonstrates its ability to synthesize realistic CAD sketches and its potential to aid the mechanical design workflow.",https://api.openreview.net/pdf/38ea22fe4cd42ce014e63702972f486b3c0b3995.pdf,graph;optimization;zero_few-shot;representation;generative model;inference;metric;flow;llm,https://scholar.google.com/scholar?q=Vitruvion:+A+Generative+Model+of+Parametric+CAD+Sketches
GeneDisco: A Benchmark for Experimental Design in Drug Discovery,2022,ICLR,"['Arash Mehrjou', 'Ashkan Soleymani', 'Andrew Jesson', 'Pascal Notin', 'Yarin Gal', 'Stefan Bauer', 'Patrick Schwab']",poster,"['batch active learning', 'drug discovery', 'benchmark']","In vitro cellular experimentation with genetic interventions, using for example CRISPR technologies, is an essential step in early-stage drug discovery and target validation that serves to assess initial hypotheses about causal associations between biological mechanisms and disease pathologies. With billions of potential hypotheses to test, the experimental design space for in vitro genetic experiments is extremely vast, and the available experimental capacity - even at the largest research institutions in the world - pales in relation to the size of this biological hypothesis space. Machine learning methods, such as active and reinforcement learning, could aid in optimally exploring the vast biological space by integrating prior knowledge from various information sources as well as extrapolating to yet unexplored areas of the experimental design space based on available data. However, there exist no standardised benchmarks and data sets for this challenging task and little research has been conducted in this area to date. Here, we introduce GeneDisco, a benchmark suite for evaluating active learning algorithms for experimental design in drug discovery. GeneDisco contains a curated set of multiple publicly available experimental data sets as well as open-source implementations of state-of-the-art active learning policies for experimental design and exploration.",https://api.openreview.net/pdf/46263f4b010fc363d4351c09cb675b69418c8680.pdf,reinforcement learning;graph;active learning;llm,https://scholar.google.com/scholar?q=GeneDisco:+A+Benchmark+for+Experimental+Design+in+Drug+Discovery
Fast Generic Interaction Detection for Model Interpretability and Compression,2022,ICLR,"['Tianjian Zhang', 'Feng Yin', 'Zhi-Quan Luo']",poster,[],"The ability of discovering feature interactions in a black-box model is vital to explainable deep learning. We propose a principled, global interaction detection method by casting our target as a multi-arm bandits problem and solving it swiftly with the UCB algorithm. This adaptive method is free of ad-hoc assumptions and among the cutting-edge methods with outstanding detection accuracy and stability. Based on the detection outcome, a lightweight and interpretable deep learning model (called ParaACE) is further built using the alternating conditional expectation (ACE) method. Our proposed ParaACE improves the prediction performance by 26 % and reduces the model size by 100+ times as compared to its Teacher model over various datasets. Furthermore, we show the great potential of our method for scientific discovery through interpreting various real datasets in the economics and smart medicine sectors. The code is available at https://github.com/zhangtj1996/ParaACE. ",https://api.openreview.net/pdf/f1581535b01f857847507915f4721abcb1e41529.pdf,adaptive;llm,https://scholar.google.com/scholar?q=Fast+Generic+Interaction+Detection+for+Model+Interpretability+and+Compression
Language model compression with weighted low-rank factorization,2022,ICLR,"['Yen-Chang Hsu', 'Ting Hua', 'Sungen Chang', 'Qian Lou', 'Yilin Shen', 'Hongxia Jin']",poster,"['model compression', 'low-rank approximation', 'transformer', 'language model']","Factorizing a large matrix into small matrices is a popular strategy for model compression. Singular value decomposition (SVD) plays a vital role in this compression strategy, approximating a learned matrix with fewer parameters. However, SVD minimizes the squared error toward reconstructing the original matrix without gauging the importance of the parameters, potentially giving a larger reconstruction error for those who affect the task accuracy more. In other words, the optimization objective of SVD is not aligned with the trained model's task accuracy. We analyze this previously unexplored problem, make observations, and address it by introducing Fisher information to weigh the importance of parameters affecting the model prediction. This idea leads to our method: Fisher-Weighted SVD (FWSVD). Although the factorized matrices from our approach do not result in smaller reconstruction errors, we find that our resulting task accuracy is much closer to the original model's performance. We perform analysis with the transformer-based language models, showing our weighted SVD largely alleviates the mismatched optimization objectives and can maintain model performance with a higher compression rate. Our method can directly compress a task-specific model while achieving better performance than other compact model strategies requiring expensive model pre-training. Moreover, the evaluation of compressing an already compact model shows our method can further reduce 9% to 30% parameters with an insignificant impact on task accuracy.",https://api.openreview.net/pdf/a5edead703a518eda031d7e25734d372b8287883.pdf,graph;optimization;zero_few-shot;transformer;multimodal;low-rank;llm,https://scholar.google.com/scholar?q=Language+model+compression+with+weighted+low-rank+factorization
Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis,2022,ICLR,"['Siyi Tang', 'Jared Dunnmon', 'Khaled Kamal Saab', 'Xuan Zhang', 'Qianying Huang', 'Florian Dubost', 'Daniel Rubin', 'Christopher Lee-Messer']",poster,"['Graph neural network', 'Self-supervision', 'Interpretability', 'Visualization', 'Neuroscience', 'Electroencephalography', 'Seizure', 'Epilepsy', 'Time Series']","Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in prior automated seizure detection and classification studies: (1) representing non-Euclidean data structure in EEGs, (2) accurately classifying rare seizure types, and (3) lacking a quantitative interpretability approach to measure model ability to localize seizures. In this study, we address these challenges by (1) representing the spatiotemporal dependencies in EEGs using a graph neural network (GNN) and proposing two EEG graph structures that capture the electrode geometry or dynamic brain connectivity, (2) proposing a self-supervised pre-training method that predicts preprocessed signals for the next time period to further improve model performance, particularly on rare seizure types, and (3) proposing a quantitative model interpretability approach to assess a model’s ability to localize seizures within EEGs. When evaluating our approach on seizure detection and classification on a large public dataset (5,499 EEGs), we find that our GNN with self-supervised pre-training achieves 0.875 Area Under the Receiver Operating Characteristic Curve on seizure detection and 0.749 weighted F1-score on seizure classification, outperforming previous methods for both seizure detection and classification. Moreover, our self-supervised pre-training strategy significantly improves classification of rare seizure types (e.g. 47 points increase in combined tonic seizure accuracy over baselines). Furthermore, quantitative interpretability analysis shows that our GNN with self-supervised pre-training precisely localizes 25.4% focal seizures, a 21.9 point improvement over existing CNNs. Finally, by superimposing the identified seizure locations on both raw EEG signals and EEG graphs, our approach could provide clinicians with an intuitive visualization of localized seizure regions.",https://api.openreview.net/pdf/17a7d200331982e9e2906bc6831d4cdc744a6f5c.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Self-Supervised+Graph+Neural+Networks+for+Improved+Electroencephalographic+Seizure+Analysis
No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models,2022,ICLR,"['Chen Liang', 'Haoming Jiang', 'Simiao Zuo', 'Pengcheng He', 'Xiaodong Liu', 'Jianfeng Gao', 'Weizhu Chen', 'Tuo Zhao']",poster,"['Training Large Transformer Models', 'Reducing Model Redundancy', 'Parameter Sensitivity', 'Adaptive Learning Rate Method', 'Model Generalization', 'Model Pruning']","Recent research has shown the existence of significant redundancy in large Transformer models. One can prune the redundant parameters without significantly sacrificing the generalization performance. However, we question whether the redundant parameters could have contributed more if they were properly trained. To answer this question, we propose a novel training strategy that encourages all parameters to be trained sufficiently. Specifically, we adaptively adjust the learning rate for each parameter according to its sensitivity, a robust gradient-based measure reflecting this parameter's contribution to the model performance. A parameter with low sensitivity is redundant, and we improve its fitting by increasing its learning rate. In contrast, a parameter with high sensitivity is well-trained, and we regularize it by decreasing its learning rate to prevent further overfitting. We conduct extensive experiments on natural language understanding, neural machine translation, and image classification to demonstrate the effectiveness of the proposed schedule. Analysis shows that the proposed schedule indeed reduces the redundancy and improves generalization performance.",https://api.openreview.net/pdf/f29d145db699800c70bb362bb205f16575e30db7.pdf,zero_few-shot;transformer;adaptive;llm,https://scholar.google.com/scholar?q=No+Parameters+Left+Behind:+Sensitivity+Guided+Adaptive+Learning+Rate+for+Training+Large+Transformer+Models
The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training,2022,ICLR,"['Shiwei Liu', 'Tianlong Chen', 'Xiaohan Chen', 'Li Shen', 'Decebal Constantin Mocanu', 'Zhangyang Wang', 'Mykola Pechenizkiy']",poster,"['random pruning', 'sparse training', 'static sparse training', 'layer-wise sparsities', 'dynamic sparse training']","Random pruning is arguably the most naive way to attain sparsity in neural networks, but has been deemed uncompetitive by either post-training pruning or sparse training. In this paper, we focus on sparse training and highlight a perhaps counter-intuitive finding, that random pruning at initialization can be quite powerful for the sparse training of modern neural networks. Without any delicate pruning criteria or carefully pursued sparsity structures, we empirically demonstrate that sparsely training a randomly pruned network from scratch can match the performance of its dense equivalent. There are two key factors that contribute to this revival: (i) $the network sizes matter$: as the original dense networks grow wider and deeper, the performance of training a randomly pruned sparse network will quickly grow to matching that of its dense equivalent, even at high sparsity ratios; (ii) $appropriate layer-wise sparsity ratios$ can be pre-chosen for sparse training, which shows to be another important performance booster. Simple as it looks,  a randomly pruned subnetwork of Wide ResNet-50 can be sparsely trained to outperforming a dense Wide ResNet-50, on ImageNet. We also observed such randomly pruned networks outperform dense counterparts in other favorable aspects, such as out-of-distribution detection, uncertainty estimation, and adversarial robustness. Overall, our results strongly suggest there is larger-than-expected room for sparse training at scale, and the benefits of sparsity might be more universal beyond carefully designed pruning. Our source code can be found at https://github.com/VITA-Group/Random_Pruning.
",https://api.openreview.net/pdf/92cd52f575aab3aac6f6934474fa0fb6f56652b9.pdf,graph;sparse;llm,https://scholar.google.com/scholar?q=The+Unreasonable+Effectiveness+of+Random+Pruning:+Return+of+the+Most+Naive+Baseline+for+Sparse+Training
Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?,2022,ICLR,"['Matthew Farrell', 'Blake Bordelon', 'Shubhendu Trivedi', 'Cengiz Pehlevan']",poster,"['representation learning', 'perceptron capacity', 'perceptual manifolds', 'equivariance', ""cover's theorem"", 'vc dimension']","Equivariance has emerged as a desirable property of representations of objects subject to identity-preserving transformations that constitute a group, such as translations and rotations. However, the expressivity of a representation constrained by group equivariance is still not fully understood. We address this gap by providing a generalization of Cover's Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects. We find that the fraction of separable dichotomies is determined by the dimension of the space that is fixed by the group action. We show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. While other operations do not change the fraction of separable dichotomies, local pooling decreases the fraction, despite being a highly nonlinear operation. Finally, we test our theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement.",https://api.openreview.net/pdf/6d3dd8ced80d47d56918b5e4c597c76e355e44fb.pdf,optimization;representation;online learning;llm,https://scholar.google.com/scholar?q=Capacity+of+Group-invariant+Linear+Readouts+from+Equivariant+Representations:+How+Many+Objects+can+be+Linearly+Classified+Under+All+Possible+Views?
Towards Deepening Graph Neural Networks: A GNTK-based Optimization Perspective,2022,ICLR,"['Wei Huang', 'Yayong Li', 'weitao Du', 'Richard Xu', 'Jie Yin', 'Ling Chen', 'Miao Zhang']",poster,"['Trainablity', 'Graph Neural Tangent Kernel', 'Critical DropEdge']","Graph convolutional networks (GCNs) and their variants have achieved great success in dealing with graph-structured data. Nevertheless, it is well known that deep GCNs suffer from the over-smoothing problem, where node representations tend to be indistinguishable as more layers are stacked up. The theoretical research to date on deep GCNs has focused primarily on expressive power rather than trainability, an optimization perspective. Compared to expressivity, trainability attempts to address a more fundamental question: Given a sufficiently expressive space of models, can we successfully find a good solution via gradient descent-based optimizers? This work fills this gap by exploiting the Graph Neural Tangent Kernel (GNTK), which governs the optimization trajectory under gradient descent for wide GCNs. We formulate the asymptotic behaviors of GNTK in the large depth, which enables us to reveal the dropping trainability of wide and deep GCNs at an exponential rate in the optimization process. Additionally, we extend our theoretical framework to analyze residual connection-based techniques, which are found to be merely able to mitigate the exponential decay of trainability mildly. Inspired by our theoretical insights on trainability, we propose Critical DropEdge, a connectivity-aware and graph-adaptive sampling method, to alleviate the exponential decay problem more fundamentally. Experimental evaluation consistently confirms using our proposed method can achieve better results compared to relevant counterparts with both infinite-width and finite-width. ",https://api.openreview.net/pdf/c18075d5d39ff0265c97344cea20791494daa455.pdf,graph;optimization;representation;adaptive;llm,https://scholar.google.com/scholar?q=Towards+Deepening+Graph+Neural+Networks:+A+GNTK-based+Optimization+Perspective
Connectome-constrained Latent Variable Model of Whole-Brain Neural Activity,2022,ICLR,"['Lu Mi', 'Richard Xu', 'Sridhama Prakhya', 'Albert Lin', 'Nir Shavit', 'Aravinthan Samuel', 'Srinivas C Turaga']",poster,"['connectome', 'latent-variable model', 'variational autoencoder', 'biophysics', 'whole-brain', 'neural activity', 'calcium imaging', 'caenorhabditis elegans', 'voltage', 'generative model', 'inference network']","The availability of both anatomical connectivity and brain-wide neural activity measurements in C. elegans make the worm a promising system for learning detailed, mechanistic models of an entire nervous system in a data-driven way. However, one faces several challenges when constructing such a model. We often do not have direct experimental access to important modeling details such as single-neuron dynamics and the signs and strengths of the synaptic connectivity. Further, neural activity can only be measured in a subset of neurons, often indirectly via calcium imaging, and significant trial-to-trial variability has been observed. To address these challenges, we introduce a connectome-constrained latent variable model (CC-LVM) of the unobserved voltage dynamics of the entire C. elegans nervous system and the observed calcium signals. We used the framework of variational autoencoders to fit parameters of the mechanistic simulation constituting the generative model of the LVM to calcium imaging observations. A variational approximate posterior distribution over latent voltage traces for all neurons is efficiently inferred using an inference network, and constrained by a prior distribution given by the biophysical simulation of neural dynamics. We applied this model to an experimental whole-brain dataset, and found that connectomic constraints enable our LVM to predict the activity of neurons whose activity were withheld significantly better than models unconstrained by a connectome. We explored models with different degrees of biophysical detail, and found that models with realistic conductance-based synapses provide markedly better predictions than current-based synapses for this system.",https://api.openreview.net/pdf/302ccb9e518f0da30bda16c4db8038467af5f38e.pdf,graph;optimization;vae;generative model;inference;llm,https://scholar.google.com/scholar?q=Connectome-constrained+Latent+Variable+Model+of+Whole-Brain+Neural+Activity
T-WaveNet: A Tree-Structured Wavelet Neural Network for Time Series Signal Analysis,2022,ICLR,"['Minhao LIU', 'Ailing Zeng', 'Qiuxia LAI', 'Ruiyuan Gao', 'Min Li', 'Jing Qin', 'Qiang Xu']",poster,[],"Time series signal analysis plays an essential role in many applications, e.g., activity recognition and healthcare monitoring.
Recently, features extracted with deep neural networks (DNNs) have shown to be more effective than conventional hand-crafted ones.
However, most existing solutions rely solely on the network to extract information carried in the raw signal, regardless of its inherent physical and statistical properties, leading to sub-optimal performance particularly under a limited amount of training data.
In this work, we propose a novel tree-structured wavelet neural network for time series signal analysis, namely \emph{T-WaveNet}, taking advantage of an inherent property of various types of signals, known as the \emph{dominant frequency range}. Specifically, with \emph{T-WaveNet}, we first conduct frequency spectrum energy analysis of the signals to get a set of dominant frequency subbands. Then, we construct a tree-structured network that iteratively decomposes the input signal into various frequency subbands with similar energies. Each node on the tree is built with an invertible neural network (INN) based wavelet transform unit. Such a disentangled representation learning method facilitates a more effective extraction of the discriminative features, as demonstrated with the comprehensive experiments on various real-life time series classification datasets. ",https://api.openreview.net/pdf/c527f1cebdb3737b281ff65a78f914a97d634bad.pdf,representation;llm,https://scholar.google.com/scholar?q=T-WaveNet:+A+Tree-Structured+Wavelet+Neural+Network+for+Time+Series+Signal+Analysis
P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts,2022,ICLR,"['Benjamin Newman', 'Prafulla Kumar Choubey', 'Nazneen Rajani']",poster,"['NLP', 'Prompting', 'Commonsense', 'information extraction', 'factual extraction', 'Large Language Models']","Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of the factual information extracted from Large Language Models (LLMs) depends on the prompts used to query them. This inconsistency is problematic because different users will query LLMs for the same information using different wording, but should receive the same, accurate responses regardless. In this work we aim to address this shortcoming by introducing P-Adapters: lightweight models that sit between the embedding layer and first attention layer of LLMs. They take LLM embeddings as input and output continuous prompts that are used to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (the ""experts"") and select one to query the LLM. These require a separate classifier trained on human-annotated data to map natural language prompts to the continuous ones. P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations. P-Adapters show between 12-26% absolute improvement in precision and 36-50% absolute improvement in consistency over a baseline of just using natural language queries alone. Finally, we investigate what makes P-Adapters successful and conclude that a significant factor is access to the LLM's embeddings of the original natural language prompt, particularly the subject of the entity pair being queried.",https://api.openreview.net/pdf/8e2c7114bf23dadb13338c9b0dcf063a536ff1b3.pdf,graph;transformer;llm,https://scholar.google.com/scholar?q=P-Adapters:+Robustly+Extracting+Factual+Information+from+Language+Models+with+Diverse+Prompts
Step-unrolled Denoising Autoencoders for Text Generation,2022,ICLR,"['Nikolay Savinov', 'Junyoung Chung', 'Mikolaj Binkowski', 'Erich Elsen', 'Aaron van den Oord']",poster,"['generative models', 'text generation', 'denoising autoencoders']","In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models. Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a sequence of tokens, starting from random inputs and improving them each time until convergence. We present a simple new improvement operator that converges in fewer iterations than diffusion methods, while qualitatively producing better samples on natural language datasets. SUNDAE achieves state-of-the-art results (among non-autoregressive methods) on the WMT'14 English-to-German translation task and good qualitative results on unconditional language modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python code from GitHub. The non-autoregressive nature of SUNDAE opens up possibilities beyond left-to-right prompted generation, by filling in arbitrary blank patterns in a template.",https://api.openreview.net/pdf/17dfe8a5d0e1934ade282b40fbfe8f2fe421ef39.pdf,generative model;llm,https://scholar.google.com/scholar?q=Step-unrolled+Denoising+Autoencoders+for+Text+Generation
LoRA: Low-Rank Adaptation of Large Language Models,2022,ICLR,"['Edward J Hu', 'yelong shen', 'Phillip Wallis', 'Zeyuan Allen-Zhu', 'Yuanzhi Li', 'Shean Wang', 'Lu Wang', 'Weizhu Chen']",poster,"['Transfer learning', 'Adaptation', 'Transformer', 'Fine-tuning', 'Low-rank', 'RoBERTa', 'DeBERTa', 'GPT-2', 'GPT-3']","An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible.
Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",https://api.openreview.net/pdf/5a54aed5265cb0399c62848f44e84c4a617a354b.pdf,zero_few-shot;transformer;inference;low-rank;llm,https://scholar.google.com/scholar?q=LoRA:+Low-Rank+Adaptation+of+Large+Language+Models
Learning 3D Representations of Molecular Chirality with Invariance to Bond Rotations,2022,ICLR,"['Keir Adams', 'Lagnajit Pattanaik', 'Connor W. Coley']",poster,"['geometric deep learning', 'equivariance', 'molecules']","Molecular chirality, a form of stereochemistry most often describing relative spatial arrangements of bonded neighbors around tetrahedral carbon centers, influences the set of 3D conformers accessible to the molecule without changing its 2D graph connectivity. Chirality can strongly alter (bio)chemical interactions, particularly protein-drug binding. Most 2D graph neural networks (GNNs) designed for molecular property prediction at best use atomic labels to naïvely treat chirality, while E(3)-invariant 3D GNNs are invariant to chirality altogether. To enable representation learning on molecules with defined stereochemistry, we design an SE(3)-invariant model that processes torsion angles of a 3D molecular conformer. We explicitly model conformational flexibility by integrating a novel type of invariance to rotations about internal molecular bonds into the architecture, mitigating the need for multi-conformer data augmentation. We test our model on four benchmarks: contrastive learning to distinguish conformers of different stereoisomers in a learned latent space, classification of chiral centers as R/S, prediction of how enantiomers rotate circularly polarized light, and ranking enantiomers by their docking scores in an enantiosensitive protein pocket. We compare our model, Chiral InterRoto-Invariant Neural Network (ChIRo), with 2D and 3D GNNs to demonstrate that our model achieves state of the art performance when learning chiral-sensitive functions from molecular structures.",https://api.openreview.net/pdf/0bd5d5f64a3a1afccdb03997dd1aad21fe9c2aaa.pdf,graph;representation;contrastive learning;augmentation;3d;llm,https://scholar.google.com/scholar?q=Learning+3D+Representations+of+Molecular+Chirality+with+Invariance+to+Bond+Rotations
Unified Visual Transformer Compression,2022,ICLR,"['Shixing Yu', 'Tianlong Chen', 'Jiayi Shen', 'Huan Yuan', 'Jianchao Tan', 'Sen Yang', 'Ji Liu', 'Zhangyang Wang']",poster,"['Vision Transformer', 'Model Compression', 'Pruning', 'Layer Skipping', 'Distillation']","Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing success in compressing convolutional neural networks, the study of Vision Transformer compression has also just emerged, and existing works focused on one or two aspects of compression. This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation. We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios/masks, and skip configurations, under a distillation loss. The optimization problem is then solved using the primal-dual algorithm. Experiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors. For example, DeiT-Tiny can be trimmed down to 50\% of the original FLOPs almost without losing accuracy. Codes are available online:~\url{https://github.com/VITA-Group/UVC}.",https://api.openreview.net/pdf/8570214b832776a06b451827ee429b3eba50359a.pdf,graph;optimization;transformer;online learning;distillation;llm,https://scholar.google.com/scholar?q=Unified+Visual+Transformer+Compression
Graph-Guided Network for Irregularly Sampled Multivariate Time Series,2022,ICLR,"['Xiang Zhang', 'Marko Zeman', 'Theodoros Tsiligkaridis', 'Marinka Zitnik']",poster,"['time seres', 'irregular time series', 'graph neural networks', 'attention mechanism', 'time series classification', 'multivariate time series', 'representation learning', 'embeddings']","In many domains, including healthcare, biology, and climate science, time series are irregularly sampled with varying time intervals between successive readouts and different subsets of variables (sensors) observed at different time points. Here, we introduce RAINDROP, a graph neural network that embeds irregularly sampled and multivariate time series while also learning the dynamics of sensors purely from observational data. RAINDROP represents every sample as a separate sensor graph and models time-varying dependencies between sensors with a novel message passing operator. It estimates the latent sensor graph structure and leverages the structure together with nearby observations to predict misaligned readouts. This model can be interpreted as a graph neural network that sends messages over graphs that are optimized for capturing time-varying dependencies among sensors. We use RAINDROP to classify time series and interpret temporal dynamics on three healthcare and human activity datasets. RAINDROP outperforms state-of-the-art methods by up to 11.4% (absolute F1-score points), including techniques that deal with irregular sampling using fixed discretization and set functions. RAINDROP shows superiority in diverse setups, including challenging leave-sensor-out settings. ",https://api.openreview.net/pdf/6486d4fdf5e5db05e22c8ecd34aaab8401569f24.pdf,graph;multimodal;llm,https://scholar.google.com/scholar?q=Graph-Guided+Network+for+Irregularly+Sampled+Multivariate+Time+Series
Pre-training Molecular Graph Representation with 3D Geometry,2022,ICLR,"['Shengchao Liu', 'Hanchen Wang', 'Weiyang Liu', 'Joan Lasenby', 'Hongyu Guo', 'Jian Tang']",poster,"['Pre-training', 'SSL', 'Molecule', '3D Geometry', '2D representation']","Molecular graph representation learning is a fundamental problem in modern drug and material discovery. Molecular graphs are typically modeled by their 2D topological structures, but it has been recently discovered that 3D geometric information plays a more vital role in predicting molecular functionalities. However, the lack of 3D information in real-world scenarios has significantly impeded the learning of geometric graph representation. To cope with this challenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework where self-supervised learning (SSL) is performed by leveraging the correspondence and consistency between 2D topological structures and 3D geometric views. GraphMVP effectively learns a 2D molecular graph encoder that is enhanced by richer and more discriminative 3D geometry. We further provide theoretical insights to justify the effectiveness of GraphMVP. Finally, comprehensive experiments show that GraphMVP can consistently outperform existing graph SSL methods. Code is available on GitHub: https://github.com/chao1224/GraphMVP.",https://api.openreview.net/pdf/a22cd0fe68ecdc04b0ffcd2cdeda6ba42dc8dffd.pdf,graph;zero_few-shot;representation;metric;multi-view;3d;llm,https://scholar.google.com/scholar?q=Pre-training+Molecular+Graph+Representation+with+3D+Geometry
An Explanation of In-context Learning as Implicit Bayesian Inference,2022,ICLR,"['Sang Michael Xie', 'Aditi Raghunathan', 'Percy Liang', 'Tengyu Ma']",poster,"['in-context learning', 'language modeling', 'pre-training', 'GPT-3']","Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.",https://api.openreview.net/pdf/34aa1a37f7afecc48d15dbab476f32a40db2fe1a.pdf,graph;zero_few-shot;transformer;bayesian;inference;llm,https://scholar.google.com/scholar?q=An+Explanation+of+In-context+Learning+as+Implicit+Bayesian+Inference
Semi-relaxed Gromov-Wasserstein divergence and applications on graphs,2022,ICLR,"['Cédric Vincent-Cuaz', 'Rémi Flamary', 'Marco Corneli', 'Titouan Vayer', 'Nicolas Courty']",poster,"['Optimal Transport', 'Graph Learning']","Comparing structured objects such as graphs is a fundamental operation
involved in many learning tasks. To this end, the Gromov-Wasserstein (GW)
distance, based on Optimal Transport (OT), has proven to be successful in
handling the specific nature of the associated objects. More specifically,
through the nodes connectivity relations, GW operates on graphs, seen as
probability measures over specific spaces. At the core of OT is the idea of
conservation of mass, which imposes a coupling between all the nodes from
the two considered graphs. We argue in this paper that this property can be
detrimental for tasks such as graph dictionary or partition learning, and we
relax it by proposing a new semi-relaxed Gromov-Wasserstein divergence.
Aside from immediate computational benefits, we discuss its properties, and
show that it can lead to an efficient graph dictionary learning algorithm.
We empirically demonstrate its relevance for complex tasks on graphs such as
partitioning, clustering and completion.",https://api.openreview.net/pdf/fe28f3e616fabe8ac2ea06baba193a597fc572a7.pdf,graph;llm,https://scholar.google.com/scholar?q=Semi-relaxed+Gromov-Wasserstein+divergence+and+applications+on+graphs
ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models,2022,ICLR,"['Louis Rouillard', 'Demian Wassermann']",poster,"['Bayesian inference', 'Hierarchical Bayesian Models', 'structured Variational Inference', 'Simulation Based Inference', 'Inference amortization', 'Neuroimaging']","Frequently, population studies feature pyramidally-organized data represented using Hierarchical Bayesian Models (HBM) enriched with plates. These models can become prohibitively large in settings such as neuroimaging, where a sample is composed of a functional MRI signal measured on 300 brain locations, across 4 measurement sessions, and 30 subjects, resulting in around 1 million latent parameters.

Such high dimensionality hampers the usage of modern, expressive flow-based techniques.

To infer parameter posterior distributions in this challenging class of problems, we designed a novel methodology that automatically produces a variational family dual to a target HBM. This variational family, represented as a neural network, consists in the combination of an attention-based hierarchical encoder feeding summary statistics to a set of normalizing flows. Our automatically-derived neural network exploits exchangeability in the plate-enriched HBM and factorizes its parameter space. The resulting architecture reduces by orders of magnitude its parameterization with respect to that of a typical flow-based representation, while maintaining expressivity.

Our method performs inference on the specified HBM in an amortized setup: once trained, it can readily be applied to a new data sample to compute the parameters' full posterior.

We demonstrate the capability and scalability of our method on simulated data, as well as a challenging high-dimensional brain parcellation experiment. We also open up several questions that lie at the intersection between normalizing flows, SBI, structured Variational Inference, and inference amortization.",https://api.openreview.net/pdf/5e3287e6e246a5cf89ad2b2824e72a35f115d662.pdf,graph;zero_few-shot;transformer;representation;bayesian;inference;flow;llm,https://scholar.google.com/scholar?q=ADAVI:+Automatic+Dual+Amortized+Variational+Inference+Applied+To+Pyramidal+Bayesian+Models
"Peek-a-Boo: What (More) is Disguised in a Randomly Weighted Neural Network, and How to Find It Efficiently",2022,ICLR,"['Xiaohan Chen', 'Jason Zhang', 'Zhangyang Wang']",poster,"['Sparse Neural Network', 'Lottery Ticket Hypothesis', 'Efficient Machine Leanring']","Sparse neural networks (NNs) are intensively investigated in literature due to their appeal in saving storage, memory, and computational costs. A recent work (Ramanujan et al., 2020) showed that, different from conventional pruning-and-finetuning pipeline, there exist hidden subnetworks in randomly initialized NNs that have good performance without training the weights. However, such ""hidden subnetworks"" have mediocre performances and require an expensive edge-popup algorithm to search for them. In this work, we define an extended class of subnetworks in randomly initialized NNs called disguised subnetworks, which are not only ""hidden"" in the random networks but also ""disguised"" -- hence can only be ""unmasked"" with certain transformations on weights. We argue that the unmasking process plays an important role in enlarging the capacity of the subnetworks and thus grants two major benefits: (i) the disguised subnetworks easily outperform the hidden counterparts; (ii) the unmasking process helps to relax the quality requirement on the sparse subnetwork mask so that the expensive edge-popup algorithm can be replaced with more efficient alternatives. On top of this new concept, we propose a novel two-stage algorithm that plays a Peek-a-Boo (PaB) game to identify the disguised subnetworks with a combination of two operations: (1) searching efficiently for a subnetwork at random initialization; (2) unmasking the disguise by learning to transform the resulting subnetwork's remaining weights. Furthermore, we show that the unmasking process can be efficiently implemented (a) without referring to any latent weights or scores; and (b) by only leveraging approximated gradients, so that the whole training algorithm is computationally light. Extensive experiments with several large models (ResNet-18, ResNet-50, and WideResNet-28) and datasets (CIFAR-10, CIFAR-100 and ImageNet) demonstrate the competency of PaB over edge-popup and other counterparts. Our codes are available at: https://github.com/VITA-Group/Peek-a-Boo.",https://api.openreview.net/pdf/60b7fbc376713b122304e2e0530ea7290974d364.pdf,graph;sparse;llm,"https://scholar.google.com/scholar?q=Peek-a-Boo:+What+(More)+is+Disguised+in+a+Randomly+Weighted+Neural+Network,+and+How+to+Find+It+Efficiently"
Visual hyperacuity with moving sensor and recurrent neural computations,2022,ICLR,"['Alexander Rivkind', 'Or Ram', 'Eldad Assa', 'Michael Kreiserman', 'Ehud Ahissar']",poster,"['visual system', 'convolutional neural networks', 'recurrent neural networks', 'active vision', 'active sensing', 'ocular drift']","Dynamical phenomena, such as recurrent neuronal activity  and perpetual motion of the eye, are typically overlooked in models of bottom-up visual perception. Recent experiments suggest that tiny inter-saccadic eye motion (""fixational drift"") enhances visual  acuity beyond the limit imposed by the density of retinal photoreceptors. Here we hypothesize that such an enhancement is enabled by recurrent neuronal computations in early visual areas. Specifically, we explore a setting involving a low-resolution dynamical sensor that moves with respect to a static scene, with drift-like tiny steps. This setting mimics a dynamical eye viewing objects in perceptually-challenging conditions. The dynamical sensory input is classified by a convolutional neural network with recurrent connectivity added to its lower layers, in analogy to recurrent connectivity in early visual areas.  Applying our system to CIFAR-10 and CIFAR-100 datasets down-sampled via 8x8 sensor, we found that (i) classification accuracy, which is drastically reduced by this down-sampling, is mostly restored to its 32x32 baseline level when using a moving sensor and recurrent connectivity, (ii) in this setting, neurons in the early layers exhibit a wide repertoire of selectivity patterns, spanning the spatiotemporal selectivity space, with neurons preferring different combinations of spatial and temporal patterning, and (iii) curved sensor's trajectories improve  visual acuity compared to straight trajectories, echoing recent experimental findings involving eye-tracking in challenging conditions. Our work sheds light on the possible role of recurrent connectivity in early vision as well as the roles of fixational drift and temporal-frequency selective cells in the visual system. It also proposes a solution for artificial image recognition in settings with limited resolution and multiple time samples, such as in edge AI applications.",https://api.openreview.net/pdf/74d601c5743a4f1b7e2e0c33b63ef39f695a6c4a.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Visual+hyperacuity+with+moving+sensor+and+recurrent+neural+computations
DARA: Dynamics-Aware Reward Augmentation in Offline Reinforcement Learning,2022,ICLR,"['Jinxin Liu', 'Zhang Hongyin', 'Donglin Wang']",poster,[],"Offline reinforcement learning algorithms promise to be applicable in settings where a fixed dataset is available and no new experience can be acquired. However, such formulation is inevitably offline-data-hungry and, in practice, collecting a large offline dataset for one specific task over one specific environment is also costly and laborious. In this paper, we thus 1) formulate the offline dynamics adaptation by using (source) offline data collected from another dynamics to relax the requirement for the extensive (target) offline data, 2) characterize the dynamics shift problem in which prior offline methods do not scale well, and 3) derive a simple dynamics-aware reward augmentation (DARA) framework from both model-free and model-based offline settings. Specifically, DARA emphasizes learning from those source transition pairs that are adaptive for the target environment and mitigates the offline dynamics shift by characterizing state-action-next-state pairs instead of the typical state-action distribution sketched by prior offline RL methods. The experimental evaluation demonstrates that DARA, by augmenting rewards in the source offline dataset, can acquire an adaptive policy for the target environment and yet significantly reduce the requirement of target offline data. With only modest amounts of target offline data, our performance consistently outperforms the prior offline RL methods in both simulated and real-world tasks. ",https://api.openreview.net/pdf/b68b505b8daf0a76ad9d4d5e4cd43976ba9864db.pdf,reinforcement learning;offline reinforcement learning;adaptive;augmentation;llm,https://scholar.google.com/scholar?q=DARA:+Dynamics-Aware+Reward+Augmentation+in+Offline+Reinforcement+Learning
On Robust Prefix-Tuning for Text Classification,2022,ICLR,"['Zonghan Yang', 'Yang Liu']",poster,"['prefix-tuning', 'pretrained language models', 'text classification', 'robustness in NLP', 'optimal control']","Recently, prefix-tuning has gained increasing attention as a parameter-efficient finetuning method for large-scale pretrained language models. The method keeps the pretrained models fixed and only updates the prefix token parameters for each downstream task. Despite being lightweight and modular, prefix-tuning still lacks robustness to textual adversarial attacks. However, most currently developed defense techniques necessitate auxiliary model update and storage, which inevitably hamper the modularity and low storage of prefix-tuning. In this work, we propose a robust prefix-tuning framework that preserves the efficiency and modularity of prefix-tuning. The core idea of our framework is leveraging the layerwise activations of the language model by correctly-classified training data as the standard for additional prefix finetuning. During the test phase, an extra batch-level prefix is tuned for each batch and added to the original prefix for robustness enhancement. Extensive experiments on three text classification benchmarks show that our framework substantially improves robustness over several strong baselines against five textual attacks of different types while maintaining comparable accuracy on clean texts. We also interpret our robust prefix-tuning framework from the optimal control perspective and pose several directions for future research.",https://api.openreview.net/pdf/02dfcabb44949137b40a59f94715b5caa4b12231.pdf,graph;zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=On+Robust+Prefix-Tuning+for+Text+Classification
"MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer",2022,ICLR,"['Sachin Mehta', 'Mohammad Rastegari']",poster,"['Vision transformer', 'Mobile', 'Edge Devices', 'Transformer', 'CNN', 'Efficient Network', 'Detection', 'Segmentation', 'ImageNet']","Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. 

Our source code is open-source and available at: https://github.com/apple/ml-cvnets",https://api.openreview.net/pdf/bbdc0b90daf74d7fa54066200459a863a1f5c4e0.pdf,zero_few-shot;transformer;representation;llm,"https://scholar.google.com/scholar?q=MobileViT:+Light-weight,+General-purpose,+and+Mobile-friendly+Vision+Transformer"
On the Learning and Learnability of Quasimetrics,2022,ICLR,"['Tongzhou Wang', 'Phillip Isola']",poster,"['embedding learning', 'quasimetric learning', 'deep learning']","Our world is full of asymmetries. Gravity and wind can make reaching a place easier than coming back. Social artifacts such as genealogy charts and citation graphs are inherently directed. In reinforcement learning and control, optimal goal-reaching strategies are rarely reversible (symmetrical). Distance functions supported on these asymmetrical structures are called quasimetrics. Despite their common appearance, little research has been done on the learning of quasimetrics. Our theoretical analysis reveals that a common class of learning algorithms, including unconstrained multilayer perceptrons (MLPs), provably fails to learn a quasimetric consistent with training data. In contrast, our proposed Poisson Quasimetric Embedding (PQE) is the first quasimetric learning formulation that both is learnable with gradient-based optimization and enjoys strong performance guarantees. Experiments on random graphs, social graphs, and offline Q-learning demonstrate its effectiveness over many common baselines.",https://api.openreview.net/pdf/e5214f2935d36f9a385665491f63d55204633f1a.pdf,reinforcement learning;offline reinforcement learning;graph;optimization;metric;llm,https://scholar.google.com/scholar?q=On+the+Learning+and+Learnability+of+Quasimetrics
A Relational Intervention Approach for Unsupervised Dynamics Generalization in Model-Based Reinforcement Learning,2022,ICLR,"['Jiaxian Guo', 'Mingming Gong', 'Dacheng Tao']",poster,"['Model-Based Reinforcement Learning', 'Unsupervised Dynamics Generalization']","The generalization of model-based reinforcement learning (MBRL) methods to environments with unseen transition dynamics is an important yet challenging problem.
Existing methods try to extract environment-specified information $Z$ from past transition segments to make the dynamics prediction model generalizable to different dynamics. However, because environments are not labelled, the extracted information inevitably contains redundant information unrelated to the dynamics in transition segments and thus fails to maintain a crucial property of $Z$: $Z$ should be similar in the same environment and dissimilar in different ones. As a result, the learned dynamics prediction function will deviate from the true one, which undermines the generalization ability. To tackle this problem, we introduce an interventional prediction module to estimate the probability of two estimated $\hat{z}_i, \hat{z}_j$ belonging to the same environment.
Furthermore, by utilizing the $Z$'s invariance within a single environment, a relational head is proposed to enforce the similarity between $\hat{{Z}}$ from the same environment. As a result, the redundant information will be reduced in $\hat{Z}$. We empirically show that $\hat{{Z}}$ estimated by our method enjoy less redundant information than previous methods, and such $\hat{{Z}}$  can significantly reduce dynamics prediction errors and improve the performance of model-based RL methods on zero-shot new environments with unseen dynamics. The codes of this method are available at \url{https://github.com/CR-Gjx/RIA}.",https://api.openreview.net/pdf/a6c6a600f9e89fe92c0e2d8df1d09d0a78dd39ad.pdf,reinforcement learning;graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=A+Relational+Intervention+Approach+for+Unsupervised+Dynamics+Generalization+in+Model-Based+Reinforcement+Learning
Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity,2022,ICLR,"['Shiwei Liu', 'Tianlong Chen', 'Zahra Atashgahi', 'Xiaohan Chen', 'Ghada Sokar', 'Elena Mocanu', 'Mykola Pechenizkiy', 'Zhangyang Wang', 'Decebal Constantin Mocanu']",poster,"['efficient ensemble', 'FreeTickets', 'dynamic sparse training', 'deep ensemble', 'dynamic sparsity']","The success of deep ensembles on improving predictive performance, uncertainty estimation, and out-of-distribution robustness has been extensively studied in the machine learning literature. Albeit the promising results, naively training multiple deep neural networks and combining their predictions at inference leads to prohibitive computational costs and memory requirements. Recently proposed efficient ensemble approaches reach the performance of the traditional deep ensembles with significantly lower costs. However, the training resources required by these approaches are still at least the same as training a single dense model. In this work, we draw a unique connection between sparse neural network training and deep ensembles, yielding a novel efficient ensemble learning framework called $FreeTickets$. Instead of training multiple dense networks and averaging them, we directly train sparse subnetworks from scratch and extract diverse yet accurate subnetworks during this efficient, sparse-to-sparse training. Our framework, $FreeTickets$, is defined as the ensemble of these relatively cheap sparse subnetworks. Despite being an ensemble method, $FreeTickets$ has even fewer parameters and training FLOPs than a single dense model. This seemingly counter-intuitive outcome is due to the ultra training/inference efficiency of dynamic sparse training. $FreeTickets$ surpasses the dense baseline in all the following criteria: prediction accuracy, uncertainty estimation, out-of-distribution (OoD) robustness, as well as efficiency for both training and inference. Impressively, $FreeTickets$ outperforms the naive deep ensemble with ResNet50 on ImageNet using around only $1/5$ of the training FLOPs required by the latter. We have released our source code at https://github.com/VITA-Group/FreeTickets.",https://api.openreview.net/pdf/0108f1e168989b51e9c31bd9e515fde8e7473904.pdf,graph;zero_few-shot;inference;sparse;llm,https://scholar.google.com/scholar?q=Deep+Ensembling+with+No+Overhead+for+either+Training+or+Testing:+The+All-Round+Blessings+of+Dynamic+Sparsity
Dynamic Token Normalization improves Vision Transformers,2022,ICLR,"['Wenqi Shao', 'Yixiao Ge', 'Zhaoyang Zhang', 'XUYUAN XU', 'Xiaogang Wang', 'Ying Shan', 'Ping Luo']",poster,"['classification', 'Normalization', 'transformer']","Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN  makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, Swin, and PVT. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN on small ViT by $1.1\%$ top-1 accuracy on ImageNet.",https://api.openreview.net/pdf/078f0ee170896cd9a0cb8a5b1aeb1b6c0c0556b3.pdf,transformer;llm,https://scholar.google.com/scholar?q=Dynamic+Token+Normalization+improves+Vision+Transformers
Symbolic Learning to Optimize: Towards Interpretability and Scalability,2022,ICLR,"['Wenqing Zheng', 'Tianlong Chen', 'Ting-Kuei Hu', 'Zhangyang Wang']",poster,"['Symbolic Regression', 'Learning To Optimize', 'Interpretability']","Recent studies on Learning to Optimize (L2O) suggest a promising path to automating and accelerating the optimization procedure for complicated tasks. Existing L2O models parameterize optimization rules by neural networks, and learn those numerical rules via meta-training. However, they face two common pitfalls: (1) scalability: the numerical rules represented by neural networks create extra memory overhead for applying L2O models, and limits their applicability to optimizing larger tasks; (2) interpretability: it is unclear what each L2O model has learned in its black-box optimization rule, nor is it straightforward to compare different L2O models in an explainable way. To avoid both pitfalls, this paper proves the concept that we can ""kill two birds by one stone"", by introducing the powerful tool of symbolic regression to L2O. In this paper, we establish a holistic symbolic representation and analysis framework for L2O, which yields a series of insights for learnable optimizers. Leveraging our findings, we further propose a lightweight L2O model that can be meta-trained on large-scale problems and outperformed human-designed and tuned optimizers. Our work is set to supply a brand-new perspective to L2O research. Codes are available at: https://github.com/VITA-Group/Symbolic-Learning-To-Optimize.",https://api.openreview.net/pdf/278475a0ef39ae7c0ff2e358f9723083c0b6b342.pdf,graph;optimization;representation;meta-learning;llm,https://scholar.google.com/scholar?q=Symbolic+Learning+to+Optimize:+Towards+Interpretability+and+Scalability
Sparsity Winning Twice: Better Robust Generalization from More Efficient Training,2022,ICLR,"['Tianlong Chen', 'Zhenyu Zhang', 'pengjun wang', 'Santosh Balachandra', 'Haoyu Ma', 'Zehao Wang', 'Zhangyang Wang']",poster,[],"Recent studies demonstrate the deep networks, even robustified by the state-of-the-art adversarial training (AT), still suffer from large robust generalization gaps, in addition to the much more expensive training costs than standard training. In this paper, we investigate this intriguing problem from a new perspective, i.e., $\textit{injecting appropriate forms of sparsity}$ during adversarial training. We introduce two alternatives for sparse adversarial training: (i) $\textit{static sparsity}$, by leveraging recent results from the lottery ticket hypothesis to identify critical sparse subnetworks arising from the early training; (ii) $\textit{dynamic sparsity}$, by allowing the sparse subnetwork to adaptively adjust its connectivity pattern (while sticking to the same sparsity ratio) throughout training. We find both static and dynamic sparse methods to yield win-win: substantially shrinking the robust generalization gap and alleviating the robust overfitting, meanwhile significantly saving training and inference FLOPs. Extensive experiments validate our proposals with multiple network architectures on diverse datasets, including CIFAR-10/100 and Tiny-ImageNet. For example, our methods reduce robust generalization gap and overfitting by $34.44\%$ and $4.02\%$, with comparable robust/standard accuracy boosts and $87.83\%$/$87.82\%$ training/inference FLOPs savings on CIFAR-100 with ResNet-18. Besides, our approaches can be organically combined with existing regularizers, establishing new state-of-the-art results in AT. All codes are included.",https://api.openreview.net/pdf/be814865ef70b50ab161933745ec83c51f8ce075.pdf,graph;zero_few-shot;adaptive;inference;sparse;llm,https://scholar.google.com/scholar?q=Sparsity+Winning+Twice:+Better+Robust+Generalization+from+More+Efficient+Training
Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice,2022,ICLR,"['Peihao Wang', 'Wenqing Zheng', 'Tianlong Chen', 'Zhangyang Wang']",poster,"['Deep ViT', 'Spectral Analysis', 'Attention Collapse', 'Patch Diversity']","Vision Transformer (ViT) has recently demonstrated promise in computer vision problems. However, unlike Convolutional Neural Networks (CNN), it is known that the performance of ViT saturates quickly with depth increasing, due to the observed attention collapse or patch uniformity. Despite a couple of empirical solutions, a rigorous framework studying on this scalability issue remains elusive. In this paper, we first establish a  rigorous theory framework to analyze ViT features from the Fourier spectrum domain. We show that the self-attention mechanism inherently amounts to a low-pass filter, which indicates when ViT scales up its depth, excessive low-pass filtering will cause feature maps to only preserve their Direct-Current (DC) component. We then propose two straightforward yet effective techniques to mitigate the undesirable low-pass limitation. The first technique, termed AttnScale, decomposes a self-attention block into low-pass and high-pass components, then rescales and combines these two filters to produce an all-pass self-attention matrix. The second technique, termed FeatScale, re-weights feature maps on separate frequency bands to amplify the high-frequency signals. Both techniques are efficient and hyperparameter-free, while effectively overcoming relevant ViT training artifacts such as attention collapse and patch uniformity. By seamlessly plugging in our techniques to multiple ViT variants, we demonstrate that they consistently help ViTs benefit from deeper architectures, bringing up to 1.1% performance gains ""for free"" (e.g., with little parameter overhead). We publicly release our codes and pre-trained models at https://github.com/VITA-Group/ViT-Anti-Oversmoothing.",https://api.openreview.net/pdf/b248407c10a8f43f933b8eb52a6221cc6f0680b2.pdf,graph;zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Anti-Oversmoothing+in+Deep+Vision+Transformers+via+the+Fourier+Domain+Analysis:+From+Theory+to+Practice
FP-DETR: Detection Transformer Advanced by Fully Pre-training,2022,ICLR,"['Wen Wang', 'Yang Cao', 'Jing Zhang', 'Dacheng Tao']",poster,"['Object Detection', 'Detection Transformer', 'Pre-training', 'Visual Prompt']","Large-scale pre-training has proven to be effective for visual representation learning on downstream tasks, especially for improving robustness and generalization. However, the recently developed detection transformers only employ pre-training on its backbone while leaving the key component, i.e., a 12-layer transformer, being trained from scratch, which prevents the model from above benefits. This separated training paradigm is mainly caused by the discrepancy between the upstream and downstream tasks. To mitigate the issue, we propose FP-DETR, a new method that Fully Pre-Trains an encoder-only transformer and smoothly fine-tunes it for object detection via a task adapter. Inspired by the success of textual prompts in NLP, we treat query positional embeddings as visual prompts to help the model attend to the target area (prompting) and recognize the object. To this end, we propose the task adapter which leverages self-attention to model the contextual relation between object query embedding. Experiments on the challenging COCO dataset demonstrate that our FP-DETR achieves competitive performance. Moreover, it enjoys better robustness to common corruptions and generalization to small-size datasets than state-of-the-art detection transformers. Code will be made publicly available at $\url{https://github.com/encounter1997/FP-DETR}$.",https://api.openreview.net/pdf/e9eb5c626773238a576771113c31684900fad1b7.pdf,graph;transformer;representation;llm,https://scholar.google.com/scholar?q=FP-DETR:+Detection+Transformer+Advanced+by+Fully+Pre-training
Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks,2022,ICLR,"['Andrea Cini', 'Ivan Marisca', 'Cesare Alippi']",poster,"['graph neural networks', 'missing data', 'time series analysis', 'time series imputation']","Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall short in capturing the nonlinear time and space dependencies existing within networks of interconnected sensors and do not take full advantage of the available - and often strong - relational information. Notably, most state-of-the-art imputation methods based on deep learning do not explicitly model relational aspects and, in any case, do not exploit processing frameworks able to adequately represent structured spatio-temporal data. Conversely, graph neural networks have recently surged in popularity as both expressive and scalable tools for processing sequential data with relational inductive biases. In this work, we present the first assessment of graph neural networks in the context of multivariate time series imputation. In particular, we introduce a novel graph neural network architecture, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing. Empirical results show that our model outperforms state-of-the-art methods in the imputation task on relevant real-world benchmarks with mean absolute error improvements often higher than 20%.",https://api.openreview.net/pdf/227091a9cda893265aa46e20fcf42675d01a8158.pdf,graph;zero_few-shot;representation;online learning;llm,https://scholar.google.com/scholar?q=Filling+the+G_ap_s:+Multivariate+Time+Series+Imputation+by+Graph+Neural+Networks
Discrete Representations Strengthen Vision Transformer Robustness,2022,ICLR,"['Chengzhi Mao', 'Lu Jiang', 'Mostafa Dehghani', 'Carl Vondrick', 'Rahul Sukthankar', 'Irfan Essa']",poster,"['vision transformer', 'robustness', 'image recognition']","Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs are overly reliant on local features (\eg, nuisances and texture) and fail to make adequate use of global context (\eg, shape and structure). As a result, ViTs fail to generalize to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vector-quantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12\% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet.",https://api.openreview.net/pdf/248e8be4fabe9aa0349e4e24290dd07abecaa424.pdf,graph;transformer;representation;llm,https://scholar.google.com/scholar?q=Discrete+Representations+Strengthen+Vision+Transformer+Robustness
Vector-quantized Image Modeling with Improved VQGAN,2022,ICLR,"['Jiahui Yu', 'Xin Li', 'Jing Yu Koh', 'Han Zhang', 'Ruoming Pang', 'James Qin', 'Alexander Ku', 'Yuanzhong Xu', 'Jason Baldridge', 'Yonghui Wu']",poster,"['VQGAN', 'Vision Transformers', 'Vector-quantized Image Modeling']","Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling (VIM) approach that involves pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple improvements over vanilla VQGAN from architecture to codebook learning, yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN further improves vector-quantized image modeling tasks, including unconditional, class-conditioned image generation and unsupervised representation learning. When trained on ImageNet at 256x256 resolution, we achieve Inception Score (IS) of 175.1 and Fr'echet Inception Distance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised pretraining, we further evaluate the pretrained Transformer by averaging intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 73.2% for a similar model size. ViM-L also outperforms iGPT-XL which is trained with extra web image data and larger model size.",https://api.openreview.net/pdf/63ce2022df1b6352ef17e394bb00cd416cf9497c.pdf,graph;zero_few-shot;transformer;representation;generative model;transfer learning;multi-task;llm,https://scholar.google.com/scholar?q=Vector-quantized+Image+Modeling+with+Improved+VQGAN
Anytime Dense Prediction with Confidence Adaptivity,2022,ICLR,"['Zhuang Liu', 'Zhiqiu Xu', 'Hung-Ju Wang', 'Trevor Darrell', 'Evan Shelhamer']",poster,"['Efficient Inference', 'Anytime Inference', 'Semantic Segmentation', 'Dense Prediction', 'Computer Vision']","Anytime inference requires a model to make a progression of predictions which might be halted at any time. Prior research on anytime visual recognition has mostly focused on image classification.We propose the first unified and end-to-end approach for anytime dense prediction. A cascade of ""exits"" is attached to the model to make multiple predictions. We redesign the exits to account for the depth and spatial resolution of the features for each exit. To reduce total computation, and make full use of prior predictions, we develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufficiently confident. Our full method, named anytime dense prediction with confidence (ADP-C), achieves the same level of final accuracy, and meanwhile significantly reduces total computation. We evaluate our method on Cityscapes semantic segmentation and MPII human pose estimation: ADP-C enables anytime inference without sacrificing accuracy while also reducing the total FLOPs of its base models by 44.4% and 59.1%. We compare with anytime inference by deep equilibrium networks and feature-based stochastic sampling, showing that ADP-C dominates both across the accuracy-computation curve. Our code is available at https://github.com/liuzhuang13/anytime.",https://api.openreview.net/pdf/32a205bf9187cb9f1f299230fb996fb4ca7c920a.pdf,adaptive;inference;segmentation;llm,https://scholar.google.com/scholar?q=Anytime+Dense+Prediction+with+Confidence+Adaptivity
Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models,2022,ICLR,"['Xiaofang Wang', 'Dan Kondratyuk', 'Eric Christiansen', 'Kris M. Kitani', 'Yair Movshovitz-Attias', 'Elad Eban']",poster,"['Ensemble', 'Cascade', 'Efficiency']","Committee-based models (ensembles or cascades) construct models by combining existing pre-trained ones. While ensembles and cascades are well-known techniques that were proposed before deep learning, they are not considered a core building block of deep model architectures and are rarely compared to in recent literature on developing efficient models. In this work, we go back to basics and conduct a comprehensive analysis of the efficiency of committee-based models. We find that even the most simplistic method for building committees from existing, independently pre-trained models can match or exceed the accuracy of state-of-the-art models while being drastically more efficient. These simple committee-based models also outperform sophisticated neural architecture search methods (e.g., BigNAS). These findings hold true for several tasks, including image classification, video classification, and semantic segmentation, and various architecture families, such as ViT, EfficientNet, ResNet, MobileNetV2, and X3D. Our results show that an EfficientNet cascade can achieve a 5.4x speedup over B7 and a ViT cascade can achieve a 2.3x speedup over ViT-L-384 while being equally accurate.",https://api.openreview.net/pdf/30614ee406abf0fbe954e884f90eb52e58cb5336.pdf,segmentation;3d;llm,https://scholar.google.com/scholar?q=Wisdom+of+Committees:+An+Overlooked+Approach+To+Faster+and+More+Accurate+Models
Surrogate Gap Minimization Improves Sharpness-Aware Training,2022,ICLR,"['Juntang Zhuang', 'Boqing Gong', 'Liangzhe Yuan', 'Yin Cui', 'Hartwig Adam', 'Nicha C Dvornek', 'sekhar tatikonda', 'James s Duncan', 'Ting Liu']",poster,"['generalization', 'sharpness-aware minimization', 'surrogate gap', 'deep learning']","The recently proposed  Sharpness-Aware  Minimization  (SAM)  improves generalization by minimizing a perturbed loss defined as the maximum loss within a neighborhood in the parameter space. However, we show that both sharp and flat minima can have a low perturbed loss, implying that SAM does not always prefer flat minima. Instead, we define a surrogate gap, a measure equivalent to the dominant eigenvalue of Hessian at a local minimum when the radius of neighborhood (to derive the perturbed loss) is small.  The surrogate gap is easy to compute and feasible for direct minimization during training. Based on the above observations, we propose Surrogate Gap Guided Sharpness-Aware Minimization (GSAM), a novel improvement over SAM with negligible computation overhead.  Conceptually, GSAM consists of two steps:  1) a gradient descent like SAM to minimize the perturbed loss, and 2) an ascent step in the orthogonal direction (after gradient decomposition) to minimize the surrogate gap and yet not affect the perturbed loss. GSAM seeks a region with both small loss (by step 1) and low sharpness (by step 2), giving rise to a model with high generalization capabilities. Theoretically, we show the convergence of GSAM and provably better generalization than SAM.Empirically, GSAM consistently improves generalization (e.g., +3.2% over SAM and +5.4% over AdamW on ImageNet top-1 accuracy for ViT-B/32). Code is released at https://sites.google.com/view/gsam-iclr22/home",https://api.openreview.net/pdf/a8445fdd027dcffe865628c3024581cae637dca1.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Surrogate+Gap+Minimization+Improves+Sharpness-Aware+Training
RegionViT: Regional-to-Local Attention for Vision Transformers,2022,ICLR,"['Chun-Fu Chen', 'Rameswar Panda', 'Quanfu Fan']",poster,"['vision transformer', 'image recognition', 'multi-scale feature']","Vision transformer (ViT) has recently shown its strong capability in achieving comparable results to convolutional neural networks (CNNs) on image classification. However, vanilla ViT simply inherits the same architecture from the natural language processing directly, which is often not optimized for vision applications. Motivated by this, in this paper, we propose a new architecture that adopts the pyramid structure and employ novel regional-to-local attention rather than global self-attention in vision transformers. More specifically, our model first generates regional tokens and local tokens from an image with different patch sizes, where each regional token is associated with a set of local tokens based on the spatial location. The regional-to-local attention includes two steps: first, the regional self-attention extracts global information among all regional tokens and then the local self-attention exchanges the information among one regional token and the associated local tokens via self-attention. Therefore, even though local self-attention confines the scope in a local region but it can still receive global information.
Extensive experiments on four vision tasks, including image classification, object and keypoint detection, semantics segmentation and action recognition, show that our approach outperforms or is on par with state-of-the-art ViT variants including many concurrent works. Our source codes and models are available at \url{https://github.com/IBM/RegionViT}.",https://api.openreview.net/pdf/a70d9722581424639326f684a4184d8a5af3dcb3.pdf,transformer;segmentation;llm,https://scholar.google.com/scholar?q=RegionViT:+Regional-to-Local+Attention+for+Vision+Transformers
Group equivariant neural posterior estimation,2022,ICLR,"['Maximilian Dax', 'Stephen R Green', 'Jonathan Gair', 'Michael Deistler', 'Bernhard Schölkopf', 'Jakob H. Macke']",poster,"['simulation-based inference', 'likelihood-free inference', 'machine learning for science', 'equivariances', 'group transformations']","Simulation-based inference with conditional neural density estimators is a powerful approach to solving inverse problems in science. However, these methods typically treat the underlying forward model as a black box, with no way to exploit geometric properties such as equivariances. Equivariances are common in scientific models, however integrating them directly into expressive inference networks (such as normalizing flows) is not straightforward. We here describe an alternative method to incorporate equivariances under joint transformations of parameters and data. Our method---called group equivariant neural posterior estimation (GNPE)---is based on self-consistently standardizing the ""pose"" of the data while estimating the posterior over parameters. It is architecture-independent, and applies both to exact and approximate equivariances. As a real-world application, we use GNPE for amortized inference of astrophysical binary black hole systems from gravitational-wave observations. We show that GNPE achieves state-of-the-art accuracy while reducing inference times by three orders of magnitude.",https://api.openreview.net/pdf/57dd5a689e8ee692e40ff4ca7c15b889e41e29c2.pdf,zero_few-shot;inference;metric;flow;llm,https://scholar.google.com/scholar?q=Group+equivariant+neural+posterior+estimation
Handling Distribution Shifts on Graphs: An Invariance Perspective,2022,ICLR,"['Qitian Wu', 'Hengrui Zhang', 'Junchi Yan', 'David Wipf']",poster,"['Representation Learning on Graphs', 'Out-of-Distribution Generalization', 'Domain Shift', 'Graph Structure Learning', 'Invariant Models']","There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given two-fold fundamental challenges: 1) the inter-connection among nodes in one graph, which induces non-IID generation of data points even under the same environment, and 2) the structural information in the input graph, which is also informative for prediction. In this paper, we formulate the OOD problem on graphs and develop a new invariant learning approach, Explore-to-Extrapolate Risk Minimization (EERM), that facilitates graph neural networks to leverage invariance principles for prediction. EERM resorts to multiple context explorers (specified as graph structure editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node-level prediction. We prove the validity of our method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution.",https://api.openreview.net/pdf/f55776307b1b806dc62cd1e642289d70df555fd2.pdf,graph;generative model;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=Handling+Distribution+Shifts+on+Graphs:+An+Invariance+Perspective
Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners,2022,ICLR,"['Ningyu Zhang', 'Luoqiu Li', 'Xiang Chen', 'Shumin Deng', 'Zhen Bi', 'Chuanqi Tan', 'Fei Huang', 'Huajun Chen']",poster,"['prompt-tuning', 'pre-trained language model', 'few-shot learning']","Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance.",https://api.openreview.net/pdf/d0850eb512cbbd2b27f6ff0ab0edf71861cd4829.pdf,transformer;llm,https://scholar.google.com/scholar?q=Differentiable+Prompt+Makes+Pre-trained+Language+Models+Better+Few-shot+Learners
CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation,2022,ICLR,"['Tongkun Xu', 'Weihua Chen', 'Pichao WANG', 'Fan Wang', 'Hao Li', 'Rong Jin']",poster,[],"Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. Most existing UDA methods focus on learning domain-invariant feature representation, either from the domain level or category level, using convolution neural networks (CNNs)-based frameworks. One fundamental problem for the category level based UDA is the production of pseudo labels for samples in target domain, which are usually too noisy for accurate domain alignment, inevitably compromising the UDA performance.  With the success of Transformer in various tasks, we find that the cross-attention in Transformer is robust to the noisy input pairs for better feature alignment, thus in this paper Transformer is adopted for the challenging UDA task. Specifically, to generate accurate input pairs, we design a two-way center-aware labeling algorithm to produce pseudo labels for target samples. Along with the pseudo labels, a weight-sharing triple-branch transformer framework is proposed to apply self-attention and cross-attention for source/target feature learning and source-target domain alignment, respectively. 
Such design explicitly enforces the framework to learn discriminative domain-specific and domain-invariant representations simultaneously. The proposed method is dubbed CDTrans (cross-domain transformer), and it provides one of the first attempts to solve UDA tasks with a pure transformer solution. Experiments show that our proposed method achieves the best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet. Code and models are available at https://github.com/CDTrans/CDTrans.",https://api.openreview.net/pdf/5af495124dcff5b772396db65ab98725f7b036b7.pdf,graph;transformer;representation;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=CDTrans:+Cross-domain+Transformer+for+Unsupervised+Domain+Adaptation
Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains,2022,ICLR,"['Qilong Zhang', 'Xiaodan Li', 'YueFeng Chen', 'Jingkuan Song', 'Lianli Gao', 'Yuan He', ""Hui Xue'""]",poster,"['practice black-box attack', 'cross-domain transferability']","Adversarial examples have posed a severe threat to deep neural networks due to their transferable nature. Currently, various works have paid great efforts to enhance the cross-model transferability, which mostly assume the substitute model is trained in the same domain as the target model.
However, in reality, the relevant information of the deployed model is unlikely to leak.
Hence, it is vital to build a more practical black-box threat model to overcome this limitation and evaluate the vulnerability of deployed models.
In this paper, with only the knowledge of the ImageNet domain, we propose a Beyond ImageNet Attack (BIA) to investigate the transferability towards black-box domains (unknown classification tasks). Specifically, we leverage a generative model to learn the adversarial function for disrupting low-level features of input images. 
Based on this framework, we further propose two variants to narrow the gap between the source and target domains from the data and model perspectives, respectively. Extensive experiments on coarse-grained and fine-grained domains demonstrate the effectiveness of our proposed methods. Notably,
our methods outperform state-of-the-art approaches by up to 7.71\% (towards coarse-grained domains) and 25.91\% (towards fine-grained domains) on average. Our code is available at \url{https://github.com/Alibaba-AAIG/Beyond-ImageNet-Attack}.",https://api.openreview.net/pdf/e3edfd55ba8b10084cbe821c5e89dd92bb887262.pdf,zero_few-shot;generative model;transfer learning;llm,https://scholar.google.com/scholar?q=Beyond+ImageNet+Attack:+Towards+Crafting+Adversarial+Examples+for+Black-box+Domains
RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning,2022,ICLR,"['Xiaojian Ma', 'Weili Nie', 'Zhiding Yu', 'Huaizu Jiang', 'Chaowei Xiao', 'Yuke Zhu', 'Song-Chun Zhu', 'Anima Anandkumar']",poster,"['visual relational reasoning', 'representation learning', 'systematic generalization']","Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.",https://api.openreview.net/pdf/9ae93c86cdada9dfdeef3cf2c7ee77363fe51c7b.pdf,graph;zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=RelViT:+Concept-guided+Vision+Transformer+for+Visual+Relational+Reasoning
On Improving Adversarial Transferability of Vision Transformers ,2022,ICLR,"['Muzammal Naseer', 'Kanchana Ranasinghe', 'Salman Khan', 'Fahad Khan', 'Fatih Porikli']",spotlight,"['Vision Transformers', 'Adversarial Perturbations']","Vision transformers (ViTs) process input images as sequences of patches via self-attention; a radically different architecture than convolutional neural networks (CNNs).  This makes it interesting to study the adversarial feature space of ViT models and their transferability. In particular, we observe that adversarial patterns found via conventional adversarial attacks show very \emph{low} black-box transferability even for large ViT models. We show that this phenomenon is only due to the sub-optimal attack procedures that do not leverage the true representation potential of ViTs. A deep ViT is composed of multiple blocks, with a consistent architecture comprising of self-attention and feed-forward layers, where each block is capable of independently producing a class token. Formulating an attack using only the last class token (conventional approach) does not directly leverage the discriminative information stored in the earlier tokens, leading to poor adversarial transferability of ViTs.Using the compositional nature of ViT models, we enhance transferability of existing attacks by introducing two novel strategies specific to the architecture of ViT models.  \emph{(i) Self-Ensemble:} We propose a method to find multiple discriminative pathways by dissecting a single ViT model into an ensemble of networks. This allows explicitly utilizing class-specific information at each ViT block. \emph{(ii) Token Refinement:} We then propose to refine the tokens to further enhance the discriminative capacity at each block of ViT.Our token refinement systematically combines the class tokens with structural information preserved within the patch tokens. An adversarial attack when applied to such refined tokens within the ensemble of classifiers found in a single vision transformer has significantly higher transferability and thereby brings out the true generalization potential of the ViT's adversarial space. Code: https://t.ly/hBbW.",https://api.openreview.net/pdf/b40e4df19f3e58593a885adc8809af1ba9864da8.pdf,zero_few-shot;transformer;representation;transfer learning;llm,https://scholar.google.com/scholar?q=On+Improving+Adversarial+Transferability+of+Vision+Transformers+
On the Connection between Local Attention and Dynamic Depth-wise Convolution,2022,ICLR,"['Qi Han', 'Zejia Fan', 'Qi Dai', 'Lei Sun', 'Ming-Ming Cheng', 'Jiaying Liu', 'Jingdong Wang']",spotlight,"['local attention', 'depth-wise convolution', 'dynamic depth-wise convolution', 'weight sharing', 'dynamic weight']","Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as dynamic weight computation. We point out that local attention resembles depth-wise convolution and its dynamic variants in sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. The main differences lie in (i) weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions and attention shares the connection weights across channels, and (ii) dynamic weight computation manners - local attention is based on dot-products between pairwise positions in the local window, and dynamic convolution is based on linear projections conducted on the center representation or the globally pooled representation. The connection between local attention and dynamic depth-wise convolution is empirically verified by the ablation study about weight sharing and dynamic weight computation in Local Vision Transformer and (dynamic) depth-wise convolution. We empirically observe that the models based on depth-wise convolution and the dynamic variants with lower computation complexity perform on-par with or slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation. Code is available at https://github.com/Atten4Vis/DemystifyLocalViT.",https://api.openreview.net/pdf/b5b230d05deb5ca8dcfd87f952bca5621cf5cced.pdf,zero_few-shot;transformer;representation;sparse;segmentation;llm,https://scholar.google.com/scholar?q=On+the+Connection+between+Local+Attention+and+Dynamic+Depth-wise+Convolution
Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory,2022,ICLR,"['Zhi Zhang', 'Zhuoran Yang', 'Han Liu', 'Pratap Tokekar', 'Furong Huang']",spotlight,"['Multi-agent Reinforcement Learning', 'Predictive State Representation', 'Dynamic Interaction Graph']","We study reinforcement learning for partially observable multi-agent systems where each agent only has access to its own observation and reward and aims to maximize its cumulative rewards. To handle partial observations, we propose graph-assisted predictive state representations (GAPSR), a scalable multi-agent representation learning framework that leverages the agent connectivity graphs to aggregate local representations computed by each agent. In addition, our representations are readily able to incorporate dynamic interaction graphs and kernel space embeddings of the predictive states, and thus have strong flexibility and representation power. 
Based on GAPSR, we propose an end-to-end  MARL algorithm that simultaneously infers the predictive representations and uses the representations as the input of a policy optimization algorithm. Empirically, we demonstrate the efficacy of the proposed algorithm provided on both a MAMuJoCo robotic learning experiment and a multi-agent particle learning environment.",https://api.openreview.net/pdf/abd7a0683441b4eb75fb4381e8ac583f2bff2b90.pdf,reinforcement learning;graph;optimization;representation;multi-agent;llm,https://scholar.google.com/scholar?q=Reinforcement+Learning+under+a+Multi-agent+Predictive+State+Representation+Model:+Method+and+Theory
Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling,2022,ICLR,['Ada Wan'],spotlight,"['fairness', 'evaluation', 'multilingual NLP / multilinguality', 'representation learning for language data', 'statistical comparisons', 'Double Descent', 'conditional language modeling', 'data-centric approach', 'diversity in AI', 'morphology', 'Transformer', 'meta evaluation', 'visualization or interpretation of learned representations', 'character encoding', 'internationalization and localization', 'robustness', 'statistical science for NLP', 'science in the era of AI/DL (AIxScience)', 'transdisciplinarity']","We perform systematically and fairly controlled experiments with the 6-layer Transformer to investigate  the hardness in conditional-language-modeling languages which have been traditionally considered morphologically rich (AR and RU) and poor (ZH). We evaluate through statistical comparisons across 30 possible language directions from the 6 languages of the United Nations Parallel Corpus across 5 data sizes on 3 representation levels --- character, byte, and word. Results show that performance is relative to the representation granularity of each of the languages, not to the language as a whole. On the character and byte levels, we are able to eliminate statistically significant performance disparity, hence demonstrating that a language cannot be intrinsically hard. The disparity that mirrors the morphological complexity hierarchy is shown to be a byproduct of word segmentation. Evidence from data statistics, along with the fact that word segmentation is qualitatively indeterminate, renders a decades-long debate on morphological complexity (unless it is being intentionally modeled in a word-based, meaning-driven context) irrelevant in the context of computing. The intent of our work is to help effect more objectivity and adequacy in evaluation as well as fairness and inclusivity in experimental setup in the area of language and computing so to uphold diversity in Machine Learning and Artificial Intelligence research. Multilinguality is real and relevant in computing not due to canonical, structural linguistic concepts such as morphology or ""words"" in our minds, but rather standards related to internationalization and localization, such as character encoding --- something which has thus far been sorely overlooked in our discourse and curricula. ",https://api.openreview.net/pdf/1bc81aec7b25823dcaab95c24c1c5c0779bd3c7c.pdf,transformer;representation;segmentation;llm,https://scholar.google.com/scholar?q=Fairness+in+Representation+for+Multilingual+NLP:+Insights+from+Controlled+Experiments+on+Conditional+Language+Modeling
ViTGAN: Training GANs with Vision Transformers,2022,ICLR,"['Kwonjoon Lee', 'Huiwen Chang', 'Lu Jiang', 'Han Zhang', 'Zhuowen Tu', 'Ce Liu']",spotlight,[],"Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such performance can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). For ViT discriminators, we observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce several novel regularization techniques for training GANs with ViTs. For ViT generators, we examine architectural choices for latent and pixel mapping layers to faciliate convergence. Empirically, our approach, named ViTGAN, achieves comparable performance to the leading CNN- based GAN models on three datasets: CIFAR-10, CelebA, and LSUN bedroom.",https://api.openreview.net/pdf/12b5f82c142cc00c08e950c5a49db1948f14aa54.pdf,transformer;generative model;llm,https://scholar.google.com/scholar?q=ViTGAN:+Training+GANs+with+Vision+Transformers
TRGP: Trust Region Gradient Projection for Continual Learning,2022,ICLR,"['Sen Lin', 'Li Yang', 'Deliang Fan', 'Junshan Zhang']",spotlight,"['trust region', 'gradient projection', 'scaled weight projection', 'continual learning', 'forward knowledge transfer', 'task correlation']","Catastrophic forgetting is one of the major challenges in continual learning. To address this issue, some existing methods put restrictive constraints on the optimization space of the new task for minimizing the interference to old tasks. However, this may lead to unsatisfactory performance for the new task, especially when the new task is strongly correlated with old tasks. To tackle this challenge, we propose Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. Particularly, we introduce a notion of 'trust region' to select the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region through a layer-wise scaling matrix. By jointly optimizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks,  TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that our approach achieves significant improvement over related state-of-the-art methods.",https://api.openreview.net/pdf/7291715ef56df6f6a92491a5977eb95a2db1ca86.pdf,optimization;transformer;transfer learning;llm,https://scholar.google.com/scholar?q=TRGP:+Trust+Region+Gradient+Projection+for+Continual+Learning
Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design,2022,ICLR,"['Wenhao Gao', 'Rocío Mercado', 'Connor W. Coley']",spotlight,"['molecular design', 'synthesis planning', 'tree generation', 'graph generation']","Molecular design and synthesis planning are two critical steps in the process of molecular discovery that we propose to formulate as a single shared task of conditional synthetic pathway generation. We report an amortized approach to generate synthetic pathways as a Markov decision process conditioned on a target molecular embedding. This approach allows us to conduct synthesis planning in a bottom-up manner and design synthesizable molecules by decoding from optimized conditional codes, demonstrating the potential to solve both problems of design and synthesis simultaneously. The approach leverages neural networks to probabilistically model the synthetic trees, one reaction step at a time, according to reactivity rules encoded in a discrete action space of reaction templates. We train these networks on hundreds of thousands of artificial pathways generated from a pool of purchasable compounds and a list of expert-curated templates. We validate our method with (a) the recovery of molecules using conditional generation, (b) the identification of synthesizable structural analogs, and (c) the optimization of molecular structures given oracle functions relevant to bioactivity and drug discovery.",https://api.openreview.net/pdf/9a1a1ce4faf67f9a0f649866c9158c2f0f055db1.pdf,reinforcement learning;optimization;zero_few-shot;generative model;llm,https://scholar.google.com/scholar?q=Amortized+Tree+Generation+for+Bottom-up+Synthesis+Planning+and+Synthesizable+Molecular+Design
EViT: Expediting Vision Transformers via Token Reorganizations,2022,ICLR,"['Youwei Liang', 'Chongjian GE', 'Zhan Tong', 'Yibing Song', 'Jue Wang', 'Pengtao Xie']",spotlight,"['Vision Transformers', 'multi-head self-attention', 'efficient inference']","Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50% while its recognition accuracy is decreased by only 0.3% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit",https://api.openreview.net/pdf/feb0c5a2e1c1fc63509c2e528ca07aa95aea2d5e.pdf,transformer;inference;active learning;llm,https://scholar.google.com/scholar?q=EViT:+Expediting+Vision+Transformers+via+Token+Reorganizations
Spanning Tree-based Graph Generation for Molecules,2022,ICLR,"['Sungsoo Ahn', 'Binghong Chen', 'Tianzhe Wang', 'Le Song']",spotlight,"['molecule generation', 'tree generation', 'graph generation', 'deep generative model', 'de novo drug design']","In this paper, we explore the problem of generating molecules using deep neural networks, which has recently gained much interest in chemistry. To this end, we propose a spanning tree-based graph generation (STGG) framework based on formulating molecular graph generation as a construction of a spanning tree and the residual edges. Such a formulation exploits the sparsity of molecular graphs and allows using compact tree-constructive operations to define the molecular graph connectivity. Based on the intermediate graph structure of the construction process, our framework can constrain its generation to molecular graphs that satisfy the chemical valence rules. We also newly design a Transformer architecture with tree-based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework in metrics such as validity, Frechet ChemNet distance, and fragment similarity. We also demonstrate the usefulness of STGG in maximizing penalized LogP value of molecules.",https://api.openreview.net/pdf/dcb1134d836d26dc8ef7d83683aa9f5b35964eae.pdf,graph;optimization;zero_few-shot;transformer;generative model;metric;llm,https://scholar.google.com/scholar?q=Spanning+Tree-based+Graph+Generation+for+Molecules
Multitask Prompted Training Enables Zero-Shot Task Generalization,2022,ICLR,"['Victor Sanh', 'Albert Webson', 'Colin Raffel', 'Stephen Bach', 'Lintang Sutawika', 'Zaid Alyafeai', 'Antoine Chaffin', 'Arnaud Stiegler', 'Arun Raja', 'Manan Dey', 'M Saiful Bari', 'Canwen Xu', 'Urmish Thakker', 'Shanya Sharma Sharma', 'Eliza Szczechla', 'Taewoon Kim', 'Gunjan Chhablani', 'Nihal Nayak', 'Debajyoti Datta', 'Jonathan Chang', 'Mike Tian-Jian Jiang', 'Han Wang', 'Matteo Manica', 'Sheng Shen', 'Zheng Xin Yong', 'Harshit Pandey', 'Rachel Bawden', 'Thomas Wang', 'Trishala Neeraj', 'Jos Rozen', 'Abheesht Sharma', 'Andrea Santilli', 'Thibault Fevry', 'Jason Alan Fries', 'Ryan Teehan', 'Teven Le Scao', 'Stella Biderman', 'Leo Gao', 'Thomas Wolf', 'Alexander M Rush']",spotlight,[],"Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models’ pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks specified in natural language. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several datasets, often outperforming models 16× its size. Further, our model attains strong performance on a subset of tasks from the BIG-Bench benchmark, outperforming models 6× its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource.",https://api.openreview.net/pdf/bec425b93713482f8e2de5d1d15b66ff95a47026.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Multitask+Prompted+Training+Enables+Zero-Shot+Task+Generalization
On the relation between statistical learning and perceptual distances,2022,ICLR,"['Alexander Hepburn', 'Valero Laparra', 'Raul Santos-Rodriguez', 'Johannes Ballé', 'Jesus Malo']",spotlight,[],"It has been demonstrated many times that the behavior of the human visual system is connected to the statistics of natural images. Since machine learning relies on the statistics of training data as well, the above connection has interesting implications when using perceptual distances (which mimic the behavior of the human visual system) as a loss function. In this paper, we aim to unravel the non-trivial relationships between the probability distribution of the data, perceptual distances, and unsupervised machine learning. To this end, we show that perceptual sensitivity is correlated with the probability of an image in its close neighborhood. We also explore the relation between distances induced by autoencoders and the probability distribution of the training data, as well as how these induced distances are correlated with human perception. Finally, we find perceptual distances do not always lead to noticeable gains in performance over Euclidean distance in common image processing tasks, except when data is scarce and the perceptual distance provides regularization. We propose this may be due to a double-counting effect of the image statistics, once in the perceptual distance and once in the training procedure.",https://api.openreview.net/pdf/12c717193deff83fed4cdbbc207d6c4ffebad63e.pdf,llm,https://scholar.google.com/scholar?q=On+the+relation+between+statistical+learning+and+perceptual+distances
When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations,2022,ICLR,"['Xiangning Chen', 'Cho-Jui Hsieh', 'Boqing Gong']",spotlight,"['Vision Transformers', 'Optimization']","Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and +11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. Model checkpoints are available at \url{https://github.com/google-research/vision_transformer}.",https://api.openreview.net/pdf/97b8505eb7034c4bfaee9c7d480a9f605be6fea8.pdf,optimization;transformer;contrastive learning;inference;sparse;transfer learning;active learning;augmentation;llm,https://scholar.google.com/scholar?q=When+Vision+Transformers+Outperform+ResNets+without+Pre-training+or+Strong+Data+Augmentations
Tighter Sparse Approximation Bounds for ReLU Neural Networks,2022,ICLR,"['Carles Domingo-Enrich', 'Youssef Mroueh']",spotlight,"['neural network', 'two-layer', 'infinite-width', 'approximation', 'sparse', 'Radon transform', 'Fourier transform', 'ReLU']","A well-known line of work (Barron, 1993; Breiman, 1993; Klusowski & Barron, 2018) provides bounds on the width $n$ of a ReLU two-layer neural network needed to approximate a function $f$ over the ball $\mathcal{B}_R(\mathbb{R}^d)$ up to error $\epsilon$, when the Fourier based quantity $C_f = \int_{\mathbb{R}^d} \|\xi\|^2 |\hat{f}(\xi)| \ d\xi$ is finite. More recently Ongie et al. (2019) used the Radon transform as a tool for analysis of infinite-width ReLU two-layer networks. In particular, they introduce the concept of Radon-based $\mathcal{R}$-norms and show that a function defined on $\mathbb{R}^d$ can be represented as an infinite-width two-layer neural network if and only if its $\mathcal{R}$-norm is finite. In this work, we extend the framework of Ongie et al. (2019) and define similar Radon-based semi-norms ($\mathcal{R}, \mathcal{U}$-norms) such that a function admits an infinite-width neural network representation on a bounded open set $\mathcal{U} \subseteq \mathbb{R}^d$ when its $\mathcal{R}, \mathcal{U}$-norm is finite. Building on this, we derive sparse (finite-width) neural network approximation bounds that refine those of Breiman (1993); Klusowski & Barron (2018). Finally, we show that infinite-width neural network representations on bounded open sets are not unique and study their structure, providing a functional view of mode connectivity.",https://api.openreview.net/pdf/26cbfd341c2fcbea04f1b531de54a26548f1ec2d.pdf,representation;sparse;llm,https://scholar.google.com/scholar?q=Tighter+Sparse+Approximation+Bounds+for+ReLU+Neural+Networks
How Do Vision Transformers Work?,2022,ICLR,"['Namuk Park', 'Songkuk Kim']",spotlight,"['vision transformer', 'self-attention', 'multi-head self-attention', 'loss landscape']","The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes. The code is available at https://github.com/xxxnell/how-do-vits-work.",https://api.openreview.net/pdf/e293cbd49c33a4924e5b45932a342361dd9845cf.pdf,zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=How+Do+Vision+Transformers+Work?
Imbedding Deep Neural Networks,2022,ICLR,"['Andrew Corbett', 'Dmitry Kangin']",spotlight,"['Neural ODEs', 'Optimal Control', 'Deep Neural Networks', 'Invariant Imbedding']","Continuous-depth neural networks, such as Neural ODEs, have refashioned the understanding of residual neural networks in terms of non-linear vector-valued optimal control problems. The common solution is to use the adjoint sensitivity method to replicate a forward-backward pass optimisation problem. We propose a new approach which explicates the network's `depth' as a fundamental variable, thus reducing the problem to a system of forward-facing initial value problems. This new method is based on the principal of `Invariant Imbedding' for which we prove a general solution, applicable to all non-linear, vector-valued optimal control problems with both running and terminal loss.
Our new architectures provide a tangible tool for inspecting the theoretical--and to a great extent unexplained--properties of network depth. They also constitute a resource of discrete implementations of Neural ODEs comparable to classes of imbedded residual neural networks. Through a series of experiments, we show the competitive performance of the proposed architectures for supervised learning and time series prediction. ",https://api.openreview.net/pdf/2696810601b722fa25baf9dd6ed1280d43c1c474.pdf,llm,https://scholar.google.com/scholar?q=Imbedding+Deep+Neural+Networks
Superclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings,2022,ICLR,"['Jingchao Ni', 'Wei Cheng', 'Zhengzhang Chen', 'Takayoshi Asakura', 'Tomoya Soma', 'Sho Kato', 'Haifeng Chen']",spotlight,"['Deep learning', 'represenation learning', 'generative model']","Learning fine-grained embeddings is essential for extending the generalizability of models pre-trained on ""coarse"" labels (e.g., animals). It is crucial to fields for which fine-grained labeling (e.g., breeds of animals) is expensive, but fine-grained prediction is desirable, such as medicine. The dilemma necessitates adaptation of a ""coarsely"" pre-trained model to new tasks with a few ""finer-grained"" training labels. However, coarsely supervised pre-training tends to suppress intra-class variation, which is vital for cross-granularity adaptation. In this paper, we develop a training framework underlain by a novel superclass-conditional Gaussian mixture model (SCGM). SCGM imitates the generative process of samples from hierarchies of classes through latent variable modeling of the fine-grained subclasses. The framework is agnostic to the encoders and only adds a few distribution related parameters, thus is efficient, and flexible to different domains. The model parameters are learned end-to-end by maximum-likelihood estimation via a principled Expectation-Maximization algorithm. Extensive experiments on benchmark datasets and a real-life medical dataset indicate the effectiveness of our method.",https://api.openreview.net/pdf/6eed00839e8f02de0ecba5b42ac5ce892def5ac0.pdf,generative model;llm,https://scholar.google.com/scholar?q=Superclass-Conditional+Gaussian+Mixture+Model+For+Learning+Fine-Grained+Embeddings
CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code,2023,ICLR,"['Nadezhda Chirkova', 'Sergey Troshin']",poster,"['source code processing', 'tokenization', 'byte-pair encoding']","Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account source code specifics. We propose subtokenziation that reduces average length by 17--40% without downstream performance drop, and show that a carefully chosen subtokenization may improve  quality by 0.5-2%, possibly with some length increase.",https://api.openreview.net/pdf/c3138a16b1e95192c50eacb849b3a42ecf8a6999.pdf,transformer;llm,https://scholar.google.com/scholar?q=CodeBPE:+Investigating+Subtokenization+Options+for+Large+Language+Model+Pretraining+on+Source+Code
Boosting Causal Discovery via Adaptive Sample Reweighting,2023,ICLR,"['An Zhang', 'Fangfu Liu', 'Wenchang Ma', 'Zhibo Cai', 'Xiang Wang', 'Tat-Seng Chua']",poster,"['Causal Structure Learning', 'Score-based Causal Discovery', 'Adaptive Sample Reweighting']","Under stringent model type and variable distribution assumptions, score-based causal discovery methods learn the directed acyclic graph (DAG) from observational data by evaluating candidate graphs over an averaged score function. Despite the great success in low-dimensional linear systems, it has been observed that these approaches overly exploits easier-to-fit samples, thus inevitably learning spurious edges. Worse still, the common homogeneity assumption of most causal discovery methods can be easily violated due to the widespread existence of heterogeneous data in the real world, resulting in performance vulnerability when noise distributions vary. We propose a simple yet effective model-agnostic framework to boost causal discovery performance by dynamically learning the adaptive weights for the Reweighted Score function, ReScore for short, where the learned weights tailors quantitatively to the important degree of each samples. Intuitively, we leverage the bilevel optimization scheme to alternatively train a standard DAG learner first, then upweight the samples that the DAG learner fails to fit well and downweight the samples that the DAG learner easily extracts the causation information from. Extensive experiments on both synthetic and real-world datasets are carried out to validate the effectiveness of ReScore. We observe consistent and significant boosts in structure learning performance. We further visualize that ReScore concurrently mitigates the influence of spurious edges and generalizes to heterogeneous data. Finally, we perform theoretical analysis to guarantee the structure identifiability and the weight adaptive properties of ReScore. Our codes are available at https://github.com/anzhang314/ReScore.",https://api.openreview.net/pdf/490a8e5885f74912244f797f7afd7060d7d2bbe9.pdf,graph;optimization;zero_few-shot;adaptive;llm,https://scholar.google.com/scholar?q=Boosting+Causal+Discovery+via+Adaptive+Sample+Reweighting
Actionable Neural Representations: Grid Cells from Minimal Constraints,2023,ICLR,"['Will Dorrell', 'Peter E. Latham', 'Timothy E. J. Behrens', 'James C. R. Whittington']",poster,"['Grid Cells', 'Representation Theory', 'Theoretical Neuroscience', 'Normative Models']","To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won't have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an  `actionable representation'. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode.",https://api.openreview.net/pdf/da785b626e7697b7ef2446f8cd28e584a45bc5b9.pdf,optimization;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Actionable+Neural+Representations:+Grid+Cells+from+Minimal+Constraints
Asynchronous Distributed Bilevel Optimization,2023,ICLR,"['Yang Jiao', 'Kai Yang', 'Tiancheng Wu', 'Dongjin Song', 'Chengtao Jian']",poster,[],"Bilevel optimization plays an essential role in many machine learning tasks, ranging from hyperparameter optimization to meta-learning. Existing studies on bilevel optimization, however, focus on either centralized or synchronous distributed setting. The centralized bilevel optimization approaches require collecting massive amount of data to a single server, which inevitably incur significant communication expenses and may give rise to data privacy risks. Synchronous distributed bilevel optimization algorithms, on the other hand, often face the straggler problem and will immediately stop working if a few workers fail to respond. As a remedy, we propose Asynchronous Distributed Bilevel Optimization (ADBO) algorithm. The proposed ADBO can tackle bilevel optimization problems with both nonconvex upper-level and lower-level objective functions, and its convergence is theoretically guaranteed. Furthermore, it is revealed through theoretic analysis that the iteration complexity of ADBO to obtain the $\epsilon$-stationary point is upper bounded by $\mathcal{O}(\frac{1}{{{\epsilon ^2}}})$. Thorough empirical studies on public datasets have been conducted to elucidate the effectiveness and efficiency of the proposed ADBO.",https://api.openreview.net/pdf/7751c6dceeca5e43226f9ae20bb2a2ab9915aa40.pdf,graph;optimization;zero_few-shot;meta-learning;llm,https://scholar.google.com/scholar?q=Asynchronous+Distributed+Bilevel+Optimization
Evolving Populations of Diverse RL Agents with MAP-Elites,2023,ICLR,"['Thomas PIERROT', 'Arthur Flajolet']",poster,[],"Quality Diversity (QD) has emerged as a powerful alternative optimization paradigm that aims at generating large and diverse collections of solutions, notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions through mutations and crossovers. While very effective for some unstructured problems, early ME implementations relied exclusively on random search to evolve the population of solutions, rendering them notoriously sample-inefficient for high-dimensional problems, such as when evolving neural networks. Follow-up works considered exploiting gradient information to guide the search in order to address these shortcomings through techniques borrowed from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While mixing RL techniques with ME unlocked state-of-the-art performance for robotics control problems that require a good amount of exploration, it also plagued these ME variants with limitations common among RL algorithms that ME was free of, such as hyperparameter sensitivity, high stochasticity as well as training instability, including when the population size increases as some components are shared across the population in recent approaches. Furthermore, existing approaches mixing ME with RL tend to be tied to a specific RL algorithm, which effectively prevents their use on problems where the corresponding RL algorithm fails. To address these shortcomings, we introduce a flexible framework that allows the use of any RL algorithm and alleviates the aforementioned limitations by evolving populations of agents (whose definition include hyperparameters and all learnable parameters) instead of just policies. We demonstrate the benefits brought about by our framework through extensive numerical experiments on a number of robotics control problems, some of which with deceptive rewards, taken from the QD-RL literature. We open source an efficient JAX-based implementation of our algorithm in the QDax library. ",https://api.openreview.net/pdf/7647093a5e985828f881513eb78bf7d36bde7a04.pdf,reinforcement learning;optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Evolving+Populations+of+Diverse+RL+Agents+with+MAP-Elites
Leveraging Large Language Models for Multiple Choice Question Answering,2023,ICLR,"['Joshua Robinson', 'David Wingate']",poster,"['NLP', 'language models', 'multiple choice question answering', 'symbol binding', 'GPT-3', 'Codex']","While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., “A”) associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.",https://api.openreview.net/pdf/1ae9b0968aa2c0f898d5082be403c3070b0c09fc.pdf,graph;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Leveraging+Large+Language+Models+for+Multiple+Choice+Question+Answering
UL2: Unifying Language Learning Paradigms,2023,ICLR,"['Yi Tay', 'Mostafa Dehghani', 'Vinh Q. Tran', 'Xavier Garcia', 'Jason Wei', 'Xuezhi Wang', 'Hyung Won Chung', 'Dara Bahri', 'Tal Schuster', 'Steven Zheng', 'Denny Zhou', 'Neil Houlsby', 'Donald Metzler']",poster,"['language models', 'pretraining', 'transformers']","Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. Finally, we show that UL2 20B works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. We release Flax-based T5X model checkpoints for the 20B model publicly.
",https://api.openreview.net/pdf/8ac86c3590d8d02420c1aec52a1ee763b2f5166d.pdf,graph;zero_few-shot;transformer;generative model;self-supervision;llm,https://scholar.google.com/scholar?q=UL2:+Unifying+Language+Learning+Paradigms
Backpropagation through Combinatorial Algorithms: Identity with Projection Works,2023,ICLR,"['Subham Sekhar Sahoo', 'Anselm Paulus', 'Marin Vlastelica', 'Vít Musil', 'Volodymyr Kuleshov', 'Georg Martius']",poster,"['combinatorial optimization', 'deep learning', 'representation learning', 'gradient descent', 'backpropagation', 'argmin differentiation', 'deep graph matching', 'retrieval']","Embedding discrete solvers as differentiable layers has given modern deep learning architectures combinatorial expressivity and discrete reasoning capabilities. The derivative of these solvers is zero or undefined, therefore a meaningful replacement is crucial for effective gradient-based learning. Prior works rely on smoothing the solver with input perturbations, relaxing the solver to continuous problems, or interpolating the loss landscape with techniques that typically require additional solver calls, introduce extra hyper-parameters, or compromise performance. We propose a principled approach to exploit the geometry of the discrete solution space to treat the solver as a negative identity on the backward pass and further provide a theoretical justification. Our experiments demonstrate that such a straightforward hyper-parameter-free approach is able to compete with previous more complex methods on numerous experiments such as backpropagation through discrete samplers, deep graph matching, and image retrieval. Furthermore, we substitute the previously proposed problem-specific and label-dependent margin with a generic regularization procedure that prevents cost collapse and increases robustness.",https://api.openreview.net/pdf/1fd744076d10f6b78516e5b369737d7e4ce6811f.pdf,graph;llm,https://scholar.google.com/scholar?q=Backpropagation+through+Combinatorial+Algorithms:+Identity+with+Projection+Works
UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining,2023,ICLR,"['Hyung Won Chung', 'Xavier Garcia', 'Adam Roberts', 'Yi Tay', 'Orhan Firat', 'Sharan Narang', 'Noah Constant']",poster,"['Keywords: multilingual', 'pretraining', 'language models', 'language sampling', 'language distribution', 'low-resource languages', 'overfitting']","Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMax sampling.",https://api.openreview.net/pdf/f9db93a34d7f56ce156cb253feba6c638acc2b21.pdf,llm,https://scholar.google.com/scholar?q=UniMax:+Fairer+and+More+Effective+Language+Sampling+for+Large-Scale+Multilingual+Pretraining
On Pre-training Language Model for Antibody,2023,ICLR,"['Danqing Wang', 'Fei YE', 'Hao Zhou']",poster,[],"Antibodies are vital proteins offering robust protection for the human body from pathogens. The development of general protein and antibody-specific pre-trained language models both facilitate antibody prediction tasks. However, there have been limited studies that comprehensively explore the representation capability of distinct pre-trained language models on different antibody tasks. To investigate the problem, we aim to answer several key questions in this paper, such as how pre-trained language models perform in antibody tasks with different specificity and how introducing specific biological mechanisms to the pre-training process can benefit the model. Additionally, we evaluate if the learned antibody pre-trained representations can be applied to real-world antibody problems, like drug discovery and immune process understanding. Previously, no benchmark available largely hindered the study to answer these questions. To aid in our investigation, we provide an AnTibody Understanding Evaluation (ATUE) benchmark. We comprehensively evaluate the performance of protein pre-trained language models by empirical study along with conclusions and new insights. Our ATUE and code are released at https://github.com/dqwang122/EATLM.",https://api.openreview.net/pdf/c0deacf9e860ac34392346ea2fea727ac90bec4a.pdf,representation;llm,https://scholar.google.com/scholar?q=On+Pre-training+Language+Model+for+Antibody
Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,2023,ICLR,"['Abulhair Saparov', 'He He']",poster,"['large language models', 'reasoning', 'question answering', 'chain-of-thought', 'in-context learning']","Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",https://api.openreview.net/pdf/e73172f359a19430928855ff049b5dd1e7a4d987.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Language+Models+Are+Greedy+Reasoners:+A+Systematic+Formal+Analysis+of+Chain-of-Thought
Understanding the Covariance Structure of Convolutional Filters,2023,ICLR,"['Asher Trockman', 'Devin Willmott', 'J Zico Kolter']",poster,"['initialization', 'init', 'covariance', 'gaussian', 'convolutional neural network', 'convmixer', 'convnext', 'transfer learning', 'spatial mixing', 'computer vision', 'convolution']","Neural network weights are typically initialized at random from univariate distributions, controlling just the variance of individual weights even in highly-structured operations like convolutions. Recent ViT-inspired convolutional networks such as ConvMixer and ConvNeXt use large-kernel depthwise convolutions whose learned filters have notable structure; this presents an opportunity to study their empirical covariances. In this work, we first observe that such learned filters have highly-structured covariance matrices, and moreover, we find that covariances calculated from small networks may be used to effectively initialize a variety of larger networks of different depths, widths, patch sizes, and kernel sizes, indicating a degree of model-independence to the covariance structure. Motivated by these findings, we then propose a learning-free multivariate initialization scheme for convolutional filters using a simple, closed-form construction of their covariance. Models using our initialization outperform those using traditional univariate initializations, and typically meet or exceed the performance of those initialized from the covariances of learned filters; in some cases, this improvement can be achieved without training the depthwise convolutional filters at all. Our code is available at https://github.com/locuslab/convcov.",https://api.openreview.net/pdf/304a7da98b79c8c15e8baa9f038951976a9ef764.pdf,llm,https://scholar.google.com/scholar?q=Understanding+the+Covariance+Structure+of+Convolutional+Filters
PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales,2023,ICLR,"['PeiFeng Wang', 'Aaron Chan', 'Filip Ilievski', 'Muhao Chen', 'Xiang Ren']",poster,"['Commonsense reasoning', 'free-text rationale', 'rationale generation', 'faithful reasoning']","Neural language models (LMs) have achieved impressive results on various language-based reasoning tasks by utilizing latent knowledge encoded in their own pretrained parameters. To make this reasoning process more explicit, recent works retrieve a rationalizing LM's internal knowledge by training or prompting it to generate free-text rationales, which can be used to guide task predictions made by either the same LM or a separate reasoning LM. However, rationalizing LMs require expensive rationale annotation and/or computation, without any assurance that their generated rationales improve LM task performance or faithfully reflect LM decision-making. In this paper, we propose PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns to faithfully reason over rationales via counterfactual regularization. First, PINTO maps out a suitable reasoning process for the task input by prompting a frozen rationalizing LM to generate a free-text rationale. Second, PINTO's reasoning LM is fine-tuned to solve the task using the generated rationale as context, while regularized to output less confident predictions when the rationale is perturbed. Across four datasets, we show that PINTO significantly improves the generalization ability of the reasoning LM, yielding higher performance on both in-distribution and out-of-distribution test sets. Also, we find that PINTO's rationales are more faithful to its task predictions than those generated by competitive baselines.",https://api.openreview.net/pdf/7e3d881a1ec0910d26a1dcbaea914860cb610c81.pdf,llm,https://scholar.google.com/scholar?q=PINTO:+Faithful+Language+Reasoning+Using+Prompt-Generated+Rationales
Linearly Mapping from Image to Text Space,2023,ICLR,"['Jack Merullo', 'Louis Castricato', 'Carsten Eickhoff', 'Ellie Pavlick']",poster,"['representation learning', 'deep learning', 'grounded language learning', 'nlp', 'dl', 'image', 'image captioning', 'language grounding', 'grounded']","The extent to which text-only language models (LMs)  learn to represent the physical, non-linguistic world is an open question. Prior work has shown that pretrained LMs can be taught to ``understand'' visual inputs when the models' parameters are updated on image captioning tasks. We test a stronger hypothesis: that the conceptual representations learned by text-only models are functionally equivalent (up to a linear transformation) to those learned by models trained on vision tasks. Specifically, we show that the image representations from vision models can be transferred as continuous prompts to frozen LMs by training only a single linear projection. Using these to prompt the LM achieves competitive performance on captioning and visual question answering tasks compared to models that tune both the image encoder and text decoder (such as the MAGMA model). We compare three image encoders with increasing amounts of linguistic supervision seen during pretraining: BEIT (no linguistic information), NF-ResNET (lexical category information), and CLIP (full natural language descriptions). We find that all three encoders perform equally well at transferring visual property information to the language model (e.g., whether an animal is large or small), but that image encoders pretrained with linguistic supervision more saliently encode category information (e.g., distinguishing hippo vs.\ elephant) and thus perform significantly better on benchmark language-and-vision tasks. Our results indicate that LMs encode conceptual information structurally similarly to vision-based models, even those that are solely trained on images.",https://api.openreview.net/pdf/bb73f5907bc91ecfb1c8ee44e7e84b62e3f33c49.pdf,representation;transfer learning;llm,https://scholar.google.com/scholar?q=Linearly+Mapping+from+Image+to+Text+Space
Causal Confusion and Reward Misidentification in Preference-Based Reward Learning,2023,ICLR,"['Jeremy Tien', 'Jerry Zhi-Yang He', 'Zackory Erickson', 'Anca Dragan', 'Daniel S. Brown']",poster,"['reward learning', 'robustness', 'preference-based learning']","Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states---resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optimizing misidentified rewards drives the policy off the reward's training distribution, resulting in high predicted (learned) rewards but low true rewards. These findings illuminate the susceptibility of preference learning to reward misidentification and causal confusion---failure to consider even one of many factors can result in unexpected, undesirable behavior. ",https://api.openreview.net/pdf/f41368bc311fd0e894120cf88134acdbc361ec94.pdf,reinforcement learning;zero_few-shot;llm,https://scholar.google.com/scholar?q=Causal+Confusion+and+Reward+Misidentification+in+Preference-Based+Reward+Learning
Compositional Task Representations for Large Language Models,2023,ICLR,"['NAN SHAO', 'Zefan Cai', 'Hanwei xu', 'Chonghua Liao', 'Yanan Zheng', 'Zhilin Yang']",poster,[],"Large language models have shown a remarkable cross-task generalization ability. Most prior work assumed that prompts effectively extract knowledge from language models to facilitate generalization to new tasks. This perspective led to numerous studies on improving prompts. In contrast, we introduce a new perspective, compositional generalization, that views each task as a composition of latent codes and generalizes to test tasks by a new composition of seen codes. To this end, we propose a novel prompt-free approach, Compositional Task Representations (CTR), that employs multi-task training to learn a discrete, compositional codebook. Empirically, our CTR substantially outperforms prompt-based methods in zero-label learning on average. According to our analysis, some of the learned CTR codes are interpretable to human and demonstrate a certain degree of controllability.
",https://api.openreview.net/pdf/ef7361f4ac0604d204d4c3f22d833e3e5d4c3163.pdf,representation;multi-task;llm,https://scholar.google.com/scholar?q=Compositional+Task+Representations+for+Large+Language+Models
Broken Neural Scaling Laws,2023,ICLR,"['Ethan Caballero', 'Kshitij Gupta', 'Irina Rish', 'David Krueger']",poster,"['Scaling Laws', 'Scaling', 'Scale', 'Big Learning', 'Deep Learning', 'Artificial Neural Networks']","We present a smoothly broken power law functional form (referred to by us as a broken neural scaling law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning (single agent and multi-agent). When compared to other functional forms for neural scaling behavior, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models and extrapolates scaling behavior that other functional forms are incapable of expressing such as the non-monotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. See arXiv for longer version of this paper. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws",https://api.openreview.net/pdf/b9c49ee6a7c5cbe69e7836796acf1b820da86174.pdf,reinforcement learning;zero_few-shot;transformer;generative model;contrastive learning;metric;multi-agent;multimodal;llm,https://scholar.google.com/scholar?q=Broken+Neural+Scaling+Laws
Safe Exploration Incurs Nearly No Additional Sample Complexity for Reward-Free RL,2023,ICLR,"['Ruiquan Huang', 'Jing Yang', 'Yingbin Liang']",poster,"['Reward-free RL', 'Safety constraint', 'Sample complexity', 'Pure exploration']","Reward-free reinforcement learning (RF-RL), a recently introduced RL paradigm, relies on random action-taking to explore the unknown environment without any reward feedback information. While the primary goal of the exploration phase in RF-RL is to reduce the uncertainty in the estimated model with minimum number of trajectories, in practice, the agent often needs to abide by certain safety constraint at the same time. It remains unclear how such safe exploration requirement would affect the corresponding sample complexity in order to achieve the desired optimality of the obtained policy in planning. In this work, we make a first attempt to answer this question. In particular, we consider the scenario where a safe baseline policy is known beforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET) framework. We then particularize the SWEET framework to the tabular and the low-rank MDP settings, and develop algorithms coined Tabular-SWEET and Low-rank-SWEET, respectively. Both algorithms leverage the concavity and continuity of the newly introduced truncated value functions, and are guaranteed to achieve zero constraint violation during exploration with high probability. Furthermore, both algorithms can provably find a near-optimal policy subject to any constraint in the planning phase. Remarkably, the sample complexities under both algorithms match or even outperform the state of the art in their constraint-free counterparts up to some constant factors, proving that safety constraint hardly increases the sample complexity for RF-RL.",https://api.openreview.net/pdf/4d89566649ca26b4992a3fab78d893feb9ca8dc1.pdf,reinforcement learning;optimization;zero_few-shot;low-rank;llm,https://scholar.google.com/scholar?q=Safe+Exploration+Incurs+Nearly+No+Additional+Sample+Complexity+for+Reward-Free+RL
Diffusion-based Image Translation using disentangled style and content representation,2023,ICLR,"['Gihyun Kwon', 'Jong Chul Ye']",poster,"['DDPM', 'CLIP', 'Image Translation', 'ViT']","Diffusion-based image translation guided by  semantic texts   or a single target image   has enabled flexible style transfer which is not limited to the specific domains. 
Unfortunately, due to the stochastic nature of diffusion models, it is often  difficult to maintain the original content of the image  during the reverse diffusion.
To address this, here we present a novel diffusion-based unsupervised image translation method, dubbed as DiffuseIT, using disentangled style and content representation.
 Specifically, inspired by the  slicing Vision Transformer, we extract intermediate keys of multihead self attention layer  from ViT model and used them as the content preservation loss. Then, an image guided style transfer is performed by matching the [CLS] classification token from the denoised samples and target image, whereas additional CLIP loss is used for the text-driven style transfer.
  To further accelerate the semantic change during the reverse  diffusion, we also propose a novel semantic divergence loss and resampling strategy. 
 Our experimental results show that the proposed method outperforms state-of-the-art baseline models in both text-guided and image-guided translation tasks. ",https://api.openreview.net/pdf/b3174c74984a2e90538982e09e40225a474d34e0.pdf,graph;transformer;representation;transfer learning;diffusion models;llm,https://scholar.google.com/scholar?q=Diffusion-based+Image+Translation+using+disentangled+style+and+content+representation
Large Language Models are Human-Level Prompt Engineers,2023,ICLR,"['Yongchao Zhou', 'Andrei Ioan Muresanu', 'Ziwen Han', 'Keiran Paster', 'Silviu Pitis', 'Harris Chan', 'Jimmy Ba']",poster,"['few-shot learning', 'automated reasoning', 'large language models']","By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the ""program,"" optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 21/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.",https://api.openreview.net/pdf/e7ec7c76cacea5d0272b93de5a569d872e7344e6.pdf,graph;zero_few-shot;generative model;llm,https://scholar.google.com/scholar?q=Large+Language+Models+are+Human-Level+Prompt+Engineers
Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus,2023,ICLR,"['Gang Li', 'Yang Li']",poster,"['vision-language', 'UI', 'few-shot', 'finetuning', 'multi-task']","Mobile UI understanding is important for enabling various interaction tasks such as UI automation and accessibility. Previous mobile UI modeling often depends on the view hierarchy information of a screen, which directly provides the structural data of the UI, with the hope to bypass challenging tasks of visual modeling from screen pixels. However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information. As a result, despite the use of view hierarchies could offer short-term gains, it may ultimately hinder the applicability and performance of the model. In this paper, we propose Spotlight, a vision-only approach for mobile UI understanding. Specifically, we enhance a vision-language model that only takes the screenshot of the UI and a region of interest on the screen---the focus---as the input. This general architecture of Spotlight is easily scalable and capable of performing a range of UI modeling tasks. Our experiments show that our model establishes SoTA results on several representative UI tasks and outperforms previous methods that use both screenshots and view hierarchies as inputs. Furthermore, we explore multi-task learning and few-shot prompting capacities of the proposed models, demonstrating promising results in the multi-task learning direction.",https://api.openreview.net/pdf/ad305a1a4c4b0a2571863546dc680f91c8b5b9f1.pdf,graph;multi-task;multimodal;llm,https://scholar.google.com/scholar?q=Spotlight:+Mobile+UI+Understanding+using+Vision-Language+Models+with+a+Focus
Decomposed Prompting: A Modular Approach for Solving Complex Tasks,2023,ICLR,"['Tushar Khot', 'Harsh Trivedi', 'Matthew Finlayson', 'Yao Fu', 'Kyle Richardson', 'Peter Clark', 'Ashish Sabharwal']",poster,"['prompting', 'decomposition', 'in-context learning', 'reasoning', 'few-shot prompts', 'multi-step reasoning']","Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired.
We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.",https://api.openreview.net/pdf/c0a9bac140b181384176f474f3533e053b8d663d.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Decomposed+Prompting:+A+Modular+Approach+for+Solving+Complex+Tasks
Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors,2023,ICLR,"['Sizhe Chen', 'Geng Yuan', 'Xinwen Cheng', 'Yifan Gong', 'Minghai Qin', 'Yanzhi Wang', 'Xiaolin Huang']",poster,"['data protection', 'poisoning attack', 'self-ensemble', 'deep neural network']","As data becomes increasingly vital, a company would be very cautious about releasing data, because the competitors could use it to train high-performance models, thereby posing a tremendous threat to the company's commercial competence. To prevent training good models on the data, we could add imperceptible perturbations to it. Since such perturbations aim at hurting the entire training process, they should reflect the vulnerability of DNN training, rather than that of a single model. Based on this new idea, we seek perturbed examples that are always unrecognized (never correctly classified) in training. In this paper, we uncover them by model checkpoints' gradients, forming the proposed self-ensemble protection (SEP), which is very effective because (1) learning on examples ignored during normal training tends to yield DNNs ignoring normal examples; (2) checkpoints' cross-model gradients are close to orthogonal, meaning that they are as diverse as DNNs with different architectures. That is, our amazing performance of ensemble only requires the computation of training one model. By extensive experiments with 9 baselines on 3 datasets and 5 architectures, SEP is verified to be a new state-of-the-art, e.g., our small $\ell_\infty=2/255$ perturbations reduce the accuracy of a CIFAR-10 ResNet18 from 94.56% to 14.68%, compared to 41.35% by the best-known method. Code is available at https://github.com/Sizhe-Chen/SEP.",https://api.openreview.net/pdf/03e69010cd3d91e35491934b95bbd8839f6416bc.pdf,llm,https://scholar.google.com/scholar?q=Self-Ensemble+Protection:+Training+Checkpoints+Are+Good+Data+Protectors
Effectively Modeling Time Series with Simple Discrete State Spaces,2023,ICLR,"['Michael Zhang', 'Khaled Kamal Saab', 'Michael Poli', 'Tri Dao', 'Karan Goel', 'Christopher Re']",poster,"['time series', 'forecasting', 'state-space models', 'time series classification']","Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express  autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix---a canonical representation for discrete-time processes---which enables SpaceTime's SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a ""closed-loop"" variation of the companion SSM, which enables SpaceTime to predict many future time-steps by generating its own layer-wise inputs. For efficient training and inference, we introduce an algorithm that reduces the memory and compute of a forward pass with the companion matrix. With sequence length $\ell$ and state-space size $d$, we go from $\tilde{O}(d \ell)$ naïvely to $\tilde{O}(d + \ell)$. In experiments, our contributions lead to state-of-the-art results on extensive and diverse benchmarks, with best or second-best AUROC on 6 / 7 ECG and speech time series classification, and best MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1) fits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more accurately on longer horizons than prior state-of-the-art, and (3) speeds up training on real-world ETTh1 data by 73% and 80% relative wall-clock time over Transformers and LSTMs.",https://api.openreview.net/pdf/ab3d042895227ba8357ce14f5695c199eaf81a23.pdf,transformer;representation;inference;llm,https://scholar.google.com/scholar?q=Effectively+Modeling+Time+Series+with+Simple+Discrete+State+Spaces
"Momentum Stiefel Optimizer, with Applications to Suitably-Orthogonal Attention, and Optimal Transport",2023,ICLR,"['Lingkai Kong', 'Yuqing Wang', 'Molei Tao']",poster,[],"The problem of optimization on Stiefel manifold, i.e., minimizing functions of (not necessarily square) matrices that satisfy orthogonality constraints, has been extensively studied. Yet, a new approach is proposed based on, for the first time, an interplay between thoughtfully designed continuous and discrete dynamics. It leads to a gradient-based optimizer with intrinsically added momentum. This method exactly preserves the manifold structure but does not require additional operation to keep momentum in the changing (co)tangent space, and thus has low computational cost and pleasant accuracy. Its generalization to adaptive learning rates is also demonstrated. Notable performances are observed in practical tasks. For instance, we found that placing orthogonal constraints on attention heads of trained-from-scratch Vision Transformer (Dosovitskiy et al., 2020) could markedly improve its performance, when our optimizer is used, and it is better that each head is made orthogonal within itself but not necessarily to other heads. This optimizer also makes the useful notion of Projection Robust Wasserstein Distance (Paty and Cuturi, 2019; Lin et al., 2020) for high-dim. optimal transport even more effective.",https://api.openreview.net/pdf/efc53aa7247c696d0a77dc0fd44bcd24095d2fee.pdf,graph;optimization;zero_few-shot;transformer;adaptive;llm,"https://scholar.google.com/scholar?q=Momentum+Stiefel+Optimizer,+with+Applications+to+Suitably-Orthogonal+Attention,+and+Optimal+Transport"
Characteristic Neural Ordinary Differential Equation,2023,ICLR,"['Xingzi Xu', 'Ali Hasan', 'Khalil Elkhalil', 'Jie Ding', 'Vahid Tarokh']",poster,"['Neural ODE', 'Differential Equation', 'Method of characteristics']","We propose Characteristic-Neural Ordinary Differential Equations (C-NODEs), a framework for extending Neural Ordinary Differential Equations (NODEs) beyond ODEs. While NODE models the evolution of latent variables as the solution to an ODE, C-NODE models the evolution of the latent variables as the solution of a family of first-order partial differential equations (PDEs) along curves on which the PDEs reduce to ODEs, referred to as characteristic curves. This reduction along characteristic curves allows for analyzing PDEs through standard techniques used for ODEs, in particular the adjoint sensitivity method. We also derive C-NODE-based continuous normalizing flows, which describe the density evolution of latent variables along multiple dimensions. Empirical results demonstrate the improvements provided by the proposed method for irregularly sampled time series prediction on MuJoCo, PhysioNet, and Human Activity datasets and classification and density estimation on CIFAR-10, SVHN, and MNIST datasets given a similar computational budget as the existing NODE methods.
The results also provide empirical evidence that the learned curves improve the system efficiency using a lower number of parameters and function evaluations compared with those of the baselines. ",https://api.openreview.net/pdf/58468a45facccea4727809a26664270da375e8d8.pdf,zero_few-shot;flow;llm,https://scholar.google.com/scholar?q=Characteristic+Neural+Ordinary+Differential+Equation
Artificial Neuronal Ensembles with Learned Context Dependent Gating,2023,ICLR,"['Matthew James Tilley', 'Michelle Miller', 'David Freedman']",poster,"['Continual Learning', 'Catastrophic Forgetting']","Biological neural networks are capable of recruiting different sets of neurons to encode different memories. However, when training artificial neural networks on a set of tasks, typically, no mechanism is employed for selectively producing anything analogous to these neuronal ensembles. Further, artificial neural networks suffer from catastrophic forgetting, where the network's performance rapidly deteriorates as tasks are learned sequentially. By contrast, sequential learning is possible for a range of biological organisms. We introduce Learned Context Dependent Gating (LXDG), a method to flexibly allocate and recall `artificial neuronal ensembles', using a particular network structure and a new set of regularization terms. Activities in the hidden layers of the network are modulated by gates, which are dynamically produced during training. The gates are outputs of networks themselves, trained with a sigmoid output activation. The regularization terms we have introduced correspond to properties exhibited by biological neuronal ensembles. The first term penalizes low gate sparsity, ensuring that only a specified fraction of the network is used. The second term ensures that previously learned gates are recalled when the network is presented with input from previously learned tasks. Finally, there is a regularization term responsible for ensuring that new tasks are encoded in gates that are as orthogonal as possible from previously used ones. We demonstrate the ability of this method to alleviate catastrophic forgetting on continual learning benchmarks. When the new regularization terms are included in the model along with Elastic Weight Consolidation (EWC) it achieves better performance on the benchmark `permuted MNIST' than with EWC alone. The benchmark `rotated MNIST' demonstrates how similar tasks recruit similar neurons to the artificial neuronal ensemble. ",https://api.openreview.net/pdf/86c094a695d14e94090e05aae069fe94004b8e07.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Artificial+Neuronal+Ensembles+with+Learned+Context+Dependent+Gating
Progressive Prompts: Continual Learning for Language Models,2023,ICLR,"['Anastasia Razdaibiedina', 'Yuning Mao', 'Rui Hou', 'Madian Khabsa', 'Mike Lewis', 'Amjad Almahairi']",poster,"['natural language processing', 'continual learning', 'prompt tuning']","We introduce Progressive Prompts – a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement >20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.",https://api.openreview.net/pdf/fd24677a6ed80ba3b6c8dbbcacfbeb6bbd507bc5.pdf,graph;zero_few-shot;transfer learning;llm,https://scholar.google.com/scholar?q=Progressive+Prompts:+Continual+Learning+for+Language+Models
Systematic Rectification of Language Models via Dead-end Analysis,2023,ICLR,"['Meng Cao', 'Mehdi Fatemi', 'Jackie CK Cheung', 'Samira Shabanian']",poster,"['Language Models', 'Detoxification', 'Dead-end Theory', 'Reinforcement Learning.']","With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses. One way to reduce the risk of LLMs generating undesired discourses is to alter the training of the LLM. This can be very restrictive due to demanding computation requirements. Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse. Here, we center detoxification on the probability that the finished discourse is ultimately considered toxic. That is, at each point, we advise against token selections proportional to how likely a finished text from this point will be toxic. To this end, we formally extend the dead-end theory from the recent reinforcement learning (RL) literature to also cover uncertain outcomes. Our approach, called rectification, utilizes a separate but significantly smaller model for detoxification, which can be applied to diverse LLMs as long as they share the same vocabulary. Importantly, our method does not require access to the internal representations of the LLM, but only the token probability distribution at each decoding step. We believe this is important since many LLMs today are hosted in servers and only accessible through APIs. When applied to various LLMs, including GPT-3, our approach generates notably better results compared to the base LLMs and other techniques in terms of the overall language and detoxification performance.",https://api.openreview.net/pdf/04f652450ee4706ff6565dc9df8580e1881050b2.pdf,reinforcement learning;representation;llm,https://scholar.google.com/scholar?q=Systematic+Rectification+of+Language+Models+via+Dead-end+Analysis
A Theoretical Framework for Inference and Learning in Predictive Coding Networks,2023,ICLR,"['Beren Millidge', 'Yuhang Song', 'Tommaso Salvatori', 'Thomas Lukasiewicz', 'Rafal Bogacz']",poster,"['predictive coding', 'backpropagation', 'target propagation', 'machine learning', 'neuroscience']","Predictive coding (PC) is an influential theory in computational neuroscience, which argues that the cortex forms unsupervised world models by implementing a hierarchical process of prediction error minimization. PC networks (PCNs) are trained in two phases. First, neural activities are updated to optimize the network's response to external stimuli. Second, synaptic weights are updated to consolidate this change in activity --- an algorithm called \emph{prospective configuration}. While previous work has shown how in various limits, PCNs can be found to approximate backpropagation (BP), recent work has demonstrated that PCNs operating in this standard regime, which does not approximate BP, nevertheless obtain competitive training and generalization performance to BP-trained networks while outperforming them on various tasks. However, little is understood theoretically about the properties and dynamics of PCNs in this regime. In this paper, we provide a comprehensive theoretical analysis of the properties of PCNs trained with prospective configuration. We first derive analytical results concerning the inference equilibrium for PCNs and a previously unknown close connection relationship to target propagation (TP). Secondly, we provide a theoretical analysis of learning in PCNs as a variant of generalized expectation-maximization and use that to prove the convergence of PCNs to critical points of the BP loss function, thus showing that deep PCNs can, in theory, achieve the same generalization performance as BP, while maintaining their unique advantages.",https://api.openreview.net/pdf/15d048b2a93e6a13bade281774bb0e064b51237c.pdf,inference;llm,https://scholar.google.com/scholar?q=A+Theoretical+Framework+for+Inference+and+Learning+in+Predictive+Coding+Networks
Promptagator: Few-shot Dense Retrieval From 8 Examples,2023,ICLR,"['Zhuyun Dai', 'Vincent Y Zhao', 'Ji Ma', 'Yi Luan', 'Jianmo Ni', 'Jing Lu', 'Anton Bakalov', 'Kelvin Guu', 'Keith Hall', 'Ming-Wei Chang']",poster,"['large language model', 'few-shot prompting', 'information retrieval']","Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other retrieval tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval problems, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To address this, we introduce Prompt-based Query Generation forRetrieval (Promptagator): for each task, we feed the few-shot examples to a large language model (LLM) and prompt it to behave as a task-specific query generator. Using this, we can synthetically generate a large number of relevant queries for any document, yielding abundant data for training task-specific retrievers --- with no reliance on traditional resources such as Natural Questions (Kwiatkowskiet al., 2019) or MS MARCO (Nguyen et al., 2016). Surprisingly, Promptagator with only 8 annotated examples enables efficient dual encoder retrievers to outperform computationally more expensive models trained on MS MARCO such as ColBERT v2 (Santhanam et al., 2022) by more than 1.2 points nDCG@10 on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 points nDCG@10 improvement. Our studies show that synthetic query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.",https://api.openreview.net/pdf/79a0f9b78ef87a8465c2f60eac8f96b996c84b38.pdf,transformer;generative model;transfer learning;llm,https://scholar.google.com/scholar?q=Promptagator:+Few-shot+Dense+Retrieval+From+8+Examples
Language models are multilingual chain-of-thought reasoners,2023,ICLR,"['Freda Shi', 'Mirac Suzgun', 'Markus Freitag', 'Xuezhi Wang', 'Suraj Srivats', 'Soroush Vosoughi', 'Hyung Won Chung', 'Yi Tay', 'Sebastian Ruder', 'Denny Zhou', 'Dipanjan Das', 'Jason Wei']",poster,"['multilingual', 'reasoning', 'large language model']","We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at AnonymousLink and the supplementary material.
",https://api.openreview.net/pdf/972d6eaf77336eece16b7ec5bdb9565b06423b8a.pdf,llm,https://scholar.google.com/scholar?q=Language+models+are+multilingual+chain-of-thought+reasoners
Recitation-Augmented Language Models,2023,ICLR,"['Zhiqing Sun', 'Xuezhi Wang', 'Yi Tay', 'Yiming Yang', 'Denny Zhou']",poster,"['Large Language Models', 'In-context Learning', 'Memorization', 'Closed-book Question Answering', 'CBQA']","We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs’ own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on three pre-trained models (In-house LM, UL2, and OPT) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at ""https://github.com/Edward-Sun/RECITE"".",https://api.openreview.net/pdf/693f49dd101c5c13e74972b49546fdff73d91ac4.pdf,generative model;augmentation;llm,https://scholar.google.com/scholar?q=Recitation-Augmented+Language+Models
Reward Design with Language Models,2023,ICLR,"['Minae Kwon', 'Sang Michael Xie', 'Kalesha Bullard', 'Dorsa Sadigh']",poster,"['reward design', 'foundation models', 'gpt3', 'reward specification', 'reinforcement learning', 'human-ai interaction']","Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by using a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperforms RL agents trained with reward functions learned via supervised learning. ",https://api.openreview.net/pdf/696171827b35dfe4e639dfe0644bf0f279f84c75.pdf,reinforcement learning;graph;zero_few-shot;multimodal;llm,https://scholar.google.com/scholar?q=Reward+Design+with+Language+Models
Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation,2023,ICLR,"['Linfeng Zhao', 'Huazhe Xu', 'Lawson L.S. Wong']",poster,[],"Differentiable planning promises end-to-end differentiability and adaptivity. However, an issue prevents it from scaling up to larger-scale problems: they need to differentiate through forward iteration layers to compute gradients, which couples forward computation and backpropagation and needs to balance forward planner performance and computational cost of the backward pass. To alleviate this issue, we propose to differentiate through the Bellman fixed-point equation to decouple forward and backward passes for Value Iteration Network and its variants, which enables constant backward cost (in planning horizon) and flexible forward budget and helps scale up to large tasks. We study the convergence stability, scalability, and efficiency of the proposed implicit version of VIN and its variants and demonstrate their superiorities on a range of planning tasks: 2D navigation, visual navigation, and 2-DOF manipulation in configuration space and workspace.",https://api.openreview.net/pdf/c9ba511ff253534e8b5c8b381259eb8b04b6406a.pdf,transformer;llm,https://scholar.google.com/scholar?q=Scaling+up+and+Stabilizing+Differentiable+Planning+with+Implicit+Differentiation
Spatio-temporal point processes with deep non-stationary kernels,2023,ICLR,"['Zheng Dong', 'Xiuyuan Cheng', 'Yao Xie']",poster,"['point process', 'neural network', 'non-stationary kernel', 'low-rank model']","Point process data are becoming ubiquitous in modern applications, such as social networks, health care, and finance. Despite the powerful expressiveness of the popular recurrent neural network (RNN) models for point process data, they may not successfully capture sophisticated non-stationary dependencies in the data due to their recurrent structures. Another popular type of deep model for point process data is based on representing the influence kernel (rather than the intensity function) by neural networks. We take the latter approach and develop a new deep non-stationary influence kernel that can model non-stationary spatio-temporal point processes. The main idea is to approximate the influence kernel with a novel and general low-rank decomposition, enabling efficient representation through deep neural networks and computational efficiency and better performance. We also take a new approach to maintain the non-negativity constraint of the conditional intensity by introducing a log-barrier penalty. We demonstrate our proposed method's good performance and computational efficiency compared with the state-of-the-art on simulated and real data. ",https://api.openreview.net/pdf/805a22ec2ddcfcaefd7076337990eb22fe609119.pdf,optimization;zero_few-shot;representation;low-rank;llm,https://scholar.google.com/scholar?q=Spatio-temporal+point+processes+with+deep+non-stationary+kernels
How gradient estimator variance and bias impact learning in neural networks,2023,ICLR,"['Arna Ghosh', 'Yuhan Helena Liu', 'Guillaume Lajoie', 'Konrad Kording', 'Blake Aaron Richards']",poster,"['Computational Neuroscience', 'learning and plasticity', 'Credit assignment', 'Imperfect gradient descent', 'Gradient approximation', 'Biologically-plausible learning', 'Neuromorphic computing', 'Neural networks']","There is growing interest in understanding how real brains may approximate gradients and how gradients can be used to train neuromorphic chips. However, neither real brains nor neuromorphic chips can perfectly follow the loss gradient, so parameter updates would necessarily use gradient estimators that have some variance and/or bias. Therefore, there is a need to understand better how variance and bias in gradient estimators impact learning dependent on network and task properties. Here, we show that variance and bias can impair learning on the training data, but some degree of variance and bias in a gradient estimator can be beneficial for generalization. We find that the ideal amount of variance and bias in a gradient estimator are dependent on several properties of the network and task: the size and activity sparsity of the network, the norm of the gradient, and the curvature of the loss landscape. As such, whether considering biologically-plausible learning algorithms or algorithms for training neuromorphic chips, researchers can analyze these properties to determine whether their approximation to gradient descent will be effective for learning given their network and task properties.",https://api.openreview.net/pdf/78403b3ae7bbdc9e73614cc0b261982b15a6a1f0.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=How+gradient+estimator+variance+and+bias+impact+learning+in+neural+networks
Evaluating Representations with Readout Model Switching,2023,ICLR,"['Yazhe Li', 'Jorg Bornschein', 'Marcus Hutter']",poster,"['Representation Learning', 'Evaluation', 'Expert Switching', 'Minumum Description Length']","Although much of the success of Deep Learning builds on learning good representations, a rigorous method to evaluate their quality is lacking. In this paper, we treat the evaluation of representations as a model selection problem and propose to use the Minimum Description Length (MDL) principle to devise an evaluation metric. Contrary to the established practice of limiting the capacity of the readout model, we design a hybrid discrete and continuous-valued model space for the readout models and employ a switching strategy to combine their predictions. The MDL score takes model complexity, as well as data efficiency into account. As a result, the most appropriate model for the specific task and representation will be chosen, making it a unified measure for comparison. The proposed metric can be efficiently computed with an online method and we present results for pre-trained vision encoders of various architectures (ResNet and ViT) and objective functions (supervised and self-supervised) on a range of downstream tasks. We compare our methods with accuracy-based approaches and show that the latter are inconsistent when multiple readout models are used. Finally, we discuss important properties revealed by our evaluations such as model scaling, preferred readout model, and data efficiency.",https://api.openreview.net/pdf/e6852d6a9b749e54b195047d6910e5c27fa9474a.pdf,zero_few-shot;transformer;representation;online learning;metric;llm,https://scholar.google.com/scholar?q=Evaluating+Representations+with+Readout+Model+Switching
Error Sensitivity Modulation based Experience Replay: Mitigating Abrupt Representation Drift in Continual Learning,2023,ICLR,"['Fahad Sarfraz', 'Elahe Arani', 'Bahram Zonooz']",poster,"['Continual Learning', 'Catastrophic Forgetting', 'Multi memory System', 'Experience Replay', 'Error-Sensitivity modulation', 'Brain inspired Algorithm', 'Representation Drift']","Humans excel at lifelong learning, as the brain has evolved to be robust to distribution shifts and noise in our ever-changing environment. Deep neural networks (DNNs), however, exhibit catastrophic forgetting and the learned representations drift drastically as they encounter a new task. This alludes to a different error-based learning mechanism in the brain. Unlike DNNs, where learning scales linearly with the magnitude of the error, the sensitivity to errors in the brain decreases as a function of their magnitude. To this end, we propose ""ESMER"" which employs a principled mechanism to modulate error sensitivity in a dual-memory rehearsal-based system. Concretely, it maintains a memory of past errors and uses it to modify the learning dynamics so that the model learns more from small consistent errors compared to large sudden errors. We also propose ""Error-Sensitive Reservoir Sampling"" to maintain episodic memory, which leverages the error history to pre-select low-loss samples as candidates for the buffer, which are better suited for retaining information. Empirical results show that ESMER effectively reduces forgetting and abrupt drift in representations at the task boundary by gradually adapting to the new task while consolidating knowledge. Remarkably, it also enables the model to learn under high levels of label noise, which is ubiquitous in real-world data streams.",https://api.openreview.net/pdf/1fbbc169fd74408bb6e6db0a1a2363e7161ae239.pdf,graph;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Error+Sensitivity+Modulation+based+Experience+Replay:+Mitigating+Abrupt+Representation+Drift+in+Continual+Learning
Differentially Private Adaptive Optimization with Delayed Preconditioners,2023,ICLR,"['Tian Li', 'Manzil Zaheer', 'Ken Liu', 'Sashank J. Reddi', 'Hugh Brendan McMahan', 'Virginia Smith']",poster,"['adaptive optimization', 'differential privacy']","Privacy costs may negate the benefits of using adaptive optimizers in differentially private model training. Prior works typically address this issue by using auxiliary information (e.g., public data) to boost the effectiveness of adaptive optimization. In this work, we explore techniques to estimate and efficiently adapt to gradient geometry in private adaptive optimization without auxiliary data. Motivated by the observation that adaptive methods can tolerate stale preconditioners, we propose differentially private adaptive training with delayed preconditioners (DP^2), a simple method that constructs delayed but less noisy preconditioners to better realize the benefits of adaptivity. Theoretically, we provide convergence guarantees for our method for both convex and non-convex problems, and analyze trade-offs between delay and privacy noise reduction. Empirically, we explore DP^2 across several real-world datasets, demonstrating that it can improve convergence speed by as much as 4× relative to non-adaptive baselines and match the performance of state-of-the-art optimization methods that require auxiliary data.",https://api.openreview.net/pdf/66a6dccdd07b9a1103315a658487355c6e004885.pdf,optimization;adaptive;llm,https://scholar.google.com/scholar?q=Differentially+Private+Adaptive+Optimization+with+Delayed+Preconditioners
Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions,2023,ICLR,"['Ruben Villegas', 'Mohammad Babaeizadeh', 'Pieter-Jan Kindermans', 'Hernan Moraldo', 'Han Zhang', 'Mohammad Taghi Saffar', 'Santiago Castro', 'Julius Kunze', 'Dumitru Erhan']",poster,"['generative models', 'video generation', 'video prediction', 'text to video']","We present Phenaki, a model capable of realistic video synthesis given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new causal model for learning video representation which compresses the video to a small discrete tokens representation. This tokenizer is auto-regressive in time, which allows it to work with video representations of different length. 
To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts.",https://api.openreview.net/pdf/fe8e106a2746992c9c2e658bdc8cb9c89cc5a39a.pdf,graph;zero_few-shot;transformer;representation;generative model;llm,https://scholar.google.com/scholar?q=Phenaki:+Variable+Length+Video+Generation+from+Open+Domain+Textual+Descriptions
FIT: A Metric for Model Sensitivity,2023,ICLR,"['Ben Zandonati', 'Adrian Alan Pol', 'Maurizio Pierini', 'Olya Sirkin', 'Tal Kopetz']",poster,"['Fisher Information', 'Quantization']","Model compression is vital to the deployment of deep learning on edge devices. Low precision representations, achieved via quantization of weights and activations, can reduce inference time and memory requirements. However, quantifying and predicting the response of a model to the changes associated with this procedure remains challenging. This response is non-linear and heterogeneous throughout the network. Understanding which groups of parameters and activations are more sensitive to quantization than others is a critical stage in maximizing efficiency. For this purpose, we propose FIT. Motivated by an information geometric perspective, FIT combines the Fisher information with a model of quantization. We find that FIT can estimate the final performance of a network without retraining. FIT effectively fuses contributions from both parameter and activation quantization into a single metric. Additionally, FIT is fast to compute when compared to existing methods, demonstrating favourable convergence properties. These properties are validated experimentally across hundreds of quantization configurations, with a focus on layer-wise mixed-precision quantization.",https://api.openreview.net/pdf/e2969454c67149d7a4865ba3bd2d0d4f7978ce21.pdf,graph;zero_few-shot;representation;inference;metric;llm,https://scholar.google.com/scholar?q=FIT:+A+Metric+for+Model+Sensitivity
Lossless Adaptation of Pretrained Vision Models For Robotic Manipulation,2023,ICLR,"['Mohit Sharma', 'Claudio Fantacci', 'Yuxiang Zhou', 'Skanda Koppula', 'Nicolas Heess', 'Jon Scholz', 'Yusuf Aytar']",poster,[],"Recent works have shown that large models pretrained on common visual learning tasks can provide useful representations for a wide range of specialized perception problems, as well as a variety of robotic manipulation tasks.  While prior work on robotic manipulation has predominantly used frozen pretrained features, we demonstrate that in robotics this approach can fail to reach optimal performance, and that fine-tuning of the full model can lead to significantly better results. Unfortunately, fine-tuning disrupts the pretrained visual representation, and causes representational drift towards the fine-tuned task thus leading to a loss of the versatility of the original model. We introduce a method for lossless adaptation to address this shortcoming of classical fine-tuning. We demonstrate that appropriate placement of our parameter efficient adapters can significantly reduce the performance gap between frozen pretrained representations and full end-to-end fine-tuning without changes to the original representation and thus preserving original capabilities of the pretrained model. We perform a comprehensive investigation across three major model architectures (ViTs, NFNets, and ResNets), supervised (ImageNet-1K classification) and self-supervised pretrained weights (CLIP, BYOL, Visual MAE) in three manipulation task domains and 35 individual tasks, and demonstrate that our claims are strongly validated in various settings. Please see real world videos at https://sites.google.com/view/robo-adapters",https://api.openreview.net/pdf/28f4471b307550d3ccc1dd10e23ed088114a0109.pdf,graph;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Lossless+Adaptation+of+Pretrained+Vision+Models+For+Robotic+Manipulation
Noise-Robust De-Duplication at Scale,2023,ICLR,"['Emily Silcock', ""Luca D'Amico-Wong"", 'Jinglin Yang', 'Melissa Dell']",poster,[],"Identifying near duplicates within large, noisy text corpora has a myriad of applications that range from de-duplicating training datasets, reducing privacy risk, and evaluating test set leakage, to identifying reproduced news articles and literature within large corpora. Across these diverse applications, the overwhelming majority of work relies on $N$-grams. Limited efforts have been made to evaluate how well $N$-gram methods perform, in part because it is unclear how one could create an unbiased evaluation dataset for a massive corpus. This study uses the unique timeliness of historical news wires to create a 27,210 document dataset, with 122,876 positive duplicate pairs, for studying noise-robust de-duplication. The time-sensitivity of news makes comprehensive hand labelling feasible - despite the massive overall size of the corpus - as duplicates occur within a narrow date range. The study then develops and evaluates a range of de-duplication methods: hashing and $N$-gram overlap (which predominate in the literature), a contrastively trained bi-encoder, and a ``re-rank'' style approach combining a bi- and cross-encoder. The neural approaches significantly outperform hashing and $N$-gram overlap. We show that the bi-encoder scales well, de-duplicating a 10 million article corpus on a single GPU card in a matter of hours. We also apply our pre-trained model to the RealNews and patent portions of C4 (Colossal Clean Crawled Corpus), illustrating that a neural approach can identify many near duplicates missed by hashing, in the presence of various types of noise. The public release of our NEWS-COPY de-duplication dataset, codebase, and the pre-trained models will facilitate further research and applications.",https://api.openreview.net/pdf/8b9428278fecebbf54dd7baa215439f4ffb8f5f8.pdf,contrastive learning;llm,https://scholar.google.com/scholar?q=Noise-Robust+De-Duplication+at+Scale
Planning with Large Language Models for Code Generation,2023,ICLR,"['Shun Zhang', 'Zhenfang Chen', 'Yikang Shen', 'Mingyu Ding', 'Joshua B. Tenenbaum', 'Chuang Gan']",poster,"['Large Language Model', 'Code Generation', 'Planning']","Existing large language model-based code generation pipelines typically use beam search or sampling algorithms during the decoding process. Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation. In this work, we propose a novel Transformer decoding algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs. Specifically, instead of simply optimizing the likelihood of the generated sequences, the Transformer makes use of a planner that generates candidate programs and tests them on public test cases. The Transformer can therefore make more informed decisions and generate tokens that will eventually lead to higher-quality programs. We also design a mechanism that shares information between the Transformer and the planner to make our algorithm computationally efficient. We empirically evaluate our framework with several large language models as backbones on public coding challenge benchmarks, showing that 1) it can generate programs that consistently achieve higher performance compared with competing baseline methods; 2) it enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objective.",https://api.openreview.net/pdf/5f8b793197851829ddf2e08915b38f1549cb5b9d.pdf,transformer;generative model;llm,https://scholar.google.com/scholar?q=Planning+with+Large+Language+Models+for+Code+Generation
Heterogeneous Neuronal and Synaptic Dynamics for Spike-Efficient Unsupervised Learning: Theory and Design Principles,2023,ICLR,"['Biswadeep Chakraborty', 'Saibal Mukhopadhyay']",poster,"['theory', 'spiking neural network', 'LIF', 'STDP', 'heterogeneity', 'memory capacity', 'spike efficiency', 'bayesian optimization']","This paper shows that the heterogeneity in neuronal and synaptic dynamics reduces the spiking activity of a Recurrent Spiking Neural Network (RSNN) while improving prediction performance, enabling spike-efficient (unsupervised) learning.
We analytically show that the diversity in neurons' integration/relaxation dynamics improves an RSNN's ability to learn more distinct input patterns (higher memory capacity), leading to improved classification and prediction performance. We further prove that heterogeneous Spike-Timing-Dependent-Plasticity (STDP) dynamics of synapses reduce spiking activity but preserve memory capacity. The analytical results motivate Heterogeneous RSNN design using Bayesian optimization to determine heterogeneity in neurons and synapses to improve $\mathcal{E}$, defined as the ratio of spiking activity and memory capacity. The empirical results on time series classification and prediction tasks show that optimized HRSNN increases performance and reduces spiking activity compared to a homogeneous RSNN.",https://api.openreview.net/pdf/9f7c495d032385cfaeae5a10554a9593efeb5a33.pdf,optimization;bayesian;llm,https://scholar.google.com/scholar?q=Heterogeneous+Neuronal+and+Synaptic+Dynamics+for+Spike-Efficient+Unsupervised+Learning:+Theory+and+Design+Principles
Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning,2023,ICLR,"['Ivona Najdenkoska', 'Xiantong Zhen', 'Marcel Worring']",poster,"['multimodal', 'few-shot learning', 'meta-learning', 'transformers', 'vision and language models']","Multimodal few-shot learning is challenging due to the large domain gap between vision and language modalities. Existing methods are trying to communicate visual concepts as prompts to frozen language models, but rely on hand-engineered task induction to reduce the hypothesis space. To make the whole process learnable, we introduce a multimodal meta-learning approach. Specifically, our approach decomposes the training of the model into a set of related multimodal few-shot tasks. We define a meta-mapper network, acting as a meta-learner, to efficiently bridge frozen large-scale vision and language models and leverage their already learned capacity. By updating the learnable parameters only of the meta-mapper, it learns to accrue shared meta-knowledge among these tasks. Thus, it can rapidly adapt to newly presented samples with only a few gradient updates. Importantly, it induces the task in a completely data-driven manner, with no need for a hand-engineered task induction. We evaluate our approach on recently proposed multimodal few-shot benchmarks, measuring how rapidly the model can bind novel visual concepts to words and answer visual questions by observing only a limited set of labeled examples. The experimental results show that our meta-learning approach outperforms the baseline across multiple datasets and various training settings while being computationally more efficient.",https://api.openreview.net/pdf/0e9bd6133a3659d2a5883ce2063de3dfff12c275.pdf,graph;zero_few-shot;meta-learning;multimodal;llm,https://scholar.google.com/scholar?q=Meta+Learning+to+Bridge+Vision+and+Language+Models+for+Multimodal+Few-Shot+Learning
Neural Implicit Shape Editing using Boundary Sensitivity,2023,ICLR,"['Arturs Berzins', 'Moritz Ibing', 'Leif Kobbelt']",poster,[],"Neural fields are receiving increased attention as a geometric representation due to their ability to compactly store detailed and smooth shapes and easily undergo topological changes. Compared to classic geometry representations, however, neural representations do not allow the user to exert intuitive control over the shape. Motivated by this, we leverage boundary sensitivity to express how perturbations in parameters move the shape boundary. This allows us to interpret the effect of each learnable parameter and study achievable deformations. With this, we perform geometric editing: finding a parameter update that best approximates a globally prescribed deformation. Prescribing the deformation only locally allows the rest of the shape to change according to some prior, such as semantics or deformation rigidity. Our method is agnostic to the model and its training and updates the NN in-place. Furthermore, we show how boundary sensitivity helps to optimize and constrain objectives (such as surface area and volume), which are difficult to compute without first converting to another representation, such as a mesh.",https://api.openreview.net/pdf/2bb2869a2fc20265557dcaa1d8fb95e2369b2d06.pdf,optimization;zero_few-shot;transformer;representation;metric;llm,https://scholar.google.com/scholar?q=Neural+Implicit+Shape+Editing+using+Boundary+Sensitivity
TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation,2023,ICLR,"['Hyesu Lim', 'Byeonggeun Kim', 'Jaegul Choo', 'Sungha Choi']",poster,"['Test time adaptation', 'Domain adaptation', 'Batch Normalization']","This paper proposes a novel batch normalization strategy for test-time adaptation. Recent test-time adaptation methods heavily rely on the modified batch normalization, i.e., transductive batch normalization (TBN), which calculates the mean and the variance from the current test batch rather than using the running mean and variance obtained from the source data, i.e., conventional batch normalization (CBN). Adopting TBN that employs test batch statistics mitigates the performance degradation caused by the domain shift. However, re-estimating normalization statistics using test data depends on impractical assumptions that a test batch should be large enough and be drawn from i.i.d. stream, and we observed that the previous methods with TBN show critical performance drop without the assumptions. In this paper, we identify that CBN and TBN are in a trade-off relationship and present a new test-time normalization (TTN) method that interpolates the statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer. Our proposed TTN improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios. TTN is widely applicable to other test-time adaptation methods that rely on updating model parameters via backpropagation. We demonstrate that adopting TTN further improves their performance and achieves state-of-the-art performance in various standard benchmarks.",https://api.openreview.net/pdf/cac516c4c0f5e33e98d8f7a4853bbbc87a45b3f1.pdf,llm,https://scholar.google.com/scholar?q=TTN:+A+Domain-Shift+Aware+Batch+Normalization+in+Test-Time+Adaptation
Generating Sequences by Learning to Self-Correct,2023,ICLR,"['Sean Welleck', 'Ximing Lu', 'Peter West', 'Faeze Brahman', 'Tianxiao Shen', 'Daniel Khashabi', 'Yejin Choi']",poster,"['Language models', 'text generation']","Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that  Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator. 
",https://api.openreview.net/pdf/492050ca3a1a68a786565a323ad790877b8af5b3.pdf,optimization;generative model;online learning;llm,https://scholar.google.com/scholar?q=Generating+Sequences+by+Learning+to+Self-Correct
$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference,2023,ICLR,"['Benfeng Xu', 'Quan Wang', 'Zhendong Mao', 'Yajuan Lyu', 'Qiaoqiao She', 'Yongdong Zhang']",poster,"['Large Language Models', 'In-Context Learning', 'K Nearest Neighbors']","In-Context Learning (ICL), which formulates target tasks as prompt completion conditioned on in-context demonstrations, has become the prevailing utilization of LLMs. In this paper, we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment. To address both challenges, we advocate a simple and effective solution, $k$NN Prompting, which first queries LLM with training data for distributed representations, then predicts test instances by simply referring to nearest neighbors. We conduct comprehensive experiments to demonstrate its two-fold superiority: 1) Calibration-Free: $k$NN Prompting does not directly align LLM output distribution with task-specific label space, instead leverages such distribution to align test and training instances. It significantly outperforms state-of-the-art calibration-based methods under comparable few-shot scenario. 2) Beyond-Context: $k$NN Prompting can further scale up effectively with as many training data as are available, continually bringing substantial improvements. The scaling trend holds across 10 orders of magnitude ranging from 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8B to 30B. It successfully bridges data scaling into model scaling, and brings new potentials for the gradient-free paradigm of LLM deployment. Code is publicly available at https://github.com/BenfengXu/KNNPrompting",https://api.openreview.net/pdf/feaebae1ecc4dd15ee54a25f37db2412f7e62789.pdf,graph;transformer;representation;inference;llm,https://scholar.google.com/scholar?q=$k$NN+Prompting:+Beyond-Context+Learning+with+Calibration-Free+Nearest+Neighbor+Inference
HypeR: Multitask Hyper-Prompted Training Enables Large-Scale Retrieval Generalization,2023,ICLR,"['ZeFeng Cai', 'Chongyang Tao', 'Tao Shen', 'Can Xu', 'Xiubo Geng', 'Xin Alex Lin', 'Liang He', 'Daxin Jiang']",poster,"['Uniformed Large-Scale Retrieval', 'Multi-Task hyper-prompted training', 'Retrieval Generalization']","Recently, large-scale text retrieval has made impressive progress, facilitating both information retrieval and downstream knowledge-intensive tasks (e.g., open-domain QA and dialogue). With a moderate amount of data, a neural text retriever can outperform traditional methods such as BM25 by a large step. However, while being applied to out-of-domain data, the performance of a neural retriever degrades considerably. Therefore, how to enable a retriever to perform more robustly across different domains or tasks  and even show strong zero-shot transfer ability is critical for building scalable IR systems. To this end, we propose HypeR, a hyper-prompted training mechanism to enable uniform retrieval across tasks of different domains. Specifically, our approach jointly trains the query encoder with a shared prompt-based parameter pool and a prompt synthesizer that dynamically composes hyper-prompt for encoding each query from different tasks or domains. Besides, to avoid the mode collapse of prompt attention distribution for different queries, we design a contrastive prompt regularization that promotes the mode of prompt attention to be aligned and uniform. Through multi-task hyper-prompted training, our retriever can master the ability to dynamically represent different types of queries and transfer knowledge across different domains and tasks. Extensive experiments show our model attains better retrieval performance across different tasks and better zero-shot transfer ability compared with various previous methods.",https://api.openreview.net/pdf/ffa8e64f13b08c527104225518e4a9fd1371ace2.pdf,zero_few-shot;transformer;contrastive learning;transfer learning;multi-task;multimodal;llm,https://scholar.google.com/scholar?q=HypeR:+Multitask+Hyper-Prompted+Training+Enables+Large-Scale+Retrieval+Generalization
Bidirectional Language Models Are Also Few-shot Learners,2023,ICLR,"['Ajay Patel', 'Bryan Li', 'Mohammad Sadegh Rasooli', 'Noah Constant', 'Colin Raffel', 'Chris Callison-Burch']",poster,"['prompting', 'prompt-based learning', 'mt5', 't5', 'machine translation', 'llm', 'large language models']","Large language models such as GPT-3 (Brown et al., 2020) can perform arbitrary tasks without undergoing fine-tuning after being prompted with only a few labeled examples. An arbitrary task can be reformulated as a natural language prompt, and a language model can be asked to generate the completion, indirectly performing the task in a paradigm known as prompt-based learning. To date, emergent prompt-based learning capabilities have mainly been demonstrated for unidirectional language models. However, bidirectional language models pre-trained on denoising objectives such as masked language modeling produce stronger learned representations for transfer learning. This motivates the possibility of prompting bidirectional models, but their pre-training objectives have made them largely incompatible with the existing prompting paradigm. We present SAP (Sequential Autoregressive Prompting), a technique that enables the prompting of bidirectional models. Utilizing the machine translation task as a case study, we prompt the bidirectional mT5 model (Xue et al., 2021) with SAP and demonstrate its few-shot and zero-shot translations outperform the few-shot translations of unidirectional models like GPT-3 and XGLM (Lin et al., 2021), despite mT5's approximately 50% fewer parameters. We further show SAP is effective on question answering and summarization. For the first time, our results demonstrate prompt-based learning is an emergent property of a broader class of language models, rather than only unidirectional models.",https://api.openreview.net/pdf/89bcb6060a2f549ce65ae9acf0cddd043ce5ea1d.pdf,zero_few-shot;representation;transfer learning;llm,https://scholar.google.com/scholar?q=Bidirectional+Language+Models+Are+Also+Few-shot+Learners
Latent Graph Inference using Product Manifolds,2023,ICLR,"['Haitz Sáez de Ocáriz Borde', 'Anees Kazi', 'Federico Barbero', 'Pietro Lio']",poster,"['Latent Graph Inference', 'Product Manifolds', 'Graph Neural Networks']","Graph Neural Networks usually rely on the assumption that the graph topology is available to the network as well as optimal for the downstream task. Latent graph inference allows models to dynamically learn the intrinsic graph structure of problems where the connectivity patterns of data may not be directly accessible. In this work, we generalize the discrete Differentiable Graph Module (dDGM) for latent graph learning. The original dDGM architecture used the Euclidean plane to encode latent features based on which the latent graphs were generated. By incorporating Riemannian geometry into the model and generating more complex embedding spaces, we can improve the performance of the latent graph inference system. In particular, we propose a computationally tractable approach to produce product manifolds of constant curvature model spaces that can encode latent features of varying structure. The latent representations mapped onto the inferred product manifold are used to compute richer similarity measures that are leveraged by the latent graph learning model to obtain optimized latent graphs. Moreover, the curvature of the product manifold is learned during training alongside the rest of the network parameters and based on the downstream task, rather than it being a static embedding space. Our novel approach is tested on a wide range of datasets, and outperforms the original dDGM model.",https://api.openreview.net/pdf/65504b4ddfce51504320874a061bd0ea2e9bf146.pdf,graph;zero_few-shot;representation;inference;llm,https://scholar.google.com/scholar?q=Latent+Graph+Inference+using+Product+Manifolds
Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection,2023,ICLR,"['Kaifeng Gao', 'Long Chen', 'Hanwang Zhang', 'Jun Xiao', 'Qianru Sun']",poster,"['Prompt Tuning', 'Video Relation Detection']","Prompt tuning with large-scale pretrained vision-language models empowers open-vocabulary prediction trained on limited base categories, e.g., object classification and detection. In this paper, we propose compositional prompt tuning with motion cues: an extended prompt tuning paradigm for compositional predictions of video data. In particular, we present Relation Prompt (RePro) for Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where conventional prompt tuning is easily biased to certain subject-object combinations and motion patterns. To this end, RePro addresses the two technical challenges of Open-VidVRD: 1) the prompt tokens should respect the two different semantic roles of subject and object, and 2) the tuning should account for the diverse spatiotemporal motion patterns of the subject-object compositions. Our RePro achieves a new state-of-the-art performance on two VidVRD benchmarks of not only the base training object and predicate categories, but also the unseen ones. Extensive ablations also demonstrate the effectiveness of the proposed compositional and multi-mode design of prompt. Code is available at https://github.com/Dawn-LX/OpenVoc-VidVRD.",https://api.openreview.net/pdf/c9c64a5b7d6d1b3b977f973ab553b08580576420.pdf,llm,https://scholar.google.com/scholar?q=Compositional+Prompt+Tuning+with+Motion+Cues+for+Open-vocabulary+Video+Relation+Detection
Understanding Neural Coding on Latent Manifolds by Sharing Features and Dividing Ensembles,2023,ICLR,"['Martin Bjerke', 'Lukas Schott', 'Kristopher T Jensen', 'Claudia Battistin', 'David A. Klindt', 'Benjamin Adric Dunn']",poster,"['neuroscience', 'neural activity', 'tuning curves', 'neural ensemble detection', 'grid cells', 'latent variable models']","Systems neuroscience relies on two complementary views of neural data, characterized by single neuron tuning curves and analysis of population activity. These two perspectives combine elegantly in neural latent variable models that constrain the relationship between latent variables and neural activity, modeled by simple tuning curve functions. This has recently been demonstrated using Gaussian processes, with applications to realistic and topologically relevant latent manifolds. Those and previous models, however, missed crucial shared coding properties of neural populations. We propose $\textit{feature sharing}$ across neural tuning curves which significantly improves performance and helps optimization. We also propose a solution to the $\textit{ensemble detection}$ problem, where different groups of neurons, i.e., ensembles, can be modulated by different latent manifolds. Achieved through a soft clustering of neurons during training, this allows for the separation of mixed neural populations in an unsupervised manner. These innovations lead to more interpretable models of neural population activity that train well and perform better even on mixtures of complex latent manifolds. Finally, we apply our method on a recently published grid cell dataset, and recover distinct ensembles, infer toroidal latents and predict neural tuning curves in a single integrated modeling framework.",https://api.openreview.net/pdf/a1c1782d5a6a71b6c2c9ce03e3bf919b9e9a2ad9.pdf,optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Understanding+Neural+Coding+on+Latent+Manifolds+by+Sharing+Features+and+Dividing+Ensembles
DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training,2023,ICLR,"['Wei Li', 'Linchao Zhu', 'Longyin Wen', 'Yi Yang']",poster,"['Zero-shot captioning', 'Decoder training', 'Multi-modal learning']","Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong zero-shot transfer capability in many discriminative tasks, e.g., image classification. Their adaptation to zero-shot image-conditioned text generation tasks has drawn increasing interest. Prior arts approach to zero-shot captioning by either utilizing the existing large language models (e.g., GPT-2) or pre-training the encoder-decoder network in an end-to-end manner. However, the large language models may not generate sensible descriptions due to the task discrepancy between captioning and language modeling, while the end-to-end pre-training requires paired data and extensive computational resources. In this work, we propose a simple framework, named DeCap, for zero-shot captioning. We introduce a lightweight visual-aware language decoder. This decoder is both data-efficient and computation-efficient: 1) it only requires the \textit{text} data for training, easing the burden on the collection of paired data. 2) it does not require end-to-end training. When trained with text-only data, the decoder takes the text embedding extracted from the off-the-shelf CLIP encoder as a prefix embedding. The challenge is that the decoder is trained on the text corpus but at the inference stage, it needs to generate captions based on visual inputs. Though the CLIP text embedding and the visual embedding are correlated, the \textit{modality gap} issue is widely observed in multi-modal contrastive models that prevents us from directly taking the visual embedding as the prefix embedding. We propose a training-free mechanism to reduce the modality gap. We project the visual embedding into the CLIP text embedding space, while the projected embedding retains the information of the visual input. Taking the projected embedding as the prefix embedding, the decoder generates high-quality descriptions that match the visual input. The experiments show that DeCap outperforms other zero-shot captioning methods and unpaired captioning methods by a large margin on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps. We apply DeCap to video captioning and achieve state-of-the-art zero-shot performance on MSR-VTT and ActivityNet-Captions. The code is available at https://github.com/dhg-wei/DeCap.",https://api.openreview.net/pdf/20281fe81003b21131076887ded62556d1c2dc19.pdf,graph;zero_few-shot;generative model;contrastive learning;inference;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=DeCap:+Decoding+CLIP+Latents+for+Zero-Shot+Captioning+via+Text-Only+Training
Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling,2023,ICLR,"['Huayu Chen', 'Cheng Lu', 'Chengyang Ying', 'Hang Su', 'Jun Zhu']",poster,"['offline reinforcement learning', 'generative models', 'diffusion models', 'behavior modeling']","In offline reinforcement learning, weighted regression is a common method to ensure the learned policy stays close to the behavior policy and to prevent selecting out-of-sample actions. In this work, we show that due to the limited distributional expressivity of policy models, previous methods might still select unseen actions during training, which deviates from their initial motivation. To address this problem, we adopt a generative approach by decoupling the learned policy into two parts: an expressive generative behavior model and an action evaluation model. The key insight is that such decoupling avoids learning an explicitly parameterized policy model with a closed-form expression. Directly learning the behavior policy allows us to leverage existing advances in generative modeling, such as diffusion-based methods, to model diverse behaviors. As for action evaluation, we combine our method with an in-sample planning technique to further avoid selecting out-of-sample actions and increase computational efficiency. Experimental results on D4RL datasets show that our proposed method achieves competitive or superior performance compared with state-of-the-art offline RL methods, especially in complex tasks such as AntMaze. We also empirically demonstrate that our method can successfully learn from a heterogeneous dataset containing multiple distinctive but similarly successful strategies, whereas previous unimodal policies fail.",https://api.openreview.net/pdf/20c4b3fc4c6f763a48b751e9c6af706646df4083.pdf,reinforcement learning;offline reinforcement learning;zero_few-shot;generative model;llm,https://scholar.google.com/scholar?q=Offline+Reinforcement+Learning+via+High-Fidelity+Generative+Behavior+Modeling
$\mathscr{N}$-WL: A New Hierarchy of Expressivity for Graph Neural Networks,2023,ICLR,"['Qing Wang', 'Dillon Ze Chen', 'Asiri Wijesinghe', 'Shouheng Li', 'Muhammad Farhan']",poster,"['Graph neural network', 'Weisfeiler-Lehman algorithm', 'k-WL hierarchy', 'graph classification']","The expressive power of Graph Neural Networks (GNNs) is fundamental for understanding their capabilities and  limitations, i.e., what graph properties can or cannot be learnt by a GNN. Since standard GNNs have been characterised to be upper-bounded by the Weisfeiler-Lehman (1-WL) algorithm, recent attempts concentrated on developing more expressive GNNs in terms of the $k$-WL hierarchy, a well-established framework for graph isormorphism tests. In this work we show that, contrary to the widely accepted view, the $k$-WL hierarchy is not well-suited for measuring expressive GNNs. This is due to limitations that are inherent to high-dimensional WL algorithms such as the lack of a natural interpretation and high computational costs, which makes it difficult to draw any firm conclusions about the expressive power of GNNs beyond 1-WL. Thus, we propose a novel hierarchy of graph isomorphism tests, namely Neighbourhood WL ($\mathscr{N}$-WL), and also establish a new theorem on the equivalence of expressivity between induced connected subgraphs and induced subgraphs within this hierarchy. Further, we design a GNN model upon $\mathscr{N}$-WL, Graph Neighbourhood Neural Network (G3N), and empirically verify its expressive power on synthetic and real-world benchmarks.",https://api.openreview.net/pdf/a1b6ae5c041645625de45e46ad10cdde373f930c.pdf,graph;llm,https://scholar.google.com/scholar?q=$\mathscr{N}$-WL:+A+New+Hierarchy+of+Expressivity+for+Graph+Neural+Networks
Explaining RL Decisions with Trajectories,2023,ICLR,"['Shripad Vilasrao Deshmukh', 'Arpan Dasgupta', 'Balaji Krishnamurthy', 'Nan Jiang', 'Chirag Agarwal', 'Georgios Theocharous', 'Jayakumar Subramanian']",poster,"['Explainable RL', 'Explainable AI', 'Offline Reinforcement Learning', 'Trajectory Attribution', 'Decision-Aware AI']","Explanation is a key component for the adoption of reinforcement learning (RL) in many real-world decision-making problems. In the literature,  the explanation is often provided by saliency attribution to the features of the RL agent's state. In this work, we propose a complementary approach to these explanations, particularly for offline RL, where we attribute the policy decisions of a trained RL agent to the trajectories encountered by it during training. To do so, we encode trajectories in offline training data individually as well as collectively (encoding a set of trajectories). We then attribute policy decisions to a set of trajectories in this encoded space by estimating the sensitivity of the decision with respect to that set.  Further, we demonstrate the effectiveness of the proposed approach in terms of quality of attributions as well as practical scalability in diverse environments that involve both discrete and continuous state and action spaces such as grid-worlds, video games (Atari) and continuous control (MuJoCo). We also conduct a human study on a simple navigation task to observe how their understanding of the task compares with data attributed for a trained RL policy.",https://api.openreview.net/pdf/8c14263279e4c45dd0a74d7e52a0c6d707338882.pdf,reinforcement learning;offline reinforcement learning;llm,https://scholar.google.com/scholar?q=Explaining+RL+Decisions+with+Trajectories
FastFill: Efficient Compatible Model Update,2023,ICLR,"['Florian Jaeckle', 'Fartash Faghri', 'Ali Farhadi', 'Oncel Tuzel', 'Hadi Pouransari']",poster,"['Compatible Representation Learning', 'Image Retrieval', 'Model Regression']","In many retrieval systems the original high dimensional data (e.g., images) is mapped to a lower dimensional feature through a learned embedding model. The task of retrieving the most similar data from a gallery set to a given query data is performed through similarity comparison on features. When the embedding model is updated, it might produce features that are not comparable/compatible with features already in the gallery computed with the old model. Subsequently, all features in the gallery need to be re-computed using the new embedding model -- a computationally expensive process called backfilling. Recently, compatible representation learning methods have been proposed to avoid back-filling. Despite their relative success, there is an inherent trade-off between new model performance and its compatibility with the old model. In this work, we introduce FastFill: a compatible model update process using feature alignment and policy based partial backfilling to promptly elevate retrieval performance. We show that previous backfilling strategies suffer from decreased performance and demonstrate the importance of both the training objective and the ordering in online partial backfilling. We propose a new training method for feature alignment between old and new embedding models using uncertainty estimation. Compared to previous works, we obtain significantly improved backfilling results on a variety of datasets: mAP on ImageNet (+4.4%), Places-365 (+2.7%), and VGG-Face2 (+1.3%). Further, we demonstrate that when updating a biased model with FastFill, the minority subgroup accuracy gap promptly vanishes with a small fraction of partial backfilling.",https://api.openreview.net/pdf/8d20dc462785f41d1a42a73d5f251100169ec180.pdf,graph;zero_few-shot;representation;online learning;llm,https://scholar.google.com/scholar?q=FastFill:+Efficient+Compatible+Model+Update
Better Teacher Better Student: Dynamic Prior Knowledge for Knowledge Distillation,2023,ICLR,"['Martin Zong', 'Zengyu Qiu', 'Xinzhu Ma', 'Kunlin Yang', 'Chunya Liu', 'Jun Hou', 'Shuai Yi', 'Wanli Ouyang']",poster,['Knowledge Distillation'],"Knowledge distillation (KD) has shown very promising capabilities in transferring learning representations from large models (teachers) to small models (students). However, as the capacity gap between students and teachers becomes larger, existing KD methods fail to achieve better results. Our work shows that the 'prior knowledge' is vital to KD, especially when applying large teachers.  Particularly, we propose the dynamic prior knowledge (DPK), which integrates part of teacher's features as the prior knowledge before the feature distillation. This means that our method also takes the teacher's feature as `input', not just `target'. Besides, we dynamically adjust the ratio of the prior knowledge during the training phase according to the feature gap, thus guiding the student in an appropriate difficulty. To evaluate the proposed method, we conduct extensive experiments on two image classification benchmarks (i.e. CIFAR100 and ImageNet) and an object detection benchmark (\i.e. MS COCO). The results demonstrate the superiority of our method in performance under varying settings. Besides, our DPK makes the performance of the student model positively correlated with that of the teacher model, which means that we can further boost the accuracy of students by applying larger teachers. More importantly, DPK provides a fast solution in teacher model selection for any given model. Our codes will be publicly available for reproducibility.",https://api.openreview.net/pdf/2fb7e9a90ff72f47a0a922f31bcff587c30cc103.pdf,representation;transfer learning;distillation;llm,https://scholar.google.com/scholar?q=Better+Teacher+Better+Student:+Dynamic+Prior+Knowledge+for+Knowledge+Distillation
Language Models are Realistic Tabular Data Generators,2023,ICLR,"['Vadim Borisov', 'Kathrin Sessler', 'Tobias Leemann', 'Martin Pawelczyk', 'Gjergji Kasneci']",poster,"['tabular data', 'tabular data generation', 'large language models', 'transformers', 'probabilistic modeling', 'deep neural networks']","Tabular data is among the oldest and most ubiquitous forms of data. However, the generation of synthetic samples with the original data’s characteristics remains a significant challenge for tabular data. While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature. To this end, we propose GReaT (Generation of Realistic Tabular data), which exploits an auto-regressive generative LLM to sample synthetic and yet highly realistic tabular data. Furthermore, GReaT can model tabular data distributions by conditioning on any subset of features; the remaining features are sampled without additional overhead. We demonstrate the effectiveness of the proposed approach in a series of experiments that quantify the validity and quality of the produced data samples from multiple angles. We find that GReaT maintains state-of-the-art performance across numerous real-world and synthetic data sets with heterogeneous feature types coming in various sizes.",https://api.openreview.net/pdf/93e938176cd4da2511c79883813c6bb7781f9804.pdf,graph;transformer;vae;generative model;llm,https://scholar.google.com/scholar?q=Language+Models+are+Realistic+Tabular+Data+Generators
Complexity-Based Prompting for Multi-step Reasoning,2023,ICLR,"['Yao Fu', 'Hao Peng', 'Ashish Sabharwal', 'Peter Clark', 'Tushar Khot']",poster,"['Chain-of-Thoughts', 'Multi-Step Reasoning', 'Large Language Models', 'Prompting']","We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on math word reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority
of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3, our approach substantially improves multi-step reasoning accuracy, with an 8.6% absolute improvement on GSM8K, and 6.4% on MathQA. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",https://api.openreview.net/pdf/45dc479ddf081da97bf30a319e61cd0509d1f701.pdf,llm,https://scholar.google.com/scholar?q=Complexity-Based+Prompting+for+Multi-step+Reasoning
Unsupervised 3D Object Learning through Neuron Activity aware Plasticity,2023,ICLR,"['Beomseok Kang', 'Biswadeep Chakraborty', 'Saibal Mukhopadhyay']",poster,['Hebbian learning'],"We present an unsupervised deep learning model for 3D object classification. Conventional Hebbian learning, a well-known unsupervised model, suffers from loss of local features leading to reduced performance for tasks with complex geometric objects. We present a deep network with a novel Neuron Activity Aware (NeAW) Hebbian learning rule that dynamically switches the neurons to be governed by Hebbian learning or anti-Hebbian learning, depending on its activity. We analytically show that NeAW Hebbian learning relieves the bias in neuron activity, allowing more neurons to attend to the representation of the 3D objects. Empirical results show that the NeAW Hebbian learning outperforms other variants of Hebbian learning and shows higher accuracy over fully supervised models when training data is limited.",https://api.openreview.net/pdf/4bb9100e1fbde4d43b7401d608de56c74c737a5a.pdf,zero_few-shot;representation;metric;3d;llm,https://scholar.google.com/scholar?q=Unsupervised+3D+Object+Learning+through+Neuron+Activity+aware+Plasticity
When Data Geometry Meets Deep Function: Generalizing Offline Reinforcement Learning,2023,ICLR,"['Jianxiong Li', 'Xianyuan Zhan', 'Haoran Xu', 'Xiangyu Zhu', 'Jingjing Liu', 'Ya-Qin Zhang']",poster,"['offline reinforcement learning', 'deep Q functions generalization']","In offline reinforcement learning (RL), one detrimental issue to policy learning is the error accumulation of deep \textit{Q} function in out-of-distribution (OOD) areas. Unfortunately, existing offline RL methods are often over-conservative, inevitably hurting generalization performance outside data distribution. In our study, one interesting observation is that deep \textit{Q} functions approximate well inside the convex hull of training data. Inspired by this, we propose a new method, \textit{DOGE (Distance-sensitive Offline RL with better GEneralization)}. DOGE marries dataset geometry with deep function approximators in offline RL, and enables exploitation in generalizable OOD areas rather than strictly constraining policy within data distribution. Specifically, DOGE trains a state-conditioned distance function that can be readily plugged into standard actor-critic methods as a policy constraint. Simple yet elegant, our algorithm enjoys better generalization compared to state-of-the-art methods on D4RL benchmarks. Theoretical analysis demonstrates the superiority of our approach to existing methods that are solely based on data distribution or support constraints. Code is available at https://github.com/Facebear-ljx/DOGE.",https://api.openreview.net/pdf/94dbfca2646bd9ee54214755138d35cafa230611.pdf,reinforcement learning;offline reinforcement learning;optimization;llm,https://scholar.google.com/scholar?q=When+Data+Geometry+Meets+Deep+Function:+Generalizing+Offline+Reinforcement+Learning
What Do Self-Supervised Vision Transformers Learn?,2023,ICLR,"['Namuk Park', 'Wonjae Kim', 'Byeongho Heo', 'Taekyung Kim', 'Sangdoo Yun']",poster,"['contrastive learning', 'masked image modeling', 'vision transformer', 'representation learning', 'self-supervised learning', 'empirical analysis']","We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM more texture-oriented. (3) CL plays a crucial role in the later layers, while MIM mainly focuses on the early layers. Upon these analyses, we find that CL and MIM can complement each other and observe that even the simplest harmonization can help leverage the advantages of both methods. The code is available at https://github.com/naver-ai/cl-vs-mim.
",https://api.openreview.net/pdf/ad19dc82b73d87a8e38ef475dc72f82e75ea328e.pdf,zero_few-shot;transformer;representation;contrastive learning;llm,https://scholar.google.com/scholar?q=What+Do+Self-Supervised+Vision+Transformers+Learn?
Scaling Forward Gradient With Local Losses,2023,ICLR,"['Mengye Ren', 'Simon Kornblith', 'Renjie Liao', 'Geoffrey Hinton']",poster,[],"Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. The standard forward gradient algorithm suffers from the curse of dimensionality in the number of parameters. In this paper, we propose to scale forward gradient by adding a large number of local greedy loss functions. We consider block-wise, patch-wise, and channel group-wise local losses, and show that activity perturbation reduces variance compared to weight perturbation. Inspired by MLPMixer, we also propose a new architecture, LocalMixer, that is more suitable for local learning. We find local learning can work well with both supervised classification and self-supervised contrastive learning. Empirically, it can match backprop on MNIST and CIFAR-10 and significantly outperform backprop-free algorithms on ImageNet.",https://api.openreview.net/pdf/8b67f407d22905f08e467f5c91f96f9eb5c64a19.pdf,zero_few-shot;transformer;contrastive learning;llm,https://scholar.google.com/scholar?q=Scaling+Forward+Gradient+With+Local+Losses
Memorization-Dilation: Modeling Neural Collapse Under Noise,2023,ICLR,"['Duc Anh Nguyen', 'Ron Levie', 'Julian Lienen', 'Eyke Hüllermeier', 'Gitta Kutyniok']",poster,"['Neural collapse', 'feature representation', 'label smoothing', 'cross entropy']","The notion of neural collapse refers to several emergent phenomena that have been empirically observed across various canonical classification problems.  During the terminal phase of training a deep neural network, the feature embedding of all examples of the same class tend to collapse to a single representation, and the features of different classes tend to separate as much as possible. Neural collapse is often studied through a simplified model, called the layer-peeled model, in which the network is assumed to have ``infinite expressivity'' and can map each data point to any arbitrary representation. In this work we study a more realistic variant of the layer-peeled model, which takes the positivity of the features into account. Furthermore, we extend this model to also incorporate the limited expressivity of the network. Empirical evidence suggests that the memorization of noisy data points leads to a degradation (dilation) of the neural collapse. Using a model of the memorization-dilation (M-D) phenomenon, we show one mechanism by which different losses lead to different performances of the trained network on noisy data. Our proofs reveal why label smoothing, a modification of cross-entropy empirically observed to produce a regularization effect, leads to improved generalization in classification tasks.",https://api.openreview.net/pdf/9f22b86b155fa265265ff7806589485c313427c8.pdf,representation;llm,https://scholar.google.com/scholar?q=Memorization-Dilation:+Modeling+Neural+Collapse+Under+Noise
Spacetime Representation Learning,2023,ICLR,"['Marc T. Law', 'James Lucas']",poster,"['pseudo-Riemannian geometry', 'spacetimes', 'Lorentz geometry', 'Lorentzian causality theory', 'Lorentzian pre-length spaces', 'directed graphs']","Much of the data we encounter in the real world can be represented as directed graphs. In this work, we introduce a general family of representations for directed graphs through connected time-oriented Lorentz manifolds, called ""spacetimes"" in general relativity. Spacetimes intrinsically contain a causal structure that indicates whether or not there exists a causal or even chronological order between points of the manifold, called events. This chronological order allows us to naturally represent directed edges via imposing the correct ordering when the nodes are embedded as events in the spacetime. Previous work in machine learning only considers embeddings lying on the simplest Lorentz manifold or does not exploit the connection between Lorentzian pre-length spaces and directed graphs. We introduce a well-defined approach to map data onto a general family of spacetimes.  We empirically evaluate our framework in the tasks of hierarchy extraction of undirected graphs, directed link prediction and representation of directed graphs.",https://api.openreview.net/pdf/36cdcef566ad431e1ed53bed1c6ffee73624b56e.pdf,graph;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Spacetime+Representation+Learning
Multi-level Protein Structure Pre-training via Prompt Learning,2023,ICLR,"['Zeyuan Wang', 'Qiang Zhang', 'Shuang-Wei HU', 'Haoran Yu', 'Xurui Jin', 'Zhichen Gong', 'Huajun Chen']",poster,"['protein representation learning', 'prompt learning', 'multi-task learning', 'multi-level structure']","A protein can focus on different structure levels to implement its functions. Each structure has its own merit and driving forces in describing some specific characteristics, and they cannot replace each other. Most existing function prediction methods take the tertiary structure as input, unintentionally ignoring the other levels of protein structures. Considering protein sequences can determine multi-level structures, in this paper, we aim to realize the comprehensive potential of protein sequences for function prediction. Specifically, we propose a new prompt-guided multi-task pre-training and fine-tuning framework, and the resulting protein model is called PromptProtein. Through the prompt-guided multi-task pre-training, we learn multiple prompt signals to steer the model to focus on different structure levels. We also design a prompt fine-tuning module to provide downstream tasks the on-demand flexibility of utilizing respective levels of structure information. Extensive experiments on function prediction and protein engineering show that PromptProtein outperforms state-of-the-art methods by large margins.",https://api.openreview.net/pdf/dc43e5fd99a5392aedf0a73a5a11819a2d7ec708.pdf,graph;multi-task;llm,https://scholar.google.com/scholar?q=Multi-level+Protein+Structure+Pre-training+via+Prompt+Learning
M-L2O: Towards Generalizable Learning-to-Optimize by Test-Time Fast Self-Adaptation,2023,ICLR,"['Junjie Yang', 'Xuxi Chen', 'Tianlong Chen', 'Zhangyang Wang', 'Yingbin Liang']",poster,"['L2O', 'Meta Learning', 'Generalization']"," Learning to Optimize (L2O) has drawn increasing attention as it often remarkably accelerates the optimization procedure of complex tasks by ""overfitting"" specific task type, leading to enhanced performance compared to analytical optimizers. Generally, L2O develops a parameterized optimization method (i.e., ""optimizer"") by learning from solving sample problems. This data-driven procedure yields L2O that can efficiently solve problems similar to those seen in training, that is, drawn from the same ""task distribution"". However, such learned optimizers often struggle when new test problems come with a substantially deviation from the training task distribution. This paper investigates a potential solution to this open challenge, by meta-training an L2O optimizer that can perform fast test-time self-adaptation to a out-of-distribution task, in only a few steps. We theoretically characterize the generalization of L2O, and further show that our proposed framework (termed as M-L2O) provably facilitates rapid task adaptation by locating well-adapted initial points for the optimizer weight. Empirical observations on several classic tasks like LASSO and Quadratic, demonstrate that M-L2O converges significantly faster than vanilla L2O with only $5$ steps of adaptation, echoing our theoretical results. Codes are available in https://github.com/VITA-Group/M-L2O.",https://api.openreview.net/pdf/28968ae7fa07510ed3a4c526367db53e51ef3115.pdf,optimization;transformer;meta-learning;llm,https://scholar.google.com/scholar?q=M-L2O:+Towards+Generalizable+Learning-to-Optimize+by+Test-Time+Fast+Self-Adaptation
3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation,2023,ICLR,"['Ho Hin Lee', 'Shunxing Bao', 'Yuankai Huo', 'Bennett A. Landman']",poster,"['Depth-wise Convolution', 'Large Kernel Convolution', 'Convolutional Neural Network', 'Hierarchical Transformer', 'Volumetric Segmentation', 'Medical Image Segmentation']","The recent 3D medical ViTs (e.g., SwinUNETR) achieve the state-of-the-art performances on several 3D volumetric data benchmarks, including 3D medical image segmentation. Hierarchical transformers (e.g., Swin Transformers) reintroduced several ConvNet priors and further enhanced the practical viability of adapting volumetric segmentation in 3D medical datasets. The effectiveness of hybrid approaches is largely credited to the large receptive field for non-local self-attention and the large number of model parameters. We hypothesize that volumetric ConvNets can simulate the large receptive field behavior of these learning approaches with fewer model parameters using depth-wise convolution. In this work, we propose a lightweight volumetric ConvNet, termed 3D UX-Net, which adapts the hierarchical transformer using ConvNet modules for robust volumetric segmentation. Specifically, we revisit volumetric depth-wise convolutions with large kernel (LK) size (e.g. starting from $7\times7\times7$) to enable the larger global receptive fields, inspired by Swin Transformer. We further substitute the multi-layer perceptron (MLP) in Swin Transformer blocks with pointwise depth convolutions and enhance model performances with fewer normalization and activation layers, thus reducing the number of model parameters. 3D UX-Net competes favorably with current SOTA transformers (e.g. SwinUNETR) using three challenging public datasets on volumetric brain and abdominal imaging: 1) MICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI Challenge 2022 AMOS. 3D UX-Net consistently outperforms SwinUNETR with improvement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice (Feta2021). We further evaluate the transfer learning capability of 3D UX-Net with AMOS2022 and demonstrates another improvement of $2.27\%$ Dice (from 0.880 to 0.900). The source code with our proposed model are available at https://github.com/MASILab/3DUX-Net.",https://api.openreview.net/pdf/66c7d9008e0864a9dd656e4b98c11672a8799de1.pdf,graph;transformer;metric;transfer learning;segmentation;3d;llm,https://scholar.google.com/scholar?q=3D+UX-Net:+A+Large+Kernel+Volumetric+ConvNet+Modernizing+Hierarchical+Transformer+for+Medical+Image+Segmentation
Soft Neighbors are Positive Supporters in Contrastive Visual Representation Learning,2023,ICLR,"['Chongjian GE', 'Jiangliu Wang', 'Zhan Tong', 'Shoufa Chen', 'Yibing Song', 'Ping Luo']",poster,"['contrastive learning', 'soft neighbors', 'visual correlation']","Contrastive learning methods train visual encoders by comparing views (e.g., often created via a group of data augmentations on the same instance) from one instance to others. Typically, the views created from one instance are set as positive, while views from other instances are negative. This binary instance discrimination is studied extensively to improve feature representations in self-supervised learning. In this paper, we rethink the instance discrimination framework and find the binary instance labeling insufficient to measure correlations between different samples. For an intuitive example, given a random image instance, there may exist other images in a mini-batch whose content meanings are the same (i.e., belonging to the same category) or partially related (i.e., belonging to a similar category). How to treat the images that correlate similarly to the current image instance leaves an unexplored problem. We thus propose to support the current image by exploring other correlated instances (i.e., soft neighbors). We first carefully cultivate a candidate neighbor set, which will be further utilized to explore the highly-correlated instances. A cross-attention module is then introduced to predict the correlation score (denoted as positiveness) of other correlated instances with respect to the current one. The positiveness score quantitatively measures the positive support from each correlated instance, and is encoded into the objective for pretext training. To this end, our proposed method benefits in discriminating uncorrelated instances while absorbing correlated instances for SSL. We evaluate our soft neighbor contrastive learning method (SNCLR) on standard visual recognition benchmarks, including image classification, object detection, and instance segmentation. The state-of-the-art recognition performance shows that SNCLR is effective in improving feature representations from both ViT and CNN encoders. ",https://api.openreview.net/pdf/fcf2bcabecc2de6a0abb518daa6c6bb2c960aa82.pdf,graph;zero_few-shot;transformer;representation;contrastive learning;augmentation;segmentation;llm,https://scholar.google.com/scholar?q=Soft+Neighbors+are+Positive+Supporters+in+Contrastive+Visual+Representation+Learning
Offline RL for Natural Language Generation with Implicit Language Q Learning,2023,ICLR,"['Charlie Victor Snell', 'Ilya Kostrikov', 'Yi Su', 'Sherry Yang', 'Sergey Levine']",poster,"['offline reinforcement learning', 'natural language processing', 'dialogue', 'controlled generation']","Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability. Our method employs a combination of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing user-specified utility functions. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language generation settings, demonstrating how it can be a more effective utility optimizer than prior approaches for end-to-end dialogue, and how it can effectively optimize high variance reward functions based on subjective judgement, such as whether to label a comment as toxic or not.",https://api.openreview.net/pdf/52d92f1f776e26f5cd3935e847b0f2b77d30b7e9.pdf,reinforcement learning;offline reinforcement learning;optimization;generative model;distillation;llm,https://scholar.google.com/scholar?q=Offline+RL+for+Natural+Language+Generation+with+Implicit+Language+Q+Learning
Automatic Chain of Thought Prompting in Large Language Models,2023,ICLR,"['Zhuosheng Zhang', 'Aston Zhang', 'Mu Li', 'Alex Smola']",poster,"['Chain of Thought Prompting', 'Large Language Models', 'In-context Learning', 'Few-shot Learning', 'Arithmetic Reasoning', 'Commonsense Reasoning', 'Symbolic Reasoning.']","Large Language Models (LLMs) can carry out complex reasoning tasks by generating intermediate reasoning steps. These steps are triggered by what is called chain-of-thought (CoT) prompting, which comes in two flavors: one leverages a simple prompt like ""Let’s think step by step"" to facilitate step-by-step reasoning before answering a question (Zero-Shot-CoT). The other uses manual demonstrations, each composed of a question and a reasoning chain that leads to an answer (Manual-CoT). Unfortunately, the superior performance of the latter strategy crucially hinges on manually generating task-specific demonstrations. This makes it far less scalable and more dependent on the talent of the CoT engineer. We show that such manual efforts may be eliminated by leveraging LLMs to generate the reasoning chains on its own. Since these generated chains often come with mistakes we propose a number of mitigation strategies. Our proposed Auto-CoT method automaticaly samples diverse questions and we perform post-processing quality control to generate usable reasoning chains from Zero-Shot-CoT. On ten public benchmark reasoning tasks, Auto-CoT performs on par with Manual-CoT without the need for human intervention. Code is available at https://github.com/amazon-research/auto-cot.
",https://api.openreview.net/pdf/f566e3690bb4730c79fee6cd769ca1c7f2dc2bc6.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Automatic+Chain+of+Thought+Prompting+in+Large+Language+Models
Re-Imagen: Retrieval-Augmented Text-to-Image Generator,2023,ICLR,"['Wenhu Chen', 'Hexiang Hu', 'Chitwan Saharia', 'William W. Cohen']",poster,"['Diffusion Model', 'Information Retrieval', 'Knowledge Grounding', 'Image Generation']","Research on text-to-image generation has witnessed significant progress in generating diverse and photo-realistic images, driven by diffusion and auto-regressive models trained on large-scale image-text data. Though state-of-the-art models can generate high-quality images of common entities, they often have difficulty generating images of uncommon entities, such as `Chortai (dog)' or `Picarones (food)'. To tackle this issue, we present the Retrieval-Augmented Text-to-Image Generator (Re-Imagen), a generative model that uses retrieved information to produce high-fidelity and faithful images, even for rare or unseen entities. Given a text prompt, Re-Imagen accesses an external multi-modal knowledge base to retrieve relevant (image, text) pairs, and uses them as references to generate the image. With this retrieval step, Re-Imagen is augmented with the knowledge of high-level semantics and low-level visual details of the mentioned entities, and thus improves its accuracy in generating the entities' visual appearances. We train Re-Imagen on a constructed dataset containing (image,text,retrieval) triples to teach the model to ground on both text prompt and retrieval. Furthermore, we develop a new sampling strategy to interleave the classifier-free guidance for text and retrieval condition to balance the text and retrieval alignment. Re-Imagen achieves new SoTA FID results on two image generation benchmarks, such as COCO (\ie, FID = 5.25) and WikiImage (\ie, FID = 5.82) without fine-tuning. To further evaluate the capabilities of the model, we introduce EntityDrawBench, a new benchmark that evaluates image generation for diverse entities, from frequent to rare, across multiple visual domains. Human evaluation on EntityDrawBench shows that Re-Imagen performs on par with the best prior models in photo-realism, but with significantly better real-world faithfulness, especially on less frequent entities. ",https://api.openreview.net/pdf/cd7624d9396c506925128bb17ad6c851714db6e4.pdf,zero_few-shot;generative model;augmentation;multimodal;llm,https://scholar.google.com/scholar?q=Re-Imagen:+Retrieval-Augmented+Text-to-Image+Generator
Link Prediction with Non-Contrastive Learning,2023,ICLR,"['William Shiao', 'Zhichun Guo', 'Tong Zhao', 'Evangelos E. Papalexakis', 'Yozen Liu', 'Neil Shah']",poster,"['graph learning', 'graph neural networks', 'non-contrastive learning', 'link prediction']","Graph neural networks (GNNs) are prominent in the graph machine learning domain, owing to their strong performance across various tasks. A recent focal area is the space of graph self-supervised learning (SSL), which aims to derive useful node representations without labeled data. Notably, many state-of-the-art graph SSL methods are contrastive methods, which use a combination of positive and negative samples to learn node representations. Owing to challenges in negative sampling (slowness and model sensitivity), recent literature introduced non-contrastive methods, which instead only use positive samples. Though such methods have shown promising performance in node-level tasks, their suitability for link prediction tasks, which are concerned with predicting link existence between pairs of nodes (and have broad applicability to recommendation systems contexts) is yet unexplored. In this work, we extensively evaluate the performance of existing non-contrastive methods for link prediction in both transductive and inductive settings. While most existing non-contrastive methods perform poorly overall, we find that, surprisingly, BGRL generally performs well in transductive settings. However, it performs poorly in the more realistic inductive settings where the model has to generalize to links to/from unseen nodes. We find that non-contrastive models tend to overfit to the training graph and use this analysis to propose T-BGRL, a novel non-contrastive framework that incorporates cheap corruptions to improve the generalization ability of the model. This simple modification strongly improves inductive performance in 5/6 of our datasets, with up to a 120% improvement in Hits@50 - all with comparable speed to other non-contrastive baselines, and up to $14\times$ faster than the best-performing contrastive baseline. Our work imparts interesting findings about non-contrastive learning for link prediction and paves the way for future researchers to further expand upon this area.",https://api.openreview.net/pdf/46cb48345cdfc877f29c5c93429f4dd9c06cbc74.pdf,graph;zero_few-shot;representation;contrastive learning;llm,https://scholar.google.com/scholar?q=Link+Prediction+with+Non-Contrastive+Learning
"A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity",2023,ICLR,"['Hongkang Li', 'Meng Wang', 'Sijia Liu', 'Pin-Yu Chen']",poster,"['Vision transformer', 'Learning', 'Generalization', 'Sample comeplxity', 'Token sparsification', 'Theory']","Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, the theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a three-layer ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover,  this paper indicates that a proper token sparsification can improve the test performance by removing label-irrelevant and/or noisy tokens, including spurious correlations. Empirical experiments on synthetic data and CIFAR-10 dataset justify our theoretical results and generalize to deeper ViTs. ",https://api.openreview.net/pdf/31d1481db9db8d93532ef30aa9a64f3a33b188b1.pdf,zero_few-shot;transformer;sparse;llm,"https://scholar.google.com/scholar?q=A+Theoretical+Understanding+of+Shallow+Vision+Transformers:+Learning,+Generalization,+and+Sample+Complexity"
Weighted Ensemble Self-Supervised Learning,2023,ICLR,"['Yangjun Ruan', 'Saurabh Singh', 'Warren Richard Morningstar', 'Alexander A Alemi', 'Sergey Ioffe', 'Ian Fischer', 'Joshua V. Dillon']",poster,"['self-supervised learning', 'ensemble', 'representation learning']","Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses.  We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that those which increase the diversity of ensemble heads lead to better downstream evaluation results. Thorough experiments yield improved prior art baselines which our method still surpasses; e.g., our overall improvement with MSN ViT-B/16 is 3.9 p.p. for 1-shot learning.",https://api.openreview.net/pdf/c3bd68473e3729fbbb9b7485d2dfe3af068d28c4.pdf,graph;zero_few-shot;representation;metric;llm,https://scholar.google.com/scholar?q=Weighted+Ensemble+Self-Supervised+Learning
Offline Congestion Games: How Feedback Type Affects Data Coverage Requirement,2023,ICLR,"['Haozhe Jiang', 'Qiwen Cui', 'Zhihan Xiong', 'Maryam Fazel', 'Simon Shaolei Du']",poster,[],"This paper investigates when one can efficiently recover an approximate Nash Equilibrium (NE) in offline congestion games. The existing dataset coverage assumption in offline general-sum games inevitably incurs a dependency on the number of actions, which can be exponentially large in congestion games. We consider three different types of feedback with decreasing revealed information. Starting from the facility-level (a.k.a., semi-bandit) feedback, we propose a novel one-unit deviation coverage condition and show a pessimism-type algorithm that can recover an approximate NE. For the agent-level (a.k.a., bandit) feedback setting, interestingly, we show the one-unit deviation coverage condition is not sufficient. On the other hand, we convert the game to multi-agent linear bandits and show that with a generalized data coverage assumption in offline linear bandits, we can efficiently recover the approximate NE. Lastly, we consider a novel type of feedback, the game-level feedback where only the total reward from all agents is revealed. Again, we show the coverage assumption for the agent-level feedback setting is insufficient in the game-level feedback setting, and with a stronger version of the data coverage assumption for linear bandits, we can recover an approximate NE. Together, our results constitute the first study of offline congestion games and imply formal separations between different types of feedback.",https://api.openreview.net/pdf/5d536b4991d001b1cb1a44b7b6d5785c2f3012d8.pdf,reinforcement learning;offline reinforcement learning;multi-agent;llm,https://scholar.google.com/scholar?q=Offline+Congestion+Games:+How+Feedback+Type+Affects+Data+Coverage+Requirement
MaskViT: Masked Visual Pre-Training for Video Prediction,2023,ICLR,"['Agrim Gupta', 'Stephen Tian', 'Yunzhi Zhang', 'Jiajun Wu', 'Roberto Martín-Martín', 'Li Fei-Fei']",poster,"['Video Prediction', 'Masked Visual Modeling', 'Visual MPC', 'Transformers']","The ability to predict future visual observations conditioned on past observations and motor commands can enable embodied agents to plan solutions to a variety of tasks in complex environments. This work shows that we can create good video prediction models by pre-training transformers via masked visual modeling. Our approach, named MaskViT, is based on two simple design decisions. First, for memory and training efficiency, we use two types of window attention: spatial and spatiotemporal. Second, during training, we mask a variable percentage of tokens instead of a fixed mask ratio. For inference, MaskViT generates all tokens via iterative refinement where we incrementally decrease the masking ratio following a mask scheduling function. On several datasets we demonstrate that MaskViT outperforms prior works in video prediction, is parameter efficient, and can generate high resolution videos ($256 \times $256). Further, we demonstrate the benefits of inference speedup (up to $512 \times$) due to iterative decoding by using MaskViT for planning on a real robot. Our work suggests that we can endow embodied agents with powerful predictive models by leveraging the general framework of masked visual modeling with minimal domain knowledge. ",https://api.openreview.net/pdf/519ede840ab88f1eeeef20446b915f73429dec70.pdf,reinforcement learning;graph;zero_few-shot;transformer;inference;llm,https://scholar.google.com/scholar?q=MaskViT:+Masked+Visual+Pre-Training+for+Video+Prediction
Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations,2023,ICLR,"['Ziyu Jiang', 'Yinpeng Chen', 'Mengchen Liu', 'Dongdong Chen', 'Xiyang Dai', 'Lu Yuan', 'Zicheng Liu', 'Zhangyang Wang']",poster,"['Mask Image Modeling', 'Contrastive learning']","Recently, both Contrastive Learning (CL) and Mask Image Modeling (MIM) demonstrate that self-supervision is powerful to learn good representations. However, naively combining them is far from success. In this paper, we start by making the empirical observation that a naive joint optimization of CL and MIM losses leads to conflicting gradient directions - more severe as the layers go deeper. This motivates us to shift the paradigm from combining loss at the end, to choosing the proper learning method per network layer. Inspired by experimental observations, we find that MIM and CL are suitable to lower and higher layers, respectively. We hence propose to combine them in a surprisingly simple, ``sequential cascade'' fashion: early layers are first trained under one MIM loss, on top of which latter layers continue to be trained under another CL loss. The proposed Layer Grafted Pre-training learns good visual representations that demonstrate superior label efficiency in downstream applications, in particular yielding strong few-shot performance besides linear evaluation. For instance, on ImageNet-1k, Layer Grafted Pre-training yields 65.5% Top-1 accuracy in terms of 1% few-shot learning with ViT-B/16, which improves MIM and CL baselines by 14.4% and 2.1% with no bells and whistles. The code is available at https://github.com/VITA-Group/layerGraftedPretraining_ICLR23.git.
",https://api.openreview.net/pdf/1d8972d5afc36b1f7508cd08a231b9a41347530b.pdf,graph;optimization;zero_few-shot;representation;contrastive learning;self-supervision;llm,https://scholar.google.com/scholar?q=Layer+Grafted+Pre-training:+Bridging+Contrastive+Learning+And+Masked+Image+Modeling+For+Label-Efficient+Representations
Discovering Latent Knowledge in Language Models Without Supervision,2023,ICLR,"['Collin Burns', 'Haotian Ye', 'Dan Klein', 'Jacob Steinhardt']",poster,"['AI safety', 'AI alignment', 'truthfulness', 'large language models', 'honesty', 'interpretability']","Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.",https://api.openreview.net/pdf/1a2f757c6a56314972af9e4b40e580d5691e0b55.pdf,zero_few-shot;multimodal;imitation learning;llm,https://scholar.google.com/scholar?q=Discovering+Latent+Knowledge+in+Language+Models+Without+Supervision
Is Attention All That NeRF Needs?,2023,ICLR,"['Mukund Varma T', 'Peihao Wang', 'Xuxi Chen', 'Tianlong Chen', 'Subhashini Venugopalan', 'Zhangyang Wang']",poster,"['Neural Radiance Field', 'Transformer', 'Neural Rendering']","We present Generalizable NeRF Transformer (GNT), a transformer-based architecture that reconstructs Neural Radiance Fields (NeRFs) and learns to render novel views on the fly from source views. While prior works on NeRFs optimize a scene representation by inverting a handcrafted rendering equation, GNT achieves neural representation and rendering that generalizes across scenes using transformers at two stages. (1) The view transformer leverages multi-view geometry as an inductive bias for attention-based scene representation, and predicts coordinate-aligned features by aggregating information from epipolar lines on the neighboring views. (2) The ray transformer renders novel views using attention to decode the features from the view transformer along the sampled points during ray marching. Our experiments demonstrate that when optimized on a single scene, GNT can successfully reconstruct NeRF without an explicit rendering formula due to the learned ray renderer. When trained on multiple scenes, GNT consistently achieves state-of-the-art performance when transferring to unseen scenes and outperform all other methods by ~10% on average. Our analysis of the learned attention maps to infer depth and occlusion indicate that attention enables learning a physically-grounded rendering. Our results show the promise of transformers as a universal modeling tool for graphics. Please refer to our project page for video results: https://vita-group.github.io/GNT/",https://api.openreview.net/pdf/d875ba2409ec78faf50cee666b3866b2b99b54f8.pdf,graph;transformer;representation;transfer learning;multimodal;multi-view;llm,https://scholar.google.com/scholar?q=Is+Attention+All+That+NeRF+Needs?
Boosting Adversarial Transferability using Dynamic Cues,2023,ICLR,"['Muzammal Naseer', 'Ahmad Mahmood', 'Salman Khan', 'Fahad Khan']",poster,"['Adversarial attacks', 'Transferability', 'Prompt learning', 'Dynamic video modeling']","The transferability of adversarial perturbations between image models has been extensively studied. In this case, an attack is generated from a known surrogate \eg, the ImageNet trained model, and transferred to change the decision of an unknown (black-box) model trained on an image dataset. However, attacks generated from image models do not capture the dynamic nature of a moving object or a changing scene due to a lack of temporal cues within image models. This leads to reduced transferability of adversarial attacks from representation-enriched \emph{image} models such as Supervised Vision Transformers (ViTs), Self-supervised ViTs (\eg, DINO), and Vision-language models (\eg, CLIP) to black-box \emph{video} models. In this work, we induce dynamic cues within the image models without sacrificing their original performance on images. To this end, we optimize \emph{temporal prompts} through frozen image models to capture motion dynamics. Our temporal prompts are the result of a learnable transformation that allows optimizing for temporal gradients during an adversarial attack to fool the motion dynamics. Specifically, we introduce spatial (image) and temporal (video) cues within the same source model through task-specific prompts. Attacking such prompts maximizes the adversarial transferability from image-to-video and image-to-image models using the attacks designed for image models. As an example, an iterative attack launched from image model Deit-B with temporal prompts reduces generalization (top1 \% accuracy) of a video model by 35\% on Kinetics-400. Our approach also improves adversarial transferability to image models by 9\% on ImageNet w.r.t the current state-of-the-art approach. Our attack results indicate that the attacker does not need specialized architectures, \eg, divided space-time attention, 3D convolutions, or multi-view convolution networks for different data modalities. Image models are effective surrogates to optimize an adversarial attack to fool black-box models in a changing environment over time. Code is available at \url{https://bit.ly/3Xd9gRQ}",https://api.openreview.net/pdf/9e990c20252d6a4dcc08a88751f2f07536fc4f76.pdf,graph;zero_few-shot;transformer;representation;transfer learning;multi-view;3d;llm,https://scholar.google.com/scholar?q=Boosting+Adversarial+Transferability+using+Dynamic+Cues
Active Image Indexing,2023,ICLR,"['Pierre Fernandez', 'Matthijs Douze', 'Herve Jegou', 'Teddy Furon']",poster,"['Indexing', 'Copy detection', 'Image similarity search', 'Watermarking']","Image copy detection and retrieval from large databases leverage two components. First, a neural network maps an image to a vector representation, that is relatively robust to various transformations of the image. Second, an efficient but approximate similarity search algorithm trades scalability (size and speed) against quality of the search, thereby introducing a source of error. 
This paper improves the robustness of image copy detection with active indexing, that optimizes the interplay of these two components.  We reduce the quantization loss of a given image representation by making imperceptible changes to the image before its release. The loss is back-propagated through the deep neural network back to the image, under perceptual constraints. These modifications make the image more retrievable. 
Our experiments show that the retrieval and copy detection of activated images is significantly improved. For instance, activation improves by $+40\%$ the Recall1@1 on various image transformations, and for several popular indexing structures based on product quantization and locality sensitivity hashing.",https://api.openreview.net/pdf/b41256b2ee29f6e5ec3e51cca015a39d47426eab.pdf,optimization;representation;active learning;llm,https://scholar.google.com/scholar?q=Active+Image+Indexing
Quasi-optimal Reinforcement Learning with Continuous Actions,2023,ICLR,"['Yuhan Li', 'Wenzhuo Zhou', 'Ruoqing Zhu']",poster,"['Continuous Treatments', 'Markov Decision Process', 'Safe Action Allocation']","Many real-world applications of reinforcement learning (RL) require making decisions in continuous action environments. In particular, determining the optimal dose level plays a vital role in developing medical treatment regimes. One challenge in adapting existing RL algorithms to medical applications, however, is that the popular infinite support stochastic policies, e.g., Gaussian policy, may assign riskily high dosages and harm patients seriously. Hence, it is important to induce a policy class whose support only contains near-optimal actions, and shrink the action-searching area for effectiveness and reliability. To achieve this, we develop a novel quasi-optimal learning algorithm, which can be easily optimized in off-policy settings with guaranteed convergence under general function approximations. Theoretically, we analyze the consistency, sample complexity, adaptability, and convergence of the proposed algorithm. We evaluate our algorithm with comprehensive simulated experiments and a dose suggestion real application to Ohio Type 1 diabetes dataset.",https://api.openreview.net/pdf/100ce9435fb820f433bb2a423491a43ed4f64862.pdf,reinforcement learning;llm,https://scholar.google.com/scholar?q=Quasi-optimal+Reinforcement+Learning+with+Continuous+Actions
More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity,2023,ICLR,"['Shiwei Liu', 'Tianlong Chen', 'Xiaohan Chen', 'Xuxi Chen', 'Qiao Xiao', 'Boqian Wu', 'Tommi Kärkkäinen', 'Mykola Pechenizkiy', 'Decebal Constantin Mocanu', 'Zhangyang Wang']",poster,"['51x51 kernel', 'Large kernel convolution', 'convolutional neural networks', 'sparsity', 'backbone']","Transformers have quickly shined in the computer vision world since the emergence of Vision Transformers (ViTs). The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models. Very recently, a couple of advanced convolutional models strike back with large kernels motivated by the local-window attention mechanism, showing appealing performance and efficiency. While one of them, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31 with improved performance, the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of advanced ViTs such as Swin Transformer. In this paper, we explore the possibility of training extreme convolutions larger than 31x31 and test whether the performance gap can be eliminated by strategically enlarging convolutions. This study ends up with a recipe for applying extremely large kernels from the perspective of sparsity, which can smoothly scale up kernels to 61x61 with better performance. Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architecture equipped with sparse factorized 51x51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architectures like ConvNeXt and RepLKNet, on ImageNet classification as well as a wide range of downstream tasks including semantic segmentation on ADE20K, object detection on PASCAL VOC 2007, and object detection/segmentation on MS COCO. Codes are available at https://github.com/VITA-Group/SLaK.",https://api.openreview.net/pdf/5c2a0ddd8a652ab99b14ffd766607bb364e763b9.pdf,graph;transformer;sparse;segmentation;llm,https://scholar.google.com/scholar?q=More+ConvNets+in+the+2020s:+Scaling+up+Kernels+Beyond+51x51+using+Sparsity
A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks,2023,ICLR,"['Xinyi Wu', 'Zhengdao Chen', 'William Wei Wang', 'Ali Jadbabaie']",poster,"['graph neural networks', 'oversmoothing', 'representational power', 'theory', 'deep learning']","Oversmoothing is a central challenge of building more powerful Graph Neural Networks (GNNs). While previous works have only demonstrated that oversmoothing is inevitable when the number of graph convolutions tends to infinity, in this paper, we precisely characterize the mechanism behind the phenomenon via a non-asymptotic analysis. Specifically, we distinguish between two different effects when applying graph convolutions—an undesirable mixing effect that homogenizes node representations in different classes, and a desirable denoising effect that homogenizes node representations in the same class. By quantifying these two effects on random graphs sampled from the Contextual Stochastic Block Model (CSBM), we show that oversmoothing happens once the mixing effect starts to dominate the denoising effect, and the number of layers required for this transition is $O(\log N/\log (\log N))$ for sufficiently dense graphs with $N$ nodes. We also extend our analysis to study the effects of Personalized PageRank (PPR), or equivalently, the effects of initial residual connections on oversmoothing. Our results suggest that while PPR mitigates oversmoothing at deeper layers, PPR-based architectures still achieve their best performance at a shallow depth and are outperformed by the graph convolution approach on certain graphs. Finally, we support our theoretical results with numerical experiments, which further suggest that the oversmoothing phenomenon observed in practice can be magnified by the difficulty of optimizing deep GNN models.",https://api.openreview.net/pdf/572cf223b8b9b3354dc46410c80596be2504b4d3.pdf,graph;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=A+Non-Asymptotic+Analysis+of+Oversmoothing+in+Graph+Neural+Networks
Compositional Semantic Parsing with Large Language Models,2023,ICLR,"['Andrew Drozdov', 'Nathanael Schärli', 'Ekin Akyürek', 'Nathan Scales', 'Xinying Song', 'Xinyun Chen', 'Olivier Bousquet', 'Denny Zhou']",poster,"['large language models', 'prompting', 'compositional generalization', 'natural language processing']","Humans can reason compositionally when presented with new tasks.  Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.",https://api.openreview.net/pdf/668ef1e66f349e87c8948f0e5e5984608ebef31d.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Compositional+Semantic+Parsing+with+Large+Language+Models
Prompting GPT-3 To Be Reliable,2023,ICLR,"['Chenglei Si', 'Zhe Gan', 'Zhengyuan Yang', 'Shuohang Wang', 'Jianfeng Wang', 'Jordan Lee Boyd-Graber', 'Lijuan Wang']",poster,"['prompting', 'GPT-3', 'large language models', 'reliability', 'robustness', 'biases', 'calibration', 'knowledge updating']","Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3’s reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM’s factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.",https://api.openreview.net/pdf/1545ad3e1d44fe3f8431c30da415e1dd55352da5.pdf,graph;llm,https://scholar.google.com/scholar?q=Prompting+GPT-3+To+Be+Reliable
Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs,2023,ICLR,"['Chenxiao Yang', 'Qitian Wu', 'Jiahua Wang', 'Junchi Yan']",poster,"['Graph Neural Networks', 'Generalization']","Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training.

This finding provides a new perspective for understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting various GNN-related research problems including expressivity, generalization, over-smoothing and heterophily. As an initial step to analyze PMLP, we show its essential difference to MLP at infinite-width limit lies in the NTK feature map in the post-training stage. Moreover, through extrapolation analysis (i.e., generalization under distribution shifts), we find that though most GNNs and their PMLP counterparts cannot extrapolate non-linear functions for extreme out-of-distribution data, they have greater potential to generalize to testing data near the training data support as natural advantages of the GNN architecture used for inference.",https://api.openreview.net/pdf/46a2475b833d9631645d3d04015622e6eecb0cf0.pdf,graph;zero_few-shot;representation;inference;flow;llm,https://scholar.google.com/scholar?q=Graph+Neural+Networks+are+Inherently+Good+Generalizers:+Insights+by+Bridging+GNNs+and+MLPs
MEDICAL IMAGE UNDERSTANDING WITH PRETRAINED VISION LANGUAGE MODELS: A COMPREHENSIVE STUDY,2023,ICLR,"['Ziyuan Qin', 'Hua Hui Yi', 'Qicheng Lao', 'Kang Li']",poster,"['Vision Language models', 'Multimodality', 'Medical images', 'Few-shot learning', 'zero-shot']","The large-scale pre-trained vision language models (VLM) have shown remarkable domain transfer capability on natural images. However, it remains unknown whether this capability can also apply to the medical image domain. This paper thoroughly studies the knowledge transferability of pre-trained VLMs to the medical domain, where we show that well-designed medical prompts are the key to elicit knowledge from pre-trained VLMs. We demonstrate that by prompting with expressive attributes that are shared between domains, the VLM can carry the knowledge across domains and improve its generalization. This mechanism empowers VLMs to recognize novel objects with fewer or without image samples. Furthermore, to avoid the laborious manual designing process, we develop three approaches for automatic generation of medical prompts, which can inject expert-level medical knowledge and image-specific information into the prompts for fine-grained grounding. We conduct extensive experiments on thirteen different medical datasets across various modalities, showing that our well-designed prompts greatly improve the zero-shot performance compared to the default prompts, and our fine-tuned models surpass the supervised models by a significant margin.",https://api.openreview.net/pdf/8e53cd494ff16bfef607704574e7a1e2c770f607.pdf,graph;zero_few-shot;generative model;transfer learning;llm,https://scholar.google.com/scholar?q=MEDICAL+IMAGE+UNDERSTANDING+WITH+PRETRAINED+VISION+LANGUAGE+MODELS:+A+COMPREHENSIVE+STUDY
CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Alignment,2023,ICLR,"['Hongwei Xue', 'Yuchong Sun', 'Bei Liu', 'Jianlong Fu', 'Ruihua Song', 'Houqiang Li', 'Jiebo Luo']",poster,[],"Pre-trained image-text models, like CLIP, have demonstrated the strong power of vision-language representation learned from a large scale of web-collected image-text data. In light of the well-learned visual features, there are works that transfer image representation to the video domain and achieve good results. However, adapting image-text pre-trained models to video-text pre-training (i.e., post-pretraining) has not demonstrated a significant advantage yet. In this paper, we tackle this challenge by raising and addressing two questions: 1) what are the factors hindering post-pretraining CLIP from improving performance on video-text tasks, and 2) how to mitigate the impact of these factors. Through a series of comparative experiments and analyses, we find that the data scale and domain gap between language sources have large impacts. By these observations, we propose an Omnisource Cross-modal Learning method equipped with a Video Proxy mechanism on the basis of CLIP, namely CLIP-ViP. Extensive results show that our approach improves the performance of CLIP on video-text retrieval by a large margin. Our model achieves state-of-the-art results on a variety of datasets, including MSR-VTT, DiDeMo, LSMDC, and ActivityNet. We release our code and pre-trained CLIP-ViP models at \url{https://github.com/microsoft/XPretrain/tree/main/CLIP-ViP}.",https://api.openreview.net/pdf/f8c079d34aee5b9409dbf8a160ba5d1d8b547b1f.pdf,graph;representation;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=CLIP-ViP:+Adapting+Pre-trained+Image-Text+Model+to+Video-Language+Alignment
Learning to Decompose Visual Features with Latent Textual Prompts,2023,ICLR,"['Feng Wang', 'Manling Li', 'Xudong Lin', 'Hairong Lv', 'Alex Schwing', 'Heng Ji']",poster,"['CLIP', 'vision-language learning', 'visual prompt']","Recent advances in pre-training vision-language models like CLIP have shown great potential in learning transferable visual representations. Nonetheless, for downstream inference, CLIP-like models suffer from either 1) degraded accuracy and robustness in the case of inaccurate text descriptions during retrieval-based inference (the challenge for zero-shot protocol); or 2) breaking the well-established vision-language alignment (the challenge for linear probing). To address them, we propose Decomposed Feature Prompting (DeFo). DeFo leverages a flexible number of learnable embeddings as textual input while maintaining the vision-language dual-model architecture, which enables the model to learn decomposed visual features with the help of feature-level textual prompts. We further use an additional linear layer to perform classification, allowing a scalable size of language inputs. Our empirical study shows DeFo's significance in improving the vision-language models. For example, DeFo obtains 73.2% test accuracy on ImageNet with a ResNet-50 backbone without tuning any pretrained weights of both the vision and language encoder, outperforming zero-shot CLIP by a large margin of 15.0%, and outperforming state-of-the-art vision-language prompt tuning method by 7.6%.",https://api.openreview.net/pdf/113fd3d7efcd01c4d918daa9d8c3b5bfc746b4da.pdf,graph;zero_few-shot;representation;inference;transfer learning;llm,https://scholar.google.com/scholar?q=Learning+to+Decompose+Visual+Features+with+Latent+Textual+Prompts
Learning with Auxiliary Activation for Memory-Efficient Training,2023,ICLR,"['Sunghyeon Woo', 'Dongsuk Jeon']",poster,"['Memory Efficient Training', 'Auxiliary Activation', 'Backpropagation', 'Deep Learning']","While deep learning has achieved great success in various fields, a large amount of memory is necessary to train deep neural networks, which hinders the development of massive state-of-the-art models. The reason is the conventional learning rule, backpropagation, should temporarily store input activations of all the layers in the network. To overcome this, recent studies suggested various memory-efficient implementations of backpropagation. However, those approaches incur computational overhead due to the recomputation of activations, slowing down neural network training. In this work, we propose a new learning rule which significantly reduces memory requirements while closely matching the performance of backpropagation. The algorithm combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation instead of actual input activation to reduce the amount of data to be temporarily stored. We mathematically show that our learning rule can reliably train the networks whose loss landscape is convex if the auxiliary activation satisfies certain conditions. Based on this observation, we suggest candidates of auxiliary activation that satisfy those conditions. Experimental results confirm that the proposed learning rule achieves competitive performance compared to backpropagation in various models such as ResNet, Transformer, BERT, ViT, and MLP-Mixer.",https://api.openreview.net/pdf/1c6f3fd1ba354ed7bd43032e4a12fb61ed2598b0.pdf,zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Learning+with+Auxiliary+Activation+for+Memory-Efficient+Training
MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer,2023,ICLR,"['Qihao Zhao', 'Yangyu Huang', 'Wei Hu', 'Fan Zhang', 'Jun Liu']",poster,[],"The recently proposed data augmentation TransMix employs attention labels to help visual transformers (ViT) achieve better robustness and performance. However, TransMix is deficient in two aspects: 1) The image cropping method of TransMix may not be suitable for vision transformer. 2) At the early stage of training, the model produces unreliable attention maps. TransMix uses unreliable attention maps to compute mixed attention labels that can affect the model. To address the aforementioned issues, we propose MaskMix and Progressive Attention Labeling (PAL) in image and label space, respectively. In detail, from the perspective of image space, we design MaskMix, which mixes two images based on a patch-like grid mask. In particular, the size of each mask patch is adjustable and is a multiple of the image patch size, which ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we design PAL, which utilizes a progressive factor to dynamically re-weight the attention weights of the mixed attention label. Finally, we combine MaskMix and Progressive Attention Labeling as our new data augmentation method, named MixPro. The experimental results show that our method can improve various ViT-based models at scales on ImageNet classification (73.8% top-1 accuracy based on DeiT-T for 300 epochs). After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection, and instance segmentation. Furthermore, compared to TransMix, MixPro also shows stronger robustness on several benchmarks.",https://api.openreview.net/pdf/0958c1042f7b6e9c288b5bec82ced0c146f4bf51.pdf,transformer;transfer learning;augmentation;segmentation;llm,https://scholar.google.com/scholar?q=MixPro:+Data+Augmentation+with+MaskMix+and+Progressive+Attention+Labeling+for+Vision+Transformer
Simplicial Hopfield networks,2023,ICLR,"['Thomas F Burns', 'Tomoki Fukai']",poster,"['Hopfield network', 'associative memory', 'attention', 'computational neuroscience', 'simplicial complex', 'topology', 'memory capacity']","Hopfield networks are artificial neural networks which store memory patterns on the states of their neurons by choosing recurrent connection weights and update rules such that the energy landscape of the network forms attractors around the memories. How many stable, sufficiently-attracting memory patterns can we store in such a network using $N$ neurons? The answer depends on the choice of weights and update rule. Inspired by setwise connectivity in biology, we extend Hopfield networks by adding setwise connections and embedding these connections in a simplicial complex. Simplicial complexes are higher dimensional analogues of graphs which naturally represent collections of pairwise and setwise relationships. We show that our simplicial Hopfield networks increase memory storage capacity. Surprisingly, even when connections are limited to a small random subset of equivalent size to an all-pairwise network, our networks still outperform their pairwise counterparts. Such scenarios include non-trivial simplicial topology. We also test analogous modern continuous Hopfield networks, offering a potentially promising avenue for improving the attention mechanism in Transformer models.",https://api.openreview.net/pdf/e23c85adef751869d6c53a9e15879e8940567192.pdf,graph;transformer;llm,https://scholar.google.com/scholar?q=Simplicial+Hopfield+networks
A Differential Geometric View and Explainability of GNN on Evolving Graphs,2023,ICLR,"['Yazheng Liu', 'Xi Zhang', 'Sihong Xie']",poster,[],"Graphs are ubiquitous in social networks and biochemistry, where Graph Neural Networks (GNN) are the state-of-the-art models for prediction. Graphs can be evolving and it is vital to formally model and understand how a trained GNN responds to graph evolution. We propose a smooth parameterization of the GNN predicted distributions using axiomatic attribution, where the distributions are on a low dimensional manifold within a high-dimensional embedding space. We exploit the differential geometric viewpoint to model distributional evolution as smooth curves on the manifold. We reparameterize families of curves on the manifold and design a convex optimization problem to find a unique curve that concisely approximates the distributional evolution for human interpretation. Extensive experiments on node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better sparsity, faithfulness, and intuitiveness of the proposed method over the state-of-the-art methods.",https://api.openreview.net/pdf/a3918635757574d63bba806cbd417f5a14b58811.pdf,graph;optimization;zero_few-shot;metric;llm,https://scholar.google.com/scholar?q=A+Differential+Geometric+View+and+Explainability+of+GNN+on+Evolving+Graphs
Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning,2023,ICLR,"['Zhen Wang', 'Rameswar Panda', 'Leonid Karlinsky', 'Rogerio Feris', 'Huan Sun', 'Yoon Kim']",poster,"['Prompt Tuning', 'Multitask Learning', 'Transfer Learning']","Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning $0.035\%$ as many task-specific parameters.",https://api.openreview.net/pdf/7506d3076a848ae366fb1e6d0213bf0328ddd8aa.pdf,zero_few-shot;transfer learning;distillation;llm,https://scholar.google.com/scholar?q=Multitask+Prompt+Tuning+Enables+Parameter-Efficient+Transfer+Learning
Self-supervision through Random Segments with Autoregressive Coding (RandSAC),2023,ICLR,"['Tianyu Hua', 'Yonglong Tian', 'Sucheng Ren', 'Michalis Raptis', 'Hang Zhao', 'Leonid Sigal']",poster,[],"Inspired by the success of self-supervised autoregressive representation learning in natural language (GPT and its variants), and advances in recent visual architecture design with Vision Transformers (ViTs), in this paper, we explore the effects various design choices have on the success of applying such training strategies for visual feature learning. Specifically, we introduce a novel strategy that we call Random Segments with Autoregressive Coding (RandSAC). In RandSAC, we group patch representations (image tokens) into hierarchically arranged segments; within each segment, tokens are predicted in parallel, similar to BERT, while across segment predictions are sequential, similar to GPT. We illustrate that randomized serialization of the segments significantly improves the performance and results in distribution over spatially-long (across-segments) and -short (within-segment) predictions which are effective for feature learning. We illustrate the pertinence of these design choices and explore alternatives on a number of datasets (e.g., CIFAR10, ImageNet). While our pre-training strategy works with vanilla Transformer, we also propose a conceptually simple, but highly effective, addition to the decoder that allows learnable skip-connections to encoder feature layers, which further improves the performance.",https://api.openreview.net/pdf/4e53d24d80bcf1d31e7eea1d99a647f689b2bf87.pdf,zero_few-shot;transformer;representation;self-supervision;llm,https://scholar.google.com/scholar?q=Self-supervision+through+Random+Segments+with+Autoregressive+Coding+(RandSAC)
Transformer-Patcher: One Mistake Worth One Neuron,2023,ICLR,"['Zeyu Huang', 'Yikang Shen', 'Xiaofeng Zhang', 'Jie Zhou', 'Wenge Rong', 'Zhang Xiong']",poster,['Sequential Model Editing'],"Large Transformer-based Pretrained Language Models (PLMs) dominate almost all Natural Language Processing (NLP) tasks. Nevertheless, they still make mistakes from time to time. For a model deployed in an industrial environment, fixing these mistakes quickly and robustly is vital to improve user experiences. Previous works formalize such problems as Model Editing (ME) and mostly focus on fixing one mistake. However, the one-mistake-fixing scenario is not an accurate abstraction of the real-world challenge. In the deployment of AI services, there are ever-emerging mistakes, and the same mistake may recur if not corrected in time. Thus a preferable solution is to rectify the mistakes as soon as they appear nonstop. Therefore, we extend the existing ME into the Sequential Model Editing (SME) to help develop more practical editing methods. Our study shows that current ME methods either fail to make a sequence of edits or to remember previous edits. We then introduce Transformer-Patcher, a novel model editor that can shift the behavior of transformer-based models by simply adding and training a few neurons in the last Feed-Forward Network layer. Experimental results on both classification and generation tasks show that Transformer-Patcher can successively correct up to thousands of errors (Reliability) and generalize to their equivalent inputs (Generality) while retaining the model’s accuracy on irrelevant inputs (Locality). Our method outperforms previous fine-tuning and HyperNetwork-based methods and achieves state-of-the-art performance for Sequential Model Editing (SME).",https://api.openreview.net/pdf/333a3ae8305ed6fdef22d29e567970ee0060978d.pdf,graph;transformer;generative model;llm,https://scholar.google.com/scholar?q=Transformer-Patcher:+One+Mistake+Worth+One+Neuron
Optimal Activation Functions for the Random Features Regression Model,2023,ICLR,"['Jianxin Wang', 'José Bento']",poster,"['Random Features Regression Model', 'Learning theory for neural networks', 'Functional analysis and variational calculus']","The asymptotic mean squared test error and sensitivity of the Random Features Regression model (RFR) have been recently studied. We build on this work and identify in closed-form the family of Activation Functions (AFs) that minimize a combination of the test error and sensitivity of the RFR under different notions of functional parsimony. We find scenarios under which the optimal AFs are linear, saturated linear functions, or expressible in terms of Hermite polynomials. Finally, we show how using optimal AFs impacts well established properties of the RFR model, such as its double descent curve, and the dependency of its optimal regularization parameter on the observation noise level.",https://api.openreview.net/pdf/8895967b0a0cc46d0f4328a9c58e36e5da0fec6b.pdf,llm,https://scholar.google.com/scholar?q=Optimal+Activation+Functions+for+the+Random+Features+Regression+Model
Hierarchical Sliced Wasserstein Distance,2023,ICLR,"['Khai Nguyen', 'Tongzheng Ren', 'Huy Nguyen', 'Litu Rout', 'Tan Minh Nguyen', 'Nhat Ho']",poster,"['Sliced Wasserstein', 'Radon Transform', 'Optimal Transport', 'Generative Models']","Sliced Wasserstein (SW) distance has been widely used in different application scenarios since it can be scaled to a large number of supports without suffering from the curse of dimensionality. The value of sliced Wasserstein distance is the average of transportation cost between one-dimensional representations (projections) of original measures that are obtained by Radon Transform (RT). Despite its efficiency in the number of supports, estimating the sliced Wasserstein requires a relatively large number of projections in high-dimensional settings. Therefore, for applications where the number of supports is relatively small compared with the dimension, e.g., several deep learning applications where the mini-batch approaches are utilized, the complexities from matrix multiplication of Radon Transform become the main computational bottleneck. To address this issue, we propose to derive projections by linearly and randomly combining a smaller number of projections which are named bottleneck projections. We explain the usage of these projections by introducing Hierarchical Radon Transform (HRT) which is constructed by applying  Radon Transform variants recursively. We then formulate the approach into a new metric between measures, named Hierarchical Sliced Wasserstein (HSW) distance. By proving the injectivity of HRT, we derive the metricity of HSW. Moreover, we investigate the theoretical properties of HSW including its connection to SW variants and its computational and sample complexities. Finally, we compare the computational cost and generative quality of HSW with the conventional SW on the task of deep generative modeling using various benchmark datasets including CIFAR10, CelebA, and Tiny ImageNet.",https://api.openreview.net/pdf/5503447c4f2c86286908b993e39c387ac34cdc2f.pdf,graph;representation;generative model;metric;llm,https://scholar.google.com/scholar?q=Hierarchical+Sliced+Wasserstein+Distance
Out-of-distribution Representation Learning for Time Series Classification,2023,ICLR,"['Wang Lu', 'Jindong Wang', 'Xinwei Sun', 'Yiqiang Chen', 'Xing Xie']",poster,"['Domain generalization', 'out-of-distribution generalization', 'time series classification']","Time series classification is an important problem in the real world. Due to its non-stationary property that the distribution changes over time, it remains challenging to build models for generalization to unseen distributions. In this paper, we propose to view time series classification from the distribution perspective. We argue that the temporal complexity of a time series dataset could attribute to unknown latent distributions that need characterize. To this end, we propose DIVERSIFY for out-of-distribution (OOD) representation learning on dynamic distributions of times series. DIVERSIFY takes an iterative process: it first obtains the ‘worst-case’ latent distribution scenario via adversarial training, then reduces the gap between these latent distributions. We then show that such an algorithm is theoretically supported. Extensive experiments are conducted on seven datasets with different OOD settings across gesture recognition, speech commands recognition, wearable stress and affect detection, and sensor-based human activity recognition. Qualitative and quantitative results demonstrate that DIVERSIFY significantly outperforms other baselines and effectively characterizes the latent distributions. Code is available at https://github.com/microsoft/robustlearn.",https://api.openreview.net/pdf/d789366a598458e3e64dd117350163e00c1c54d1.pdf,graph;representation;llm,https://scholar.google.com/scholar?q=Out-of-distribution+Representation+Learning+for+Time+Series+Classification
Self-Consistency Improves Chain of Thought Reasoning in Language Models,2023,ICLR,"['Xuezhi Wang', 'Jason Wei', 'Dale Schuurmans', 'Quoc V Le', 'Ed H. Chi', 'Sharan Narang', 'Aakanksha Chowdhery', 'Denny Zhou']",poster,"['Language models', 'natural language processing', 'reasoning']","Chain-of-thought prompting combined with pretrained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out all possible reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer.  Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",https://api.openreview.net/pdf/9d06013867701125040af03996c3aefddc8d58d1.pdf,graph;llm,https://scholar.google.com/scholar?q=Self-Consistency+Improves+Chain+of+Thought+Reasoning+in+Language+Models
HotProtein: A Novel Framework for Protein Thermostability Prediction and Editing,2023,ICLR,"['Tianlong Chen', 'Chengyue Gong', 'Daniel Jesus Diaz', 'Xuxi Chen', 'Jordan Tyler Wells', 'qiang liu', 'Zhangyang Wang', 'Andrew Ellington', 'Alex Dimakis', 'Adam Klivans']",poster,"['Protein Thermostability', 'Protein Editing', 'Dataset', 'Structure-aware Pre-training', 'Factorized Sparse Tuning']","The molecular basis of protein thermal stability is only partially understood and has major significance for drug and vaccine discovery.  The lack of datasets and standardized benchmarks considerably limits learning-based discovery methods. We present \texttt{HotProtein}, a large-scale protein dataset with \textit{growth temperature} annotations of thermostability, containing $182$K amino acid sequences and $3$K folded structures from $230$ different species with a wide temperature range $-20^{\circ}\texttt{C}\sim 120^{\circ}\texttt{C}$. Due to functional domain differences and data scarcity within each species, existing methods fail to generalize well on our dataset. We address this problem through a novel learning framework, consisting of ($1$) Protein structure-aware pre-training (SAP) which leverages 3D information to enhance sequence-based pre-training; ($2$) Factorized sparse tuning (FST) that utilizes low-rank and sparse priors as an implicit regularization, together with feature augmentations. Extensive empirical studies demonstrate that our framework improves thermostability prediction compared to other deep learning models. Finally, we introduce a novel editing algorithm to efficiently generate positive amino acid mutations that improve thermostability. Codes are available in https://github.com/VITA-Group/HotProtein.",https://api.openreview.net/pdf/2e8f5a95a911c55a1dbf828fed9a9e23a319b027.pdf,zero_few-shot;sparse;augmentation;low-rank;3d;llm,https://scholar.google.com/scholar?q=HotProtein:+A+Novel+Framework+for+Protein+Thermostability+Prediction+and+Editing
Average Sensitivity of Decision Tree Learning,2023,ICLR,"['Satoshi Hara', 'Yuichi Yoshida']",poster,"['decision tree', 'average sensitivity', 'trustworthy machine learning']","A decision tree is a fundamental model used in data mining and machine learning. In practice, the training data used to construct a decision tree may change over time or contain noise, and a drastic change in the learned tree structure owing to such data perturbation is unfavorable. For example, in data mining, a change in the tree implies a change in the extracted knowledge, which raises the question of whether the extracted knowledge is truly reliable or is only a noisy artifact. To alleviate this issue, we design decision tree learning algorithms that are stable against insignificant perturbations in the training data. Specifically, we adopt the notion of average sensitivity as a stability measure, and design an algorithm with low average sensitivity that outputs a decision tree whose accuracy is close to the optimal decision tree. The experimental results on real-world datasets demonstrate that the proposed algorithm enables users to select suitable decision trees considering the trade-off between average sensitivity and accuracy.",https://api.openreview.net/pdf/db4880381e5ca7a65fa5ae7f37e99287c8978e9d.pdf,zero_few-shot;decision trees;llm,https://scholar.google.com/scholar?q=Average+Sensitivity+of+Decision+Tree+Learning
The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers,2023,ICLR,"['Zonglin Li', 'Chong You', 'Srinadh Bhojanapalli', 'Daliang Li', 'Ankit Singh Rawat', 'Sashank J. Reddi', 'Ke Ye', 'Felix Chern', 'Felix Yu', 'Ruiqi Guo', 'Sanjiv Kumar']",poster,"['Transformers', 'Sparse', 'Calibration', 'Robustness', 'Label Noise', 'Efficiency']","This paper studies a curious phenomenon that machine learning model with Transformer architectures have sparse activation maps. By activation map we refer to the intermediate output of the multi-layer perceptrons (MLPs) after a ReLU activation function, and by ""sparse"" we mean that on average very few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each input to MLP. Moreover, larger Transformers with more layers and wider MLP hidden dimensions are sparser as measured by the percentage of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and evaluation data, for Transformers of various configurations, at layers of all depth levels. We discuss how sparsity immediately implies a way to significantly reduce the FLOP count and improve efficiency for Transformers. Moreover, we demonstrate perhaps surprisingly that enforcing an even sparser activation via Top-k thresholding with a small k brings a collection of desired properties, namely less sensitivity to noisy training data, more robustness to input corruptions, and better calibration for their prediction confidence.",https://api.openreview.net/pdf/4120f135fba7001a85237ce4a81740c0626abb31.pdf,transformer;sparse;llm,https://scholar.google.com/scholar?q=The+Lazy+Neuron+Phenomenon:+On+Emergence+of+Activation+Sparsity+in+Transformers
What Can we Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers?,2023,ICLR,"['Ido Galil', 'Mohammed Dabbah', 'Ran El-Yaniv']",poster,"['selective prediction', 'selective classification', 'reject option', 'risk coverage trade-off', 'deep learning', 'neural networks']","When deployed for risk-sensitive tasks, deep neural networks must include an uncertainty estimation mechanism.
Here we examine the relationship between deep architectures and their respective training regimes, with their corresponding selective prediction and uncertainty estimation performance. We consider some of the most popular estimation performance metrics previously proposed including AUROC, ECE, AURC as well as coverage for selective accuracy constraint. 
We present a novel and comprehensive study of selective prediction and the uncertainty estimation performance of 523 existing pretrained deep ImageNet classifiers that are available in popular repositories.
We identify numerous and previously unknown factors that affect uncertainty estimation and examine the relationships between the different metrics. We find that distillation-based training regimes consistently yield better uncertainty estimations than other training schemes such as vanilla training, pretraining on a larger dataset and adversarial training.
Moreover, we find a subset of ViT models that outperform any other models in terms of uncertainty estimation performance.
For example, we discovered an unprecedented 99% top-1 selective accuracy on ImageNet at 47% coverage
(and 95% top-1 accuracy at 80%) for a ViT model, whereas a competing EfficientNet-V2-XL cannot obtain these accuracy constraints at any level of coverage. 
Our companion paper, also published in ICLR 2023 (A framework for benchmarking class-out-of-distribution detection and its application to ImageNet), examines the performance of these classifiers in a class-out-of-distribution setting.",https://api.openreview.net/pdf/511b590fa52d8862dc31d6bf812574202375626b.pdf,optimization;metric;distillation;llm,https://scholar.google.com/scholar?q=What+Can+we+Learn+From+The+Selective+Prediction+And+Uncertainty+Estimation+Performance+Of+523+Imagenet+Classifiers?
Unicom: Universal and Compact Representation Learning for Image Retrieval,2023,ICLR,"['Xiang An', 'Jiankang Deng', 'Kaicheng Yang', 'Jaiwei Li', 'Ziyong Feng', 'Jia Guo', 'Jing Yang', 'Tongliang Liu']",poster,"['Cluster Discrimination', 'Image Retrieval']","Modern image retrieval methods typically rely on fine-tuning pre-trained encoders to extract image-level descriptors.
However, the most widely used models are pre-trained on ImageNet-1K with limited classes. The pre-trained feature representation is therefore not universal enough to generalize well to the diverse open-world classes. 
In this paper, we first cluster the large-scale \laion{} into one million pseudo classes based on the joint textual and visual features extracted by the CLIP model. Due to the confusion of label granularity, the automatically clustered dataset inevitably contains heavy inter-class conflict. To alleviate such conflict, we randomly select partial inter-class prototypes to construct the margin-based softmax loss. To further enhance the low-dimensional feature representation, we randomly select partial feature dimensions when calculating the similarities between embeddings and class-wise prototypes. The dual random partial selections are with respect to the class dimension and the feature dimension of the prototype matrix, making the classification conflict-robust and the feature embedding compact. Our method significantly outperforms state-of-the-art unsupervised and supervised image retrieval approaches on multiple benchmarks. The code and pre-trained models are released to facilitate future research \url{https://github.com/deepglint/unicom}. ",https://api.openreview.net/pdf/e97855587823f27b93c38bd29a902d48e3a7c676.pdf,graph;zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Unicom:+Universal+and+Compact+Representation+Learning+for+Image+Retrieval
Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning,2023,ICLR,"['XIANGYU PENG', 'Chen Xing', 'Prafulla Kumar Choubey', 'Chien-Sheng Wu', 'Caiming Xiong']",poster,"['prompt tuning', 'natural language processing', 'few-shot learning']","Prompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance of prompt tuning by transferring knowledge from soft prompts of source tasks with abundant training samples. Recognizing the good generalization capabilities of ensemble methods in low-data regime, we first experiment and show that a simple ensemble of model predictions based on different source prompts, outperforms existing multi-prompt knowledge transfer approaches such as source prompt fusion in the few-shot setting. Motivated by this observation, we further investigate model ensembles and  propose Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the contribution of each source model for each target sample separately when ensembling source model outputs. Through this way, SESoM inherits the superior generalization of ensemble methods and simultaneously captures the sample-specific competence of each source prompt.  We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-\{base, large, XL\}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.",https://api.openreview.net/pdf/0da427b46bc6cf66b7284999d1ecd003cb8d4c3d.pdf,graph;zero_few-shot;metric;transfer learning;llm,https://scholar.google.com/scholar?q=Model+ensemble+instead+of+prompt+fusion:+a+sample-specific+knowledge+transfer+method+for+few-shot+prompt+tuning
Unified Discrete Diffusion for Simultaneous Vision-Language Generation,2023,ICLR,"['Minghui Hu', 'Chuanxia Zheng', 'Zuopeng Yang', 'Tat-Jen Cham', 'Heliang Zheng', 'Chaoyue Wang', 'Dacheng Tao', 'Ponnuthurai N. Suganthan']",poster,"['Multi-modal', 'Image generation', 'Image Caption.']","The recently developed discrete diffusion model performs extraordinarily well in generation tasks, especially in the text-to-image task, showing great potential for modeling multimodal signals. In this paper, we leverage these properties and present a unified multimodal generation model, which can perform text-based, image-based, and even vision-language simultaneous generation using a single model. Specifically, we unify the discrete diffusion process for multimodal signals by proposing a unified Markov transition matrix and a unified objective. Moreover, we design a multimodal mutual attention module to highlight the inter-modal linkages, which is vital for multimodal generation. Extensive experiments indicate that our proposed method can perform comparably to the state-of-the-art solutions in various generation tasks.",https://api.openreview.net/pdf/d386e35cf9491b25a9017b063c051ef1d750c527.pdf,transformer;generative model;multimodal;diffusion models;llm,https://scholar.google.com/scholar?q=Unified+Discrete+Diffusion+for+Simultaneous+Vision-Language+Generation
Planckian Jitter: countering the color-crippling effects of color jitter on self-supervised training,2023,ICLR,"['Simone Zini', 'Alex Gomez-Villa', 'Marco Buzzelli', 'Bartłomiej Twardowski', 'Andrew D. Bagdanov', 'Joost van de weijer']",poster,"['Contrastive Learning', 'Self-Supervised Learning', 'Color Features', 'Illuminant Invariance']","Several recent works on self-supervised learning are trained by mapping different augmentations of the same image to the same feature representation. The data augmentations used are of crucial importance to the quality of learned feature representations. In this paper, we analyze how the color jitter traditionally used in data augmentation negatively impacts the quality of the color features in learned feature representations. To address this problem, we propose a more realistic, physics-based color data augmentation - which we call Planckian Jitter - that creates realistic variations in chromaticity and produces a model robust to illumination changes that can be commonly observed in real life, while maintaining the ability to discriminate image content based on color information.
Experiments confirm that such a representation is complementary to the representations learned with the currently-used color jitter augmentation and that a simple concatenation leads to significant performance gains on a wide range of downstream datasets. 
In addition, we present a color sensitivity analysis that documents the impact of different training methods on model neurons and shows that the performance of the learned features is robust with respect to illuminant variations.
Official code available at: https://github.com/TheZino/PlanckianJitter",https://api.openreview.net/pdf/14392e7c217114a74674c714c248cd648e9b92d3.pdf,zero_few-shot;representation;augmentation;llm,https://scholar.google.com/scholar?q=Planckian+Jitter:+countering+the+color-crippling+effects+of+color+jitter+on+self-supervised+training
Masked Image Modeling with Denoising Contrast,2023,ICLR,"['Kun Yi', 'Yixiao Ge', 'Xiaotong Li', 'Shusheng Yang', 'Dian Li', 'Jianping Wu', 'Ying Shan', 'Xiaohu Qie']",poster,"['masked image modeling', 'self-supervised learning', 'image pre-training']","Since the development of self-supervised visual representation learning from contrastive learning to masked image modeling (MIM), there is no significant difference in essence, that is, how to design proper pretext tasks for vision dictionary look-up. MIM recently dominates this line of research with state-of-the-art performance on vision Transformers (ViTs), where the core is to enhance the patch-level visual context capturing of the network via denoising auto-encoding mechanism. Rather than tailoring image tokenizers with extra training stages as in previous works, we unleash the great potential of contrastive learning on de- noising auto-encoding and introduce a pure MIM method, ConMIM, to produce simple intra-image inter-patch contrastive constraints as the sole learning objectives for masked patch prediction. We further strengthen the denoising mechanism with asymmetric designs, including image perturbations and model progress rates, to improve the network pre-training. ConMIM-pretrained models with various scales achieve competitive results on downstream image classification, semantic segmentation, object detection, and instance segmentation tasks, e.g., on ImageNet-1K classification, we achieve 83.9% top-1 accuracy with ViT-Small and 85.3% with ViT-Base without extra data for pre-training. Code will be available at https://github.com/TencentARC/ConMIM.
",https://api.openreview.net/pdf/0a992b36167d39752c459212325882e0c62c458e.pdf,optimization;zero_few-shot;transformer;representation;contrastive learning;metric;segmentation;llm,https://scholar.google.com/scholar?q=Masked+Image+Modeling+with+Denoising+Contrast
Masked Unsupervised Self-training for Label-free Image Classification ,2023,ICLR,"['Junnan Li', 'Silvio Savarese', 'Steven Hoi']",poster,"['zero-shot classification', 'unsupervised learning', 'self-training', 'CLIP', 'masked image modeling']","State-of-the-art computer vision models are mostly trained with supervised learning using human-labeled images, which limits their scalability due to the expensive annotation cost. While self-supervised representation learning has achieved impressive progress, it still requires a second stage of finetuning on labeled data. On the other hand, models pre-trained with large-scale text supervision (e.g., CLIP) have enabled zero-shot transfer to downstream image classification tasks. However, the zero-shot performance of CLIP-like models are often insufficient for real-world adoption. In this paper, we aim to leverage the abundant unlabeled data from a target domain to improve the performance of a pre-trained zero-shot classifier, by unsupervised finetuning of the pre-trained model. We propose Masked Unsupervised Self-Training (MUST), a new approach which leverages two different and complimentary sources of training signals: pseudo-labels and raw images. MUST jointly optimizes three objectives to learn both class-level global feature and pixel-level local feature and enforces a regularization between the two. We demonstrate the efficacy of MUST on 8 downstream tasks across a variety of domains, where it improves upon CLIP by a large margin. MUST also outperforms supervised few-shot adaptation methods. It achieves a top-1 accuracy of 77.7% on ImageNet using ViT-B, +9.4% higher than CLIP, and +6.2% higher than 16-shot CLIP adaptation. Our code is available at https://github.com/salesforce/MUST.",https://api.openreview.net/pdf/4727dfde7523443e21ec3f97b19eeba777450c3e.pdf,graph;zero_few-shot;representation;transfer learning;llm,https://scholar.google.com/scholar?q=Masked+Unsupervised+Self-training+for+Label-free+Image+Classification+
Trainable Weight Averaging: Efficient Training by Optimizing Historical Solutions,2023,ICLR,"['Tao Li', 'Zhehao Huang', 'Qinghua Tao', 'Yingwen Wu', 'Xiaolin Huang']",poster,"['efficient training', 'weight averaging', 'optimization']","Stochastic gradient descent (SGD) and its variants are considered as the de-facto methods to train deep neural networks (DNNs). While recent improvements to SGD mainly focus on the descent algorithm itself, few works pay attention to utilizing the historical solutions---as an iterative method, SGD has gone through substantial explorations before convergence. Recently, an interesting attempt is stochastic weight averaging (SWA), which significantly improves the generalization by simply averaging the solutions at the tail stage of training. In this paper, we realize that the averaging coefficients could be determined in a trainable manner and propose Trainable Weight Averaging (TWA), a novel optimization method in the reduced subspace spanned by historical solutions. TWA has much greater flexibility and can be applied to the head stage of training to achieve training efficiency while preserving good generalization capability. Further, we propose a distributed training scheme to resolve the memory burden of large-scale training with efficient parallel computation. In the extensive numerical experiments, (i) TWA achieves consistent improvements over SWA with less sensitivity to learning rate; (ii) applying TWA in the head stage of training largely speeds up the convergence, resulting in over $40\%$ time saving on CIFAR and $30\%$ on ImageNet with improved generalization compared with regular training.",https://api.openreview.net/pdf/d32020780d5a44be516cb5ee9c13e38535e964ed.pdf,graph;optimization;transformer;llm,https://scholar.google.com/scholar?q=Trainable+Weight+Averaging:+Efficient+Training+by+Optimizing+Historical+Solutions
Human MotionFormer: Transferring Human Motions with Vision Transformers,2023,ICLR,"['Hongyu Liu', 'Xintong Han', 'Chenbin Jin', 'Lihui Qian', 'Huawei Wei', 'Zhe Lin', 'Faqiang Wang', 'Haoye Dong', 'Yibing Song', 'Jia Xu', 'Qifeng Chen']",poster,[],"Human motion transfer aims to transfer motions from a target dynamic person to a source static one for motion synthesis. An accurate matching between the source person and the target motion in both large and subtle motion changes is vital for improving the transferred motion quality. In this paper, we propose Human MotionFormer, a hierarchical ViT framework that leverages global and local perceptions to capture large and subtle motion matching, respectively. It consists of two ViT encoders to extract input features (i.e., a target motion image and a source human image) and a ViT decoder with several cascaded blocks for feature matching and motion transfer. In each block, we set the target motion feature as Query and the source person as Key and Value, calculating the cross-attention maps to conduct a global feature matching. Further, we introduce a convolutional layer to improve the local perception after the global cross-attention computations. This matching process is implemented in both warping and generation branches to guide the motion transfer. During training, we propose a mutual learning loss to enable the co-supervision between warping and generation branches for better motion representations. Experiments show that our Human MotionFormer sets the new state-of-the-art performance both qualitatively and quantitatively. Project page: https://github.com/KumapowerLIU/Human-MotionFormer.",https://api.openreview.net/pdf/d7f8c668602a7ac86b440d7547893cb07f0881f6.pdf,transformer;representation;generative model;transfer learning;llm,https://scholar.google.com/scholar?q=Human+MotionFormer:+Transferring+Human+Motions+with+Vision+Transformers
Understanding Zero-shot Adversarial Robustness for Large-Scale Models,2023,ICLR,"['Chengzhi Mao', 'Scott Geng', 'Junfeng Yang', 'Xin Wang', 'Carl Vondrick']",poster,"['Adversarial Robustness', 'Zero-Shot Recognition']","Pretrained large-scale vision-language models like CLIP have exhibited strong generalization over unseen tasks. Yet imperceptible adversarial perturbations can significantly reduce CLIP's performance on new tasks. In this work, we identify and explore the problem of adapting large-scale models for zero-shot adversarial robustness. We first identify two key factors during model adaption--training losses and adaptation methods--that affect the model's zero-shot adversarial robustness. We then propose a text-guided contrastive adversarial training loss, which aligns the text embeddings and the adversarial visual features with contrastive learning on a small set of training data. We apply this training loss to two adaption methods, model finetuning and visual prompt tuning. We find that visual prompt tuning is more effective in the absence of texts, while finetuning wins in the existence of text guidance. Overall, our approach significantly improves the zero-shot adversarial robustness over CLIP, seeing an average improvement of 31 points over ImageNet and 15 zero-shot datasets. We hope this work can shed light on understanding the zero-shot adversarial robustness of large-scale models. ",https://api.openreview.net/pdf/3260fd25cd7bfa3805e9cf3a86f91b87e701984d.pdf,zero_few-shot;contrastive learning;llm,https://scholar.google.com/scholar?q=Understanding+Zero-shot+Adversarial+Robustness+for+Large-Scale+Models
GReTo: Remedying dynamic graph topology-task discordance via target homophily,2023,ICLR,"['Zhengyang Zhou', 'qihe huang', 'Gengyu Lin', 'Kuo Yang', 'LEI BAI', 'Yang Wang']",poster,"['Dynamic graph', 'graph homophily theory', 'Graph Neural Network', 'topology-task discordance']","Dynamic graphs are ubiquitous across disciplines where observations usually change over time. Regressions on dynamic graphs often contribute to diverse critical tasks, such as climate early-warning and traffic controlling. Existing homophily Graph Neural Networks (GNNs) adopt physical connections or feature similarity as adjacent matrix to perform node-level aggregations. However, on dynamic graphs with diverse node-wise relations, exploiting a pre-defined fixed topology for message passing inevitably leads to the aggregations of target-deviated neighbors. We designate such phenomenon as the topology-task discordance, which naturally challenges the homophily assumption. In this work, we revisit node-wise relationships and explore novel homophily measurements on dynamic graphs with both signs and distances, capturing multiple node-level spatial relations and temporal evolutions. We discover that advancing homophily aggregations to signed target-oriented message passing can effectively resolve the discordance and promote aggregation capacity. Therefore, a GReTo is proposed, which performs signed message passing in immediate neighborhood, and exploits both local environments and target awareness to realize high-order message propagation. Empirically, our solution achieves significant improvements against best baselines, notably improving 24.79% on KnowAir and 3.60% on Metr-LA. ",https://api.openreview.net/pdf/48156cfb8ea4e505cf281af3482fc7dbf314ac48.pdf,graph;llm,https://scholar.google.com/scholar?q=GReTo:+Remedying+dynamic+graph+topology-task+discordance+via+target+homophily
Modeling content creator incentives on algorithm-curated platforms,2023,ICLR,"['Jiri Hron', 'Karl Krauth', 'Michael Jordan', 'Niki Kilbertus', 'Sarah Dean']",oral,"['Nash equilibria', 'producer incentives', 'attention monetizing platforms', 'recommenders', 'differentiable games', 'exposure game']","Content creators compete for user attention. Their reach crucially depends on algorithmic choices made by developers on online platforms. To maximize exposure, many creators adapt strategically, as evidenced by examples like the sprawling search engine optimization industry. This begets competition for the finite user attention pool. We formalize these dynamics in what we call an exposure game, a model of incentives induced by modern algorithms including factorization and (deep) two-tower architectures. We prove that seemingly innocuous algorithmic choices—e.g., non-negative vs. unconstrained factorization—significantly affect the existence and character of (Nash) equilibria in exposure games. We proffer use of creator behavior models like ours for an (ex-ante) pre-deployment audit. Such an audit can identify misalignment between desirable and incentivized content, and thus complement post-hoc measures like content filtering and moderation. To this end, we propose tools for numerically finding equilibria in exposure games, and illustrate results of an audit on the MovieLens and LastFM datasets. Among else, we find that the strategically produced content exhibits strong dependence between algorithmic exploration and content diversity, and between model expressivity and bias towards gender-based user and creator groups.",https://api.openreview.net/pdf/12c4dfbbd1516c36a132fe1e8e1205b88da0540b.pdf,graph;optimization;transformer;online learning;llm,https://scholar.google.com/scholar?q=Modeling+content+creator+incentives+on+algorithm-curated+platforms
WikiWhy: Answering and Explaining Cause-and-Effect Questions,2023,ICLR,"['Matthew Ho', 'Aditya Sharma', 'Justin Chang', 'Michael Saxon', 'Sharon Levy', 'Yujie Lu', 'William Yang Wang']",oral,"['NLP', 'Question Answering', 'LLM', 'Dataset', 'Explanation']","As large language models (LLMs) grow larger and more sophisticated, assessing their ""reasoning"" capabilities in natural language grows more challenging. Recent question answering (QA) benchmarks that attempt to assess reasoning are often limited by a narrow scope of covered situations and subject matters. We introduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining why an answer is true in natural language. WikiWhy contains over 9,000 ""why"" question-answer-rationale triples, grounded on Wikipedia facts across a diverse set of topics. Each rationale is a set of supporting statements connecting the question to the answer. WikiWhy serves as a benchmark for the reasoning capabilities of LLMs because it demands rigorous explicit rationales for each answer to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized. GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements.",https://api.openreview.net/pdf/dd230e9938db73b0fff7ee629cb682af034688fc.pdf,graph;llm,https://scholar.google.com/scholar?q=WikiWhy:+Answering+and+Explaining+Cause-and-Effect+Questions
Git Re-Basin: Merging Models modulo Permutation Symmetries,2023,ICLR,"['Samuel Ainsworth', 'Jonathan Hayase', 'Siddhartha Srinivasa']",oral,[],"The success of deep learning is due in large part to our ability to solve certain massive non-convex optimization problems with relative ease. Though non-convex optimization is NP-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes often contain (nearly) a single basin after accounting for all possible permutation symmetries of hidden units a la Entezari et al. 2021. We introduce three algorithms to permute the units of one model to bring them into alignment with a reference model in order to merge the two models in weight space. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained ResNet models on CIFAR-10. Additionally, we identify intriguing phenomena relating model width and training time to mode connectivity. Finally, we discuss shortcomings of the linear mode connectivity hypothesis, including a counterexample to the single basin theory.",https://api.openreview.net/pdf/b212b96bd3f13e202965581f6173495898534b76.pdf,graph;optimization;llm,https://scholar.google.com/scholar?q=Git+Re-Basin:+Merging+Models+modulo+Permutation+Symmetries
Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification,2023,ICLR,"['Takashi Ishida', 'Ikko Yamane', 'Nontawat Charoenphakdee', 'Gang Niu', 'Masashi Sugiyama']",oral,"['Bayes error', 'best achievable error', 'irreducible error']","There is a fundamental limitation in the prediction performance that a machine learning model can achieve due to the inevitable uncertainty of the prediction target. In classification problems, this can be characterized by the Bayes error, which is the best achievable error with any classifier. The Bayes error can be used as a criterion to evaluate classifiers with state-of-the-art performance and can be used to detect test set overfitting. We propose a simple and direct Bayes error estimator, where we just take the mean of the labels that show \emph{uncertainty} of the class assignments. Our flexible approach enables us to perform Bayes error estimation even for weakly supervised data. In contrast to others, our method is model-free and even instance-free. Moreover, it has no hyperparameters and gives a more accurate estimate of the Bayes error than several baselines empirically. Experiments using our method suggest that recently proposed deep networks such as the Vision Transformer may have reached, or is about to reach, the Bayes error for benchmark datasets. Finally, we discuss how we can study the inherent difficulty of the acceptance/rejection decision for scientific articles, by estimating the Bayes error of the ICLR papers from 2017 to 2023.",https://api.openreview.net/pdf/adf5cd1db7eb1218ea6e605d13c786cdf71eab45.pdf,zero_few-shot;transformer;bayesian;llm,https://scholar.google.com/scholar?q=Is+the+Performance+of+My+Deep+Network+Too+Good+to+Be+True?+A+Direct+Approach+to+Estimating+the+Bayes+Error+in+Binary+Classification
"When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?",2023,ICLR,"['Mert Yuksekgonul', 'Federico Bianchi', 'Pratyusha Kalluri', 'Dan Jurafsky', 'James Zou']",oral,"['vision-language models', 'clip', 'contrastive learning', 'retrieval', 'vision-language pretraining', 'multimodal representation learning']","Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode the compositional relationships between objects and attributes. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. ARO consists of \emph{Visual Genome Attribution}, to test the understanding of objects' properties; \emph{Visual Genome Relation}, to test for relational understanding; and \emph{COCO-Order \& Flickr30k-Order}, to test for order sensitivity in VLMs. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We present the settings where  state-of-the-art VLMs behave like bags-of-words---i.e. when they have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large scale datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the evaluation and training procedures. We demonstrate that it is possible to perform well on image-text retrieval over existing datasets without using the composition and order information. This further motivates the value of using ARO to benchmark VLMs. Given that contrastive pretraining optimizes for retrieval on large datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring understanding of order and compositionality. ",https://api.openreview.net/pdf/ced77554985af011f5544a8798a3035d4b6ab52b.pdf,contrastive learning;llm,"https://scholar.google.com/scholar?q=When+and+Why+Vision-Language+Models+Behave+like+Bags-Of-Words,+and+What+to+Do+About+It?"
On the Sensitivity of Reward Inference to Misspecified Human Models,2023,ICLR,"['Joey Hong', 'Kush Bhatia', 'Anca Dragan']",oral,"['reward learning', 'inverse reinforcement learning', 'misspecification']","Inferring reward functions from human behavior is at the center of value alignment – aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases in behavior that lead to arbitrarily large errors in the inferred reward. However, and arguably more importantly, we are also able to identify reasonable assumptions under which the reward inference error can be bounded linearly in the error in the human model. Finally, we verify our theoretical insights in discrete and continuous control tasks with simulated and human data.",https://api.openreview.net/pdf/787489763506d1437ac7b05b15f89ea0beb8c3b1.pdf,inference;llm,https://scholar.google.com/scholar?q=On+the+Sensitivity+of+Reward+Inference+to+Misspecified+Human+Models
Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions,2023,ICLR,"['Sitan Chen', 'Sinho Chewi', 'Jerry Li', 'Yuanzhi Li', 'Adil Salim', 'Anru Zhang']",oral,"['diffusion models', 'score-based generative models', 'sampling', 'score estimation', 'Langevin', 'stochastic differential equations']","We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does *not* reduce the complexity of SGMs.",https://api.openreview.net/pdf/f0dc173be132440952bd7d8221b096d0a0ecf2c7.pdf,generative model;diffusion models;llm,https://scholar.google.com/scholar?q=Sampling+is+as+easy+as+learning+the+score:+theory+for+diffusion+models+with+minimal+data+assumptions
Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching,2023,ICLR,"['Donggyun Kim', 'Jinwoo Kim', 'Seongwoong Cho', 'Chong Luo', 'Seunghoon Hong']",oral,"['few-shot learning', 'dense prediction tasks']","Dense prediction tasks are a fundamental class of problems in computer vision. As supervised methods suffer from high pixel-wise labeling cost, a few-shot learning solution that can learn any dense task from a few labeled images is desired. Yet, current few-shot learning methods target a restricted set of tasks such as semantic segmentation, presumably due to challenges in designing a general and unified model that is able to flexibly and efficiently adapt to arbitrary tasks of unseen semantics. We propose Visual Token Matching (VTM), a universal few-shot learner for arbitrary dense prediction tasks. It employs non-parametric matching on patch-level embedded tokens of images and labels that encapsulates all tasks. Also, VTM flexibly adapts to any task with a tiny amount of task-specific parameters that modulate the matching algorithm. We implement VTM as a powerful hierarchical encoder-decoder architecture involving ViT backbones where token matching is performed at multiple feature hierarchies. We experiment VTM on a challenging variant of Taskonomy dataset and observe that it robustly few-shot learns various unseen dense prediction tasks. Surprisingly, it is competitive with fully supervised baselines using only 10 labeled examples of novel tasks ($0.004\%$ of full supervision) and sometimes outperforms using $0.1\%$ of full supervision. Codes are available at https://github.com/GitGyun/visual_token_matching.",https://api.openreview.net/pdf/45149e96f3e88087d3e81a1ff08f0d2b5e719921.pdf,graph;zero_few-shot;metric;segmentation;llm,https://scholar.google.com/scholar?q=Universal+Few-shot+Learning+of+Dense+Prediction+Tasks+with+Visual+Token+Matching
ReAct: Synergizing Reasoning and Acting in Language Models,2023,ICLR,"['Shunyu Yao', 'Jeffrey Zhao', 'Dian Yu', 'Nan Du', 'Izhak Shafran', 'Karthik R Narasimhan', 'Yuan Cao']",oral,"['Language model', 'agent', 'reasoning', 'decision making']","While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",https://api.openreview.net/pdf/bc117919562a4ccddbe5c5b24ee364d14289cdee.pdf,reinforcement learning;zero_few-shot;generative model;active learning;llm,https://scholar.google.com/scholar?q=ReAct:+Synergizing+Reasoning+and+Acting+in+Language+Models
Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness,2023,ICLR,"['Joel Dapello', 'Kohitij Kar', 'Martin Schrimpf', 'Robert Baldwin Geary', 'Michael Ferguson', 'David Daniel Cox', 'James J. DiCarlo']",oral,"['Computer Vision', 'Primate Vision', 'Adversarial Robustness', 'Behavioral Alignment', 'Inferior Temporal Cortex']","While some state-of-the-art artificial neural network systems in computer vision are strikingly accurate models of the corresponding primate visual processing, there are still many discrepancies between these models and the behavior of primates on object recognition tasks. Many current models suffer from extreme sensitivity to adversarial attacks and often do not align well with the image-by-image behavioral error patterns observed in humans. Previous research has provided strong evidence that primate object recognition behavior can be very accurately predicted by neural population activity in the inferior temporal (IT) cortex, a brain area in the late stages of the visual processing hierarchy. Therefore, here we directly test whether making the late stage representations of models more similar to that of macaque IT produces new models that exhibit more robust, primate-like behavior. We conducted chronic, large-scale multi-electrode recordings  across the IT cortex in six non-human primates (rhesus macaques). We then use these data to fine-tune (end-to-end) the model ""IT"" representations such that they are more aligned with the biological IT representations, while preserving accuracy on object recognition tasks. We generate a cohort of models with a range of IT similarity scores validated on held-out animals across two image sets with distinct statistics. Across a battery of optimization conditions, we observed a strong correlation between the models' IT-likeness and alignment with human behavior, as well as an increase in its adversarial robustness. We further assessed the limitations of this approach and find that the improvements in behavioral alignment and adversarial robustness generalize across different image statistics, but not to object categories outside of those covered in our IT training set. Taken together, our results demonstrate that building models that are more aligned with the primate brain leads to more robust and human-like behavior, and call for larger neural data-sets to further augment these gains.",https://api.openreview.net/pdf/9c4c1940dba43cb5ad6502b7a23339d19d3a9a49.pdf,optimization;representation;multimodal;llm,https://scholar.google.com/scholar?q=Aligning+Model+and+Macaque+Inferior+Temporal+Cortex+Representations+Improves+Model-to-Human+Behavioral+Alignment+and+Adversarial+Robustness
Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning,2023,ICLR,"['Antonia Creswell', 'Murray Shanahan', 'Irina Higgins']",oral,"['System 2', 'Logical reasoning', 'Language Models', 'Large Language Models', 'Reasoning', 'Neuro-symbolic', 'Neural Symbolic', 'Interpretability']","Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 46 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",https://api.openreview.net/pdf/4c8f591f9bb58ccd07ed826e0e57885bc4227b12.pdf,inference;llm,https://scholar.google.com/scholar?q=Selection-Inference:+Exploiting+Large+Language+Models+for+Interpretable+Logical+Reasoning
Near-optimal Coresets for Robust Clustering,2023,ICLR,"['Lingxiao Huang', 'Shaofeng H.-C. Jiang', 'Jianing Lou', 'Xuan Wu']",oral,"['clustering', 'outlier', 'robustness', 'coreset']","We consider robust clustering problems in $\mathbb{R}^d$, specifically $k$-clustering problems (e.g., $k$-Median and $k$-Means) with $m$ \emph{outliers}, where the cost for a given center set $C \subset \mathbb{R}^d$ aggregates the distances from $C$ to all but the furthest $m$ data points, instead of all points as in classical clustering. We focus on the $\epsilon$-coreset for robust clustering, a small proxy of the dataset that preserves the clustering cost within $\epsilon$-relative error for all center sets. Our main result is an $\epsilon$-coreset of size $O(m + \mathrm{poly}(k \epsilon^{-1}))$ that can be constructed in near-linear time. This significantly improves previous results, which either suffers an exponential dependence on $(m + k)$ [Feldman and Schulman, SODA'12], or has a weaker bi-criteria guarantee [Huang et al., FOCS'18]. Furthermore, we show this dependence in $m$ is nearly-optimal, and the fact that it is isolated from other factors may be crucial for dealing with large number of outliers. We construct our coresets by adapting to the outlier setting a recent framework [Braverman et al., FOCS'22] which was designed for capacity-constrained clustering, overcoming a new challenge that the participating terms in the cost, particularly the excluded $m$ outlier points, are dependent on the center set $C$. We validate our coresets on various datasets, and we observe a superior size-accuracy tradeoff compared with popular baselines including uniform sampling and sensitivity sampling. We also achieve a significant speedup of existing approximation algorithms for robust clustering using our coresets.",https://api.openreview.net/pdf/697bd8e4cac416b91757762ed8f0209073062f6d.pdf,optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=Near-optimal+Coresets+for+Robust+Clustering
Learning on Large-scale Text-attributed Graphs via Variational Inference,2023,ICLR,"['Jianan Zhao', 'Meng Qu', 'Chaozhuo Li', 'Hao Yan', 'Qian Liu', 'Rui Li', 'Xing Xie', 'Jian Tang']",oral,"['Language Model', 'Graph Neural Network', 'Node Classification']","This paper studies learning on text-attributed graphs (TAGs), where each node is associated with a text description. An ideal solution for such a problem would be integrating both the text and graph structure information with large language models and graph neural networks (GNNs). However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and  GNNs together. In this paper, we propose an efficient and effective solution to learning on large text-attributed graphs by fusing graph structure and language learning with a variational Expectation-Maximization (EM) framework, called GLEM. Instead of simultaneously training large language models and GNNs on big graphs, GLEM proposes to alternatively update the two modules in the E-step and M-step. Such a procedure allows training the two modules separately while simultaneously allowing the two modules to interact and mutually enhance each other. Extensive experiments on multiple data sets demonstrate the efficiency and effectiveness of the proposed approach.",https://api.openreview.net/pdf/d5933681412eb0329ac9f838744d30d98d4f8c3d.pdf,graph;zero_few-shot;inference;llm,https://scholar.google.com/scholar?q=Learning+on+Large-scale+Text-attributed+Graphs+via+Variational+Inference
PaLI: A Jointly-Scaled Multilingual Language-Image Model,2023,ICLR,"['Xi Chen', 'Xiao Wang', 'Soravit Changpinyo', 'AJ Piergiovanni', 'Piotr Padlewski', 'Daniel Salz', 'Sebastian Goodman', 'Adam Grycner', 'Basil Mustafa', 'Lucas Beyer', 'Alexander Kolesnikov', 'Joan Puigcerver', 'Nan Ding', 'Keran Rong', 'Hassan Akbari', 'Gaurav Mishra', 'Linting Xue', 'Ashish V Thapliyal', 'James Bradbury', 'Weicheng Kuo', 'Mojtaba Seyedhosseini', 'Chao Jia', 'Burcu Karagol Ayan', 'Carlos Riquelme Ruiz', 'Andreas Peter Steiner', 'Anelia Angelova', 'Xiaohua Zhai', 'Neil Houlsby', 'Radu Soricut']",oral,[],"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI, a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",https://api.openreview.net/pdf/1870a0455d0e7a6ed7d8f02e8e156cf63f5d6b6a.pdf,zero_few-shot;transformer;multimodal;llm,https://scholar.google.com/scholar?q=PaLI:+A+Jointly-Scaled+Multilingual+Language-Image+Model
"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",2023,ICLR,"['Albert Qiaochu Jiang', 'Sean Welleck', 'Jin Peng Zhou', 'Timothee Lacroix', 'Jiacheng Liu', 'Wenda Li', 'Mateja Jamnik', 'Guillaume Lample', 'Yuhuai Wu']",oral,[],"The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from $20.9\%$ to $39.3\%$ on a collection of mathematical competition problems.",https://api.openreview.net/pdf/cfd03f19d20263d9c1d1cc026a2b3528392fc857.pdf,zero_few-shot;llm,"https://scholar.google.com/scholar?q=Draft,+Sketch,+and+Prove:+Guiding+Formal+Theorem+Provers+with+Informal+Proofs"
REVISITING PRUNING AT INITIALIZATION THROUGH THE LENS OF RAMANUJAN GRAPH,2023,ICLR,"['Duc N.M Hoang', 'Shiwei Liu', 'Radu Marculescu', 'Zhangyang Wang']",oral,"['pruning at initialization', 'graph theory', 'Ramanujan Graph', 'sparse neural networks']","Pruning neural networks at initialization (PaI) has received an upsurge of interest due to its end-to-end saving potential. PaI is able to find sparse subnetworks at initialization that can achieve comparable performance to the full networks. These methods can surpass the trivial baseline of random pruning but suffer from a significant performance gap compared to post-training pruning. Previous approaches firmly rely on weights, gradients, and sanity checks as primary signals when conducting PaI analysis. To better understand the underlying mechanism of PaI, we propose to interpret it through the lens of the Ramanujan Graph - a class of expander graphs that are sparse while being highly connected. It is often believed there should be a strong correlation between the Ramanujan graph and PaI since both are about finding sparse and well-connected neural networks. However, the finer-grained link relating highly sparse and connected networks to their relative performance (i.e., ranking of difference sparse structures at the same specific global sparsity) is still missing. We observe that not only the Ramanujan property for sparse networks shows no significant relationship to PaI’s relative performance, but maximizing it can also lead to the formation of pseudo-random graphs with no structural meanings. We reveal the underlying cause to be Ramanujan Graph’s strong assumption on the upper bound of the largest nontrivial eigenvalue (µˆ) of layers belonging to highly sparse networks. We hence propose Iterative Mean Difference of Bound (IMDB) as a mean to relax the µˆ upper bound. Likewise, we also show there exists a lower bound for µˆ, which we call the Normalized Random Coefficient (NaRC), that gives us an accurate assessment for when sparse but highly connected structure degenerates into naive randomness. Finally, we systematically analyze the behavior of various PaI methods and demonstrate the utility of our proposed metrics in characterizing PaI performance. We show that subnetworks preserving better the IMDB property correlate higher in performance, while NaRC provides us with a possible mean to locate the region where highly connected, highly sparse, and non-trivial Ramanujan expanders exist. Our code is available at: https://github.com/VITA-Group/ramanujan-on-pai.",https://api.openreview.net/pdf/f73064906e38441e21dd0a622065469ef3f5b5bd.pdf,graph;zero_few-shot;metric;sparse;llm,https://scholar.google.com/scholar?q=REVISITING+PRUNING+AT+INITIALIZATION+THROUGH+THE+LENS+OF+RAMANUJAN+GRAPH
Symbolic Physics Learner: Discovering governing equations via Monte Carlo tree search,2023,ICLR,"['Fangzheng Sun', 'Yang Liu', 'Jian-Xun Wang', 'Hao Sun']",oral,"['symbolic regression', 'Monte Carlo tree search', 'governing equations', 'nonlinear dynamics']","Nonlinear dynamics is ubiquitous in nature and commonly seen in various science and engineering disciplines. Distilling analytical expressions that govern nonlinear dynamics from limited data remains vital but challenging. To tackle this fundamental issue, we propose a novel Symbolic Physics Learner (SPL) machine to discover the mathematical structure of nonlinear dynamics. The key concept is to interpret mathematical operations and system state variables by computational rules and symbols, establish symbolic reasoning of mathematical formulas via expression trees, and employ a Monte Carlo tree search (MCTS) agent to explore optimal expression trees based on measurement data. The MCTS agent obtains an optimistic selection policy through the traversal of expression trees, featuring the one that maps to the arithmetic expression of underlying physics. Salient features of the proposed framework include search flexibility and enforcement of parsimony for discovered equations. The efficacy and superiority of the SPL machine are demonstrated by numerical examples, compared with state-of-the-art baselines.",https://api.openreview.net/pdf/0c815f206ac64432f9caf1f36b816f9e368dee15.pdf,reinforcement learning;graph;online learning;distillation;llm,https://scholar.google.com/scholar?q=Symbolic+Physics+Learner:+Discovering+governing+equations+via+Monte+Carlo+tree+search
Visual Classification via Description from Large Language Models,2023,ICLR,"['Sachit Menon', 'Carl Vondrick']",oral,"['vision-language models', 'CLIP', 'prompting', 'GPT-3', 'large language models', 'zero-shot recognition', 'multimodal']","Vision-language models such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what the model ``thinks"" it is seeing to make its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline. ",https://api.openreview.net/pdf/d171255a976821dd4ebfacb7a012082c4b888b7a.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Visual+Classification+via+Description+from+Large+Language+Models
Rethinking the Expressive Power of GNNs via Graph Biconnectivity,2023,ICLR,"['Bohang Zhang', 'Shengjie Luo', 'Liwei Wang', 'Di He']",oral,"['Graph Neural Networks', 'Expressive Power', 'Weisfeiler-Lehman test', 'Graph Transformer', 'Biconnectivity']","Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs with respect to the Weisfeiler-Lehman (WL) test, for most of them, there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022), for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.",https://api.openreview.net/pdf/be0ebeff1b3c008481709874f052f374a1d68dec.pdf,graph;transformer;metric;llm,https://scholar.google.com/scholar?q=Rethinking+the+Expressive+Power+of+GNNs+via+Graph+Biconnectivity
Token Merging: Your ViT But Faster,2023,ICLR,"['Daniel Bolya', 'Cheng-Yang Fu', 'Xiaoliang Dai', 'Peizhao Zhang', 'Christoph Feichtenhofer', 'Judy Hoffman']",oral,"['token merging', 'token pruning', 'inference speed', 'training speed', 'throughput', 'off-the-shelf', 'fine tuning']","We introduce Token Merging (ToMe), a simple method to increase the throughput of existing ViT models without needing to train. ToMe gradually combines similar tokens in a transformer using a general and light-weight matching algorithm that is as fast as pruning while being more accurate. Off-the-shelf, ToMe can 2x the throughput of state-of-the-art ViT-L @ 512 and ViT-H @ 518 models on images and 2.2x the throughput of ViT-L on video with only a 0.2-0.3% accuracy drop in each case. ToMe can also easily be applied during training, improving in practice training speed up to 2x for MAE fine-tuning on video. Training with ToMe further minimizes accuracy drop, leading to 2x the throughput of ViT-B on audio for only a 0.4% mAP drop. Qualitatively, we find that ToMe merges object parts into one token, even over multiple frames of video. Overall, ToMe’s accuracy and speed are competitive with state-of-the-art on images, video, and audio.",https://api.openreview.net/pdf/ef10c4387f0309b8f942d720fdb3ed5bc6ec5b30.pdf,graph;transformer;llm,https://scholar.google.com/scholar?q=Token+Merging:+Your+ViT+But+Faster
Image as Set of Points,2023,ICLR,"['Xu Ma', 'Yuqian Zhou', 'Huan Wang', 'Can Qin', 'Bin Sun', 'Chang Liu', 'Yun Fu']",oral,"['Clustering', 'Image Processing', 'Context Cluster', 'Representation']","
What is an image, and how to extract latent features? 
Convolutional Networks (ConvNets) consider an image as organized pixels in a rectangular shape and extract features via convolutional operation in a local region; Vision Transformers (ViTs) treat an image as a sequence of patches and extract features via attention mechanism in a global range. In this work, we introduce a straightforward and promising paradigm for visual representation, which is called Context Clusters. Context clusters (CoCs) view an image as a set of unorganized points and extract features via a simplified clustering algorithm. In detail, each point includes the raw feature (e.g., color) and positional information (e.g., coordinates), and a simplified clustering algorithm is employed to group and extract deep features hierarchically. Our CoCs are convolution- and attention-free, only relying on clustering algorithm for spatial interaction. Owing to the simple design, we show CoCs endow gratifying interpretability via the visualization of the clustering process.  
Our CoCs aim at providing a new perspective on image and visual representation, which may enjoy broad applications in different domains and exhibit profound insights. Even though we are not targeting SOTA performance, COCs still achieve comparable or even better performance than ConvNets or ViTs on several benchmarks.",https://api.openreview.net/pdf/839da9c992ee84a8fa5be183d987fa55966e54ff.pdf,transformer;representation;llm,https://scholar.google.com/scholar?q=Image+as+Set+of+Points
Selective Annotation Makes Language Models Better Few-Shot Learners,2023,ICLR,"['Hongjin SU', 'Jungo Kasai', 'Chen Henry Wu', 'Weijia Shi', 'Tianlu Wang', 'Jiayi Xin', 'Rui Zhang', 'Mari Ostendorf', 'Luke Zettlemoyer', 'Noah A. Smith', 'Tao Yu']",poster,"['few-shot learning', 'language models', 'in-context learning', 'active learning']","Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9%/11.4% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks.",https://api.openreview.net/pdf/6e58f3c9c108d2d4b15102e79b138f8a4177b0ad.pdf,graph;zero_few-shot;generative model;llm,https://scholar.google.com/scholar?q=Selective+Annotation+Makes+Language+Models+Better+Few-Shot+Learners
Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?,2023,ICLR,"['Runpei Dong', 'Zekun Qi', 'Linfeng Zhang', 'Junbo Zhang', 'Jianjian Sun', 'Zheng Ge', 'Li Yi', 'Kaisheng Ma']",poster,"['Representation Learning', 'Cross-Modal Learning', '3D Point Clouds']","The success of deep learning heavily relies on large-scale data with comprehensive labels, which is more expensive and time-consuming to fetch in 3D compared to 2D images or natural languages. This promotes the potential of utilizing models pretrained with data more than 3D as teachers for cross-modal knowledge transferring. In this paper, we revisit masked modeling in a unified fashion of knowledge distillation, and we show that foundational Transformers pretrained with 2D images or natural languages can help self-supervised 3D representation learning through training Autoencoders as Cross-Modal Teachers (ACT). The pretrained Transformers are transferred as cross-modal 3D teachers using discrete variational autoencoding self-supervision, during which the Transformers are frozen with prompt tuning for better knowledge inheritance. The latent features encoded by the 3D teachers are used as the target of masked point modeling, wherein the dark knowledge is distilled to the 3D Transformer students as foundational geometry understanding. Our ACT pretrained 3D learner achieves state-of-the-art generalization capacity across various downstream benchmarks, e.g., 88.21% overall accuracy on ScanObjectNN. Codes have been released at https://github.com/RunpeiDong/ACT.",https://api.openreview.net/pdf/79fadd685bdeb6df98bf287dbb0824d4b1885bb9.pdf,zero_few-shot;transformer;representation;transfer learning;distillation;multimodal;3d;self-supervision;llm,https://scholar.google.com/scholar?q=Autoencoders+as+Cross-Modal+Teachers:+Can+Pretrained+2D+Image+Transformers+Help+3D+Representation+Learning?
Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam,2023,ICLR,"['Yucheng Lu', 'Conglong Li', 'Minjia Zhang', 'Christopher De Sa', 'Yuxiong He']",poster,[],"1-bit gradient compression and local steps are two representative techniques that enable drastic communication reduction in distributed SGD. Their benefits, however, remain an open question on Adam-based large model pre-training (e.g. BERT and GPT). In this paper, we demonstrate the non-linearity in Adam causes slow convergence even when 1-bit compression or local steps are individually applied. To alleviate this limitation, we propose \textbf{0/1 Adam} that linearizes each Adam step via approximating its optimizer states using their stale estimates and linear correlation. \textbf{0/1 Adam} performs an Adam-like step to preserve the adaptivity, while its linearity allows utilizing 1-bit compression and local steps simultaneously for wall-clock time speed up. We provide convergence guarantee for \textbf{0/1 Adam} on smooth non-convex objectives. On various large-scale benchmarks such as BERT-Base, BERT-Large, GPT-2 pre-training and ImageNet, we demonstrate on up to 128 GPUs that \textbf{0/1 Adam} is able to reduce up to 87\% of data volume, 54\% of communication rounds, and achieve up to 2$\times$ higher training throughput and end-to-end training time reduction compared to the state-of-the-art baseline 1-bit Adam; while enjoying the same statistical convergence speed and end task model accuracy on GLUE dataset and ImageNet validation set. ",https://api.openreview.net/pdf/71e3ec182ebfd2023724f92a59fe1d09a34cb114.pdf,zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Maximizing+Communication+Efficiency+for+Large-scale+Training+via+0/1+Adam
"Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning",2023,ICLR,"['Sasha Salter', 'Kristian Hartikainen', 'Walter Goodwin', 'Ingmar Posner']",poster,"['Skills', 'Transfer Learning', 'Reinforcement Learning']","The ability to discover behaviours from past experience and transfer them to new tasks is a hallmark of intelligent agents acting sample-efficiently in the real world. Equipping embodied reinforcement learners with the same ability may be crucial for their successful deployment in robotics. While hierarchical and KL-regularized reinforcement learning individually hold promise here, arguably a hybrid approach could combine their respective benefits. Key to these fields is the use of information asymmetry across architectural modules to bias which skills are learnt. While asymmetry choice has a large influence on transferability, existing methods base their choice primarily on intuition in a domain-independent, potentially sub-optimal, manner. In this paper, we theoretically and empirically show the crucial expressivity-transferability trade-off of skills across sequential tasks, controlled by information asymmetry. Given this insight, we introduce Attentive Priors for Expressive and Transferable Skills (APES), a hierarchical KL-regularized method, heavily benefiting from both priors and hierarchy. Unlike existing approaches, APES automates the choice of asymmetry by learning it in a data-driven, domain-dependent, way based on our expressivity-transferability theorems. Experiments over complex transfer domains of varying levels of extrapolation and sparsity, such as robot block stacking, demonstrate the criticality of the correct asymmetric choice, with APES drastically outperforming previous methods.",https://api.openreview.net/pdf/c363ed9425c2f59718edea214b3ce8625578da08.pdf,reinforcement learning;metric;transfer learning;llm,"https://scholar.google.com/scholar?q=Priors,+Hierarchy,+and+Information+Asymmetry+for+Skill+Transfer+in+Reinforcement+Learning"
Generate rather than Retrieve: Large Language Models are Strong Context Generators,2023,ICLR,"['Wenhao Yu', 'Dan Iter', 'Shuohang Wang', 'Yichong Xu', 'Mingxuan Ju', 'Soumya Sanyal', 'Chenguang Zhu', 'Michael Zeng', 'Meng Jiang']",poster,"['prompt learning', 'large language model', 'open-domain question answering']","Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextual documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, in order to generate diverse documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead.",https://api.openreview.net/pdf/a4086708f91faaf89fdf42039c5895dfd8a6a372.pdf,generative model;llm,https://scholar.google.com/scholar?q=Generate+rather+than+Retrieve:+Large+Language+Models+are+Strong+Context+Generators
Reliability of CKA as a Similarity Measure in Deep Learning,2023,ICLR,"['MohammadReza Davari', 'Stefan Horoi', 'Amine Natik', 'Guillaume Lajoie', 'Guy Wolf', 'Eugene Belilovsky']",poster,"['Representation Learning', 'Similarity Measures', 'Centered Kernel Alignment (CKA)']","Comparing learned neural representations in neural networks is a challenging but important problem, which has been approached in different ways. The Centered Kernel Alignment (CKA) similarity metric, particularly its linear variant, has recently become a popular approach and has been widely used to compare representations of a network's different layers, of architecturally similar networks trained differently, or of models with different architectures trained on the same data. A wide variety of claims about similarity and dissimilarity of these various representations have been made using CKA results. In this work we present analysis that formally characterizes CKA sensitivity to a large class of simple transformations, which can naturally occur in the context of modern machine learning. This provides a concrete explanation to CKA sensitivity to outliers, which has been observed in past works, and to transformations that preserve the linear separability of the data, an important generalization attribute. We empirically investigate several weaknesses of the CKA similarity metric, demonstrating situations in which it gives unexpected or counterintuitive results. Finally we study approaches for modifying representations to maintain functional behaviour while changing the CKA value. Our results illustrate that, in many cases, the CKA value can be easily manipulated without substantial changes to the functional behaviour of the models, and call for caution when leveraging activation alignment metrics.",https://api.openreview.net/pdf/402c531d6593a053e759e45ba18de2f2cc84dd23.pdf,graph;zero_few-shot;representation;metric;llm,https://scholar.google.com/scholar?q=Reliability+of+CKA+as+a+Similarity+Measure+in+Deep+Learning
Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,2023,ICLR,"['Denny Zhou', 'Nathanael Schärli', 'Le Hou', 'Jason Wei', 'Nathan Scales', 'Xuezhi Wang', 'Dale Schuurmans', 'Claire Cui', 'Olivier Bousquet', 'Quoc V Le', 'Ed H. Chi']",poster,"['large language models', 'natural language processing', 'prompting', 'reasoning', 'compositional generalization']","Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split)  with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting.  This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",https://api.openreview.net/pdf/328fd7b9b742a2398905672f07b91af643001cb5.pdf,llm,https://scholar.google.com/scholar?q=Least-to-Most+Prompting+Enables+Complex+Reasoning+in+Large+Language+Models
Denoising Masked Autoencoders Help Robust Classification,2023,ICLR,"['QuanLin Wu', 'Hang Ye', 'Yuntian Gu', 'Huishuai Zhang', 'Liwei Wang', 'Di He']",poster,"['self-supervised', 'certified robustness', 'randomized smoothing']","In this paper, we propose a new self-supervised method, which is called denoising masked autoencoders (DMAE), for learning certified robust classifiers of images. In DMAE, we corrupt each image by adding Gaussian noises to each pixel value and randomly masking several patches. A Transformer-based encoder-decoder model is then trained to reconstruct the original image from the corrupted one. In this learning paradigm, the encoder will learn to capture relevant semantics for the downstream tasks, which is also robust to Gaussian additive noises. We show that the pre-trained encoder can naturally be used as the base classifier in Gaussian smoothed models, where we can analytically compute the certified radius for any data point. Although the proposed method is simple, it yields significant performance improvement in downstream classification tasks. We show that the DMAE ViT-Base model, which just uses 1/10 parameters of the model developed in recent work (Carlini et al., 2022), achieves competitive or better certified accuracy in various settings. The DMAE ViT-Large model significantly surpasses all previous results, establishing a new state-of-the-art on ImageNet dataset. We further demonstrate that the pre-trained model has good transferability to the CIFAR-10 dataset, suggesting its wide adaptability. Models and code are available at
https://github.com/quanlin-wu/dmae.",https://api.openreview.net/pdf/eb383598b65499174e21e2475c8f9f0442264ccb.pdf,graph;zero_few-shot;transformer;transfer learning;llm,https://scholar.google.com/scholar?q=Denoising+Masked+Autoencoders+Help+Robust+Classification
SCoMoE: Efficient Mixtures of Experts with Structured Communication,2023,ICLR,"['zhiyuan zeng', 'Deyi Xiong']",poster,[],"  Mixture-of-Experts (MoE) models are promising architectures for massively multilingual neural machine translation and large language models due to the advantage of sublinear scaling. However, the training of large MoE models is usually bottlenecked by the all-to-all communication (Lepikhin et al., 2020). To reduce the communication cost, we propose SCoMoE, an MoE architecture with structured all-to-all communication, inspired by the hierarchical architecture of the communication topology. SCoMoE encourages data to be communicated across devices through fast intra-accelerator/node communication channels, reducing communication throughput in the slow inter-node communication channel. We slice the data on the sequence dimension (SCoMoE-Seq) into three communication groups and project the data on the feature dimension (SCoMoE-Feat) into low-dimensional representations. To compensate the potential performance drop caused by the routing locality in SCoMoE, we further propose a token clustering approach to aggregating related tokens from different devices before the MoE layers. The sigmoid gating in the balanced router used in the token clustering is substituted with the softmax gating with differential sorting. Experiments on bilingual and massively multilingual machine translation demonstrate that SCoMoE achieves a speedup of 1.44x over GShard with comparable performance, and substantially outperforms Gshard (2.8 BLEU) on OPUS-100 with a speedup of 1.25x.",https://api.openreview.net/pdf/ac600913a3de976ce9df830677f9ddacb13a4838.pdf,zero_few-shot;transformer;representation;llm,https://scholar.google.com/scholar?q=SCoMoE:+Efficient+Mixtures+of+Experts+with+Structured+Communication
Learning to Linearize Deep Neural Networks  for Secure and Efficient Private Inference,2023,ICLR,"['Souvik Kundu', 'Shunlin Lu', 'Yuke Zhang', 'Jacqueline Tiffany Liu', 'Peter Anthony Beerel']",poster,"['Efficient private inference', 'cryptographic inference', 'machine learning as a service', 'efficient cryptographic inference', 'automated ReLU reduction']","The large number of ReLU non-linearity operations in existing deep neural networks makes them ill-suited for latency-efficient private inference (PI). Existing techniques to reduce ReLU operations often involve manual effort and sacrifice significant accuracy. In this paper, we first present a novel measure of non-linearity layers’ ReLU sensitivity, enabling mitigation of the time-consuming manual efforts in identifying the same. Based on this sensitivity, we then present SENet, a three-stage training method that for a given ReLU budget, automatically assigns per-layer ReLU counts, decides the ReLU locations for each layer’s activation map, and trains a model with significantly fewer ReLUs to potentially yield latency and communication efficient PI. Experimental evaluations with multiple models on various datasets show SENet’s superior performance both in terms of reduced ReLUs and improved classification accuracy compared to existing alternatives. In particular, SENet can yield models that require up to ∼2× fewer ReLUs while yielding similar accuracy. For a similar ReLU budget SENet can yield models with ∼2.32% improved classification accuracy, evaluated on CIFAR-100.",https://api.openreview.net/pdf/35f3e94fd4c6765836d89bf0ee8307a99020a02c.pdf,inference;llm,https://scholar.google.com/scholar?q=Learning+to+Linearize+Deep+Neural+Networks++for+Secure+and+Efficient+Private+Inference
Masked Frequency Modeling for Self-Supervised Visual Pre-Training,2023,ICLR,"['Jiahao Xie', 'Wei Li', 'Xiaohang Zhan', 'Ziwei Liu', 'Yew-Soon Ong', 'Chen Change Loy']",poster,"['unsupervised learning', 'self-supervised learning', 'representation learning', 'masked frequency modeling']","We present Masked Frequency Modeling (MFM), a unified frequency-domain-based approach for self-supervised pre-training of visual models. Instead of randomly inserting mask tokens to the input embeddings in the spatial domain, in this paper, we shift the perspective to the frequency domain. Specifically, MFM first masks out a portion of frequency components of the input image and then predicts the missing frequencies on the frequency spectrum. Our key insight is that predicting masked components in the frequency domain is more ideal to reveal underlying image patterns rather than predicting masked patches in the spatial domain, due to the heavy spatial redundancy. Our findings suggest that with the right configuration of mask-and-predict strategy, both the structural information within high-frequency components and the low-level statistics among low-frequency counterparts are useful in learning good representations. For the first time, MFM demonstrates that, for both ViT and CNN, a simple non-Siamese framework can learn meaningful representations even using none of the following: (i) extra data, (ii) extra model, (iii) mask token. Experimental results on image classification and semantic segmentation, as well as several robustness benchmarks show the competitive performance and advanced robustness of MFM compared with recent masked image modeling approaches. Furthermore, we also comprehensively investigate the effectiveness of classical image restoration tasks for representation learning from a unified frequency perspective and reveal their intriguing relations with our MFM approach. Project page: https://www.mmlab-ntu.com/project/mfm/index.html.",https://api.openreview.net/pdf/3612ffc6cb79e236fa2961f3a99e5138d9a04f36.pdf,zero_few-shot;representation;segmentation;llm,https://scholar.google.com/scholar?q=Masked+Frequency+Modeling+for+Self-Supervised+Visual+Pre-Training
Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning,2023,ICLR,"['Pan Lu', 'Liang Qiu', 'Kai-Wei Chang', 'Ying Nian Wu', 'Song-Chun Zhu', 'Tanmay Rajpurohit', 'Peter Clark', 'Ashwin Kalyan']",poster,"['Mathematical Reasoning', 'Tabular Math Word Problems', 'Prompt Learning', 'Policy Gradient']","Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples. The data and code are available at https://promptpg.github.io.",https://api.openreview.net/pdf/1f2f51f57875ec48e1bb27c936aa39ee2e65d06e.pdf,metric;multimodal;llm,https://scholar.google.com/scholar?q=Dynamic+Prompt+Learning+via+Policy+Gradient+for+Semi-structured+Mathematical+Reasoning
TempCLR: Temporal Alignment Representation with Contrastive Learning,2023,ICLR,"['Yuncong Yang', 'Jiawei Ma', 'Shiyuan Huang', 'Long Chen', 'Xudong Lin', 'Guangxing Han', 'Shih-Fu Chang']",poster,"['Representation learning', 'Global Sequence Alignment', 'Zero/Few-shot Transfer']","Video representation learning has been successful in video-text pre-training for zero-shot transfer, where each sentence is trained to be close to the paired video clips in a common feature space. For long videos, given a paragraph of description where the sentences describe different segments of the video, by matching all sentence-clip pairs,  the paragraph and the full video are aligned implicitly. However, such unit-level similarity measure may ignore the global temporal context over a long time span, which inevitably limits the generalization ability. In this paper, we propose a contrastive learning framework TempCLR to compare the full video and the paragraph explicitly. As the video/paragraph is formulated as a sequence of clips/sentences, under the constraint of their temporal order, we use dynamic time warping to compute the minimum cumulative cost over sentence-clip pairs as the sequence-level distance. To explore the temporal dynamics, we break the consistency of temporal order by shuffling the video clips or sentences according to the temporal granularity. In this way, we obtain the representations for clips/sentences, which perceive the temporal information and thus facilitate the sequence alignment. In addition to pre-training on the video and paragraph, our approach can also generalize on the matching between different video instances. We evaluate our approach on video retrieval, action step localization, and few-shot action recognition, and achieve consistent performance gain over all three tasks. Detailed ablation studies are provided to justify the approach design. ",https://api.openreview.net/pdf/f4df84ba55e6a74dd51327e33ccbe729ed2f166c.pdf,graph;optimization;zero_few-shot;representation;contrastive learning;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=TempCLR:+Temporal+Alignment+Representation+with+Contrastive+Learning
Effective Self-supervised Pre-training on Low-compute Networks without Distillation,2023,ICLR,"['Fuwen Tan', 'Fatemeh Sadat Saleh', 'Brais Martinez']",poster,"['Self-supervised learning', 'Low-compute network']","Despite the impressive progress of self-supervised learning (SSL), its applicability to low-compute networks has received limited attention. Reported performance has trailed behind standard supervised pre-training by a large margin, barring self-supervised learning from making an impact on models that are deployed on device. Most prior works attribute this poor performance to the capacity bottleneck of the low-compute networks and opt to bypass the problem through the use of knowledge distillation (KD). In this work, we revisit SSL for efficient neural networks, taking a closer at what are the detrimental factors causing the practical limitations, and whether they are intrinsic to the self-supervised low-compute setting. We find that, contrary to accepted knowledge, there is no intrinsic architectural bottleneck, we diagnose that the performance bottleneck is related to the model complexity vs regularization strength trade-off. In particular, we start by empirically observing that the use of local views can have a dramatic impact on the effectiveness of the SSL methods. This hints at view sampling being one of the performance bottlenecks for SSL on low-capacity networks. We hypothesize that the view sampling strategy for large neural networks, which requires matching views in very diverse spatial scales and contexts, is too demanding for low-capacity architectures. We systematize the design of the view sampling mechanism, leading to a new training methodology that consistently improves the performance across different SSL methods (e.g. MoCo-v2, SwAV or DINO), different low-size networks (convolution-based networks, e.g. MobileNetV2, ResNet18, ResNet34 and vision transformer, e.g. ViT-Ti), and different tasks (linear probe, object detection, instance segmentation and semi-supervised learning). Our best models establish new state-of-the-art for SSL methods on low-compute networks despite not using a KD loss term. Code is publicly available at github.com/saic-fi/SSLight.",https://api.openreview.net/pdf/ac14c4b70b2efd3b6c4e244137ae56017267bf5d.pdf,graph;zero_few-shot;transformer;distillation;segmentation;llm,https://scholar.google.com/scholar?q=Effective+Self-supervised+Pre-training+on+Low-compute+Networks+without+Distillation
Can discrete information extraction prompts generalize across language models?,2023,ICLR,"['Nathanaël Carraz Rakotonirina', 'Roberto Dessi', 'Fabio Petroni', 'Sebastian Riedel', 'Marco Baroni']",poster,"['prompting', 'prompt analysis', 'language model interfaces', 'prompt generalizations']","We study whether automatically-induced prompts that effectively extract information from a language model can also be used, out-of-the-box, to probe other language models for the same information. After confirming that discrete prompts induced with the AutoPrompt algorithm outperform manual and semi-manual prompts on the slot-filling task, we demonstrate a drop in performance for AutoPrompt prompts learned on a model and tested on another. We introduce a way to induce prompts by mixing language models at training time that results in prompts that generalize well across models. We conduct an extensive analysis of the induced prompts, finding that the more general prompts include a larger proportion of existing English words and have a less order-dependent and more uniform distribution of information across their component tokens. Our work provides preliminary evidence that it's possible to generate discrete prompts that can be induced once and used with a number of different models, and gives insights on the properties characterizing such prompts.",https://api.openreview.net/pdf/027788e7f8d7f512b53ca6e6935d18aa5150e77f.pdf,llm,https://scholar.google.com/scholar?q=Can+discrete+information+extraction+prompts+generalize+across+language+models?
LPT: Long-tailed Prompt Tuning  for Image Classification,2023,ICLR,"['Bowen Dong', 'Pan Zhou', 'Shuicheng Yan', 'Wangmeng Zuo']",poster,[],"For long-tailed classification tasks, most works often pretrain a big model on a large-scale (unlabeled) dataset, and then fine-tune the whole pretrained  model for  adapting to long-tailed data. Though promising, fine-tuning the whole pretrained model tends to suffer from high cost in computation and deployment of different models for different tasks, as well as weakened generalization capability for overfitting to certain features of long-tailed data. To alleviate these issues, we propose an effective Long-tailed Prompt Tuning (LPT) method for long-tailed classification tasks. LPT introduces several trainable prompts into a frozen pretrained model to adapt it to long-tailed data. For better effectiveness, we divide prompts into two groups: 1) a shared prompt for the whole long-tailed dataset to learn general features and to adapt a pretrained model into the target long-tailed domain; and 2) group-specific prompts to gather group-specific features for the samples which have similar features and also to empower the pretrained model with fine-grained discrimination ability. Then we design a two-phase training paradigm to learn these prompts. In the first phase, we train the shared prompt via conventional supervised prompt tuning to adapt a pretrained model to the desired long-tailed domain. In the second phase, we use the learnt shared prompt as query to select a small best matched set for a group of similar samples from the group-specific prompt set to dig the common features of these similar samples, and then optimize these prompts with a dual sampling strategy and the asymmetric Gaussian Clouded Logit loss. By only fine-tuning a few prompts while fixing the pretrained model, LPT can reduce training cost and deployment cost by storing a few prompts, and enjoys a strong generalization ability of the pretrained model. Experiments show that on various long-tailed benchmarks, with only $\sim$1.1\% extra trainable parameters, LPT achieves comparable or higher performance than previous whole model fine-tuning methods, and is more robust to domain-shift.",https://api.openreview.net/pdf/fcaa69d5aae6afe2bc1093525d8040992a2a38fd.pdf,zero_few-shot;metric;llm,https://scholar.google.com/scholar?q=LPT:+Long-tailed+Prompt+Tuning++for+Image+Classification
DamoFD: Digging into Backbone Design on Face Detection,2023,ICLR,"['Yang Liu', 'Jiankang Deng', 'Fei Wang', 'Lei Shang', 'Xuansong Xie', 'Baigui Sun']",poster,"['Face Detection', 'Neural Architecture Search', 'Network Expressivity']","Face detection (FD) has achieved remarkable success over the past few years, yet,
these leaps often arrive when consuming enormous computation costs. Moreover,
when considering a realistic situation, i.e., building a lightweight face detector
under a computation-scarce scenario, such heavy computation cost limits the application
of the face detector. To remedy this, several pioneering works design
tiny face detectors through off-the-shelf neural architecture search (NAS) technologies,
which are usually applied to the classification task. Thus, the searched
architectures are sub-optimal for the face detection task since some design criteria
between detection and classification task are different. As a representative, the
face detection backbone design needs to guarantee the stage-level detection ability
while it is not required for the classification backbone. Furthermore, the detection
backbone consumes a vast body of inference budgets in the whole detection framework.
Considering the intrinsic design requirement and the virtual importance role
of the face detection backbone, we thus ask a critical question: How to employ
NAS to search FD-friendly backbone architecture? To cope with this question,
we propose a distribution-dependent stage-aware ranking score (DDSAR-Score)
to explicitly characterize the stage-level expressivity and identify the individual
importance of each stage, thus satisfying the aforementioned design criterion of
the FD backbone. Based on our proposed DDSAR-Score, we conduct comprehensive
experiments on the challenging Wider Face benchmark dataset and achieve
dominant performance across a wide range of compute regimes. In particular,
compared to the tiniest face detector SCRFD-0.5GF, our method is +2.5 % better
in Average Precision (AP) score when using the same amount of FLOPs. The
code is avaliable at https://github.com/ly19965/FaceMaas/tree/master/face_project/face_detection/DamoFD.",https://api.openreview.net/pdf/de07bb8709651bf73dcec7308c3610ce657af47a.pdf,graph;inference;llm,https://scholar.google.com/scholar?q=DamoFD:+Digging+into+Backbone+Design+on+Face+Detection
Learning to Compose Soft Prompts for Compositional Zero-Shot Learning,2023,ICLR,"['Nihal V. Nayak', 'Peilin Yu', 'Stephen Bach']",poster,"['compositional zero-shot learning', 'prompts', 'foundation models']","We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) like CLIP. We develop CSP for compositional zero-shot learning, the task of predicting unseen attribute-object compositions (e.g., old cat and young tiger). VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets. CSP treats the attributes and objects that define classes as learnable tokens of vocabulary. During training, the vocabulary is tuned to recognize classes that compose tokens in multiple ways (e.g., old cat and white cat). At test time, we recompose the learned attribute-object vocabulary in new combinations to recognize novel classes. We show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9 percentage points on AUC. CSP also outperforms CoOp, a soft prompting method that fine-tunes the prefix context tokens, by an average of 5.8 percentage points on AUC. We perform additional experiments to show that CSP improves generalization to higher-order attribute-attribute-object compositions (e.g., old white cat) and combinations of pretrained attributes and fine-tuned objects. The code is available at https://github.com/BatsResearch/csp.",https://api.openreview.net/pdf/0e212bc778de607b8b27f2b040f6b63d6261d59a.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Learning+to+Compose+Soft+Prompts+for+Compositional+Zero-Shot+Learning
TaskPrompter: Spatial-Channel Multi-Task Prompting for Dense Scene Understanding,2023,ICLR,"['Hanrong Ye', 'Dan Xu']",poster,"['Multi-task Learning', 'Scene Understanding', 'Computer Vision']","Learning effective representations simultaneously from multiple tasks in a unified network framework is a fundamental paradigm for multi-task dense visual scene understanding. This requires joint modeling (i) task-generic and (ii) task-specific representations, and (iii) cross-task representation interactions. Existing works typically model these three perspectives with separately designed structures, using shared network modules for task-generic learning, different modules for task-specific learning, and establishing connections among these components for cross-task interactions. It is barely explored in the literature to model these three perspectives in each network layer in an end-to-end manner, which can not only minimize the effort of carefully designing empirical structures for the three multi-task representation learning objectives, but also greatly improve the representation learning capability of the multi-task network since all the model capacity will be used to optimize the three objectives together. In this paper, we propose TaskPrompter, a novel spatial-channel multi-task prompting transformer framework to achieve this target. Specifically, we design a set of spatial-channel task prompts and learn their spatial- and channel interactions with the shared image tokens in each transformer layer with attention mechanism, as aggregating spatial and channel information is critical for dense prediction tasks. Each task prompt learns task-specific representation for one task, while all the prompts can jointly contribute to the learning of the shared image token representations, and the interactions between different task prompts model the cross-task relationship. To decode dense predictions for multiple tasks with the learned spatial-channel task prompts from transformer, we accordingly design a dense task prompt decoding mechanism, which queries the shared image tokens using task prompts to obtain spatial- and channel-wise task-specific representations. Extensive experiments on two challenging multi-task dense scene understanding benchmarks (i.e. NYUD-V2 and PASCAL-Context) show the superiority of the proposed framework and TaskPrompter establishes significant state-of-the-art performances on multi-task dense predictions. Codes and models are made publicly available at https://github.com/prismformore/Multi-Task-Transformer.",https://api.openreview.net/pdf/ebe8ec2526826ab8aca7189c0b3935036603f11c.pdf,graph;transformer;representation;multi-task;llm,https://scholar.google.com/scholar?q=TaskPrompter:+Spatial-Channel+Multi-Task+Prompting+for+Dense+Scene+Understanding
Suppressing the Heterogeneity: A Strong Feature Extractor for Few-shot Segmentation,2023,ICLR,"['Zhengdong Hu', 'Yifan Sun', 'Yi Yang']",poster,"['deep learning', 'computer vision', 'few-shot learning', 'few-shot semantic segmentation']","This paper tackles the Few-shot Semantic Segmentation (FSS) task with focus on learning the feature extractor. Somehow the feature extractor has been overlooked by recent state-of-the-art methods, which directly use a deep model pretrained on ImageNet for feature extraction (without further fine-tuning). Under this background, we think the FSS feature extractor deserves exploration and observe the heterogeneity (i.e., the intra-class diversity in the raw images) as a critical challenge hindering the intra-class feature compactness. The heterogeneity has three levels from coarse to fine: 1) Sample-level: the inevitable distribution gap between the support and query images makes them heterogeneous from each other. 2) Region-level: the background in FSS actually contains multiple regions with different semantics. 3) Patch-level: some neighboring patches belonging to a same class may appear quite different from each other. Motivated by these observations, we propose a feature extractor with Multi-level Heterogeneity Suppressing (MuHS). MuHS leverages the attention mechanism in transformer backbone to effectively suppress all these three-level heterogeneity. Concretely, MuHS reinforces the attention / interaction between different samples (query and support), different regions and neighboring patches by constructing cross-sample attention, cross-region interaction and a novel masked image segmentation (inspired by the recent masked image modeling), respectively. We empirically show that 1) MuHS brings consistent improvement for various FSS heads and 2) using a simple linear classification head, MuHS sets new states of the art on multiple FSS datasets, validating the importance of FSS feature learning.",https://api.openreview.net/pdf/41a97d1a38e6c62bc037998a39c3201184ec1317.pdf,graph;transformer;segmentation;llm,https://scholar.google.com/scholar?q=Suppressing+the+Heterogeneity:+A+Strong+Feature+Extractor+for+Few-shot+Segmentation
Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning,2023,ICLR,"['Shaofeng Zhang', 'Feng Zhu', 'Rui Zhao', 'Junchi Yan']",poster,"['self supervised learning', 'contrastive learning']","We propose ADCLR: \underline{A}ccurate and \underline{D}ense \underline{C}ontrastive \underline{R}epresentation \underline{L}earning, a novel self-supervised learning framework for learning accurate and dense vision representation. To extract spatial-sensitive information, ADCLR introduces query patches for contrasting  in addition with global contrasting. Compared with previous dense contrasting methods, ADCLR mainly enjoys three merits: i) achieving both global-discriminative and spatial-sensitive representation, ii) model-efficient (no extra parameters in addition to the global contrasting baseline), and iii) correspondence-free and thus simpler to implement. Our approach achieves new state-of-the-art performance for contrastive methods. On classification tasks, for ViT-S, ADCLR achieves 78.1\% top-1 accuracy on ImageNet with linear probing, outperforming our baseline (DINO) without our devised techniques as plug-in, by 1.1\%. For ViT-B, ADCLR achieves 79.8\%, 84.0\% accuracy on ImageNet by linear probing and finetune, outperforming DINO by 0.6\%, 0.4\% accuracy. For dense tasks, on MS-COCO, ADCLR achieves significant improvements of 44.3\% AP on object detection, 39.7\% AP on instance segmentation, outperforming previous SOTA method SelfPatch by 2.2\% and 1.2\%, respectively. On ADE20K, ADCLR outperforms SelfPatch by 1.0\% mIoU, 1.2\% mAcc on the segmentation task.",https://api.openreview.net/pdf/2827ac68856f272d96ac55b06f6692f5bd6b1eff.pdf,zero_few-shot;representation;contrastive learning;segmentation;llm,https://scholar.google.com/scholar?q=Patch-Level+Contrasting+without+Patch+Correspondence+for+Accurate+and+Dense+Contrastive+Representation+Learning
Human-Guided Fair Classification for Natural Language Processing,2023,ICLR,"['Florian E. Dorner', 'Momchil Peychev', 'Nikola Konstantinov', 'Naman Goel', 'Elliott Ash', 'Martin Vechev']",spotlight,"['Individual Fairness', 'Style Transfer', 'NLP', 'Crowdsourcing', 'Human Evaluation']","Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensitive attributes. We then validate the generated pairs via an extensive crowdsourcing study, which confirms that a lot of these pairs align with human intuition about fairness in the context of toxicity classification. Finally, we show how limited amounts of human feedback can be leveraged to learn a similarity specification that can be used to train downstream fairness-aware models. ",https://api.openreview.net/pdf/09b5568016529de9fe0127852626c933cb6af627.pdf,graph;zero_few-shot;metric;transfer learning;llm,https://scholar.google.com/scholar?q=Human-Guided+Fair+Classification+for+Natural+Language+Processing
A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias,2023,ICLR,"['Puja Trivedi', 'Danai Koutra', 'Jayaraman J. Thiagarajan']",spotlight,"['Transfer Learning', 'Robustness', 'Adaptation', 'Data Augmentation']","Advances in the expressivity of pretrained models have increased interest in the design of adaptation protocols which enable safe and effective transfer learning. Going beyond conventional linear probing (LP) and fine tuning (FT) strategies, protocols that can effectively control feature distortion, i.e., the failure to update features orthogonal to the in-distribution, have been found to achieve improved out-of-distribution generalization (OOD). In order to limit this distortion, the LP+FT protocol, which first learns a linear probe and then uses this initialization for subsequent FT, was proposed. However, in this paper, we find when adaptation protocols (LP, FT, LP+FT) are also evaluated on a variety of safety objectives (e.g., calibration, robustness, etc.), a complementary perspective to feature distortion is helpful to explain protocol behavior. To this end, we study the susceptibility of protocols to simplicity bias (SB), i.e. the well-known propensity of deep neural networks to rely upon simple features, as SB has recently been shown to underlie several problems in robust generalization. Using a synthetic dataset, we demonstrate the susceptibility of existing protocols to SB. Given the strong effectiveness of LP+FT, we then propose modified linear probes that help mitigate SB, and lead to better initializations for subsequent FT. We verify the effectiveness of the proposed LP+FT variants for decreasing SB in a controlled setting, and their ability to improve OOD generalization and safety on three adaptation datasets.",https://api.openreview.net/pdf/a96c8869749346661838b9c685006ca0e44e9011.pdf,transfer learning;llm,https://scholar.google.com/scholar?q=A+Closer+Look+at+Model+Adaptation+using+Feature+Distortion+and+Simplicity+Bias
Ask Me Anything: A simple strategy for prompting language models,2023,ICLR,"['Simran Arora', 'Avanika Narayan', 'Mayee F Chen', 'Laurel Orr', 'Neel Guha', 'Kush Bhatia', 'Ines Chami', 'Christopher Re']",spotlight,"['large language models', 'prompt-engineering', 'in-context learning']","Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly crafted ""perfect prompt"" for a task. To mitigate the high degree of effort, we instead ask whether collecting multiple decent, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed method, Ask Me Anything (AMA). We first develop an understanding of the effective prompt formats, finding question-answering (QA) prompts, which encourage open-ended generation (""Who went to the park?"") tend to outperform those that restrict the model outputs (""John went to the park. True or False?""). AMA recursively uses the LLM to transform task inputs to the effective QA format. AM generates multiple questions per input and applies these prompts to collect several noisy ""votes"" for the input's true label. We find the prompts have varying accuracies and dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions. We evaluate AMA across open-source model families (EleutherAI, BLOOM, OPT, and T0) and sizes (125M-175B parameters), demonstrating an average performance lift of 10.2\% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B  on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama_prompting.",https://api.openreview.net/pdf/5b1bcdac167fa4b294480f303ac3722afa8a9aac.pdf,zero_few-shot;generative model;transfer learning;llm,https://scholar.google.com/scholar?q=Ask+Me+Anything:+A+simple+strategy+for+prompting+language+models
Scale-invariant Bayesian Neural Networks with Connectivity Tangent Kernel,2023,ICLR,"['SungYub Kim', 'Sihwan Park', 'Kyung-Su Kim', 'Eunho Yang']",spotlight,[],"Studying the loss landscapes of neural networks is critical to identifying generalizations and avoiding overconfident predictions. Flatness, which measures the perturbation resilience of pre-trained parameters for loss values, is widely acknowledged as an essential predictor of generalization. While the concept of flatness has been formalized as a PAC-Bayes bound, it has been observed that the generalization bounds can vary arbitrarily depending on the scale of the model parameters. Despite previous attempts to address this issue, generalization bounds remain vulnerable to function-preserving scaling transformations or are limited to impractical network structures. In this paper, we introduce new PAC-Bayes prior and posterior distributions invariant to scaling transformations, achieved through the \textit{decomposition of perturbations into scale and connectivity components}. In this way, this approach expands the range of networks to which the resulting generalization bound can be applied, including those with practical transformations such as weight decay with batch normalization. Moreover, we demonstrate that scale-dependency issues of flatness can adversely affect the uncertainty calibration of Laplace approximation, and we propose a solution using our invariant posterior. Our proposed invariant posterior allows for effective measurement of flatness and calibration with low complexity while remaining invariant to practical parameter transformations, also applying it as a reliable predictor of neural network generalization.",https://api.openreview.net/pdf/a23231458e686af85e35285f01c8e2aa3ee3cf3b.pdf,zero_few-shot;transformer;bayesian;llm,https://scholar.google.com/scholar?q=Scale-invariant+Bayesian+Neural+Networks+with+Connectivity+Tangent+Kernel
SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing,2023,ICLR,"['Sheng Li', 'Geng Yuan', 'Yue Dai', 'Youtao Zhang', 'Yanzhi Wang', 'Xulong Tang']",spotlight,[],"There has been a proliferation of artificial intelligence applications, where model training is key to promising high-quality services for these applications. However, the model training process is both time-intensive and energy-intensive, inevitably affecting the user's demand for application efficiency. Layer freezing, an efficient model training technique, has been proposed to improve training efficiency. Although existing layer freezing methods demonstrate the great potential to reduce model training costs, they still remain shortcomings such as lacking generalizability and compromised accuracy. For instance, existing layer freezing methods either require the freeze configurations to be manually defined before training, which does not apply to different networks, or use heuristic freezing criteria that is hard to guarantee decent accuracy in different scenarios. Therefore, there lacks a generic and smart layer freezing method that can automatically perform ``in-situation'' layer freezing for different networks during training processes. To this end, we propose a generic and efficient training framework (SmartFRZ). The core proposed technique in SmartFRZ is attention-guided layer freezing, which can automatically select the appropriate layers to freeze without compromising accuracy. Experimental results show that SmartFRZ effectively reduces the amount of computation in training and achieves significant training acceleration, and outperforms the state-of-the-art layer freezing approaches.",https://api.openreview.net/pdf/df204364dd4dd09467e128971f66612149d1171b.pdf,transformer;llm,https://scholar.google.com/scholar?q=SmartFRZ:+An+Efficient+Training+Framework+using+Attention-Based+Layer+Freezing
Binding Language Models in Symbolic Languages,2023,ICLR,"['Zhoujun Cheng', 'Tianbao Xie', 'Peng Shi', 'Chengzu Li', 'Rahul Nadkarni', 'Yushi Hu', 'Caiming Xiong', 'Dragomir Radev', 'Mari Ostendorf', 'Luke Zettlemoyer', 'Noah A. Smith', 'Tao Yu']",spotlight,"['semantic parsing', 'large language model', 'neural symbolic', 'code generation']","Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at anonymized.",https://api.openreview.net/pdf/d226e827fb59bcd4253c7eb8ce07d339ef5d519d.pdf,graph;zero_few-shot;llm,https://scholar.google.com/scholar?q=Binding+Language+Models+in+Symbolic+Languages
"Learning MLPs on Graphs: A Unified View of Effectiveness, Robustness, and Efficiency",2023,ICLR,"['Yijun Tian', 'Chuxu Zhang', 'Zhichun Guo', 'Xiangliang Zhang', 'Nitesh Chawla']",spotlight,"['Graph Representation Learning', 'Knowledge Distillation']","While Graph Neural Networks (GNNs) have demonstrated their efficacy in dealing with non-Euclidean structural data, they are difficult to be deployed in real applications due to the scalability constraint imposed by the multi-hop data dependency. Existing methods attempt to address this scalability issue by training student multi-layer perceptrons (MLPs) exclusively on node content features using labels derived from the teacher GNNs. However, the trained MLPs are neither effective nor robust. In this paper, we ascribe the lack of effectiveness and robustness to three significant challenges: 1) the misalignment between content feature and label spaces, 2) the strict hard matching to teacher's output, and 3) the sensitivity to node feature noises. To address the challenges, we propose NOSMOG, a novel method to learn NOise-robust Structure-aware MLPs On Graphs, with remarkable effectiveness, robustness, and efficiency. Specifically, we first address the misalignment by complementing node content with position features to capture the graph structural information. We then design an innovative representational similarity distillation strategy to inject soft node similarities into MLPs. Finally, we introduce adversarial feature augmentation to ensure stable learning against feature noises. Extensive experiments and theoretical analyses demonstrate the superiority of NOSMOG by comparing it to GNNs and the state-of-the-art method in both transductive and inductive settings across seven datasets. Codes are available at https://github.com/meettyj/NOSMOG.",https://api.openreview.net/pdf/38ae9a6d49f8c8de73c63c07dd1bb214b1e1d1e0.pdf,graph;optimization;representation;augmentation;distillation;llm,"https://scholar.google.com/scholar?q=Learning+MLPs+on+Graphs:+A+Unified+View+of+Effectiveness,+Robustness,+and+Efficiency"
"Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization",2023,ICLR,"['Rajkumar Ramamurthy', 'Prithviraj Ammanabrolu', 'Kianté Brantley', 'Jack Hessel', 'Rafet Sifa', 'Christian Bauckhage', 'Hannaneh Hajishirzi', 'Yejin Choi']",spotlight,"['natural language processing', 'reinforcement learning', 'language models', 'feedback learning']","We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP?

To help answer this, we first introduce an open-source modular library, $RL4LMs$ (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the $GRUE$ (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, $NLPO$ (Natural Language Policy Optimization)} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.",https://api.openreview.net/pdf/e7b48c662a15dbddc1a3d5c9a2b338c13189506e.pdf,reinforcement learning;graph;optimization;generative model;llm,"https://scholar.google.com/scholar?q=Is+Reinforcement+Learning+(Not)+for+Natural+Language+Processing:+Benchmarks,+Baselines,+and+Building+Blocks+for+Natural+Language+Policy+Optimization"
Disentanglement with Biological Constraints: A Theory of Functional Cell Types,2023,ICLR,"['James C. R. Whittington', 'Will Dorrell', 'Surya Ganguli', 'Timothy Behrens']",spotlight,"['Disentangling', 'neurosciece', 'representation learning', 'hippocampus', 'cortex']","Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentanglement in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why single neurons in the brain often represent single human-interpretable factors, and steps towards an understanding task structure shapes the structure of brain representation.",https://api.openreview.net/pdf/263cc7c08a3dc723c277184b709aa922ef1fc5d5.pdf,optimization;representation;vae;llm,https://scholar.google.com/scholar?q=Disentanglement+with+Biological+Constraints:+A+Theory+of+Functional+Cell+Types
Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation,2023,ICLR,"['Lorenz Kuhn', 'Yarin Gal', 'Sebastian Farquhar']",spotlight,"['uncertainty estimation', 'natural language generation']","We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of ""semantic equivalence""—different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy—an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines. ",https://api.openreview.net/pdf/535617ff3bfc4d3297922f5c320dee127d1ce6cc.pdf,graph;generative model;llm,https://scholar.google.com/scholar?q=Semantic+Uncertainty:+Linguistic+Invariances+for+Uncertainty+Estimation+in+Natural+Language+Generation
DINO as a von Mises-Fisher mixture model,2023,ICLR,"['Hariprasath Govindarajan', 'Per Sidén', 'Jacob Roll', 'Fredrik Lindsten']",spotlight,"['self-supervised learning', 'vision transformers', 'mixture models']","Self-distillation methods using Siamese networks are popular for self-supervised pre-training. DINO is one such method based on a cross-entropy loss between $K$-dimensional probability vectors, obtained by applying a softmax function to the dot product between representations and learnt prototypes. Given the fact that the learned representations are $L^2$-normalized, we show that DINO and its derivatives, such as iBOT, can be interpreted as a mixture model of von Mises-Fisher components. With this interpretation, DINO assumes equal precision for all components when the prototypes are also $L^2$-normalized. Using this insight we propose DINO-vMF, that adds appropriate normalization constants when computing the cluster assignment probabilities. Unlike DINO, DINO-vMF is stable also for the larger ViT-Base model with unnormalized prototypes. We show that the added flexibility of the mixture model is beneficial in terms of better image representations. The DINO-vMF pre-trained model consistently performs better than DINO on a range of downstream tasks. We obtain similar improvements for iBOT-vMF vs iBOT and thereby show the relevance of our proposed modification also for other methods derived from DINO.",https://api.openreview.net/pdf/7c0fa9125fa53c842a7c216fd9f1b16ee517710f.pdf,zero_few-shot;representation;distillation;llm,https://scholar.google.com/scholar?q=DINO+as+a+von+Mises-Fisher+mixture+model
TEMPERA: Test-Time Prompt Editing via Reinforcement Learning,2023,ICLR,"['Tianjun Zhang', 'Xuezhi Wang', 'Denny Zhou', 'Dale Schuurmans', 'Joseph E. Gonzalez']",spotlight,[],"Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning.  As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.",https://api.openreview.net/pdf/1cf8438e4df114f4ea13408da5952d5636eabb99.pdf,reinforcement learning;zero_few-shot;generative model;adaptive;inference;llm,https://scholar.google.com/scholar?q=TEMPERA:+Test-Time+Prompt+Editing+via+Reinforcement+Learning
A probabilistic framework for task-aligned intra- and inter-area neural manifold estimation,2023,ICLR,"['Edoardo Balzani', 'Jean-Paul G Noel', 'Pedro Herrero-Vidal', 'Dora E Angelaki', 'Cristina Savin']",spotlight,"['neuroscience', 'dimensionality reduction', 'probabilistic methods', 'inter-area interactions']","Latent manifolds provide a compact characterization of neural population activity and of shared co-variability across brain areas. Nonetheless, existing statistical tools for extracting neural manifolds face limitations in terms of interpretability of latents with respect to task variables, and can be hard to apply to datasets with no trial repeats. Here we propose a novel probabilistic framework that allows for interpretable partitioning of population variability within and across areas in the context of naturalistic behavior. Our approach for task aligned manifold estimation (TAME-GP) explicitly partitions variability into private and shared sources which can themselves be subdivided in task-relevant and task irrelevant components, uses a realistic Poisson noise model, and introduces temporal smoothing of latent trajectories in the form of a Gaussian Process prior. This TAME-GP graphical model allows for robust estimation of task-relevant variability in local population responses, and of shared co-variability between brain areas. We demonstrate the efficiency of our estimator on within model and biologically motivated simulated data. We also apply it to several datasets of neural population recordings during behavior. Overall, our results demonstrate the capacity of TAME-GP to capture meaningful intra- and inter-area neural variability with single trial resolution.",https://api.openreview.net/pdf/9e46e84807837851d4357f969ea06aa885cf4f5a.pdf,graph;zero_few-shot;multimodal;llm,https://scholar.google.com/scholar?q=A+probabilistic+framework+for+task-aligned+intra-+and+inter-area+neural+manifold+estimation
Revisiting adapters with adversarial training,2023,ICLR,"['Sylvestre-Alvise Rebuffi', 'Francesco Croce', 'Sven Gowal']",spotlight,"['adapters', 'adversarial', 'robustness', 'soup']","While adversarial training is generally used as a defense mechanism, recent works show that it can also act as a regularizer. By co-training a neural network on clean and adversarial inputs, it is possible to improve classification accuracy on the clean, non-adversarial inputs. We demonstrate that, contrary to previous findings, it is not necessary to separate batch statistics when co-training on clean and adversarial inputs, and that it is sufficient to use adapters with few domain-specific parameters for each type of input. We establish that using the classification token of a Vision Transformer (ViT) as an adapter is enough to match the classification performance of dual normalization layers, while using significantly less additional parameters. First, we improve upon the top-1 accuracy of a non-adversarially trained ViT-B16 model by +1.12% on ImageNet (reaching 83.76% top-1 accuracy). Second, and more importantly, we show that training with adapters enables model soups through linear combinations of the clean and adversarial tokens. These model soups, which we call adversarial model soups, allow us to trade-off between clean and robust accuracy without sacrificing efficiency. Finally, we show that we can easily adapt the resulting models in the face of distribution shifts. Our ViT-B16 obtains top-1 accuracies on ImageNet variants that are on average +4.00% better than those obtained with Masked Autoencoders.",https://api.openreview.net/pdf/c986093cab366dcc82865df98b5906e39dc7c493.pdf,zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Revisiting+adapters+with+adversarial+training
ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion,2023,ICLR,"['Aleksandar Pavlović', 'Emanuel Sallinger']",spotlight,"['knowledge graph embedding', 'knowledge graph completion', 'composition', 'hierarchy', 'geometric interpretation']","Knowledge graphs are inherently incomplete. Therefore substantial research has been directed toward knowledge graph completion (KGC), i.e., predicting missing triples from the information represented in the knowledge graph (KG). KG embedding models (KGEs) have yielded promising results for KGC, yet any current KGE is incapable of: (1) fully capturing vital inference patterns (e.g., composition), (2) capturing prominent patterns jointly (e.g., hierarchy and composition), and (3) providing an intuitive interpretation of captured patterns. In this work, we propose ExpressivE, a fully expressive spatio-functional KGE that solves all these challenges simultaneously. ExpressivE embeds pairs of entities as points and relations as hyper-parallelograms in the virtual triple space $\mathbb{R}^{2d}$. This model design allows ExpressivE not only to capture a rich set of inference patterns jointly but additionally to display any supported inference pattern through the spatial relation of hyper-parallelograms, offering an intuitive and consistent geometric interpretation of ExpressivE embeddings and their captured patterns. Experimental results on standard KGC benchmarks reveal that ExpressivE is competitive with state-of-the-art KGEs and even significantly outperforms them on WN18RR.",https://api.openreview.net/pdf/071ed2e450ebd00e88fdcae80a0773cfe4c7aec8.pdf,graph;zero_few-shot;inference;metric;llm,https://scholar.google.com/scholar?q=ExpressivE:+A+Spatio-Functional+Embedding+For+Knowledge+Graph+Completion
Training language models to summarize narratives improves brain alignment,2023,ICLR,"['Khai Loong Aw', 'Mariya Toneva']",spotlight,"['language', 'nlp', 'neuroscience', 'fMRI', 'interpretability']","Building systems that achieve a deeper understanding of language is one of the central goals of natural language processing (NLP). Towards this goal, recent works have begun to train language models on narrative datasets which require extracting the most critical information by integrating across long contexts. However, it is still an open question whether these models are learning a deeper understanding of the text, or if the models are simply learning a heuristic to complete the task. This work investigates this further by turning to the one language processing system that truly understands complex language: the human brain. We show that training language models for deeper narrative understanding results in richer representations that have improved alignment to human brain activity. We further find that the improvements in brain alignment are larger for character names than for other discourse features, which indicates that these models are learning important narrative elements. Taken together, these results suggest that this type of training can indeed lead to deeper language understanding. These findings have consequences both for cognitive neuroscience by revealing some of the significant factors behind brain-NLP alignment, and for NLP by highlighting that understanding of long-range context can be improved beyond language modeling.",https://api.openreview.net/pdf/c67334d169d975ca4c1f56fc722f9eb680ebf5b9.pdf,representation;llm,https://scholar.google.com/scholar?q=Training+language+models+to+summarize+narratives+improves+brain+alignment
Efficient recurrent architectures through activity sparsity and sparse back-propagation through time,2023,ICLR,"['Anand Subramoney', 'Khaleelulla Khan Nazeer', 'Mark Schöne', 'Christian Mayr', 'David Kappel']",spotlight,"['RNN', 'GRU', 'recurrent network', 'language modeling', 'dvs', 'gesture recognition', 'activity sparsity', 'efficiency']","Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and  low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events.  We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at https://github.com/KhaleelKhan/EvNN/.",https://api.openreview.net/pdf/388663b8b91354ae6d68fc7eb3580a2b5f3431fa.pdf,optimization;zero_few-shot;sparse;llm,https://scholar.google.com/scholar?q=Efficient+recurrent+architectures+through+activity+sparsity+and+sparse+back-propagation+through+time
Not All Tasks Are Born Equal: Understanding Zero-Shot Generalization,2023,ICLR,"['Jing Zhou', 'Zongyu Lin', 'Yanan Zheng', 'Jian Li', 'Zhilin Yang']",spotlight,"['Zero-Shot Learning', 'Multi-Task Learning', 'Transfer Learning']","Recent work has achieved remarkable zero-shot performance with multi-task prompted pretraining, but little has been understood. For the first time, we show that training on a small number of key tasks beats using all the training tasks, while removing these key tasks substantially hurts performance. We also find that these key tasks are mostly question answering (QA) tasks. These novel findings combined deepen our understanding about zero-shot generalization—training on certain tasks such as QA encodes general knowledge transferable to a wide range of tasks. In addition, to automate this procedure, we devise a method that (1) identifies key training tasks without observing the test tasks by examining the pairwise generalization results and (2) resamples training tasks for better data distribution. Empirically, our approach achieves improved results across various model scales and tasks.",https://api.openreview.net/pdf/1919d0fcb91288c02556462ba37d25845db2452f.pdf,zero_few-shot;transfer learning;multi-task;llm,https://scholar.google.com/scholar?q=Not+All+Tasks+Are+Born+Equal:+Understanding+Zero-Shot+Generalization
Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models,2023,ICLR,"['Xiaoman Pan', 'Wenlin Yao', 'Hongming Zhang', 'Dian Yu', 'Dong Yu', 'Jianshu Chen']",spotlight,"['Semi-parametric language model', 'text-to-text model', 'mixture-of-experts', 'natural language understanding']","Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge:  entity,  dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly, we find that KiC can be identified as a special mixture-of-experts (MoE) model, where the knowledge selector plays the role of a router that is used to determine the sequence-to-expert assignment in MoE. This key observation inspires us to develop a novel algorithm for training KiC with an instance-adaptive knowledge selector. As a knowledge-rich semi-parametric language model, KiC only needs a much smaller parametric part to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+ different tasks, we show that KiC-Large with 770M parameters easily outperforms large language models that are 4-39x larger. In addition, KiC also exhibits emergent abilities at a much smaller model scale compared to the fully-parametric models.",https://api.openreview.net/pdf/2f1a38a721bba2dcfa96af632678ce02c41b26bd.pdf,zero_few-shot;adaptive;metric;augmentation;llm,https://scholar.google.com/scholar?q=Knowledge-in-Context:+Towards+Knowledgeable+Semi-Parametric+Language+Models
CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis,2023,ICLR,"['Erik Nijkamp', 'Bo Pang', 'Hiroaki Hayashi', 'Lifu Tu', 'Huan Wang', 'Yingbo Zhou', 'Silvio Savarese', 'Caiming Xiong']",spotlight,"['Program synthesis', 'multi-turn generation', 'code generation', 'large language models', 'generative models']","Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.",https://api.openreview.net/pdf/003bbce081e6ee9edeead69fcdba6fbe3882de42.pdf,zero_few-shot;generative model;llm,https://scholar.google.com/scholar?q=CodeGen:+An+Open+Large+Language+Model+for+Code+with+Multi-Turn+Program+Synthesis
ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning,2023,ICLR,"['Olga Golovneva', 'Moya Peng Chen', 'Spencer Poff', 'Martin Corredor', 'Luke Zettlemoyer', 'Maryam Fazel-Zarandi', 'Asli Celikyilmaz']",spotlight,"['step-by-step reasoning', 'evaluation']","Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality — among other traits — by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics.",https://api.openreview.net/pdf/3f6164615b8f835462171508e65f188740d76ee8.pdf,graph;generative model;metric;llm,https://scholar.google.com/scholar?q=ROSCOE:+A+Suite+of+Metrics+for+Scoring+Step-by-Step+Reasoning
Re-calibrating Feature Attributions for Model Interpretation,2023,ICLR,"['Peiyu Yang', 'NAVEED AKHTAR', 'Zeyi Wen', 'Mubarak Shah', 'Ajmal Saeed Mian']",spotlight,"['Feature Attribution', 'Explainable Artifical Intelligence']","The ability to interpret machine learning models is critical for high-stakes applications. Due to its desirable theoretical properties, path integration is a widely used scheme for feature attribution to interpret model predictions. However, the methods implementing this scheme currently rely on absolute attribution scores to eventually provide sensible interpretations. This not only contradicts the premise that the features with larger attribution scores are more relevant to the model prediction, but also conflicts with the theoretical settings for which the desirable properties of the attributions are proven. We address this by devising a method to first compute an appropriate reference for the path integration scheme. This reference further helps in identifying valid interpolation points on a desired integration path. The reference is computed in a gradient ascending direction on the model's loss surface, while the interpolations are performed by analyzing the model gradients and variations between the reference and the input. The eventual integration is effectively performed along a non-linear path. Our scheme can be incorporated into the existing integral-based attribution methods. We also devise an effective sampling and integration procedure that enables employing our scheme with multi-reference path integration efficiently. We achieve a marked performance boost for a range of integral-based attribution methods on both local and global evaluation metrics by enhancing them with our scheme. Our extensive results also show improved sensitivity, sanity preservation and model robustness with the proposed re-calibration of the attribution techniques with our method.",https://api.openreview.net/pdf/27e850e1a146543993bd508afba29de6ac36bbdb.pdf,metric;llm,https://scholar.google.com/scholar?q=Re-calibrating+Feature+Attributions+for+Model+Interpretation
DocPrompting: Generating Code by Retrieving the Docs,2023,ICLR,"['Shuyan Zhou', 'Uri Alon', 'Frank F. Xu', 'Zhengbao Jiang', 'Graham Neubig']",spotlight,"['code generation', 'retrieval-conditioned generation']","Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code
to keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an NL intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.",https://api.openreview.net/pdf/c9881a374e0bce9d005809d63e83dfdae53d9d40.pdf,graph;generative model;llm,https://scholar.google.com/scholar?q=DocPrompting:+Generating+Code+by+Retrieving+the+Docs
Quantifying Memorization Across Neural Language Models,2023,ICLR,"['Nicholas Carlini', 'Daphne Ippolito', 'Matthew Jagielski', 'Katherine Lee', 'Florian Tramer', 'Chiyuan Zhang']",spotlight,"['memorization', 'large language models', 'duplication']","Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others).
We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.",https://api.openreview.net/pdf/6b4201e769d9dc79c8462750821d94951ee50a84.pdf,zero_few-shot;active learning;llm,https://scholar.google.com/scholar?q=Quantifying+Memorization+Across+Neural+Language+Models
Differentially Private $L_2$-Heavy Hitters in the Sliding Window Model,2023,ICLR,"['Jeremiah Blocki', 'Seunghoon Lee', 'Tamalika Mukherjee', 'Samson Zhou']",spotlight,"['differential privacy', 'heavy hitters', 'streaming algorithms', 'sliding window model']","The data management of large companies often prioritize more recent data, as a source of higher accuracy prediction than outdated data. For example, the Facebook data policy retains user search histories for $6$ months while the Google data retention policy states that browser information may be stored for up to $9$ months. These policies are captured by the sliding window model, in which only the most recent $W$ statistics form the underlying dataset. In this paper, we consider the problem of privately releasing the $L_2$-heavy hitters in the sliding window model, which include $L_p$-heavy hitters for $p\le 2$ and in some sense are the strongest possible guarantees that can be achieved using polylogarithmic space, but cannot be handled by existing techniques due to the sub-additivity of the $L_2$ norm. Moreover, existing non-private sliding window algorithms use the smooth histogram framework, which has high sensitivity. To overcome these barriers, we introduce the first differentially private algorithm for $L_2$-heavy hitters in the sliding window model by initiating a number of $L_2$-heavy hitter algorithms across the stream with significantly lower threshold. Similarly, we augment the algorithms with an approximate frequency tracking algorithm with significantly higher accuracy. We then use smooth sensitivity and statistical distance arguments to show that we can add noise proportional to an estimation of the $L_2$ norm. To the best of our knowledge, our techniques are the first to privately release statistics that are related to a sub-additive function in the sliding window model, and may be of independent interest to future differentially private algorithmic design in the sliding window model.",https://api.openreview.net/pdf/7a0d8905677bac47b853d1dfdaa37542861101da.pdf,zero_few-shot;llm,https://scholar.google.com/scholar?q=Differentially+Private+$L_2$-Heavy+Hitters+in+the+Sliding+Window+Model
Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!,2023,ICLR,"['Shiwei Liu', 'Tianlong Chen', 'Zhenyu Zhang', 'Xuxi Chen', 'Tianjin Huang', 'AJAY KUMAR JAISWAL', 'Zhangyang Wang']",spotlight,"['Sparse Neural Networks', 'Benchmark', 'Sparsity', 'Neural Network Pruning']","Sparse Neural Networks (SNNs) have received voluminous attention predominantly due to growing computational and memory footprints of consistently exploding parameter count in large-scale models. Similar to their dense counterparts, recent SNNs generalize just as well and are equipped with numerous favorable benefits (e.g., low complexity, high scalability, and robustness), sometimes even better than the original dense networks. As research effort is focused on developing increasingly sophisticated sparse algorithms, it is startling that a comprehensive benchmark to evaluate the effectiveness of these algorithms has been highly overlooked. In absence of a carefully crafted evaluation benchmark, most if not all, sparse algorithms are evaluated against fairly simple and naive tasks (eg. CIFAR-10/100, ImageNet, GLUE, etc.), which can potentially camouflage many advantages as well unexpected predicaments of SNNs. In pursuit of a more general evaluation and unveiling the true potential of sparse algorithms, we introduce “Sparsity May Cry” Benchmark (SMC-Bench), a collection of carefully-curated 4 diverse tasks with 10 datasets, that accounts for capturing a wide range of domain-specific and sophisticated knowledge. Our systemic evaluation of the most representative sparse algorithms reveals an important obscured observation: the state-of-the-art magnitude- and/or gradient-based sparse algorithms seemingly fail to perform on SMC-Bench when applied out-of-the-box, sometimes at significantly trivial sparsity as low as 5%. The observations seek the immediate attention of the sparsity research community to reconsider the highly proclaimed benefits of SNNs. We further conduct a thorough investigation into the reasons for the failure of common SNNs. Our analysis points out that such failure is intimately related to the “lazy regime” of large model training, which hints us with stronger pruning recipes that alleviate the failure on SMC-Bench (though still more or less suffering). By incorporating these well-thought and diverse tasks, SMC-Bench is designed to favor and encourage the development of more scalable and generalizable sparse algorithms. We open-source SMC-Bench to assist researchers in building next-generation sparse algorithms that scale and generalize: https://github.com/VITA-Group/SMC-Bench.",https://api.openreview.net/pdf/60f68324a3ec40c50412c32d7f1d7ee813d44b35.pdf,graph;zero_few-shot;transformer;generative model;sparse;llm,https://scholar.google.com/scholar?q=Sparsity+May+Cry:+Let+Us+Fail+(Current)+Sparse+Neural+Networks+Together!
Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers,2023,ICLR,"['Tianlong Chen', 'Zhenyu Zhang', 'AJAY KUMAR JAISWAL', 'Shiwei Liu', 'Zhangyang Wang']",spotlight,"['Sparse Mixture-of-Experts', 'Random Routing', 'Transformer Training', 'Dropout']","Despite their remarkable achievement, gigantic transformers encounter significant drawbacks, including exorbitant computational and memory footprints during training, as well as severe collapse evidenced by a high degree of parameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown promise to mitigate the issue of training efficiency, yet they are prone to (1) $\textit{redundant experts}$ due to representational collapse; and (2) $\textit{poor expert scalability for inference and downstream fine-tuning}$, primarily due to overfitting of the learned routing policy to the number of activated experts during training. As recent research efforts are predominantly focused on improving routing policies to encourage expert specializations, this work focuses on $\textit{exploring the overlooked scalability bottleneck of SMoEs}$ and leveraging it to effectively $\textbf{scale dense transformers}$. To this end, we propose a new plug-and-play training framework, $\textbf{SMoE-Dropout}$, to enable scaling transformers to better accuracy in their full capacity without collapse. Specifically, SMoE-Dropout consists of a $\textit{randomly initialized and fixed}$ router network to activate experts and gradually increases the activated expert number as training progresses over time. Transformers trained by SMoE-Dropout naturally exhibit a $\textbf{``self-slimmable”}$ property subject to resource availability, offering smooth and consistent performance boosts with an increase in activated experts during inference or fine-tuning. Our extensive experiments across diverse transformer architectures on a variety of tasks demonstrate the superior performance and substantial computation savings of SMoE-Dropout, compared to dense training baselines with equivalent parameter counts. In particular, our trained BERT outperforms its densely trained counterpart with consistent improvements of {$1.03\%$, $0.78\%$, $1.09\%$} on challenging reasoning tasks {$\texttt{ASDiv-A}$, $\texttt{MAWPS}$, $\texttt{SVAMP}$}, respectively. Codes and models are available in https://github.com/VITA-Group/Random-MoE-as-Dropout.",https://api.openreview.net/pdf/9a22d737856844ae4058be999052c67e4e975671.pdf,graph;transformer;representation;inference;sparse;llm,https://scholar.google.com/scholar?q=Sparse+MoE+as+the+New+Dropout:+Scaling+Dense+and+Self-Slimmable+Transformers
Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning,2023,ICLR,"['Jiahui Gao', 'Renjie Pi', 'LIN Yong', 'Hang Xu', 'Jiacheng Ye', 'Zhiyong Wu', 'WEIZHONG ZHANG', 'Xiaodan Liang', 'Zhenguo Li', 'Lingpeng Kong']",spotlight,"['Pre-Trained Language Model', 'Prompt-Based Learning', 'Efficient Zero-Shot Learning']","There is a rising interest in further exploring the zero-shot learning potential of large pre-trained language models (PLMs). A new paradigm called data-generation-based zero-shot learning has achieved impressive success. In this paradigm, the synthesized data from the PLM acts as the carrier of knowledge, which is used to train a task-specific model with orders of magnitude fewer parameters than the PLM, achieving both higher performance and efficiency than prompt-based zero-shot learning methods on PLMs. The main hurdle of this approach is that the synthesized data from PLM usually contains a significant portion of low-quality samples. Fitting on such data will greatly hamper the performance of the task-specific model, making it unreliable for deployment. Previous methods remedy this issue mainly by filtering synthetic data using heuristic metrics(e.g., output confidence), or refining the data with the help of a human expert, which comes with excessive manual tuning or expensive costs. In this paper, we propose a novel noise-robust re-weighting framework SunGen to automatically construct high-quality data for zero-shot classification problems. Our framework features the ability to learn the sample weights indicating data quality without requiring any human annotation. We theoretically and empirically verify the ability of our method to help construct good-quality synthetic datasets. Notably, SunGen-LSTM yields a 9.8% relative improvement than the baseline on average accuracy across eight different established text classification tasks.",https://api.openreview.net/pdf/82812310fbf1dff5ce1f72fe99e2d46523ca8d5a.pdf,zero_few-shot;generative model;metric;llm,https://scholar.google.com/scholar?q=Self-Guided+Noise-Free+Data+Generation+for+Efficient+Zero-Shot+Learning
D4FT: A Deep Learning Approach to Kohn-Sham Density Functional Theory,2023,ICLR,"['Tianbo Li', 'Min Lin', 'Zheyuan Hu', 'Kunhao Zheng', 'Giovanni Vignale', 'Kenji Kawaguchi', 'A.H. Castro Neto', 'Kostya S. Novoselov', 'Shuicheng YAN']",spotlight,"['AI for Science', 'Quantum Chemisty', 'Density Functional Theory', 'Deep Learning', 'Kohn-Sham Equation.']","Kohn-Sham Density Functional Theory (KS-DFT) has been traditionally solved by the Self-Consistent Field (SCF) method. Behind the SCF loop is the physics intuition of solving a system of non-interactive single-electron wave functions under an effective potential. In this work, we propose a deep learning approach to KS-DFT. First, in contrast to the conventional SCF loop, we propose to directly minimize the total energy by reparameterizing the orthogonal constraint as a feed-forward computation. We prove that such an approach has the same expressivity as the SCF method, yet reduces the computational complexity from O(N^4) to O(N^3). Second, the numerical integration which involves a summation over the quadrature grids can be amortized to the optimization steps. At each step, stochastic gradient descent (SGD) is performed with a sampled minibatch of the grids. Extensive experiments are carried out to demonstrate the advantage of our approach in terms of efficiency and stability. In addition, we show that our approach enables us to explore more complex neural-based wave functions. ",https://api.openreview.net/pdf/2224ef90a640f03ebd92a397a2ffd6bc277a8b16.pdf,optimization;active learning;llm,https://scholar.google.com/scholar?q=D4FT:+A+Deep+Learning+Approach+to+Kohn-Sham+Density+Functional+Theory
Prompt-to-Prompt Image Editing with Cross-Attention Control,2023,ICLR,"['Amir Hertz', 'Ron Mokady', 'Jay Tenenbaum', 'Kfir Aberman', 'Yael Pritch', 'Daniel Cohen-or']",spotlight,"['Image generation', 'Image editing', 'Diffusion models', 'Attention layer', 'Computer vision', 'Machine learning']","Recent large-scale text-driven synthesis diffusion models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Therefore, it is only natural to build upon these synthesis models to provide text-driven image editing capabilities. However, Editing is challenging for these generative models, since an innate property of an editing technique is to preserve some content from the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. We analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we propose to control the attention maps along the diffusion process. Our approach enables us to monitor the synthesis process by editing the textual prompt only, paving the way to a myriad of caption-based editing applications such as localized editing by replacing a word, global editing by adding a specification, and even controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts with different text-to-image models, demonstrating high-quality synthesis and fidelity to the edited prompts.",https://api.openreview.net/pdf/a6e78444f28f4790c2b8eb24364ced3ce736feb0.pdf,graph;zero_few-shot;transformer;generative model;diffusion models;llm,https://scholar.google.com/scholar?q=Prompt-to-Prompt+Image+Editing+with+Cross-Attention+Control
DiffEdit: Diffusion-based semantic image editing with mask guidance,2023,ICLR,"['Guillaume Couairon', 'Jakob Verbeek', 'Holger Schwenk', 'Matthieu Cord']",spotlight,"['computer vision', 'image editing', 'diffusion models']","Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. 
Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. 
DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images.",https://api.openreview.net/pdf/3d837329e3740d349726e77482e1be2f69278a1b.pdf,graph;optimization;zero_few-shot;generative model;inference;diffusion models;llm,https://scholar.google.com/scholar?q=DiffEdit:+Diffusion-based+semantic+image+editing+with+mask+guidance
Corrupted Image Modeling for Self-Supervised Visual Pre-Training,2023,ICLR,"['Yuxin Fang', 'Li Dong', 'Hangbo Bao', 'Xinggang Wang', 'Furu Wei']",spotlight,"['Self-supervised Learning', 'Representation Learning', 'Vision Transformer']","We introduce Corrupted Image Modeling (CIM) for self-supervised visual pre-training. CIM uses an auxiliary generator with a small trainable BEiT to corrupt the input image instead of using artificial [MASK] tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our approach achieves compelling results in vision benchmarks, such as ImageNet classification and ADE20K semantic segmentation.",https://api.openreview.net/pdf/4f86e1c43a4f5b420e19c75c8be820279b0b46a9.pdf,graph;zero_few-shot;representation;segmentation;llm,https://scholar.google.com/scholar?q=Corrupted+Image+Modeling+for+Self-Supervised+Visual+Pre-Training
Hungry Hungry Hippos: Towards Language Modeling with State Space Models,2023,ICLR,"['Daniel Y Fu', 'Tri Dao', 'Khaled Kamal Saab', 'Armin W Thomas', 'Atri Rudra', 'Christopher Re']",spotlight,"['language modeling', 'state space models', 'efficiency']","State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.",https://api.openreview.net/pdf/b3774a7e6b7bda0783528bf1dc8e2600707d797f.pdf,zero_few-shot;transformer;llm,https://scholar.google.com/scholar?q=Hungry+Hungry+Hippos:+Towards+Language+Modeling+with+State+Space+Models
Relational Attention: Generalizing Transformers for Graph-Structured Tasks,2023,ICLR,"['Cameron Diao', 'Ricky Loynd']",spotlight,"['Graph Neural Networks', 'Transformers', 'Graph Representation Learning', 'Neural Algorithmic Reasoning']","Transformers flexibly operate over sets of real-valued vectors representing task-specific entities and their attributes, where each vector might encode one word-piece token and its position in a sequence, or some piece of information that carries no position at all. As set processors, transformers are at a disadvantage in reasoning over more general graph-structured data where nodes represent entities and edges represent relations between entities. To address this shortcoming, we generalize transformer attention to consider and update edge vectors in each transformer layer. We evaluate this relational transformer on a diverse array of graph-structured tasks, including the large and challenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically outperforms state-of-the-art graph neural networks expressly designed to reason over graph-structured data. Our analysis demonstrates that these gains are attributable to relational attention's inherent ability to leverage the greater expressivity of graphs over sets.",https://api.openreview.net/pdf/49232cd55923175bab0a33ca81d281c76edcfaad.pdf,graph;transformer;llm,https://scholar.google.com/scholar?q=Relational+Attention:+Generalizing+Transformers+for+Graph-Structured+Tasks
Learning to Estimate Shapley Values with Vision Transformers,2023,ICLR,"['Ian Connick Covert', 'Chanwoo Kim', 'Su-In Lee']",spotlight,"['ViTs', 'Shapley values', 'amortization', 'explainability']","Transformers have become a default architecture in computer vision, but understanding what drives their predictions remains a challenging problem. Current explanation approaches rely on attention values or input gradients, but these provide a limited view of a model’s dependencies. Shapley values offer a theoretically sound alternative, but their computational cost makes them impractical for large, high-dimensional models. In this work, we aim to make Shapley values practical for vision transformers (ViTs). To do so, we first leverage an attention masking approach to evaluate ViTs with partial information, and we then develop a procedure to generate Shapley value explanations via a separate, learned explainer model. Our experiments compare Shapley values to many baseline methods (e.g., attention rollout, GradCAM, LRP), and we find that our approach provides more accurate explanations than existing methods for ViTs.",https://api.openreview.net/pdf/63a91ca98681923ceee596aa7d3254f49445c743.pdf,graph;transformer;llm,https://scholar.google.com/scholar?q=Learning+to+Estimate+Shapley+Values+with+Vision+Transformers
A framework for benchmarking Class-out-of-distribution detection and its application to ImageNet,2023,ICLR,"['Ido Galil', 'Mohammed Dabbah', 'Ran El-Yaniv']",spotlight,"['benchmarking', 'out of distribution', 'class out of distribution', 'OOD', 'OOD detection']","When deployed for risk-sensitive tasks, deep neural networks must be able to detect instances with labels from outside the distribution for which they were trained.
In this paper we present a novel framework to benchmark the ability of image classifiers to detect class-out-of-distribution instances
(i.e., instances whose true labels do not appear in the training distribution) at various levels of detection difficulty.
We apply this technique to ImageNet, and benchmark 525 pretrained, publicly available, ImageNet-1k classifiers. 
The code for generating a benchmark for any ImageNet-1k classifier, along with the benchmarks prepared for the above-mentioned 525 models is available at https://github.com/mdabbah/COOD_benchmarking.

The usefulness of the proposed framework and its advantage over alternative existing benchmarks is demonstrated by analyzing the results obtained for these models, which reveals numerous novel observations including:
(1) knowledge distillation consistently improves class-out-of-distribution (C-OOD) detection performance; (2) a subset of ViTs performs better C-OOD detection than any other model; (3) the language–-vision CLIP model achieves good zero-shot detection performance, with its best instance outperforming 96% of all other models evaluated; (4) accuracy and in-distribution ranking are positively correlated to C-OOD detection; and 
(5) we compare various confidence functions for C-OOD detection.
Our companion paper, also published in ICLR 2023 (What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers), examines the uncertainty estimation performance (ranking, calibration, and selective prediction performance) of these classifiers in an in-distribution setting.",https://api.openreview.net/pdf/973b16b739dacc7aaa862e3a74f9469f31742eb0.pdf,zero_few-shot;distillation;llm,https://scholar.google.com/scholar?q=A+framework+for+benchmarking+Class-out-of-distribution+detection+and+its+application+to+ImageNet
Mass-Editing Memory in a Transformer,2023,ICLR,"['Kevin Meng', 'Arnab Sen Sharma', 'Alex J Andonian', 'Yonatan Belinkov', 'David Bau']",spotlight,"['language models', 'GPT', 'transformers', 'model editing', 'factual associations', 'memory']","Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by an order of magnitude. Our code and data will be open-sourced upon publication.",https://api.openreview.net/pdf/5d2ff18d2f074c0f0b7bda40d118bb08e13bcd43.pdf,transformer;llm,https://scholar.google.com/scholar?q=Mass-Editing+Memory+in+a+Transformer
HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer,2023,ICLR,"['Xiaosong Zhang', 'Yunjie Tian', 'Lingxi Xie', 'Wei Huang', 'Qi Dai', 'Qixiang Ye', 'Qi Tian']",spotlight,"['Hierarchical vision transformers', 'self-supervised learning', 'masked image modeling']","There has been a debate on the choice of plain vs. hierarchical vision transformers, where researchers often believe that the former (e.g., ViT) has a simpler design but the latter (e.g., Swin) enjoys higher recognition accuracy. Recently, the emerge of masked image modeling (MIM), a self-supervised visual pre-training method, raised a new challenge to vision transformers in terms of flexibility, i.e., part of image patches or tokens are to be discarded, which seems to claim the advantages of plain vision transformers. In this paper, we delve deep into the comparison between ViT and Swin, revealing that (i) the performance gain of Swin is mainly brought by a deepened backbone and relative positional encoding, (ii) the hierarchical design of Swin can be simplified into hierarchical patch embedding (proposed in this work), and (iii) other designs such as shifted-window attentions can be removed. By removing the unnecessary operations, we come up with a new architecture named HiViT (short for hierarchical ViT), which is simpler and more efficient than Swin yet further improves its performance on fully-supervised and self-supervised visual representation learning. In particular, after pre-trained using masked autoencoder (MAE) on ImageNet-1K, HiViT-B reports a 84.6% accuracy on ImageNet-1K classification, a 53.3% box AP on COCO detection, and a 52.8% mIoU on ADE20K segmentation, significantly surpassing the baseline. Code is available at https://github.com/zhangxiaosong18/hivit.",https://api.openreview.net/pdf/7835ef364a3e5f77397911e7f2f90b3aa3630f8b.pdf,zero_few-shot;transformer;representation;segmentation;llm,https://scholar.google.com/scholar?q=HiViT:+A+Simpler+and+More+Efficient+Design+of+Hierarchical+Vision+Transformer
"A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics",2023,ICLR,"['Qing Li', 'Siyuan Huang', 'Yining Hong', 'Yixin Zhu', 'Ying Nian Wu', 'Song-Chun Zhu']",spotlight,"['Systematic Generalization', 'Concept Learning']","Inspired by humans' exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, HINT, to examine machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models' limitations, we undertake extensive experiments with various sequence-to-sequence models, including RNNs, Transformers, and GPT-3 (with the chain of thought prompting). The results indicate that current models struggle to extrapolate to long-range syntactic dependency and semantics. Models exhibit a considerable gap toward human-level generalization when evaluated with new concepts in a few-shot setting. Moreover, we discover that it is infeasible to solve HINT by merely scaling up the dataset and the model size; this strategy contributes little to the extrapolation of syntax and semantics. Finally, in zero-shot GPT-3 experiments, the chain of thought prompting exhibits impressive results and significantly boosts the test accuracy. We believe the HINT dataset and the experimental findings are of great interest to the learning community on systematic generalization.%",https://api.openreview.net/pdf/09b973c9c84fd934195e0c087cb7af065e9c6829.pdf,zero_few-shot;transformer;llm,"https://scholar.google.com/scholar?q=A+Minimalist+Dataset+for+Systematic+Generalization+of+Perception,+Syntax,+and+Semantics"
A Holistic View of Label Noise Transition Matrix in Deep Learning and Beyond,2023,ICLR,"['LIN Yong', 'Renjie Pi', 'WEIZHONG ZHANG', 'Xiaobo Xia', 'Jiahui Gao', 'Xiao Zhou', 'Tongliang Liu', 'Bo Han']",spotlight,[],"In this paper, we explore learning statistically consistent classifiers under label noise by estimating the noise transition matrix T. We first provide a holistic view of existing T-estimation methods including those with or without anchor point assumptions.  We unified them into the Minimum Geometric Envelope Operator (MGEO) framework, which tries to find the smallest T (in terms of a certain metric) that elicits a convex hull to enclose the posteriors of all the training data. Although MGEO methods show appealing theoretical properties and empirical results, we find them prone to failing when the noisy posterior estimation is imperfect, which is inevitable in practice. Specifically, we show that MGEO methods are in-consistent even with infinite samples if the noisy posterior is not estimated accurately. In view of this, we make the first effort to address this issue by proposing a novel T-estimation framework via the lens of bilevel optimization, and term it RObust Bilevel OpTimzation (ROBOT). ROBOT paves a new road beyond MGEO framework, which enjoys strong theoretical properties: identifibility, consistency and finite-sample generalization guarantees. Notably, ROBOT neither requires the perfect posterior estimation nor assumes the existence of anchor points. We further theoretically demonstrate that ROBOT is more robust in the case where MGEO methods fail. Experimentally, our framework also shows superior performance across multiple benchmarks.",https://api.openreview.net/pdf/405fab9c74f731a957a6e9ee24c23a06a6809b77.pdf,optimization;metric;llm,https://scholar.google.com/scholar?q=A+Holistic+View+of+Label+Noise+Transition+Matrix+in+Deep+Learning+and+Beyond
GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation,2023,ICLR,"['Chenhongyi Yang', 'Jiarui Xu', 'Shalini De Mello', 'Elliot J. Crowley', 'Xiaolong Wang']",spotlight,"['Visual Recognition', 'Vision transformer architecture']","We present the Group Propagation Vision Transformer (GPViT): a novel non- hierarchical (i.e. non-pyramidal) transformer model designed for general visual recognition with high-resolution features. High-resolution features (or tokens) are a natural fit for tasks that involve perceiving fine-grained details such as detection and segmentation, but exchanging global information between these features is expensive in memory and computation because of the way self-attention scales. We provide a highly efficient alternative Group Propagation Block (GP Block) to exchange global information. In each GP Block, features are first grouped to- gether by a fixed number of learnable group tokens; we then perform Group Propagation where global information is exchanged between the grouped fea- tures; finally, global information in the updated grouped features is returned back to the image features through a transformer decoder. We evaluate GPViT on a variety of visual recognition tasks including image classification, semantic seg- mentation, object detection, and instance segmentation. Our method achieves significant performance gains over previous works across all tasks, especially on tasks that require high-resolution outputs, for example, our GPViT-L3 out- performs Swin Transformer-B by 2.0 mIoU on ADE20K semantic segmentation with only half as many parameters. Code and pre-trained models are available at https://github.com/ChenhongyiYang/GPViT.",https://api.openreview.net/pdf/9542365fc4380de76797ec856ed324fe9acf8f79.pdf,graph;transformer;segmentation;llm,https://scholar.google.com/scholar?q=GPViT:+A+High+Resolution+Non-Hierarchical+Vision+Transformer+with+Group+Propagation
Vision Transformer Adapter for Dense Predictions,2023,ICLR,"['Zhe Chen', 'Yuchen Duan', 'Wenhai Wang', 'Junjun He', 'Tong Lu', 'Jifeng Dai', 'Yu Qiao']",spotlight,"['Plain Vision Transformer', 'Adapter', 'Dense Prediction']","This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer (ViT). Unlike recently advanced variants that incorporate vision-specific inductive biases into their architectures, the plain ViT suffers inferior performance on dense predictions due to weak prior assumptions. To address this issue, we propose the ViT-Adapter, which allows plain ViT to achieve comparable performance to vision-specific transformers. Specifically, the backbone in our framework is a plain ViT that can learn powerful representations from large-scale multi-modal data. When transferring to downstream tasks, a pre-training-free adapter is used to introduce the image-related inductive biases into the model, making it suitable for these tasks. We verify ViT-Adapter on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Notably, without using extra detection data, our ViT-Adapter-L yields state-of-the-art 60.9 box AP and 53.0 mask AP on COCO test-dev. We hope that the ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. Code and models will be released at https://github.com/czczup/ViT-Adapter.",https://api.openreview.net/pdf/a1a7cac48a3e0fa0d2a12b5a46c5b2463fe22a38.pdf,zero_few-shot;transformer;representation;transfer learning;segmentation;multimodal;llm,https://scholar.google.com/scholar?q=Vision+Transformer+Adapter+for+Dense+Predictions
Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors,2023,ICLR,"['Jianfei Yang', 'Xiangyu Peng', 'Kai Wang', 'Zheng Zhu', 'Jiashi Feng', 'Lihua Xie', 'Yang You']",spotlight,"['model adaptation', 'black-box predictors', 'transfer learning']","Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an unlabeled target domain supervised by a black-box predictor trained on a source domain. It does not require access to both the source-domain data and the predictor parameters, thus addressing the data privacy and portability issues of standard domain adaptation methods. Existing DABP approaches mostly rely on knowledge distillation (KD) from the black-box predictor, i.e., training the model with its noisy target-domain predictions, which however inevitably introduces the confirmation bias accumulated from the prediction noises and leads to degrading performance. To mitigate such bias, we propose a new strategy, \textit{divide-to-adapt}, that purifies cross-domain knowledge distillation by proper domain division. This is inspired by an observation we make for the first time in domain adaptation: the target domain usually contains easy-to-adapt and hard-to-adapt samples that have different levels of domain discrepancy w.r.t. the source domain, and deep models tend to fit easy-to-adapt samples first. Leveraging easy-to-adapt samples with less noise can help KD alleviate the negative effect of prediction noises from black-box predictors. In this sense, the target domain can be divided into an easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain at the early stage of training. Then the adaptation is achieved by semi-supervised learning. We further reduce distribution discrepancy between subdomains and develop weak-strong augmentation strategy to filter the predictor errors progressively. As such, our method is a simple yet effective solution to reduce error accumulation in cross-domain knowledge distillation for DABP. Moreover, we prove that the target error of DABP is bounded by the noise ratio of two subdomains, i.e., the confirmation bias, which provides the theoretical justifications for our method. Extensive experiments demonstrate our method achieves state of the art on all DABP benchmarks, outperforming the existing best approach by 7.0\% on VisDA-17, and is even comparable with the standard domain adaptation methods that use the source-domain data.",https://api.openreview.net/pdf/f6acdba3a448c8c49b38089c9fcca2175f862634.pdf,graph;zero_few-shot;augmentation;distillation;multimodal;llm,https://scholar.google.com/scholar?q=Divide+to+Adapt:+Mitigating+Confirmation+Bias+for+Domain+Adaptation+of+Black-Box+Predictors
PLOT: Prompt Learning with Optimal Transport for Vision-Language Models,2023,ICLR,"['Guangyi Chen', 'Weiran Yao', 'Xiangchen Song', 'Xinyue Li', 'Yongming Rao', 'Kun Zhang']",spotlight,[],"With the increasing attention to large vision-language models such as CLIP, there has been a significant amount of effort dedicated to building efficient prompts. Unlike conventional methods of only learning one single prompt, we propose to learn multiple comprehensive prompts to describe diverse characteristics of categories such as intrinsic attributes or extrinsic contexts. However, directly matching each prompt to the same visual feature is problematic, as it pushes the prompts to converge to one point. To solve this problem, we propose to apply optimal transport to match the vision and text modalities. Specifically, we first model images and the categories with visual and textual feature sets. Then, we apply a two-stage optimization strategy to learn the prompts. In the inner loop, we optimize the optimal transport distance to align visual features and prompts by the Sinkhorn algorithm, while in the outer loop, we learn the prompts by this distance from the supervised data. Extensive experiments are conducted on the few-shot recognition task and the improvement demonstrates the superiority of our method. The code is available at https://github.com/CHENGY12/PLOT.",https://api.openreview.net/pdf/ddf150416bd1ce46f5512042c2aaa162c8ad10b7.pdf,optimization;transformer;llm,https://scholar.google.com/scholar?q=PLOT:+Prompt+Learning+with+Optimal+Transport+for+Vision-Language+Models
LAVA: Data Valuation without Pre-Specified Learning Algorithms,2023,ICLR,"['Hoang Anh Just', 'Feiyang Kang', 'Tianhao Wang', 'Yi Zeng', 'Myeongseob Ko', 'Ming Jin', 'Ruoxi Jia']",spotlight,"['data valuation', 'optimal transport', 'model agnostic', 'data-driven']","Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden. 

This work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning algorithm. Our main results are as follows. $\textbf{(1)}$ We develop a proxy for the validation performance associated with a training set based on a non-conventional $\textit{class-wise}$ $\textit{Wasserstein distance}$ between the training and the validation set. We show that the distance characterizes the upper bound of the validation performance for any given model under certain Lipschitz conditions. $\textbf{(2)}$ We develop a novel method to value individual data based on the sensitivity analysis of the $\textit{class-wise}$ Wasserstein distance. Importantly, these values can be directly obtained $\textit{for free}$ from the output of off-the-shelf optimization solvers once the Wasserstein distance is computed. $\textbf{(3) }$We evaluate our new data valuation framework over various use cases related to detecting low-quality data
and show that, surprisingly, the learning-agnostic feature of our framework enables a $\textit{significant improvement}$ over the state-of-the-art performance while being $\textit{orders of magnitude faster.}$ ",https://api.openreview.net/pdf/8a4a49f404d172df902842781f95ef52ed70433e.pdf,optimization;zero_few-shot;llm,https://scholar.google.com/scholar?q=LAVA:+Data+Valuation+without+Pre-Specified+Learning+Algorithms
Neuro-Symbolic Procedural Planning with Commonsense Prompting,2023,ICLR,"['Yujie Lu', 'Weixi Feng', 'Wanrong Zhu', 'Wenda Xu', 'Xin Eric Wang', 'Miguel Eckstein', 'William Yang Wang']",spotlight,"['Procedural Planning', 'Commonsense Knowledge', 'Prompting', 'Neuro-Symbolic']","Procedural planning aims to implement complex high-level goals by decomposition into simpler low-level steps. Although procedural planning is a basic skill set for humans in daily life, it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures. Previous methods require manual exemplars to acquire procedural planning knowledge from LLMs in the zero-shot setting. However, such elicited pre-trained knowledge in LLMs induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural PLANner (PLAN) that elicits procedural planning knowledge from the LLMs with commonsense-infused prompting. To mitigate spurious goal-step correlations, we use symbolic program executors on the latent procedural representations to formalize prompts from commonsense knowledge bases as a causal intervention toward the Structural Causal Model. Both automatic and human evaluations on WikiHow and RobotHow show the superiority of PLAN on procedural planning without further training or manual exemplars.",https://api.openreview.net/pdf/3af66a16e02e6ec05187d765b1d2da8cabae2719.pdf,zero_few-shot;representation;llm,https://scholar.google.com/scholar?q=Neuro-Symbolic+Procedural+Planning+with+Commonsense+Prompting
Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language,2023,ICLR,"['Andy Zeng', 'Maria Attarian', 'brian ichter', 'Krzysztof Marcin Choromanski', 'Adrian Wong', 'Stefan Welker', 'Federico Tombari', 'Aveek Purohit', 'Michael S Ryoo', 'Vikas Sindhwani', 'Johnny Lee', 'Vincent Vanhoucke', 'Pete Florence']",spotlight,"['prompt engineering', 'multimodal applications', 'visual language models', 'large language models', 'commonsense reasoning']","We investigate how multimodal prompt engineering can use language as the intermediate representation to combine complementary knowledge from different pretrained (potentially multimodal) language models for a variety of tasks. This approach is both distinct from and complementary to the dominant paradigm of joint multimodal training. It also recalls a traditional systems-building view as in classical NLP pipelines, but with prompting large pretrained multimodal models. We refer to these as Socratic Models (SMs): a modular class of systems in which multiple pretrained models may be composed zero-shot via multimodal-informed prompting to capture new multimodal capabilities, without additional finetuning. We show that these systems provide competitive state-of-the-art performance for zero-shot image captioning and video-to-text retrieval, and also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes), and (iii) robot perception and planning. We hope this work provides (a) results for stronger zero-shot baseline performance with analysis also highlighting their limitations, (b) new perspectives for building multimodal systems powered by large pretrained models, and (c) practical application advantages in certain regimes limited by data scarcity, training compute, or model access.",https://api.openreview.net/pdf/92b6e024f8a9e971e8041aa14e06de2802245730.pdf,graph;zero_few-shot;representation;multimodal;llm,https://scholar.google.com/scholar?q=Socratic+Models:+Composing+Zero-Shot+Multimodal+Reasoning+with+Language
Multi-lingual Evaluation of Code Generation Models,2023,ICLR,"['Ben Athiwaratkun', 'Sanjay Krishna Gouda', 'Zijian Wang', 'Xiaopeng Li', 'Yuchen Tian', 'Ming Tan', 'Wasi Uddin Ahmad', 'Shiqi Wang', 'Qing Sun', 'Mingyue Shang', 'Sujan Kumar Gonugondla', 'Hantian Ding', 'Varun Kumar', 'Nathan Fulton', 'Arash Farahani', 'Siddhartha Jain', 'Robert Giaquinto', 'Haifeng Qian', 'Murali Krishna Ramanathan', 'Ramesh Nallapati', 'Baishakhi Ray', 'Parminder Bhatia', 'Sudipta Sengupta', 'Dan Roth', 'Bing Xiang']",spotlight,"['code generation', 'execution-based evaluation', 'test-based evaluation', 'language models', 'multi-lingual code generation benchmark', 'code insertion', 'code summarization', 'robustness for code', 'code translation', 'zero-shot code translation', 'multi-lingual', 'mono-lingual', 'language models.']","We present two new benchmarks, MBXP and Multilingual HumanEval, designed to evaluate code completion models in over 10 programming languages. These datasets are generated using a conversion framework that transpiles prompts and test cases from the original MBPP and HumanEval datasets into the corresponding data in the target language. By using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of  few-shot prompting to teach the model new languages, and zero-shot translation abilities. In addition, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks.",https://api.openreview.net/pdf/c2ba4659e44c45ec67969ec9a74097a37184ad62.pdf,graph;zero_few-shot;generative model;llm,https://scholar.google.com/scholar?q=Multi-lingual+Evaluation+of+Code+Generation+Models
Human Motion Diffusion Model,2023,ICLR,"['Guy Tevet', 'Sigal Raab', 'Brian Gordon', 'Yoni Shafir', 'Daniel Cohen-or', 'Amit Haim Bermano']",spotlight,[],"Natural and expressive human motion generation is the holy grail of computer animation.
It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. 
Diffusion models are promising candidates for the human motion domain since they
have already shown remarkable generative capabilities in other domains, and their many-to-many nature. 
In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for human motion data.  MDM is transformer-based, combining insights from motion generation literature. 
A notable design-choice is that it predicts the sample itself rather than the noise in each step to facilitate the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion, action-to-motion, and unconditioned motion generation. ",https://api.openreview.net/pdf/f0e30bdff6d93fdd5a01526aaea18c2fec384fc0.pdf,graph;zero_few-shot;transformer;generative model;metric;diffusion models;llm,https://scholar.google.com/scholar?q=Human+Motion+Diffusion+Model
STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK,2023,ICLR,"['Danilo Neves Ribeiro', 'Shen Wang', 'Xiaofei Ma', 'Henghui Zhu', 'Rui Dong', 'Deguang Kong', 'Juliette Burger', 'Anjelica Ramos', 'zhiheng huang', 'William Yang Wang', 'George Karypis', 'Bing Xiang', 'Dan Roth']",spotlight,"['natural language understanding', 'question answering', 'structured explanations', 'soft reasoning', 'dataset']","We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark. Unlike most existing question-answering (QA) datasets, we expect models to not only answer questions, but also produce step-by-step structured explanations describing how premises in the question are used to produce intermediate conclusions that can prove the correctness of a certain answer. We perform extensive evaluation with popular language models such as few-shot prompting GPT-3 and fine-tuned T5. We find that these models still lag behind human performance when producing such structured reasoning steps. We believe this work will provide a way for the community to better train and test systems on multi-step reasoning and explanations in natural language.",https://api.openreview.net/pdf/1b74d54ce93b0d4d1558e20806f96d4b743468ea.pdf,multi-task;llm,https://scholar.google.com/scholar?q=STREET:+A+MULTI-TASK+STRUCTURED+REASONING+AND+EXPLANATION+BENCHMARK
Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class-Incremental Learning,2023,ICLR,"['Yibo Yang', 'Haobo Yuan', 'Xiangtai Li', 'Zhouchen Lin', 'Philip Torr', 'Dacheng Tao']",spotlight,"['few-shot class-incremental learning', 'neural collapse']","Few-shot class-incremental learning (FSCIL) has been a challenging problem as only a few training samples are accessible for each novel class in the new sessions. Finetuning the backbone or adjusting the classifier prototypes trained in the prior sessions would inevitably cause a misalignment between the feature and classifier of old classes, which explains the well-known catastrophic forgetting problem. In this paper, we deal with this misalignment dilemma in FSCIL inspired by the recently discovered phenomenon named neural collapse, which reveals that the last-layer features of the same class will collapse into a vertex, and the vertices of all classes are aligned with the classifier prototypes, which are formed as a simplex equiangular tight frame (ETF). It corresponds to an optimal geometric structure for classification due to the maximized Fisher Discriminant Ratio. We propose a neural collapse inspired framework for FSCIL. A group of classifier prototypes are pre-assigned as a simplex ETF for the whole label space, including the base session and all the incremental sessions. During training, the classifier prototypes are not learnable, and we adopt a novel loss function that drives the features into their corresponding prototypes. Theoretical analysis shows that our method holds the neural collapse optimality and does not break the feature-classifier alignment in an incremental fashion. Experiments on the miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our proposed framework outperforms the state-of-the-art performances. Code address: https://github.com/NeuralCollapseApplications/FSCIL ",https://api.openreview.net/pdf/0ffbc09764bcd3fed340d49d2404429bae5277f5.pdf,graph;metric;multimodal;llm,https://scholar.google.com/scholar?q=Neural+Collapse+Inspired+Feature-Classifier+Alignment+for+Few-Shot+Class-Incremental+Learning
