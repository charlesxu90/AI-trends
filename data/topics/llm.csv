title,year,source,authors,class,keywords,abstract,pdf_link,topic,google_scholar_link
Coarse-Fine Networks for Temporal Activity Detection in Videos,2021,CVPR,Kumara Kahatapitiya;Michael S. Ryoo,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Kahatapitiya_Coarse-Fine_Networks_for_Temporal_Activity_Detection_in_Videos_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=Coarse-Fine+Networks+for+Temporal+Activity+Detection+in+Videos
Globally Optimal Relative Pose Estimation With Gravity Prior,2021,CVPR,Yaqing Ding;Daniel Barath;Jian Yang;Hui Kong;Zuzana Kukelova,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Ding_Globally_Optimal_Relative_Pose_Estimation_With_Gravity_Prior_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=Globally+Optimal+Relative+Pose+Estimation+With+Gravity+Prior
Repetitive Activity Counting by Sight and Sound,2021,CVPR,Yunhua Zhang;Ling Shao;Cees G. M. Snoek,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Repetitive_Activity_Counting_by_Sight_and_Sound_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=Repetitive+Activity+Counting+by+Sight+and+Sound
VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization,2021,CVPR,Seunghwan Choi;Sunghyun Park;Minsoo Lee;Jaegul Choo,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Choi_VITON-HD_High-Resolution_Virtual_Try-On_via_Misalignment-Aware_Normalization_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=VITON-HD:+High-Resolution+Virtual+Try-On+via+Misalignment-Aware+Normalization
Multi-Label Activity Recognition Using Activity-Specific Features and Activity Correlations,2021,CVPR,Yanyi Zhang;Xinyu Li;Ivan Marsic,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Multi-Label_Activity_Recognition_Using_Activity-Specific_Features_and_Activity_Correlations_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=Multi-Label+Activity+Recognition+Using+Activity-Specific+Features+and+Activity+Correlations
Joint Deep Model-Based MR Image and Coil Sensitivity Reconstruction Network (Joint-ICNet) for Fast MRI,2021,CVPR,Yohan Jun;Hyungseob Shin;Taejoon Eo;Dosik Hwang,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Jun_Joint_Deep_Model-Based_MR_Image_and_Coil_Sensitivity_Reconstruction_Network_CVPR_2021_paper.pdf,llm,https://scholar.google.com/scholar?q=Joint+Deep+Model-Based+MR+Image+and+Coil+Sensitivity+Reconstruction+Network+(Joint-ICNet)+for+Fast+MRI
Dataset Curation Beyond Accuracy,2021,ICLR,"['Johan Bjorck', 'Carla P Gomes']",poster,"['crowd-sourcing', 'calibration', 'dataset', 'uncertainty']","Neural networks are known to be data-hungry, and collecting large labeled datasets is often a crucial step in deep learning deployment. Researchers have studied dataset aspects such as distributional shift and labeling cost, primarily using downstream prediction accuracy for evaluation. In sensitive real-world applications such as medicine and self-driving cars, not only is the accuracy important, but also the calibration -- the extent that model uncertainty reflects the actual correctness likelihood. It has recently been shown that modern neural networks are ill-calibrated. In this work, we take a complementary approach -- studying how dataset properties, rather than architecture, affects calibration. For the common issue of dataset imbalance, we show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. We also study the effects of label quality, showing how label noise dramatically increases calibration error. Furthermore, poor calibration can come from small dataset sizes, which we motive via results on network expressivity. Our experiments demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation.",/pdf/19547af72952e4c1e55f808961f95c872390e5fb.pdf,llm,https://scholar.google.com/scholar?q=Dataset+Curation+Beyond+Accuracy
"With False Friends Like These, Who Can Have Self-Knowledge?",2021,ICLR,"['Lue Tao', 'Songcan Chen']",poster,"['Robustness', 'Adversarial Risk', 'Neural Networks', 'Machine Learning Security']","Adversarial examples arise from excessive sensitivity of a model. Commonly studied adversarial examples are malicious inputs, crafted by an adversary from correctly classified examples, to induce misclassification. This paper studies an intriguing, yet far overlooked consequence of the excessive sensitivity, that is, a misclassified example can be easily perturbed to help the model to produce correct output. Such perturbed examples look harmless, but actually can be maliciously utilized by a false friend to make the model self-satisfied. Thus we name them hypocritical examples. With false friends like these, a poorly performed model could behave like a state-of-the-art one. Once a deployer trusts the hypocritical performance and uses the ""well-performed"" model in real-world applications, potential security concerns appear even in benign environments. In this paper, we formalize the hypocritical risk for the first time and propose a defense method specialized for hypocritical examples by minimizing the tradeoff between natural risk and an upper bound of hypocritical risk. Moreover, our theoretical analysis reveals connections between adversarial risk and hypocritical risk. Extensive experiments verify the theoretical results and the effectiveness of our proposed methods.",/pdf/a5829dd51dc29c9441bd1979f9fb1a8dc004a99a.pdf,llm,"https://scholar.google.com/scholar?q=With+False+Friends+Like+These,+Who+Can+Have+Self-Knowledge?"
"Laplacian Eigenspaces, Horocycles and Neuron Models on Hyperbolic Spaces",2021,ICLR,['Ming-Xi Wang'],poster,"['hyperbolic learning', 'hyperbolic neural network', 'Poincare embedding']","We use hyperbolic Poisson kernel to construct the horocycle neuron model on hyperbolic spaces, which is a spectral generalization of the classical neuron model. We prove a universal approximation theorem for horocycle neurons. As a corollary, this theorem leads to a state-of-the-art result on the expressivity of neurons of the hyperbolic MLR. Our experiments get state-of-the-art results on the Poincare-embedding tree classification task and the two-dimensional visualization of images.",/pdf/6980ca2ff050b0760638c7b77236e9e254bbf027.pdf,llm,"https://scholar.google.com/scholar?q=Laplacian+Eigenspaces,+Horocycles+and+Neuron+Models+on+Hyperbolic+Spaces"
Perceptual Adversarial Robustness: Defense Against Unseen Threat Models,2021,ICLR,"['Cassidy Laidlaw', 'Sahil Singla', 'Soheil Feizi']",poster,[],"A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the definition of adversarial attacks that are imperceptible to human eyes. Most current attacks and defenses try to get around this issue by considering restrictive adversarial threat models such as those bounded by $L_2$ or $L_\infty$ distance, spatial perturbations, etc. However, models that are robust against any of these restrictive threat models are still fragile against other threat models, i.e. they have poor generalization to unforeseen attacks. Moreover, even if a model is robust against the union of several restrictive threat models, it is still susceptible to other imperceptible adversarial examples that are not contained in any of the constituent threat models. To resolve these issues, we propose adversarial training against the set of all imperceptible adversarial examples. Since this set is intractable to compute without a human in the loop, we approximate it using deep neural networks. We call this threat model the neural perceptual threat model (NPTM); it includes adversarial examples with a bounded neural perceptual distance (a neural network-based approximation of the true perceptual distance) to natural images. Through an extensive perceptual study, we show that the neural perceptual distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model.

Under the NPTM, we develop novel perceptual adversarial attacks and defenses. Because the NPTM is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five diverse adversarial attacks: $L_2$, $L_\infty$, spatial, recoloring, and JPEG. We find that PAT achieves state-of-the-art robustness against the union of these five attacks—more than doubling the accuracy over the next best model—without training against any of them. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial training defense with this property.

Code and data are available at https://github.com/cassidylaidlaw/perceptual-advex",/pdf/eb768a2d394baa94aebe2efba341cc5a5244428d.pdf,llm,https://scholar.google.com/scholar?q=Perceptual+Adversarial+Robustness:+Defense+Against+Unseen+Threat+Models
Evaluations and Methods for Explanation through Robustness Analysis,2021,ICLR,"['Cheng-Yu Hsieh', 'Chih-Kuan Yeh', 'Xuanqing Liu', 'Pradeep Kumar Ravikumar', 'Seungyeon Kim', 'Sanjiv Kumar', 'Cho-Jui Hsieh']",poster,"['Interpretability', 'Explanations', 'Adversarial Robustness']","Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel set of evaluation criteria for such feature based explanations by robustness analysis. In contrast to existing evaluations which require us to specify some way to ""remove"" features that could inevitably introduces biases and artifacts, we make use of the subtler notion of smaller adversarial perturbations. By optimizing towards our proposed evaluation criteria, we obtain new explanations that are loosely necessary and sufficient for a prediction. We further extend the explanation to extract the set of features that would move the current prediction to a target class by adopting targeted adversarial attack for the robustness analysis. Through experiments across multiple domains and a user study, we validate the usefulness of our evaluation criteria and our derived explanations.",/pdf/d6f38258eb28ef886c3aee4efb08046abbe76e5a.pdf,llm,https://scholar.google.com/scholar?q=Evaluations+and+Methods+for+Explanation+through+Robustness+Analysis
BREEDS: Benchmarks for Subpopulation Shift,2021,ICLR,"['Shibani Santurkar', 'Dimitris Tsipras', 'Aleksander Madry']",poster,"['benchmarks', 'distribution shift', 'hierarchy', 'robustness']","We develop a methodology for assessing the robustness of models to subpopulation shift---specifically, their ability to generalize to novel data subpopulations that were not observed during training. Our approach leverages the class structure underlying existing datasets to control the data subpopulations that comprise the training and test distributions. This enables us to synthesize realistic distribution shifts whose sources can be precisely controlled and characterized, within existing
large-scale datasets. Applying this methodology to the ImageNet dataset, we create a suite of subpopulation shift benchmarks of varying granularity. We then validate that the corresponding shifts are tractable by obtaining human baselines. Finally, we utilize these benchmarks to measure the sensitivity of standard model architectures as well as the effectiveness of existing train-time robustness interventions. ",/pdf/267e1b0387f6edaaa3b1145def1009b6803d55b6.pdf,llm,https://scholar.google.com/scholar?q=BREEDS:+Benchmarks+for+Subpopulation+Shift
Selectivity considered harmful: evaluating the causal impact of class selectivity in DNNs,2021,ICLR,"['Matthew L Leavitt', 'Ari S. Morcos']",poster,"['interpretability', 'explainability', 'empirical analysis', 'deep learning', 'selectivity']","The properties of individual neurons are often analyzed in order to understand the biological and artificial neural networks in which they're embedded. Class selectivity—typically defined as how different a neuron's responses are across different classes of stimuli or data samples—is commonly used for this purpose. However, it remains an open question whether it is necessary and/or sufficient for deep neural networks (DNNs) to learn class selectivity in individual units. We investigated the causal impact of class selectivity on network function by directly regularizing for or against class selectivity. Using this regularizer to reduce class selectivity across units in convolutional neural networks increased test accuracy by over 2% in ResNet18 and 1% in ResNet50 trained on Tiny ImageNet. For ResNet20 trained on CIFAR10 we could reduce class selectivity by a factor of 2.5 with no impact on test accuracy, and reduce it nearly to zero with only a small (~2%) drop in test accuracy. In contrast, regularizing to increase class selectivity significantly decreased test accuracy across all models and datasets. These results indicate that class selectivity in individual units is neither sufficient nor strictly necessary, and can even impair DNN performance. They also encourage caution when focusing on the properties of single units as representative of the mechanisms by which DNNs function.",/pdf/17afa91e20d55de3a216e07f59e5ab74d27027a6.pdf,llm,https://scholar.google.com/scholar?q=Selectivity+considered+harmful:+evaluating+the+causal+impact+of+class+selectivity+in+DNNs
Mapping the Timescale Organization of Neural Language Models,2021,ICLR,"['Hsiang-Yun Sherry Chien', 'Jinhan Zhang', 'Christopher Honey']",poster,"['natural language processing', 'LSTM', 'timescale', 'hierarchy', 'temporal context']","In the human brain, sequences of language input are processed within a distributed and hierarchical architecture, in which higher stages of processing encode contextual information over longer timescales. In contrast, in recurrent neural networks which perform natural language processing, we know little about how the multiple timescales of contextual information are functionally organized. Therefore, we applied tools developed in neuroscience to map the “processing timescales” of individual units within a word-level LSTM language model. This timescale-mapping method assigned long timescales to units previously found to track long-range syntactic dependencies. Additionally, the mapping revealed a small subset of the network (less than 15% of units) with long timescales and whose function had not previously been explored. We next probed the functional organization of the network by examining the relationship between the processing timescale of units and their network connectivity. We identified two classes of long-timescale units: “controller” units composed a densely interconnected subnetwork and strongly projected to the rest of the network, while “integrator” units showed the longest timescales in the network, and expressed projection profiles closer to the mean projection profile. Ablating integrator and controller units affected model performance at different positions within a sentence, suggesting distinctive functions of these two sets of units. Finally, we tested the generalization of these results to a character-level LSTM model and models with different architectures. In summary, we demonstrated a model-free technique for mapping the timescale organization in recurrent neural networks, and we applied this method to reveal the timescale and functional organization of neural language models",/pdf/13cd0af93335a87135e32f629677ce93f146f5bb.pdf,llm,https://scholar.google.com/scholar?q=Mapping+the+Timescale+Organization+of+Neural+Language+Models
Learning to live with Dale's principle: ANNs with separate excitatory and inhibitory units,2021,ICLR,"['Jonathan Cornford', 'Damjan Kalajdzievski', 'Marco Leite', 'Amélie Lamarquette', 'Dimitri Michael Kullmann', 'Blake Aaron Richards']",poster,[]," The units in artificial neural networks (ANNs) can be thought of as abstractions of biological neurons, and ANNs are increasingly used in neuroscience research. However, there are many important differences between ANN units and real neurons. One of the most notable is the absence of Dale's principle, which ensures that biological neurons are either exclusively excitatory or inhibitory. Dale's principle is typically left out of ANNs because its inclusion impairs learning. This is problematic, because one of the great advantages of ANNs for neuroscience research is their ability to learn complicated, realistic tasks. Here, by taking inspiration from feedforward inhibitory interneurons in the brain we show that we can develop ANNs with separate populations of excitatory and inhibitory units that learn just as well as standard ANNs. We call these networks Dale's ANNs (DANNs). We present two insights that enable DANNs to learn well: (1) DANNs are related to normalization schemes, and can be initialized such that the inhibition centres and standardizes the excitatory activity, (2) updates to inhibitory neuron parameters should be scaled using corrections based on the Fisher Information matrix. These results demonstrate how ANNs that respect Dale's principle can be built without sacrificing learning performance, which is important for future work using ANNs as models of the brain. The results may also have interesting implications for how inhibitory plasticity in the real brain operates.",/pdf/4ec70d1b966600fcb426c0ea0982e93dd870f226.pdf,llm,https://scholar.google.com/scholar?q=Learning+to+live+with+Dale's+principle:+ANNs+with+separate+excitatory+and+inhibitory+units
H-divergence: A Decision-Theoretic Probability Discrepancy Measure ,2021,ICLR,"['Shengjia Zhao', 'Abhishek Sinha', 'Yutong He', 'Aidan Perreault', 'Jiaming Song', 'Stefano Ermon']",poster,"['probability divergence', 'two sample test', 'maximum mean discrepancy']","Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. Based on ideas from decision theory, we investigate a new class of discrepancies that are based on the optimal decision loss. Two probability distributions are different if the optimal decision loss is higher on the mixture distribution than on each individual distribution. We show that this generalizes popular notions of discrepancy measurements such as the Jensen Shannon divergence and the maximum mean discrepancy. We apply our approach to two-sample tests, which evaluates whether two sets of samples come from the same distribution. On various benchmark and real datasets, we demonstrate that tests based on our generalized notion of discrepancy is able to achieve superior test power. We also apply our approach to sample quality evaluation as an alternative to the FID score, and to understanding the effects of climate change on different social and economic activities.",/pdf/29ba5a99028c5547427fa70d85282376463a9d0c.pdf,llm,https://scholar.google.com/scholar?q=H-divergence:+A+Decision-Theoretic+Probability+Discrepancy+Measure+
Gradient Starvation: A Learning Proclivity in Neural Networks,2021,NIPS,"['Mohammad Pezeshki', 'Sékou-Oumar Kaba', 'Yoshua Bengio', 'Aaron Courville', 'Doina Precup', 'Guillaume Lajoie']",poster,"['generalization', 'neural networks', 'dynamics', 'OOD']","We identify and formalize a fundamental gradient descent phenomenon resulting in a learning proclivity in over-parameterized neural networks. Gradient Starvation arises when cross-entropy loss is minimized by capturing only a subset of features relevant for the task, despite the presence of other predictive features that fail to be discovered. This work provides a theoretical explanation for the emergence of such feature imbalance in neural networks. Using tools from Dynamical Systems theory, we identify simple properties of learning dynamics during gradient descent that lead to this imbalance, and prove that such a situation can be expected given certain statistical structure in training data. Based on our proposed formalism, we develop guarantees for a novel regularization method aimed at decoupling feature learning dynamics, improving accuracy and robustness in cases hindered by gradient starvation. We illustrate our findings with simple and real-world out-of-distribution (OOD) generalization experiments.",https://api.openreview.net/pdf/21c50d2f86dc6f460965f26c29beb5681b6e510d.pdf,llm,https://scholar.google.com/scholar?q=Gradient+Starvation:+A+Learning+Proclivity+in+Neural+Networks
Uniform Concentration Bounds toward a Unified  Framework for Robust Clustering,2021,NIPS,"['Debolina Paul', 'Saptarshi Chakraborty', 'Swagatam Das', 'Jason Xu']",spotlight,"['k-means', 'median of means', 'metric entropy bounds', 'Bregman divergences', 'Iterative optimization']","Recent advances in center-based clustering continue to improve upon the drawbacks of Lloyd's celebrated $k$-means algorithm over $60$ years after its introduction. Various methods seek to address poor local minima, sensitivity to outliers, and data that are not well-suited to Euclidean measures of fit, but many are supported largely empirically. Moreover, combining such approaches in a piecemeal manner can result in ad hoc methods, and the limited theoretical results supporting each individual contribution may no longer hold. Toward addressing these issues in a principled way, this paper proposes a cohesive robust framework for center-based clustering under a general class of dissimilarity measures. In particular, we present a rigorous theoretical treatment within a Median-of-Means (MoM) estimation framework, showing that it subsumes several popular $k$-means variants. In addition to unifying existing methods, we derive uniform concentration bounds that complete their analyses, and bridge these results to the MoM framework via Dudley's chaining arguments. Importantly, we neither require any assumptions on the distribution of the outlying observations nor on the relative number of observations $n$ to features $p$. We establish strong consistency and an error rate of $O(n^{-1/2})$ under mild conditions, surpassing the best-known results in the literature. The methods are empirically validated thoroughly on real and synthetic datasets. ",https://api.openreview.net/pdf/453f82f5781b7ab06f4b1abdf295f913a9c0492a.pdf,llm,https://scholar.google.com/scholar?q=Uniform+Concentration+Bounds+toward+a+Unified++Framework+for+Robust+Clustering
Unadversarial Examples: Designing Objects for Robust Vision,2021,NIPS,"['Hadi Salman', 'Andrew Ilyas', 'Logan Engstrom', 'Sai Vemprala', 'Aleksander Madry', 'Ashish Kapoor']",poster,"['adversarial robustness', 'adversarial examples', 'computer vision']","We study a class of computer vision settings wherein one can modify the design of the objects being recognized. We develop a framework that leverages this capability---and deep networks' unusual sensitivity to input perturbations---to design ``robust objects,'' i.e., objects that are explicitly optimized to be confidently classified. Our framework yields improved performance on standard benchmarks, a simulated robotics environment, and physical-world experiments. ",https://api.openreview.net/pdf/1cc83c21b173d77ddd6cec1a946b38cd3219c62c.pdf,llm,https://scholar.google.com/scholar?q=Unadversarial+Examples:+Designing+Objects+for+Robust+Vision
Directed Spectrum Measures Improve Latent Network Models Of Neural Populations,2021,NIPS,"['Neil Gallagher', 'Kafui Dzirasa', 'David Carlson']",poster,"['functional brain networks', 'linear factor models', 'directed connectivity', 'directed spectrum']","Systems neuroscience aims to understand how networks of neurons distributed throughout the brain mediate computational tasks. One popular approach to identify those networks is to first calculate measures of neural activity (e.g. power spectra) from multiple brain regions, and then apply a linear factor model to those measures. Critically, despite the established role of directed communication between brain regions in neural computation, measures of directed communication have been rarely utilized in network estimation because they are incompatible with the implicit assumptions of the linear factor model approach. Here, we develop a novel spectral measure of directed communication called the Directed Spectrum (DS). We prove that it is compatible with the implicit assumptions of linear factor models, and we provide a method to estimate the DS. We demonstrate that latent linear factor models of DS measures better capture underlying brain networks in both simulated and real neural recording data compared to available alternatives. Thus, linear factor models of the Directed Spectrum offer neuroscientists a simple and effective way to explicitly model directed communication in networks of neural populations.",https://api.openreview.net/pdf/5d5942b8a57a1b4eca409476bdd28481b4329155.pdf,llm,https://scholar.google.com/scholar?q=Directed+Spectrum+Measures+Improve+Latent+Network+Models+Of+Neural+Populations
Instance-optimal Mean Estimation Under Differential Privacy,2021,NIPS,"['Ziyue Huang', 'Yuting Liang', 'Ke Yi']",poster,"['Mean estimation', 'differential privacy', 'instance optimality']","Mean estimation under differential privacy is a fundamental problem, but worst-case optimal mechanisms do not offer meaningful utility guarantees in practice when the global sensitivity is very large.  Instead, various heuristics have been proposed to reduce the error on real-world data that do not resemble the worst-case instance.  This paper takes a principled approach, yielding a mechanism that is instance-optimal in a strong sense.  In addition to its theoretical optimality, the mechanism is also simple and practical, and adapts to a variety of data characteristics without the need of parameter tuning.  It easily extends to the local and shuffle model as well.",https://api.openreview.net/pdf/3552edcb4c56eb621531e15c8bb69b6cb280f64a.pdf,llm,https://scholar.google.com/scholar?q=Instance-optimal+Mean+Estimation+Under+Differential+Privacy
Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence,2021,NIPS,"['Antoine Labatie', 'Dominic Masters', 'Zach Eaton-Rosen', 'Carlo Luschi']",poster,"['Batch-Independent Normalization', 'Batch Normalization', 'Layer Normalization', 'Group Normalization', 'Instance Normalization', 'Deep Learning Theory', 'CNNs', 'ResNets', 'ResNeXts', 'EfficientNets']","We investigate the reasons for the performance degradation incurred with batch-independent normalization. We find that the prototypical techniques of layer normalization and instance normalization both induce the appearance of failure modes in the neural network's pre-activations: (i) layer normalization induces a collapse towards channel-wise constant functions; (ii) instance normalization induces a lack of variability in instance statistics, symptomatic of an alteration of the expressivity. To alleviate failure mode (i) without aggravating failure mode (ii), we introduce the technique ""Proxy Normalization"" that normalizes post-activations using a proxy distribution. When combined with layer normalization or group normalization, this batch-independent normalization emulates batch normalization's behavior and consistently matches or exceeds its performance.",https://api.openreview.net/pdf/7261daa0c245e94148ba4496f7ae3a06a7457b80.pdf,llm,https://scholar.google.com/scholar?q=Proxy-Normalizing+Activations+to+Match+Batch+Normalization+while+Removing+Batch+Dependence
Adversarial Neuron Pruning Purifies Backdoored Deep Models,2021,NIPS,"['Dongxian Wu', 'Yisen Wang']",poster,"['backdoor attacks', 'backdoor defense', 'AI security', 'trustworthy machine learning']","As deep neural networks (DNNs) are growing larger, their requirements for computational resources become huge, which makes outsourcing training more popular. Training in a third-party platform, however, may introduce potential risks that a malicious trainer will return backdoored DNNs, which behave normally on clean samples but output targeted misclassifications whenever a trigger appears at the test time. Without any knowledge of the trigger, it is difficult to distinguish or recover benign DNNs from backdoored ones. In this paper, we first identify an unexpected sensitivity of backdoored DNNs, that is, they are much easier to collapse and tend to predict the target label on clean samples when their neurons are adversarially perturbed. Based on these observations, we propose a novel model repairing method, termed Adversarial Neuron Pruning (ANP), which prunes some sensitive neurons to purify the injected backdoor. Experiments show, even with only an extremely small amount of clean data (e.g., 1%), ANP effectively removes the injected backdoor without causing obvious performance degradation.",https://api.openreview.net/pdf/d24a110cbbd5437697488a5e5906b77e31f725b9.pdf,llm,https://scholar.google.com/scholar?q=Adversarial+Neuron+Pruning+Purifies+Backdoored+Deep+Models
An Analysis of Constant Step Size SGD in the Non-convex Regime: Asymptotic Normality and Bias,2021,NIPS,"['Lu Yu', 'Krishna Balasubramanian', 'Stanislav Volgushev', 'Murat A Erdogdu']",poster,"['Constant Step Size SGD', 'Non-convex optimization', 'Polyak-Ruppert averaging', 'CLT']"," Structured non-convex learning problems, for which critical points have favorable statistical properties, arise frequently in statistical machine learning. Algorithmic convergence and statistical estimation rates are well-understood for such problems. However, quantifying the uncertainty associated with the underlying training algorithm is not well-studied in the non-convex setting. In order to address this shortcoming, in this work, we establish an asymptotic normality result for the constant step size stochastic gradient descent (SGD)  algorithm---a widely used algorithm in practice. Specifically, based on the relationship between SGD and Markov Chains  [DDB19], we show that the average of SGD iterates is asymptotically normally distributed around the expected value of their unique invariant distribution, as long as the non-convex and non-smooth objective function satisfies a dissipativity property. We also characterize the bias between this expected value and the critical points of the objective function under various local regularity conditions. Together, the above two results could be leveraged to construct confidence intervals for non-convex problems that are trained using the SGD algorithm.",https://api.openreview.net/pdf/ba96ed2cb9ba54ae274bb1c0f3ef7be07c8a2a8f.pdf,llm,https://scholar.google.com/scholar?q=An+Analysis+of+Constant+Step+Size+SGD+in+the+Non-convex+Regime:+Asymptotic+Normality+and+Bias
Remember What You Want to Forget: Algorithms for Machine Unlearning,2021,NIPS,"['Ayush Sekhari', 'Jayadev Acharya', 'Gautam Kamath', 'Ananda Theertha Suresh']",poster,"['Machine unlearning', 'theory', 'differential privacy', 'generalization guarantee', 'right to be forgotten']","We study the problem of unlearning datapoints from a learnt model. The learner first receives a dataset $S$ drawn i.i.d. from an unknown distribution, and outputs a model $\widehat{w}$ that performs well on  unseen samples from the same distribution. However, at some point in the future, any training datapoint $z \in S$ can request to be unlearned, thus prompting the learner to modify its output model while still ensuring the same accuracy guarantees.  We initiate a rigorous study of generalization in machine unlearning, where the goal is to perform well on previously unseen datapoints. Our focus is on both computational and storage complexity. 

For the setting of convex losses, we provide an unlearning algorithm that can unlearn up to $O(n/d^{1/4})$ samples, where $d$ is the problem dimension. In comparison, in general, differentially private learning (which implies unlearning) only guarantees deletion of $O(n/d^{1/2})$ samples. This demonstrates a novel separation between differential privacy and machine unlearning. ",https://api.openreview.net/pdf/9ffa59f7e3575ede5ea770a7fcc42de14ca52b44.pdf,llm,https://scholar.google.com/scholar?q=Remember+What+You+Want+to+Forget:+Algorithms+for+Machine+Unlearning
Learning Robust Hierarchical Patterns of Human Brain across Many fMRI Studies,2021,NIPS,"['Dushyant Sahoo', 'Christos Davatzikos']",poster,"['Hierarchical Latent Factor Modeling', 'Matrix Factorization', 'Domain Adaptation', 'fMRI analysis']","Multi-site fMRI studies face the challenge that the pooling introduces systematic non-biological site-specific variance due to hardware, software, and environment. In this paper, we propose to reduce site-specific variance in the estimation of hierarchical Sparsity Connectivity Patterns (hSCPs) in fMRI data via a simple yet effective matrix factorization while preserving biologically relevant variations. Our method leverages unsupervised adversarial learning to improve the reproducibility of the components. Experiments on simulated datasets display that the proposed method can estimate components with higher accuracy and reproducibility, while preserving age-related variation on a multi-center clinical data set. ",https://api.openreview.net/pdf/228f3c1726ff36275eb6606f0cd35966b4ceaad4.pdf,llm,https://scholar.google.com/scholar?q=Learning+Robust+Hierarchical+Patterns+of+Human+Brain+across+Many+fMRI+Studies
Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos,2022,CVPR,Muheng Li;Lei Chen;Yueqi Duan;Zhilan Hu;Jianjiang Feng;Jie Zhou;Jiwen Lu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Bridge-Prompt_Towards_Ordinal_Action_Understanding_in_Instructional_Videos_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Bridge-Prompt:+Towards+Ordinal+Action+Understanding+in+Instructional+Videos
Prompt Distribution Learning,2022,CVPR,Yuning Lu;Jianzhuang Liu;Yonggang Zhang;Yajing Liu;Xinmei Tian,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Prompt+Distribution+Learning
Learning To Prompt for Continual Learning,2022,CVPR,Zifeng Wang;Zizhao Zhang;Chen-Yu Lee;Han Zhang;Ruoxi Sun;Xiaoqi Ren;Guolong Su;Vincent Perot;Jennifer Dy;Tomas Pfister,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Learning_To_Prompt_for_Continual_Learning_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Learning+To+Prompt+for+Continual+Learning
Align and Prompt: Video-and-Language Pre-Training With Entity Prompts,2022,CVPR,Dongxu Li;Junnan Li;Hongdong Li;Juan Carlos Niebles;Steven C.H. Hoi,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Align_and_Prompt_Video-and-Language_Pre-Training_With_Entity_Prompts_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Align+and+Prompt:+Video-and-Language+Pre-Training+With+Entity+Prompts
Learning To Recognize Procedural Activities With Distant Supervision,2022,CVPR,Xudong Lin;Fabio Petroni;Gedas Bertasius;Marcus Rohrbach;Shih-Fu Chang;Lorenzo Torresani,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_Learning_To_Recognize_Procedural_Activities_With_Distant_Supervision_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Learning+To+Recognize+Procedural+Activities+With+Distant+Supervision
Learning To Prompt for Open-Vocabulary Object Detection With Vision-Language Model,2022,CVPR,Yu Du;Fangyun Wei;Zihe Zhang;Miaojing Shi;Yue Gao;Guoqi Li,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Du_Learning_To_Prompt_for_Open-Vocabulary_Object_Detection_With_Vision-Language_Model_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Learning+To+Prompt+for+Open-Vocabulary+Object+Detection+With+Vision-Language+Model
UnweaveNet: Unweaving Activity Stories,2022,CVPR,Will Price;Carl Vondrick;Dima Damen,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Price_UnweaveNet_Unweaving_Activity_Stories_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=UnweaveNet:+Unweaving+Activity+Stories
"A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes",2022,CVPR,Mazda Moayeri;Phillip Pope;Yogesh Balaji;Soheil Feizi,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Moayeri_A_Comprehensive_Study_of_Image_Classification_Model_Sensitivity_to_Foregrounds_CVPR_2022_paper.pdf,llm,"https://scholar.google.com/scholar?q=A+Comprehensive+Study+of+Image+Classification+Model+Sensitivity+to+Foregrounds,+Backgrounds,+and+Visual+Attributes"
Dual-AI: Dual-Path Actor Interaction Learning for Group Activity Recognition,2022,CVPR,Mingfei Han;David Junhao Zhang;Yali Wang;Rui Yan;Lina Yao;Xiaojun Chang;Yu Qiao,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Dual-AI_Dual-Path_Actor_Interaction_Learning_for_Group_Activity_Recognition_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Dual-AI:+Dual-Path+Actor+Interaction+Learning+for+Group+Activity+Recognition
Conditional Prompt Learning for Vision-Language Models,2022,CVPR,Kaiyang Zhou;Jingkang Yang;Chen Change Loy;Ziwei Liu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Conditional+Prompt+Learning+for+Vision-Language+Models
Pyramid Adversarial Training Improves ViT Performance,2022,CVPR,Charles Herrmann;Kyle Sargent;Lu Jiang;Ramin Zabih;Huiwen Chang;Ce Liu;Dilip Krishnan;Deqing Sun,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Herrmann_Pyramid_Adversarial_Training_Improves_ViT_Performance_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=Pyramid+Adversarial+Training+Improves+ViT+Performance
DenseCLIP: Language-Guided Dense Prediction With Context-Aware Prompting,2022,CVPR,Yongming Rao;Wenliang Zhao;Guangyi Chen;Yansong Tang;Zheng Zhu;Guan Huang;Jie Zhou;Jiwen Lu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.pdf,llm,https://scholar.google.com/scholar?q=DenseCLIP:+Language-Guided+Dense+Prediction+With+Context-Aware+Prompting
"JRDB-Act: A Large-Scale Dataset for Spatio-Temporal Action, Social Group and Activity Detection",2022,CVPR,Mahsa Ehsanpour;Fatemeh Saleh;Silvio Savarese;Ian Reid;Hamid Rezatofighi,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Ehsanpour_JRDB-Act_A_Large-Scale_Dataset_for_Spatio-Temporal_Action_Social_Group_and_CVPR_2022_paper.pdf,llm,"https://scholar.google.com/scholar?q=JRDB-Act:+A+Large-Scale+Dataset+for+Spatio-Temporal+Action,+Social+Group+and+Activity+Detection"
Causal Prediction Can Induce Performative Stability,2022,ICML,['Bogdan Kulynych'],poster,"['performative prediction', 'causal features']","Predictive models affect the world through inducing a strategic response or reshaping the environment in which they are deployed---a property called performativity. This results in the need to constantly adapt and re-design the model. We formalize one possible mechanism through which performativity can arise using the language of causal modeling. We show that using features which form a Markov blanket of the target variable for prediction closes the feedback loop in this setting. Thus, a predictive model that takes as input such causal features might not require any further adaptation after deployment even if it changes the environment.",https://api.openreview.net/pdf/1a0415d75c8525c31b22306ebc9d5c87a0df977b.pdf,llm,https://scholar.google.com/scholar?q=Causal+Prediction+Can+Induce+Performative+Stability
From Kepler to Newton: Explainable AI for Science Discovery,2022,ICML,"['Zelong Li', 'Jianchao Ji', 'Yongfeng Zhang']",poster,"['Explainable AI', 'Science Discovery', 'Scientific Method', 'Technological Singularity', 'Human and Nature']","The Observation --- Hypothesis --- Prediction --- Experimentation loop paradigm for scientific research has been practiced by researchers for years towards scientific discoveries. However, with data explosion in both mega-scale and milli-scale research, it has been sometimes very difficult to manually analyze the data and propose new hypotheses to drive the cycle for scientific discovery.
In this paper, we discuss the role of Explainable AI in scientific discovery process by demonstrating an Explainable AI-based paradigm for science discovery. The key is to use Explainable AI to help derive data or model interpretations, hypotheses, as well as scientific discoveries or insights. We show how computational and data-intensive methodology---together with experimental and theoretical methodology---can be seamlessly integrated for scientific research. To demonstrate the AI-based science discovery process, and to pay our respect to some of the greatest minds in human history, we show how Kepler's laws of planetary motion and Newton's law of universal gravitation can be rediscovered by (Explainable) AI based on Tycho Brahe's astronomical observation data, whose works were leading the scientific revolution in the 16-17th century. This work also highlights the important role of Explainable AI (as compared to Blackbox AI) in science discovery to help humans prevent or better prepare for the possible technological singularity that may happen in the future, since science is not only about the know how, but also the know why.",https://api.openreview.net/pdf/ab0ae14763754c5ae34a50a68f1303185d8ed8a9.pdf,llm,https://scholar.google.com/scholar?q=From+Kepler+to+Newton:+Explainable+AI+for+Science+Discovery
Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization,2022,NIPS,"['Zijie Zhang', 'Yang Zhou', 'Xin Zhao', 'Tianshi Che', 'Lingjuan Lyu']",poster,"['Certified machine unlearning', 'randomized gradient smoothing', 'gradient quantization', 'theoretical guarantee', 'prompt unlearning']","The right to be forgotten calls for efficient machine unlearning techniques that make trained machine learning models forget a cohort of data. The combination of training and unlearning operations in traditional machine unlearning methods often leads to the expensive computational cost on large-scale data. This paper presents a prompt certified machine unlearning algorithm, PCMU, which executes one-time operation of simultaneous training and unlearning in advance for a series of machine unlearning requests, without the knowledge of the removed/forgotten data. First, we establish a connection between randomized smoothing for certified robustness on classification and randomized smoothing for certified machine unlearning on gradient quantization. Second, we propose a prompt certified machine unlearning model based on randomized data smoothing and gradient quantization. We theoretically derive the certified radius R regarding the data change before and after data removals and the certified budget of data removals about R. Last but not least, we present another practical framework of randomized gradient smoothing and quantization, due to the dilemma of producing high confidence certificates in the first framework. We theoretically demonstrate the certified radius R' regarding the gradient change, the correlation between two types of certified radii, and the certified budget of data removals about R'. ",https://api.openreview.net/pdf/1bb465dd4bd4e92ca20c6267bf92d11fa9b835aa.pdf,llm,https://scholar.google.com/scholar?q=Prompt+Certified+Machine+Unlearning+with+Randomized+Gradient+Smoothing+and+Quantization
Autoformalization with Large Language Models,2022,NIPS,"['Yuhuai Wu', 'Albert Qiaochu Jiang', 'Wenda Li', 'Markus Norman Rabe', 'Charles E Staats', 'Mateja Jamnik', 'Christian Szegedy']",poster,"['Large language models', 'Autoformalization', 'Formal Math', 'miniF2F.']","Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence.
While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from~$29.6\%$ to~$35.2\%$.",https://api.openreview.net/pdf/51c4ad17de2b76b8e59b8d486a53165e6c650a4e.pdf,llm,https://scholar.google.com/scholar?q=Autoformalization+with+Large+Language+Models
Fine-tuning language models to find agreement among humans with diverse preferences,2022,NIPS,"['Michiel A. Bakker', 'Martin J Chadwick', 'Hannah Sheahan', 'Michael Henry Tessler', 'Lucy Campbell-Gillingham', 'Jan Balaguer', 'Nat McAleese', 'Amelia Glaese', 'John Aslanides', 'Matthew Botvinick', 'Christopher Summerfield']",poster,"['large language models', 'LLMs', 'alignment', 'NLP', 'fine-tuning', 'reward modelling', 'preference modelling', 'human-centered AI']","Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a single ""generic"" user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g., ""should we raise taxes on the rich?""), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs ($>70\%$) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions ($>65\%$). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another.",https://api.openreview.net/pdf/fde9d1ed13707b7018f8ffa9dcf20ea356556b21.pdf,llm,https://scholar.google.com/scholar?q=Fine-tuning+language+models+to+find+agreement+among+humans+with+diverse+preferences
SoftPatch: Unsupervised Anomaly Detection with Noisy Data,2022,NIPS,"['Jiang Xi', 'Jianlin Liu', 'Jinbao Wang', 'Qiang Nie', 'Kai WU', 'Yong Liu', 'Chengjie Wang', 'Feng Zheng']",poster,"['Anomaly Detection', 'Noisy Label', 'Outlier Detection']","Although mainstream unsupervised anomaly detection (AD) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data. Training with noisy data is an inevitable problem in real-world anomaly detection but is seldom discussed. This paper considers label-level noise in image sensory anomaly detection for the first time. To solve this problem, we proposed a memory-based unsupervised AD method, SoftPatch, which efficiently denoises the data at the patch level. Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction. The scores are then stored in the memory bank to soften the anomaly detection boundary. Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset. Comprehensive experiments in various noise scenes demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the MVTecAD and BTAD benchmarks and is comparable to those methods under the setting without noise.",https://api.openreview.net/pdf/c9ac87f209016e331b8a7096676670bc741354b3.pdf,llm,https://scholar.google.com/scholar?q=SoftPatch:+Unsupervised+Anomaly+Detection+with+Noisy+Data
Counterfactual Neural Temporal Point Process for Estimating Causal Influence of Misinformation on Social Media,2022,NIPS,"['Yizhou Zhang', 'Defu Cao', 'Yan Liu']",poster,"['Causal Inference', 'Temporal Point Process', 'Misinformation Influence', 'Counterfactual Analysis', 'Fake News', 'Social Media', 'Deep Learning']","Recent years have witnessed the rise of misinformation campaigns that spread specific narratives on social media to manipulate public opinions on different areas, such as politics and healthcare. Consequently, an effective and efficient automatic methodology to estimate the influence of the misinformation on user beliefs and activities is needed. However, existing works on misinformation impact estimation either rely on small-scale psychological experiments or can only discover the correlation between user behaviour and misinformation. To address these issues, in this paper, we build up a causal framework that model the causal effect of misinformation from the perspective of temporal point process. To adapt the large-scale data, we design an efficient yet precise way to estimate the \textbf{Individual Treatment Effect} (ITE) via neural temporal point process and gaussian mixture models. Extensive experiments on synthetic dataset verify the effectiveness and efficiency of our model. We further apply our model on a real-world dataset of social media posts and engagements about COVID-19 vaccines. The experimental results indicate that our model recognized identifiable causal effect of misinformation that hurts people's subjective emotions toward the vaccines.",https://api.openreview.net/pdf/e5410e2b83f80dd34e01e8d79d6deeafeff11af4.pdf,llm,https://scholar.google.com/scholar?q=Counterfactual+Neural+Temporal+Point+Process+for+Estimating+Causal+Influence+of+Misinformation+on+Social+Media
On Scrambling Phenomena for Randomly Initialized Recurrent Networks ,2022,NIPS,"['Vaggos Chatziafratis', 'Ioannis Panageas', 'Clayton Sanford', 'Stelios Andrew Stavroulakis']",poster,"['RNNs', 'Scrambling', 'Jacobian', 'dynamical systems', 'trajectories', 'chaos', 'initialization', 'recurrent networks']","Recurrent Neural Networks (RNNs) frequently exhibit complicated dynamics, and their sensitivity to the initialization process often renders them notoriously hard to train. Recent works have shed light on such phenomena analyzing when exploding or vanishing gradients may occur, either of which is detrimental for training dynamics. In this paper, we point to a formal connection between RNNs and chaotic dynamical systems and prove a qualitatively stronger phenomenon about RNNs than what exploding gradients seem to suggest. Our main result proves that under standard initialization (e.g., He, Xavier etc.), RNNs will exhibit \textit{Li-Yorke chaos} with \textit{constant} probability \textit{independent} of the network's width. This explains the experimentally observed phenomenon of \textit{scrambling}, under which trajectories of nearby points may appear to be arbitrarily close during some timesteps, yet will be far away in future timesteps. In stark contrast to their feedforward counterparts, we show that chaotic behavior in RNNs is preserved under small perturbations and that their expressive power remains exponential in the number of feedback iterations. Our technical arguments rely on viewing RNNs as random walks under non-linear activations, and studying the existence of certain types of higher-order fixed points called \textit{periodic points} in order to establish phase transitions from order to chaos.",https://api.openreview.net/pdf/5bb1c9f65a31c1e072674bdc912e15b18b40b686.pdf,llm,https://scholar.google.com/scholar?q=On+Scrambling+Phenomena+for+Randomly+Initialized+Recurrent+Networks+
MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields,2022,NIPS,"['Ilyes Batatia', 'David Peter Kovacs', 'Gregor N. C. Simm', 'Christoph Ortner', 'Gabor Csanyi']",poster,"['GNN', 'graph neural network', 'equivariance', 'higher order', 'message passing neural network', 'point clouds', 'molecules']","Creating fast and accurate force fields is a long-standing challenge in computational chemistry and materials science. Recently, Equivariant Message Passing Neural Networks (MPNNs) have emerged as a powerful tool for building machine learning interatomic potentials, outperforming other approaches in terms of accuracy. However, they suffer from high computational cost and poor scalability. Moreover, most MPNNs only pass two-body messages leading to an intricate relationship between the number of layers and the expressivity of the features. This work introduces MACE, a new equivariant MPNN model that uses higher order messages, and demonstrates that this leads to an improved learning law. We show that by using four-body messages, the required number of message passing iterations reduces to just one, resulting in a fast and highly parallelizable model, reaching or exceeding state of the art accuracy on the rMD17 and 3BPA benchmark tasks. Our implementation is available at https://github.com/ACEsuit/mace.",https://api.openreview.net/pdf/a1ade50912bdaaf92de0d04429aead357c05975c.pdf,llm,https://scholar.google.com/scholar?q=MACE:+Higher+Order+Equivariant+Message+Passing+Neural+Networks+for+Fast+and+Accurate+Force+Fields
Single-phase deep learning in cortico-cortical networks,2022,NIPS,"['Will Greedy', 'Heng Wei Zhu', 'Joseph Oliver Pemberton', 'Jack Mellor', 'Rui Ponte Costa']",poster,"['cortical microcircuits', 'deep learning', 'synaptic plasticity', 'biologically plausible learning', 'neuroscience']","The error-backpropagation (backprop) algorithm remains the most common solution to the credit assignment problem in artificial neural networks. In neuroscience, it is unclear whether the brain could adopt a similar strategy to correctly modify its synapses. Recent models have attempted to bridge this gap while being consistent with a range of experimental observations. However, these models are either unable to effectively backpropagate error signals across multiple layers or require a multi-phase learning process, neither of which are reminiscent of learning in the brain. Here, we introduce a new model, Bursting Cortico-Cortical Networks (BurstCCN), which solves these issues by integrating known properties of cortical networks namely bursting activity, short-term plasticity (STP) and dendrite-targeting interneurons. BurstCCN relies on burst multiplexing via connection-type-specific STP to propagate backprop-like error signals within deep cortical networks. These error signals are encoded at distal dendrites and induce burst-dependent plasticity as a result of excitatory-inhibitory top-down inputs. First, we demonstrate that our model can effectively backpropagate errors through multiple layers using a single-phase learning process. Next, we show both empirically and analytically that learning in our model approximates backprop-derived gradients. Finally, we demonstrate that our model is capable of learning complex image classification tasks (MNIST and CIFAR-10). Overall, our results suggest that cortical features across sub-cellular, cellular, microcircuit and systems levels jointly underlie single-phase efficient deep learning in the brain.",https://api.openreview.net/pdf/38d05ea1f460e815411925394853516bb2feed62.pdf,llm,https://scholar.google.com/scholar?q=Single-phase+deep+learning+in+cortico-cortical+networks
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,2022,NIPS,"['Jason Wei', 'Xuezhi Wang', 'Dale Schuurmans', 'Maarten Bosma', 'brian ichter', 'Fei Xia', 'Ed H. Chi', 'Quoc V Le', 'Denny Zhou']",poster,"['Language models', 'natural language processing', 'reasoning']","We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",https://api.openreview.net/pdf/69e0946271fd64932a087332874bd380fd2152a3.pdf,llm,https://scholar.google.com/scholar?q=Chain-of-Thought+Prompting+Elicits+Reasoning+in+Large+Language+Models
Momentum Aggregation for Private Non-convex ERM,2022,NIPS,"['Hoang Tran', 'Ashok Cutkosky']",poster,"['differential privacy', 'momentum', 'non-convex optimization', 'ERM', 'tree-aggregation']","We introduce new algorithms and convergence guarantees for privacy-preserving non-convex Empirical Risk Minimization (ERM) on smooth $d$-dimensional objectives. We develop an improved sensitivity analysis of stochastic gradient descent on smooth objectives that exploits the recurrence of examples in different epochs. By combining this new approach with recent analysis of momentum with private aggregation techniques, we provide an $(\epsilon,\delta)$-differential private algorithm that finds a gradient of norm $O\left(\frac{d^{1/3}}{(\epsilon N)^{2/3}}\right)$ in $O\left(\frac{N^{7/3}\epsilon^{4/3}}{d^{2/3}}\right)$ gradient evaluations, improving the previous best gradient bound of $\tilde O\left(\frac{d^{1/4}}{\sqrt{\epsilon N}}\right)$.",https://api.openreview.net/pdf/cdecfff58c3dd8e81d2b8f6abcb014b9f90df323.pdf,llm,https://scholar.google.com/scholar?q=Momentum+Aggregation+for+Private+Non-convex+ERM
Memorization Without Overfitting:  Analyzing the Training Dynamics of Large Language Models,2022,NIPS,"['Kushal Tirumala', 'Aram H. Markosyan', 'Luke Zettlemoyer', 'Armen Aghajanyan']",poster,[],"Despite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples. Together, these findings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.",https://api.openreview.net/pdf/759ba7b66f25eb8aceac5ae59a4cfe2b76b2abca.pdf,llm,https://scholar.google.com/scholar?q=Memorization+Without+Overfitting:++Analyzing+the+Training+Dynamics+of+Large+Language+Models
Posterior and Computational Uncertainty in Gaussian Processes,2022,NIPS,"['Jonathan Wenger', 'Geoff Pleiss', 'Marvin Pförtner', 'Philipp Hennig', 'John Patrick Cunningham']",poster,"['Gaussian processes', 'computational uncertainty', 'numerical methods', 'probabilistic numerics', 'probabilistic linear solvers']","Gaussian processes scale prohibitively with the size of the dataset. In response, many approximation methods have been developed, which inevitably introduce approximation error. This additional source of uncertainty, due to limited computation, is entirely ignored when using the approximate posterior. Therefore in practice, GP models are often as much about the approximation method as they are about the data. Here, we develop a new class of methods that provides consistent estimation of the combined uncertainty arising from both the finite number of data observed and the finite amount of computation expended. The most common GP approximations map to an instance in this class, such as methods based on the Cholesky factorization, conjugate gradients, and inducing points. For any method in this class, we prove (i) convergence of its posterior mean in the associated RKHS, (ii) decomposability of its combined posterior covariance into mathematical and computational covariances, and (iii) that the combined variance is a tight worst-case bound for the squared error between the method's posterior mean and the latent function. Finally, we empirically demonstrate the consequences of ignoring computational uncertainty and show how implicitly modeling it improves generalization performance on benchmark datasets.",https://api.openreview.net/pdf/8fb82faf34781812b11a6a533384c837d44552a4.pdf,llm,https://scholar.google.com/scholar?q=Posterior+and+Computational+Uncertainty+in+Gaussian+Processes
Discrete-Convex-Analysis-Based Framework for Warm-Starting Algorithms with Predictions,2022,NIPS,"['Shinsaku Sakaue', 'Taihei Oki']",poster,"['combinatorial optimization', 'discrete convex analysis', 'algorithms with predictions', 'time complexity']","Augmenting algorithms with learned predictions is a promising approach for going beyond worst-case bounds. Dinitz, Im, Lavastida, Moseley, and Vassilvitskii~(2021) have demonstrated that warm-starts with learned dual solutions can improve the time complexity of the Hungarian method for weighted perfect bipartite matching. We extend and improve their framework in a principled manner via \textit{discrete convex analysis} (DCA), a discrete analog of convex analysis. We show the usefulness of our DCA-based framework by applying it to weighted perfect bipartite matching, weighted matroid intersection, and discrete energy minimization for computer vision. Our DCA-based framework yields time complexity bounds that depend on the $\ell_\infty$-distance from a predicted solution to an optimal solution, which has two advantages relative to the previous $\ell_1$-distance-dependent bounds: time complexity bounds are smaller, and learning of predictions is more sample efficient. We also discuss whether to learn primal or dual solutions from the DCA perspective.",https://api.openreview.net/pdf/3a2f50d639d1448c94a2940d154c4d4e9240851e.pdf,llm,https://scholar.google.com/scholar?q=Discrete-Convex-Analysis-Based+Framework+for+Warm-Starting+Algorithms+with+Predictions
Comparing Distributions by Measuring Differences that Affect Decision Making,2022,ICLR,"['Shengjia Zhao', 'Abhishek Sinha', 'Yutong He', 'Aidan Perreault', 'Jiaming Song', 'Stefano Ermon']",oral,"['probability divergence', 'two sample test', 'generative model']","Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task -- two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution. By suitably choosing the decision task, this generalizes the Jensen-Shannon divergence and the maximum mean discrepancy family. We apply our approach to two-sample tests, and on various benchmarks, we achieve superior test power compared to competing methods. In addition, a modeler can directly specify their preferences when comparing distributions through the decision loss. We apply this property to understanding the effects of climate change on different social and economic activities, evaluating sample quality, and selecting features targeting different decision tasks.",https://api.openreview.net/pdf/e99719a7a6796b569cc6afdf6f42024d0df2fbea.pdf,llm,https://scholar.google.com/scholar?q=Comparing+Distributions+by+Measuring+Differences+that+Affect+Decision+Making
VC dimension of partially quantized neural networks in the overparametrized regime,2022,ICLR,"['Yutong Wang', 'Clayton Scott']",poster,"['VC dimension', 'quantized neural networks', 'classification', 'minimax theory', 'overparametrization']","Vapnik-Chervonenkis (VC) theory has so far been unable to explain the small generalization error of overparametrized neural networks. Indeed, existing applications of VC theory to large networks obtain upper bounds on VC dimension that are proportional to the number of weights, and for a large class of networks, these upper bound are known to be tight. In this work, we focus on a class of partially quantized networks that we refer to as hyperplane arrangement neural networks (HANNs). Using a sample compression analysis, we show that HANNs can have VC dimension significantly smaller than the number of weights, while being highly expressive. In particular, empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classification with Lipschitz posterior class probability. We further demonstrate the expressivity of HANNs empirically. On a panel of 121 UCI datasets, overparametrized HANNs are able to match the performance of state-of-the-art full-precision models.",https://api.openreview.net/pdf/9760187606b3496a5f4a0fe752a22416bb4a2e21.pdf,llm,https://scholar.google.com/scholar?q=VC+dimension+of+partially+quantized+neural+networks+in+the+overparametrized+regime
The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks,2022,ICLR,"['Rahim Entezari', 'Hanie Sedghi', 'Olga Saukh', 'Behnam Neyshabur']",poster,"['Permutation', 'Invariance', 'Mode Connectivity', 'Energy Barrier', 'Loss landscape', 'Deep Learning']","In this paper, we conjecture that if the permutation invariance of neural networks is taken into account, SGD solutions will likely have no barrier in the linear interpolation between them. Although it is a bold conjecture, we show how extensive empirical attempts fall short of refuting it. We further provide a preliminary theoretical result to support our conjecture. Our conjecture has implications for the lottery ticket hypothesis, distributed training, and ensemble methods. The source code is available at \url{https://github.com/rahimentezari/PermutationInvariance}.",https://api.openreview.net/pdf/a575dfe6923ea65c17895fd63d13fc299132536f.pdf,llm,https://scholar.google.com/scholar?q=The+Role+of+Permutation+Invariance+in+Linear+Mode+Connectivity+of+Neural+Networks
Non-Linear Operator Approximations for Initial Value Problems,2022,ICLR,"['Gaurav Gupta', 'Xiongye Xiao', 'Radu Balan', 'Paul Bogdan']",poster,"['exponential operators', 'initial value problem', 'pade approximation', 'multiwavelets', 'partial differential equations']","Time-evolution of partial differential equations is the key to model several dynamical processes, events forecasting but the operators associated with such problems are non-linear. We propose a Padé approximation based exponential neural operator scheme for efficiently learning the map between a given initial condition and activities at a later time. The multiwavelets bases are used for space discretization. By explicitly embedding the exponential operators in the model, we reduce the training parameters and make it more data-efficient which is essential in dealing with scarce real-world datasets. The Padé exponential operator uses a $\textit{recurrent structure with shared parameters}$ to model the non-linearity compared to recent neural operators that rely on using multiple linear operator layers in succession. We show theoretically that the gradients associated with the recurrent Padé network are bounded across the recurrent horizon. We perform experiments on non-linear systems such as Korteweg-de Vries (KdV) and Kuramoto–Sivashinsky (KS) equations to show that the proposed approach achieves the best performance and at the same time is data-efficient. We also show that urgent real-world problems like Epidemic forecasting (for example, COVID-19) can be formulated as a 2D time-varying operator problem. The proposed Padé exponential operators yield better prediction results ($\textbf{53\%} (\textbf{52\%})$ better MAE than best neural operator (non-neural operator deep learning model)) compared to state-of-the-art forecasting models.",https://api.openreview.net/pdf/64034193e19bb69648d9e3ed9301e225d08d08fe.pdf,llm,https://scholar.google.com/scholar?q=Non-Linear+Operator+Approximations+for+Initial+Value+Problems
On the relation between statistical learning and perceptual distances,2022,ICLR,"['Alexander Hepburn', 'Valero Laparra', 'Raul Santos-Rodriguez', 'Johannes Ballé', 'Jesus Malo']",spotlight,[],"It has been demonstrated many times that the behavior of the human visual system is connected to the statistics of natural images. Since machine learning relies on the statistics of training data as well, the above connection has interesting implications when using perceptual distances (which mimic the behavior of the human visual system) as a loss function. In this paper, we aim to unravel the non-trivial relationships between the probability distribution of the data, perceptual distances, and unsupervised machine learning. To this end, we show that perceptual sensitivity is correlated with the probability of an image in its close neighborhood. We also explore the relation between distances induced by autoencoders and the probability distribution of the training data, as well as how these induced distances are correlated with human perception. Finally, we find perceptual distances do not always lead to noticeable gains in performance over Euclidean distance in common image processing tasks, except when data is scarce and the perceptual distance provides regularization. We propose this may be due to a double-counting effect of the image statistics, once in the perceptual distance and once in the training procedure.",https://api.openreview.net/pdf/12c717193deff83fed4cdbbc207d6c4ffebad63e.pdf,llm,https://scholar.google.com/scholar?q=On+the+relation+between+statistical+learning+and+perceptual+distances
Imbedding Deep Neural Networks,2022,ICLR,"['Andrew Corbett', 'Dmitry Kangin']",spotlight,"['Neural ODEs', 'Optimal Control', 'Deep Neural Networks', 'Invariant Imbedding']","Continuous-depth neural networks, such as Neural ODEs, have refashioned the understanding of residual neural networks in terms of non-linear vector-valued optimal control problems. The common solution is to use the adjoint sensitivity method to replicate a forward-backward pass optimisation problem. We propose a new approach which explicates the network's `depth' as a fundamental variable, thus reducing the problem to a system of forward-facing initial value problems. This new method is based on the principal of `Invariant Imbedding' for which we prove a general solution, applicable to all non-linear, vector-valued optimal control problems with both running and terminal loss.
Our new architectures provide a tangible tool for inspecting the theoretical--and to a great extent unexplained--properties of network depth. They also constitute a resource of discrete implementations of Neural ODEs comparable to classes of imbedded residual neural networks. Through a series of experiments, we show the competitive performance of the proposed architectures for supervised learning and time series prediction. ",https://api.openreview.net/pdf/2696810601b722fa25baf9dd6ed1280d43c1c474.pdf,llm,https://scholar.google.com/scholar?q=Imbedding+Deep+Neural+Networks
UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining,2023,ICLR,"['Hyung Won Chung', 'Xavier Garcia', 'Adam Roberts', 'Yi Tay', 'Orhan Firat', 'Sharan Narang', 'Noah Constant']",poster,"['Keywords: multilingual', 'pretraining', 'language models', 'language sampling', 'language distribution', 'low-resource languages', 'overfitting']","Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMax sampling.",https://api.openreview.net/pdf/f9db93a34d7f56ce156cb253feba6c638acc2b21.pdf,llm,https://scholar.google.com/scholar?q=UniMax:+Fairer+and+More+Effective+Language+Sampling+for+Large-Scale+Multilingual+Pretraining
Understanding the Covariance Structure of Convolutional Filters,2023,ICLR,"['Asher Trockman', 'Devin Willmott', 'J Zico Kolter']",poster,"['initialization', 'init', 'covariance', 'gaussian', 'convolutional neural network', 'convmixer', 'convnext', 'transfer learning', 'spatial mixing', 'computer vision', 'convolution']","Neural network weights are typically initialized at random from univariate distributions, controlling just the variance of individual weights even in highly-structured operations like convolutions. Recent ViT-inspired convolutional networks such as ConvMixer and ConvNeXt use large-kernel depthwise convolutions whose learned filters have notable structure; this presents an opportunity to study their empirical covariances. In this work, we first observe that such learned filters have highly-structured covariance matrices, and moreover, we find that covariances calculated from small networks may be used to effectively initialize a variety of larger networks of different depths, widths, patch sizes, and kernel sizes, indicating a degree of model-independence to the covariance structure. Motivated by these findings, we then propose a learning-free multivariate initialization scheme for convolutional filters using a simple, closed-form construction of their covariance. Models using our initialization outperform those using traditional univariate initializations, and typically meet or exceed the performance of those initialized from the covariances of learned filters; in some cases, this improvement can be achieved without training the depthwise convolutional filters at all. Our code is available at https://github.com/locuslab/convcov.",https://api.openreview.net/pdf/304a7da98b79c8c15e8baa9f038951976a9ef764.pdf,llm,https://scholar.google.com/scholar?q=Understanding+the+Covariance+Structure+of+Convolutional+Filters
PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales,2023,ICLR,"['PeiFeng Wang', 'Aaron Chan', 'Filip Ilievski', 'Muhao Chen', 'Xiang Ren']",poster,"['Commonsense reasoning', 'free-text rationale', 'rationale generation', 'faithful reasoning']","Neural language models (LMs) have achieved impressive results on various language-based reasoning tasks by utilizing latent knowledge encoded in their own pretrained parameters. To make this reasoning process more explicit, recent works retrieve a rationalizing LM's internal knowledge by training or prompting it to generate free-text rationales, which can be used to guide task predictions made by either the same LM or a separate reasoning LM. However, rationalizing LMs require expensive rationale annotation and/or computation, without any assurance that their generated rationales improve LM task performance or faithfully reflect LM decision-making. In this paper, we propose PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns to faithfully reason over rationales via counterfactual regularization. First, PINTO maps out a suitable reasoning process for the task input by prompting a frozen rationalizing LM to generate a free-text rationale. Second, PINTO's reasoning LM is fine-tuned to solve the task using the generated rationale as context, while regularized to output less confident predictions when the rationale is perturbed. Across four datasets, we show that PINTO significantly improves the generalization ability of the reasoning LM, yielding higher performance on both in-distribution and out-of-distribution test sets. Also, we find that PINTO's rationales are more faithful to its task predictions than those generated by competitive baselines.",https://api.openreview.net/pdf/7e3d881a1ec0910d26a1dcbaea914860cb610c81.pdf,llm,https://scholar.google.com/scholar?q=PINTO:+Faithful+Language+Reasoning+Using+Prompt-Generated+Rationales
Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors,2023,ICLR,"['Sizhe Chen', 'Geng Yuan', 'Xinwen Cheng', 'Yifan Gong', 'Minghai Qin', 'Yanzhi Wang', 'Xiaolin Huang']",poster,"['data protection', 'poisoning attack', 'self-ensemble', 'deep neural network']","As data becomes increasingly vital, a company would be very cautious about releasing data, because the competitors could use it to train high-performance models, thereby posing a tremendous threat to the company's commercial competence. To prevent training good models on the data, we could add imperceptible perturbations to it. Since such perturbations aim at hurting the entire training process, they should reflect the vulnerability of DNN training, rather than that of a single model. Based on this new idea, we seek perturbed examples that are always unrecognized (never correctly classified) in training. In this paper, we uncover them by model checkpoints' gradients, forming the proposed self-ensemble protection (SEP), which is very effective because (1) learning on examples ignored during normal training tends to yield DNNs ignoring normal examples; (2) checkpoints' cross-model gradients are close to orthogonal, meaning that they are as diverse as DNNs with different architectures. That is, our amazing performance of ensemble only requires the computation of training one model. By extensive experiments with 9 baselines on 3 datasets and 5 architectures, SEP is verified to be a new state-of-the-art, e.g., our small $\ell_\infty=2/255$ perturbations reduce the accuracy of a CIFAR-10 ResNet18 from 94.56% to 14.68%, compared to 41.35% by the best-known method. Code is available at https://github.com/Sizhe-Chen/SEP.",https://api.openreview.net/pdf/03e69010cd3d91e35491934b95bbd8839f6416bc.pdf,llm,https://scholar.google.com/scholar?q=Self-Ensemble+Protection:+Training+Checkpoints+Are+Good+Data+Protectors
Language models are multilingual chain-of-thought reasoners,2023,ICLR,"['Freda Shi', 'Mirac Suzgun', 'Markus Freitag', 'Xuezhi Wang', 'Suraj Srivats', 'Soroush Vosoughi', 'Hyung Won Chung', 'Yi Tay', 'Sebastian Ruder', 'Denny Zhou', 'Dipanjan Das', 'Jason Wei']",poster,"['multilingual', 'reasoning', 'large language model']","We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at AnonymousLink and the supplementary material.
",https://api.openreview.net/pdf/972d6eaf77336eece16b7ec5bdb9565b06423b8a.pdf,llm,https://scholar.google.com/scholar?q=Language+models+are+multilingual+chain-of-thought+reasoners
TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation,2023,ICLR,"['Hyesu Lim', 'Byeonggeun Kim', 'Jaegul Choo', 'Sungha Choi']",poster,"['Test time adaptation', 'Domain adaptation', 'Batch Normalization']","This paper proposes a novel batch normalization strategy for test-time adaptation. Recent test-time adaptation methods heavily rely on the modified batch normalization, i.e., transductive batch normalization (TBN), which calculates the mean and the variance from the current test batch rather than using the running mean and variance obtained from the source data, i.e., conventional batch normalization (CBN). Adopting TBN that employs test batch statistics mitigates the performance degradation caused by the domain shift. However, re-estimating normalization statistics using test data depends on impractical assumptions that a test batch should be large enough and be drawn from i.i.d. stream, and we observed that the previous methods with TBN show critical performance drop without the assumptions. In this paper, we identify that CBN and TBN are in a trade-off relationship and present a new test-time normalization (TTN) method that interpolates the statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer. Our proposed TTN improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios. TTN is widely applicable to other test-time adaptation methods that rely on updating model parameters via backpropagation. We demonstrate that adopting TTN further improves their performance and achieves state-of-the-art performance in various standard benchmarks.",https://api.openreview.net/pdf/cac516c4c0f5e33e98d8f7a4853bbbc87a45b3f1.pdf,llm,https://scholar.google.com/scholar?q=TTN:+A+Domain-Shift+Aware+Batch+Normalization+in+Test-Time+Adaptation
Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection,2023,ICLR,"['Kaifeng Gao', 'Long Chen', 'Hanwang Zhang', 'Jun Xiao', 'Qianru Sun']",poster,"['Prompt Tuning', 'Video Relation Detection']","Prompt tuning with large-scale pretrained vision-language models empowers open-vocabulary prediction trained on limited base categories, e.g., object classification and detection. In this paper, we propose compositional prompt tuning with motion cues: an extended prompt tuning paradigm for compositional predictions of video data. In particular, we present Relation Prompt (RePro) for Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where conventional prompt tuning is easily biased to certain subject-object combinations and motion patterns. To this end, RePro addresses the two technical challenges of Open-VidVRD: 1) the prompt tokens should respect the two different semantic roles of subject and object, and 2) the tuning should account for the diverse spatiotemporal motion patterns of the subject-object compositions. Our RePro achieves a new state-of-the-art performance on two VidVRD benchmarks of not only the base training object and predicate categories, but also the unseen ones. Extensive ablations also demonstrate the effectiveness of the proposed compositional and multi-mode design of prompt. Code is available at https://github.com/Dawn-LX/OpenVoc-VidVRD.",https://api.openreview.net/pdf/c9c64a5b7d6d1b3b977f973ab553b08580576420.pdf,llm,https://scholar.google.com/scholar?q=Compositional+Prompt+Tuning+with+Motion+Cues+for+Open-vocabulary+Video+Relation+Detection
Complexity-Based Prompting for Multi-step Reasoning,2023,ICLR,"['Yao Fu', 'Hao Peng', 'Ashish Sabharwal', 'Peter Clark', 'Tushar Khot']",poster,"['Chain-of-Thoughts', 'Multi-Step Reasoning', 'Large Language Models', 'Prompting']","We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on math word reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority
of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3, our approach substantially improves multi-step reasoning accuracy, with an 8.6% absolute improvement on GSM8K, and 6.4% on MathQA. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",https://api.openreview.net/pdf/45dc479ddf081da97bf30a319e61cd0509d1f701.pdf,llm,https://scholar.google.com/scholar?q=Complexity-Based+Prompting+for+Multi-step+Reasoning
Optimal Activation Functions for the Random Features Regression Model,2023,ICLR,"['Jianxin Wang', 'José Bento']",poster,"['Random Features Regression Model', 'Learning theory for neural networks', 'Functional analysis and variational calculus']","The asymptotic mean squared test error and sensitivity of the Random Features Regression model (RFR) have been recently studied. We build on this work and identify in closed-form the family of Activation Functions (AFs) that minimize a combination of the test error and sensitivity of the RFR under different notions of functional parsimony. We find scenarios under which the optimal AFs are linear, saturated linear functions, or expressible in terms of Hermite polynomials. Finally, we show how using optimal AFs impacts well established properties of the RFR model, such as its double descent curve, and the dependency of its optimal regularization parameter on the observation noise level.",https://api.openreview.net/pdf/8895967b0a0cc46d0f4328a9c58e36e5da0fec6b.pdf,llm,https://scholar.google.com/scholar?q=Optimal+Activation+Functions+for+the+Random+Features+Regression+Model
Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,2023,ICLR,"['Denny Zhou', 'Nathanael Schärli', 'Le Hou', 'Jason Wei', 'Nathan Scales', 'Xuezhi Wang', 'Dale Schuurmans', 'Claire Cui', 'Olivier Bousquet', 'Quoc V Le', 'Ed H. Chi']",poster,"['large language models', 'natural language processing', 'prompting', 'reasoning', 'compositional generalization']","Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split)  with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting.  This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",https://api.openreview.net/pdf/328fd7b9b742a2398905672f07b91af643001cb5.pdf,llm,https://scholar.google.com/scholar?q=Least-to-Most+Prompting+Enables+Complex+Reasoning+in+Large+Language+Models
Can discrete information extraction prompts generalize across language models?,2023,ICLR,"['Nathanaël Carraz Rakotonirina', 'Roberto Dessi', 'Fabio Petroni', 'Sebastian Riedel', 'Marco Baroni']",poster,"['prompting', 'prompt analysis', 'language model interfaces', 'prompt generalizations']","We study whether automatically-induced prompts that effectively extract information from a language model can also be used, out-of-the-box, to probe other language models for the same information. After confirming that discrete prompts induced with the AutoPrompt algorithm outperform manual and semi-manual prompts on the slot-filling task, we demonstrate a drop in performance for AutoPrompt prompts learned on a model and tested on another. We introduce a way to induce prompts by mixing language models at training time that results in prompts that generalize well across models. We conduct an extensive analysis of the induced prompts, finding that the more general prompts include a larger proportion of existing English words and have a less order-dependent and more uniform distribution of information across their component tokens. Our work provides preliminary evidence that it's possible to generate discrete prompts that can be induced once and used with a number of different models, and gives insights on the properties characterizing such prompts.",https://api.openreview.net/pdf/027788e7f8d7f512b53ca6e6935d18aa5150e77f.pdf,llm,https://scholar.google.com/scholar?q=Can+discrete+information+extraction+prompts+generalize+across+language+models?
