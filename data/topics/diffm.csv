title,year,source,authors,class,keywords,abstract,pdf_link,topic,google_scholar_link
Unifying Graph Convolutional Neural Networks and Label Propagation,2021,ICLR,"['Hongwei Wang', 'Jure Leskovec']",poster,"['graph convolutional neural networks', 'label propagation', 'semi-supervised node classification']","Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, it is unclear how LPA and GCN can be combined under a unified framework to improve node classification. Here we study the relationship between LPA and GCN in terms of feature/label influence, in which we characterize how much the initial feature/label of one node influences the final feature/label of another node in GCN/LPA. Based on our theoretical analysis, we propose an end-to-end model that combines GCN and LPA. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning the weights for edges based on node labels, which is more task-oriented than existing feature-based attention models and topology-based diffusion models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art graph neural networks in terms of node classification accuracy.",/pdf/c87b8a8d5bd52b657693e324583e35898d09f617.pdf,graph;transformer;diffusion models,https://scholar.google.com/scholar?q=Unifying+Graph+Convolutional+Neural+Networks+and+Label+Propagation
DiffWave: A Versatile Diffusion Model for Audio Synthesis,2021,ICLR,"['Zhifeng Kong', 'Wei Ping', 'Jiaji Huang', 'Kexin Zhao', 'Bryan Catanzaro']",poster,"['diffusion probabilistic models', 'audio synthesis', 'speech synthesis', 'generative models']","In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.",/pdf/d27840fc3a835c4da4a9d13c4227c7a0d8a9b3c5.pdf,graph;generative model;diffusion models,https://scholar.google.com/scholar?q=DiffWave:+A+Versatile+Diffusion+Model+for+Audio+Synthesis
Improved Denoising Diffusion Probabilistic Models,2021,ICLR,"['Alexander Quinn Nichol', 'Prafulla Dhariwal']",poster,"['neural networks', 'generative models', 'log-likelihood', 'diffusion models', 'denoising diffusion probabilistic models', 'image generation']","We explore denoising diffusion probabilistic models, a class of generative models which have recently been shown to produce excellent samples in the image and audio domains. While these models produce excellent samples, it has yet to be shown that they can achieve competitive log-likelihoods. We show that, with several small modifications, diffusion models can achieve competitive log-likelihoods in the image domain while maintaining high sample quality. Additionally, our models allow for sampling with an order of magnitude fewer diffusion steps with only a modest difference in sample quality. Finally, we explore how sample quality and log-likelihood scale with the number of diffusion steps and the amount of model capacity. We conclude that denoising diffusion probabilistic models are a promising class of generative models with excellent scaling properties and sample quality.",/pdf/fa8f0835a611b1883d6159ceb872a40790c65408.pdf,zero_few-shot;transformer;generative model;diffusion models,https://scholar.google.com/scholar?q=Improved+Denoising+Diffusion+Probabilistic+Models
Variational Diffusion Models,2021,NIPS,"['Diederik P Kingma', 'Tim Salimans', 'Ben Poole', 'Jonathan Ho']",poster,"['Diffusion Models', 'Generative Models', 'Density Estimation']","Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum.",https://api.openreview.net/pdf/d78a8632e42c43b39dfca68d20e6297c6f420c44.pdf,optimization;zero_few-shot;generative model;diffusion models,https://scholar.google.com/scholar?q=Variational+Diffusion+Models
D2C: Diffusion-Decoding Models for Few-Shot Conditional Generation,2021,NIPS,"['Abhishek Sinha', 'Jiaming Song', 'Chenlin Meng', 'Stefano Ermon']",poster,['generative models'],"Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAE) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks, conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study. We release our code at https://github.com/jiamings/d2c.",https://api.openreview.net/pdf/b5d48c3e6432212d5ca261b4bea43ad8d6eb63fe.pdf,optimization;zero_few-shot;representation;vae;generative model;contrastive learning;diffusion models,https://scholar.google.com/scholar?q=D2C:+Diffusion-Decoding+Models+for+Few-Shot+Conditional+Generation
Structured Denoising Diffusion Models in Discrete State-Spaces,2021,NIPS,"['Jacob Austin', 'Daniel D. Johnson', 'Jonathan Ho', 'Daniel Tarlow', 'Rianne van den Berg']",poster,"['diffusion models', 'generative models', 'text generation', 'probabilistic models']","Denoising diffusion probabilistic models (DDPMs) [Ho et al. 2021] have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. [2021], by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss.  For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.",https://api.openreview.net/pdf/bfc46a732decab8f9cdfbc9eaaffdbc7d55b31e2.pdf,zero_few-shot;transformer;generative model;diffusion models,https://scholar.google.com/scholar?q=Structured+Denoising+Diffusion+Models+in+Discrete+State-Spaces
CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation,2021,NIPS,"['YUSUKE TASHIRO', 'Jiaming Song', 'Yang Song', 'Stefano Ermon']",poster,"['time series imputation', 'generative modeling', 'deep learning', 'score-based diffusion models']","The imputation of missing values in time series has many applications in healthcare and finance. While autoregressive models are natural candidates for time series imputation, score-based diffusion models have recently outperformed existing counterparts including autoregressive models in many tasks such as image generation and audio synthesis, and would be promising for time series imputation. In this paper, we propose Conditional Score-based Diffusion model (CSDI), a novel time series imputation method that utilizes score-based diffusion models conditioned on observed data. Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. On healthcare and environmental data, CSDI improves by 40-65% over existing probabilistic imputation methods on popular performance metrics. In addition, deterministic imputation by CSDI reduces the error by 5-20% compared to the state-of-the-art deterministic imputation methods. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines. The code is available at https://github.com/ermongroup/CSDI.",https://api.openreview.net/pdf/0ae3d2f98f55676a534b11240f6e80335b7bc58e.pdf,generative model;metric;diffusion models,https://scholar.google.com/scholar?q=CSDI:+Conditional+Score-based+Diffusion+Models+for+Probabilistic+Time+Series+Imputation
Diffusion Models Beat GANs on Image Synthesis,2021,NIPS,"['Prafulla Dhariwal', 'Alexander Quinn Nichol']",spotlight,"['generative models', 'diffusion models', 'score-based models', 'denoising diffusion probabilistic models', 'image generation', 'neural networks', 'attention']","We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet 256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512.",https://api.openreview.net/pdf/fac47484c51c0a9ca609c04dfef93927c49cea18.pdf,generative model;diffusion models,https://scholar.google.com/scholar?q=Diffusion+Models+Beat+GANs+on+Image+Synthesis
Maximum Likelihood Training of Score-Based Diffusion Models,2021,NIPS,"['Yang Song', 'Conor Durkan', 'Iain Murray', 'Stefano Ermon']",spotlight,"['generative models', 'density estimation', 'score matching', 'score-based generative models', 'diffusion models', 'stochastic differential equations', 'normalizing flows', 'neural ODEs', 'likelihood', 'continuous normalizing flows']","Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing flows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a specific weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet $32\times 32$ without any data augmentation, on a par with state-of-the-art autoregressive models on these tasks. ",https://api.openreview.net/pdf/756ca91c16908b457e64e0f988e3fb8c54fc9f6e.pdf,zero_few-shot;flow;augmentation;diffusion models,https://scholar.google.com/scholar?q=Maximum+Likelihood+Training+of+Score-Based+Diffusion+Models
Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions,2021,NIPS,"['Emiel Hoogeboom', 'Didrik Nielsen', 'Priyank Jaini', 'Patrick Forré', 'Max Welling']",poster,"['categorical', 'normalizing flows', 'diffusion']","Generative flows and diffusion models have been predominantly trained on ordinal data, for example natural images. This paper introduces two extensions of flows and diffusion for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our method outperforms existing dequantization approaches on text modelling and modelling on image segmentation maps in log-likelihood. ",https://api.openreview.net/pdf/67d22eeb9190dd02385704def81b849ee83b8b12.pdf,zero_few-shot;generative model;flow;segmentation;diffusion models,https://scholar.google.com/scholar?q=Argmax+Flows+and+Multinomial+Diffusion:+Learning+Categorical+Distributions
Dynamic Dual-Output Diffusion Models,2022,CVPR,Yaniv Benny;Lior Wolf,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Benny_Dynamic_Dual-Output_Diffusion_Models_CVPR_2022_paper.pdf,diffusion models,https://scholar.google.com/scholar?q=Dynamic+Dual-Output+Diffusion+Models
High-Resolution Image Synthesis With Latent Diffusion Models,2022,CVPR,Robin Rombach;Andreas Blattmann;Dominik Lorenz;Patrick Esser;Björn Ommer,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf,diffusion models,https://scholar.google.com/scholar?q=High-Resolution+Image+Synthesis+With+Latent+Diffusion+Models
Generating High Fidelity Data From Low-Density Regions Using Diffusion Models,2022,CVPR,Vikash Sehwag;Caner Hazirbas;Albert Gordo;Firat Ozgenel;Cristian Canton,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Sehwag_Generating_High_Fidelity_Data_From_Low-Density_Regions_Using_Diffusion_Models_CVPR_2022_paper.pdf,zero_few-shot;diffusion models,https://scholar.google.com/scholar?q=Generating+High+Fidelity+Data+From+Low-Density+Regions+Using+Diffusion+Models
Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems Through Stochastic Contraction,2022,CVPR,Hyungjin Chung;Byeongsu Sim;Jong Chul Ye,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Chung_Come-Closer-Diffuse-Faster_Accelerating_Conditional_Diffusion_Models_for_Inverse_Problems_Through_Stochastic_CVPR_2022_paper.pdf,diffusion models,https://scholar.google.com/scholar?q=Come-Closer-Diffuse-Faster:+Accelerating+Conditional+Diffusion+Models+for+Inverse+Problems+Through+Stochastic+Contraction
Perception Prioritized Training of Diffusion Models,2022,CVPR,Jooyoung Choi;Jungbeom Lee;Chaehun Shin;Sungwon Kim;Hyunwoo Kim;Sungroh Yoon,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Choi_Perception_Prioritized_Training_of_Diffusion_Models_CVPR_2022_paper.pdf,diffusion models,https://scholar.google.com/scholar?q=Perception+Prioritized+Training+of+Diffusion+Models
DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation,2022,CVPR,Gwanghyun Kim;Taesung Kwon;Jong Chul Ye,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_DiffusionCLIP_Text-Guided_Diffusion_Models_for_Robust_Image_Manipulation_CVPR_2022_paper.pdf,diffusion models,https://scholar.google.com/scholar?q=DiffusionCLIP:+Text-Guided+Diffusion+Models+for+Robust+Image+Manipulation
Vector Quantized Diffusion Model for Text-to-Image Synthesis,2022,CVPR,Shuyang Gu;Dong Chen;Jianmin Bao;Fang Wen;Bo Zhang;Dongdong Chen;Lu Yuan;Baining Guo,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_Vector_Quantized_Diffusion_Model_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf,diffusion models,https://scholar.google.com/scholar?q=Vector+Quantized+Diffusion+Model+for+Text-to-Image+Synthesis
CARD: Classification and Regression Diffusion Models,2022,NIPS,"['Xizewen Han', 'Huangjie Zheng', 'Mingyuan Zhou']",poster,[],"Learning the distribution of a continuous or categorical response variable y given its covariates x is a fundamental problem in statistics and machine learning. Deep neural network-based supervised learning algorithms have made great progress in predicting the mean of y given x, but they are often criticized for their ability to accurately capture the uncertainty of their predictions. In this paper, we introduce classification and regression diffusion (CARD) models, which combine a denoising diffusion-based conditional generative model and a pre-trained conditional mean estimator, to accurately predict the distribution of y given x.  We demonstrate the outstanding ability of CARD in conditional distribution prediction with both toy examples and real-world datasets, the experimental results on which show that CARD, in general, outperforms state-of-the-art methods, including Bayesian neural network-based one, designed for uncertainty estimation, especially when the conditional distribution of y given x is multi-modal. In addition, we utilize the stochastic nature of the generative model outputs to obtain a finer granularity in model confidence assessment at the instance level for classification tasks.",https://api.openreview.net/pdf/aad8cc52d1ab38485c05dbb5577ca801955ea620.pdf,generative model;bayesian;multimodal;diffusion models,https://scholar.google.com/scholar?q=CARD:+Classification+and+Regression+Diffusion+Models
Diffusion Models as Plug-and-Play Priors,2022,NIPS,"['Alexandros Graikos', 'Nikolay Malkin', 'Nebojsa Jojic', 'Dimitris Samaras']",poster,"['diffusion models', 'conditional generation', 'image segmentation']","We consider the problem of inferring high-dimensional data $x$ in a model that consists of a prior $p(x)$ and an auxiliary differentiable constraint $c(x,y)$ on $x$ given some additional information $y$. In this paper, the prior is an independently trained denoising diffusion generative model. The auxiliary constraint is expected to have a differentiable form, but can come from diverse sources. The possibility of such inference turns diffusion models into plug-and-play modules, thereby allowing a range of potential applications in adapting models to new domains and tasks, such as conditional generation or image segmentation. The structure of diffusion models allows us to perform approximate inference by iterating differentiation through the fixed denoising network enriched with different amounts of noise at each step. Considering many noised versions of $x$ in evaluation of its fitness is a novel search mechanism that may lead to new algorithms for solving combinatorial optimization problems. The code is available at https://github.com/AlexGraikos/diffusion_priors.",https://api.openreview.net/pdf/0118a2f5ccd38ec90c6225a8a72f4c3f1a6fec7a.pdf,optimization;zero_few-shot;generative model;inference;segmentation;diffusion models,https://scholar.google.com/scholar?q=Diffusion+Models+as+Plug-and-Play+Priors
Deep Equilibrium Approaches to Diffusion Models,2022,NIPS,"['Ashwini Pokle', 'Zhengyang Geng', 'J Zico Kolter']",poster,"['diffusion models', 'deep equilibrium models', 'model inversion']","Diffusion-based generative models are extremely effective in generating high-quality images, with generated samples often surpassing the quality of those produced by other models under several metrics.  One distinguishing feature of these models, however, is that they typically require long sampling chains in order to produce high-fidelity images.  This presents a challenge not only from the lenses of sampling time, but also from the inherent difficulty in backpropagating through these chains in order to accomplish tasks such as model inversion, i.e., approximately finding latent states that generate known images.  In this paper, we look at diffusion models through a different perspective, that of a (deep) equilibrium (DEQ) fixed point model. Specifically, we extend the recent denoising diffusion implicit model (DDIM), and model the entire sampling chain as a joint, multi-variate fixed point system. This setup provides an elegant unification of diffusion and equilibrium models, and shows benefits in 1) single-shot image sampling, as it replaces the fully-serial typical sampling process with a parallel one; and 2) model inversion, where we can leverage fast gradients in the DEQ setting to much more quickly find the noise that generates a given image.  The approach is also orthogonal and thus complementary to other methods used to reduce the sampling time, or improve model inversion.  We demonstrate our method's strong performance across several datasets, including CIFAR10, CelebA, and LSUN Bedroom and Churches.",https://api.openreview.net/pdf/0ca15db2f78d0e731f11f1e36c4f3071cd864f9d.pdf,generative model;metric;diffusion models,https://scholar.google.com/scholar?q=Deep+Equilibrium+Approaches+to+Diffusion+Models
BinauralGrad: A Two-Stage Conditional Diffusion Probabilistic Model for Binaural Audio Synthesis,2022,NIPS,"['Yichong Leng', 'Zehua Chen', 'Junliang Guo', 'Haohe Liu', 'Jiawei Chen', 'Xu Tan', 'Danilo Mandic', 'Lei He', 'Xiangyang Li', 'Tao Qin', 'sheng zhao', 'Tie-Yan Liu']",poster,"['Binaural audio synthesis', 'Audio warping', 'Two-stage framework', 'Conditional diffusion probabilistic model']","Binaural audio plays a significant role in constructing immersive augmented and virtual realities. As it is expensive to record binaural audio from the real world, synthesizing them from mono audio has attracted increasing attention. This synthesis process involves not only the basic physical warping of the mono audio, but also room reverberations and head/ear related filtration, which, however, are difficult to accurately simulate in traditional digital signal processing. In this paper, we formulate the synthesis process from a different perspective by decomposing the binaural audio into a common part that shared by the left and right channels as well as a specific part that differs in each channel. Accordingly, we propose BinauralGrad, a novel two-stage framework equipped with diffusion models to synthesize them respectively. Specifically, in the first stage, the common information of the binaural audio is generated with a single-channel diffusion model conditioned on the mono audio, based on which the binaural audio is generated by a two-channel diffusion model in the second stage. Combining this novel perspective of two-stage synthesis with advanced generative models (i.e., the diffusion models), the proposed BinauralGrad is able to generate accurate and high-fidelity binaural audio samples. Experiment results show that on a benchmark dataset, BinauralGrad outperforms the existing baselines by a large margin in terms of both object and subject evaluation metrics (Wave L2: $0.128$ vs. $0.157$, MOS: $3.80$ vs. $3.61$). The generated audio samples\footnote{\url{https://speechresearch.github.io/binauralgrad}} and code\footnote{\url{https://github.com/microsoft/NeuralSpeech/tree/master/BinauralGrad}} are available online.",https://api.openreview.net/pdf/604d31618224981d44640e82a08046b6c5f06266.pdf,graph;transformer;generative model;online learning;metric;augmentation;diffusion models,https://scholar.google.com/scholar?q=BinauralGrad:+A+Two-Stage+Conditional+Diffusion+Probabilistic+Model+for+Binaural+Audio+Synthesis
Flexible Diffusion Modeling of Long Videos,2022,NIPS,"['William Harvey', 'Saeid Naderiparizi', 'Vaden Masrani', 'Christian Dietrich Weilbach', 'Frank Wood']",poster,"['generative modeling', 'denoising diffusion probabilistic model', 'video modeling']",We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames.  We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length.  We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator.,https://api.openreview.net/pdf/758bc7d447bbd3b46f16006486f88a7b12a24562.pdf,zero_few-shot;generative model;metric;sparse;diffusion models,https://scholar.google.com/scholar?q=Flexible+Diffusion+Modeling+of+Long+Videos
Improving Diffusion Models for Inverse Problems using Manifold Constraints,2022,NIPS,"['Hyungjin Chung', 'Byeongsu Sim', 'Dohoon Ryu', 'Jong Chul Ye']",poster,"['Diffusion model', 'Inverse problem', 'Manifold constraint']","Recently, diffusion models have been used to solve various inverse problems in an unsupervised manner with appropriate modifications to the sampling process. However, the current solvers, which recursively apply a reverse diffusion step followed by a projection-based measurement consistency step, often produce sub-optimal results. By studying the generative sampling path, here we show that current solvers throw the sample path off the data manifold, and hence the error accumulates. To address this, we propose an additional correction term inspired by the manifold constraint, which can be used synergistically with the previous solvers to make the iterations close to the manifold. The proposed manifold constraint is straightforward to implement within a few lines of code, yet boosts the performance by a surprisingly large margin. With extensive experiments, we show that our method is superior to the previous methods both theoretically and empirically, producing promising results in many applications such as image inpainting, colorization, and sparse-view computed tomography. Code available https://github.com/HJ-harry/MCG_diffusion",https://api.openreview.net/pdf/f2338580f32aae78d068cb65c4723e358746b8b8.pdf,graph;optimization;zero_few-shot;generative model;sparse;diffusion models,https://scholar.google.com/scholar?q=Improving+Diffusion+Models+for+Inverse+Problems+using+Manifold+Constraints
Maximum Likelihood Training of Implicit Nonlinear Diffusion Model,2022,NIPS,"['Dongjun Kim', 'Byeonghu Na', 'Se Jung Kwon', 'Dongsoo Lee', 'Wanmo Kang', 'Il-chul Moon']",poster,"['Diffusion Model', 'Score-based Model', 'Generative Model', 'Image Generation']","Whereas diverse variations of diffusion models exist, extending the linear diffusion into a nonlinear diffusion process is investigated by very few works. The nonlinearity effect has been hardly understood, but intuitively, there would be promising diffusion patterns to efficiently train the generative distribution towards the data distribution. This paper introduces a data-adaptive nonlinear diffusion process for score-based diffusion models. The proposed Implicit Nonlinear Diffusion Model (INDM) learns by combining a normalizing flow and a diffusion process. Specifically, INDM implicitly constructs a nonlinear diffusion on the data space by leveraging a linear diffusion on the latent space through a flow network. This flow network is key to forming a nonlinear diffusion, as the nonlinearity depends on the flow network. This flexible nonlinearity improves the learning curve of INDM to nearly Maximum Likelihood Estimation (MLE) against the non-MLE curve of DDPM++, which turns out to be an inflexible version of INDM with the flow fixed as an identity mapping. Also, the discretization of INDM shows the sampling robustness. In experiments, INDM achieves the state-of-the-art FID of 1.75 on CelebA. We release our code at https://github.com/byeonghu-na/INDM.",https://api.openreview.net/pdf/bfd050f3af752cba05f980f98e3da8ffee8cce13.pdf,graph;zero_few-shot;generative model;online learning;adaptive;flow;diffusion models,https://scholar.google.com/scholar?q=Maximum+Likelihood+Training+of+Implicit+Nonlinear+Diffusion+Model
Conditional Diffusion Process for Inverse Halftoning,2022,NIPS,"['Hao Jiang', 'Yadong MU']",poster,[],"Inverse halftoning is a technique used to recover realistic images from ancient prints (\textit{e.g.}, photographs, newspapers, books). The rise of deep learning has led to the gradual incorporation of neural network designs into inverse halftoning methods. Most of existing inverse halftoning approaches adopt the U-net architecture, which uses an encoder to encode halftone prints, followed by a decoder for image reconstruction. However, the mainstream supervised learning paradigm with element-wise regression commonly adopted in U-net based methods has poor generalization ability in practical applications. Specifically, when there is a large gap between the dithering patterns of the training and test halftones, the reconstructed continuous-tone images have obvious artifacts. This is an important issue in practical applications, since the algorithms for generating halftones are ever-evolving. Even for the same algorithm, different parameter choices will result in different halftone dithering patterns. In this paper, we propose the first generative halftoning method in the literature, which regards the black pixels in halftones as physically moving particles, and makes the randomly distributed particles move under some certain guidance through reverse diffusion process, so as to obtain desired halftone patterns. In particular, we propose a Conditional Diffusion model for image Halftoning (CDH), which consists of a halftone dithering process and an inverse halftoning process. By changing the initial state of the diffusion model, our method can generate visually plausible halftones with different dithering patterns under the condition of image gray level and Laplacian prior. To avoid introducing redundant patterns and undesired artifacts, we propose a meta-halftone guided network to incorporate blue noise guidance in the diffusion process. In this way, halftone images subject to more diverse distributions are fed into the inverse halftoning model, which helps the model to learn a more robust mapping from halftone distributions to continuous-tone distributions, thereby improving the generalization ability to unseen samples. Quantitative and qualitative experimental results demonstrate that the proposed method achieves state-of-the-art results.",https://api.openreview.net/pdf/55e76913a7239520c9e768d0a18b51699def3fd7.pdf,graph;zero_few-shot;generative model;meta-learning;diffusion models,https://scholar.google.com/scholar?q=Conditional+Diffusion+Process+for+Inverse+Halftoning
Diffusion-LM Improves Controllable Text Generation,2022,NIPS,"['Xiang Lisa Li', 'John Thickstun', 'Ishaan Gulrajani', 'Percy Liang', 'Tatsunori Hashimoto']",poster,"['controllable text generation', 'controlled generation', 'infilling', 'language model', 'diffusion model']","Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.",https://api.openreview.net/pdf/22d95846dcf1ef4a64faf610534b3d5d41776897.pdf,graph;generative model;diffusion models,https://scholar.google.com/scholar?q=Diffusion-LM+Improves+Controllable+Text+Generation
Score-based Generative Modeling Secretly Minimizes the Wasserstein Distance,2022,NIPS,"['Dohyun Kwon', 'Ying Fan', 'Kangwook Lee']",poster,"['score-based generative models', 'optimal transport', 'Wasserstein distance']","Score-based generative models are shown to achieve remarkable empirical performances in various applications such as image generation and audio synthesis. However, a theoretical understanding of score-based diffusion models is still incomplete. Recently, Song et al. showed that the training objective of score-based generative models is equivalent to minimizing the Kullback-Leibler divergence of the generated distribution from the data distribution. In this work, we show that score-based models also minimize the Wasserstein distance between them. Specifically, we prove that the Wasserstein distance is upper bounded by the square root of the objective function up to multiplicative constants and a fixed constant offset. Our proof is based on a novel application of the theory of optimal transport, which can be of independent interest to the society. Our numerical experiments support our findings. By analyzing our upper bounds, we provide a few techniques to obtain tighter upper bounds. ",https://api.openreview.net/pdf/86ab5fb1577ee8e97e4d5226025d9ff07d78cda3.pdf,generative model;diffusion models,https://scholar.google.com/scholar?q=Score-based+Generative+Modeling+Secretly+Minimizes+the+Wasserstein+Distance
Riemannian Diffusion Models,2022,NIPS,"['Chin-Wei Huang', 'Milad Aghajohari', 'Joey Bose', 'Prakash Panangaden', 'Aaron Courville']",poster,"['diffusion models', 'density estimation', 'generative models', 'Riemannian manifolds', 'variational inference']","Diffusion models are recent state-of-the-art methods for image generation and likelihood estimation. In this work, we generalize continuous-time diffusion models to arbitrary Riemannian manifolds and derive a variational framework for likelihood estimation. Computationally, we propose new methods for computing the Riemannian divergence which is needed for likelihood estimation. Moreover, in generalizing the Euclidean case, we prove that maximizing this variational lower-bound is equivalent to Riemannian score matching. Empirically, we demonstrate the expressive power of Riemannian diffusion models on a wide spectrum of smooth manifolds, such as spheres, tori, hyperboloids, and orthogonal groups. Our proposed method achieves new state-of-the-art likelihoods on all benchmarks.",https://api.openreview.net/pdf/3fe351d3fad6f10afe170abca444c73c60105225.pdf,zero_few-shot;generative model;diffusion models,https://scholar.google.com/scholar?q=Riemannian+Diffusion+Models
"First Hitting Diffusion Models for Generating Manifold, Graph and Categorical Data",2022,NIPS,"['Mao Ye', 'Lemeng Wu', 'qiang liu']",poster,['diffusion model'],"We propose a family of First Hitting Diffusion Models (FHDM), deep generative models that generate data with a diffusion process that terminates at a random first hitting time. This yields an extension of the standard fixed-time diffusion models that terminate at a pre-specified deterministic time. Although standard diffusion models are designed for continuous unconstrained data, FHDM is naturally designed to learn distributions on continuous as well as a range of discrete and structure domains. Moreover, FHDM  enables instance-dependent terminate time and accelerates the diffusion process to sample higher quality data with fewer diffusion steps. Technically, we train FHDM by maximum likelihood estimation on diffusion trajectories augmented from observed data with conditional first hitting processes (i.e., bridge) derived based on Doob's $h$-transform, deviating from the commonly used time-reversal mechanism. 
We apply FHDM to generate data in various domains such as point cloud (general continuous distribution),  climate and geographical events on earth (continuous distribution on the sphere),  unweighted graphs (distribution of binary matrices), and segmentation maps of 2D images (high-dimensional categorical distribution). We observe considerable improvement compared with the state-of-the-art approaches in both quality and speed. ",https://api.openreview.net/pdf/53d71b7f1b7c25d8d3110e4689668c39f7f2a453.pdf,graph;optimization;generative model;augmentation;segmentation;diffusion models,"https://scholar.google.com/scholar?q=First+Hitting+Diffusion+Models+for+Generating+Manifold,+Graph+and+Categorical+Data"
EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations,2022,NIPS,"['Min Zhao', 'Fan Bao', 'Chongxuan Li', 'Jun Zhu']",poster,"['Image to image translation', 'diffusion probabilistic models', 'stochastic differential equation', 'product of experts']","Score-based diffusion models (SBDMs) have achieved the SOTA FID results in unpaired image-to-image translation (I2I). However, we notice that existing methods totally ignore the training data in the source domain, leading to sub-optimal solutions for unpaired I2I.  To this end, we propose energy-guided stochastic differential equations (EGSDE) that employs an energy function pretrained on both the source and target domains to guide the inference process of a pretrained SDE for realistic and faithful unpaired I2I. Building upon two feature extractors, we carefully design the energy function such that it encourages the transferred image to preserve the domain-independent features and discard domain-specific ones. Further, we provide an alternative explanation of the EGSDE as a product of experts, where each of the three experts (corresponding to the SDE and two feature extractors) solely contributes to faithfulness or realism. Empirically, we compare EGSDE to a large family of baselines on three widely-adopted unpaired I2I tasks under four metrics. EGSDE not only consistently outperforms existing SBDMs-based methods in almost all settings but also achieves the SOTA realism results without harming the faithful performance. Furthermore, EGSDE allows for flexible trade-offs between realism and faithfulness and we improve the realism results further (e.g., FID of 51.04 in Cat $\to$ Dog and FID of 50.43 in Wild $\to$ Dog on AFHQ) by tuning hyper-parameters. The code is available at https://github.com/ML-GSAI/EGSDE.",https://api.openreview.net/pdf/dc061a855ccd081dd217865dede7a4a69267f62e.pdf,zero_few-shot;inference;metric;transfer learning;diffusion models,https://scholar.google.com/scholar?q=EGSDE:+Unpaired+Image-to-Image+Translation+via+Energy-Guided+Stochastic+Differential+Equations
Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs,2022,NIPS,"['Cristian Bodnar', 'Francesco Di Giovanni', 'Benjamin Paul Chamberlain', 'Pietro Lio', 'Michael M. Bronstein']",poster,"['sheaf', 'graph', 'neural network', 'diffusion', 'heterophily', 'oversmoothing', 'algebraic topology']","Cellular sheaves equip graphs with a ``geometrical'' structure by assigning vector spaces and linear maps to nodes and edges. Graph Neural Networks (GNNs) implicitly assume a graph with a trivial underlying sheaf. This choice is reflected in the structure of the graph Laplacian operator, the properties of the associated diffusion equation, and the characteristics of the convolutional models that discretise this equation. In this paper, we use cellular sheaf theory to show that the underlying geometry of the graph is deeply linked with the performance of GNNs in heterophilic settings and their oversmoothing behaviour. By considering a hierarchy of increasingly general sheaves, we study how the ability of the sheaf diffusion process to achieve linear separation of the classes in the infinite time limit expands. At the same time, we prove that when the sheaf is non-trivial, discretised parametric diffusion processes have greater control than GNNs over their asymptotic behaviour. On the practical side, we study how sheaves can be learned from data. The resulting sheaf diffusion models have many desirable properties that address the limitations of classical graph diffusion equations (and corresponding GNN models) and obtain competitive results in heterophilic settings. Overall, our work provides new connections between GNNs and algebraic topology and would be of interest to both fields.",https://api.openreview.net/pdf/4d50264651ceea1f282801d3bb8b2696797d254d.pdf,graph;metric;diffusion models,https://scholar.google.com/scholar?q=Neural+Sheaf+Diffusion:+A+Topological+Perspective+on+Heterophily+and+Oversmoothing+in+GNNs
GENIE: Higher-Order Denoising Diffusion Solvers,2022,NIPS,"['Tim Dockhorn', 'Arash Vahdat', 'Karsten Kreis']",poster,"['Diffusion Models', 'Score-based Generative Models', 'Generative Learning', 'ODE Solvers', 'Higher-Order Solvers']","Denoising diffusion models (DDMs) have emerged as a powerful class of generative models. A forward diffusion process slowly perturbs the data, while a deep model learns to gradually denoise. Synthesis amounts to solving a differential equation (DE) defined by the learnt model. Solving the DE requires slow iterative solvers for high-quality generation. In this work, we propose Higher-Order Denoising Diffusion Solvers (GENIE): Based on truncated Taylor methods, we derive a novel higher-order solver that significantly accelerates synthesis. Our solver relies on higher-order gradients of the perturbed data distribution, that is, higher-order score functions. In practice, only Jacobian-vector products (JVPs) are required and we propose to extract them from the first-order score network via automatic differentiation. We then distill the JVPs into a separate neural network that allows us to efficiently compute the necessary higher-order terms for our novel sampler during synthesis. We only need to train a small additional head on top of the first-order score network. We validate GENIE on multiple image generation benchmarks and demonstrate that GENIE outperforms all previous solvers. Unlike recent methods that fundamentally alter the generation process in DDMs, our GENIE solves the true generative DE and still enables applications such as encoding and guided sampling. Project page and code: https://nv-tlabs.github.io/GENIE.",https://api.openreview.net/pdf/8c388937b1f24c5d401925c2c7b5615a4c7e5d2e.pdf,zero_few-shot;generative model;distillation;diffusion models,https://scholar.google.com/scholar?q=GENIE:+Higher-Order+Denoising+Diffusion+Solvers
Can Push-forward Generative Models Fit Multimodal Distributions?,2022,NIPS,"['Antoine Salmona', 'Valentin De Bortoli', 'Julie Delon', 'Agnès Desolneux']",poster,"['Generative Models', 'GAN', 'VAE', 'Diffusion Models', 'Score-based Models', 'Expressivity', 'Lipschitz Mappings']","Many generative models synthesize data by transforming a standard Gaussian random variable using a deterministic neural network. Among these models are the Variational Autoencoders and the Generative Adversarial Networks. In this work, we call them ""push-forward"" models and study their expressivity. We formally demonstrate that the Lipschitz constant of these generative networks has to be large in order to fit multimodal distributions. More precisely, we show that the total variation distance and the Kullback-Leibler divergence between the generated 
and the data distribution are bounded from below by a constant depending on the mode separation and the Lipschitz constant. Since constraining the Lipschitz constants of neural networks is a common way to stabilize generative models, there is a provable trade-off between the ability of push-forward models to approximate multimodal distributions and the stability of their training. We validate our findings on one-dimensional and image datasets and empirically show that the recently introduced diffusion models do not suffer of such limitation.",https://api.openreview.net/pdf/a1a6f024413fa9d8afd17e996b306215f206e7cb.pdf,optimization;zero_few-shot;vae;generative model;multimodal;diffusion models;llm,https://scholar.google.com/scholar?q=Can+Push-forward+Generative+Models+Fit+Multimodal+Distributions?
A Continuous Time Framework for Discrete Denoising Models,2022,NIPS,"['Andrew Campbell', 'Joe Benton', 'Valentin De Bortoli', 'Tom Rainforth', 'George Deligiannidis', 'Arnaud Doucet']",poster,"['diffusion', 'score-based', 'score', 'markov chain', 'markov', 'continuous time']",We provide the first complete continuous time framework for denoising diffusion models of discrete data. This is achieved by formulating the forward noising process and corresponding reverse time generative process as Continuous Time Markov Chains (CTMCs). The model can be efficiently trained using a continuous time version of the ELBO. We simulate the high dimensional CTMC using techniques developed in chemical physics and exploit our continuous time framework to derive high performance samplers that we show can outperform discrete time methods for discrete data. The continuous time treatment also enables us to derive a novel theoretical result bounding the error between the generated sample distribution and the true data distribution.,https://api.openreview.net/pdf/46bfe0f57debf7104404250e39e9a5a81c5d9d4a.pdf,generative model;diffusion models,https://scholar.google.com/scholar?q=A+Continuous+Time+Framework+for+Discrete+Denoising+Models
LION: Latent Point Diffusion Models for 3D Shape Generation,2022,NIPS,"['Xiaohui Zeng', 'Arash Vahdat', 'Francis Williams', 'Zan Gojcic', 'Or Litany', 'Sanja Fidler', 'Karsten Kreis']",poster,"['3D Shape Synthesis', 'Generative Learning', 'Diffusion Models', 'Point Cloud Generation', 'Denoising Diffusion', 'Variational Autoencoder']","Denoising diffusion models (DDMs) have shown promising results in 3D point cloud synthesis. To advance 3D DDMs and make them useful for digital artists, we require (i) high generation quality, (ii) flexibility for manipulation and applications such as conditional synthesis and shape interpolation, and (iii) the ability to output smooth surfaces or meshes. To this end, we introduce the hierarchical Latent Point Diffusion Model (LION) for 3D shape generation. LION is set up as a variational autoencoder (VAE) with a hierarchical latent space that combines a global shape latent representation with a point-structured latent space. For generation, we train two hierarchical DDMs in these latent spaces. The hierarchical VAE approach boosts performance compared to DDMs that operate on point clouds directly, while the point-structured latents are still ideally suited for DDM-based modeling. Experimentally, LION achieves state-of-the-art generation performance on multiple ShapeNet benchmarks. Furthermore, our VAE framework allows us to easily use LION for different relevant tasks: LION excels at multimodal shape denoising and voxel-conditioned synthesis, and it can be adapted for text- and image-driven 3D generation. We also demonstrate shape autoencoding and latent shape interpolation, and we augment LION with modern surface reconstruction techniques to generate smooth 3D meshes. We hope that LION provides a powerful tool for artists working with 3D shapes due to its high-quality generation, flexibility, and surface reconstruction. Project page and code: https://nv-tlabs.github.io/LION.",https://api.openreview.net/pdf/ee4ea47be942e2d2dd1be8180aa6d9674d8beee4.pdf,zero_few-shot;representation;vae;generative model;multimodal;diffusion models;3d,https://scholar.google.com/scholar?q=LION:+Latent+Point+Diffusion+Models+for+3D+Shape+Generation
"MCVD - Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation",2022,NIPS,"['Vikram Voleti', 'Alexia Jolicoeur-Martineau', 'Christopher Pal']",poster,"['score-based denoising diffusion', 'video prediction', 'video generation', 'video interpolation']","Video prediction is a challenging task. The quality of video frames from current state-of-the-art (SOTA) generative models tends to be poor and generalization beyond the training data is difficult. 
Furthermore, existing prediction frameworks are typically not capable of simultaneously handling other video-related tasks such as unconditional generation or interpolation. In this work, we devise a general-purpose framework called Masked Conditional Video Diffusion (MCVD) for all of these video synthesis tasks using a probabilistic conditional score-based denoising diffusion model, conditioned on past and/or future frames. We train the model in a manner where we randomly and independently mask all the past frames or all the future frames. This novel but straightforward setup allows us to train a single model that is capable of executing a broad range of video tasks, specifically: future/past prediction -- when only future/past frames are masked; unconditional generation -- when both past and future frames are masked; and interpolation -- when neither past nor future frames are masked. Our experiments show that this approach can generate high-quality frames for diverse types of videos. Our MCVD models are built from simple non-recurrent 2D-convolutional architectures, conditioning on blocks of frames and generating blocks of frames. We generate videos of arbitrary lengths autoregressively in a block-wise manner. Our approach yields SOTA results across standard video prediction and interpolation benchmarks, with computation times for training models measured in 1-12 days using $\le$ 4 GPUs. 

Project page: \url{https://mask-cond-video-diffusion.github.io}

Code: \url{https://mask-cond-video-diffusion.github.io/}",https://api.openreview.net/pdf/f743b397db99894c0814944df9e860c16db360a2.pdf,graph;zero_few-shot;generative model;diffusion models,"https://scholar.google.com/scholar?q=MCVD+-+Masked+Conditional+Video+Diffusion+for+Prediction,+Generation,+and+Interpolation"
Retrieval-Augmented Diffusion Models,2022,NIPS,"['Andreas Blattmann', 'Robin Rombach', 'Kaan Oktay', 'Jonas Müller', 'Björn Ommer']",poster,"['Image Synthesis', 'Deep Generative Models', 'Retrieval-Augmented Models']","Novel architectures have recently improved generative image synthesis leading to excellent visual quality in various tasks. Much of this success is due to the scalability of these architectures and hence caused by a dramatic increase in model complexity and in the computational resources invested in training these models. Our work questions the underlying paradigm of compressing large training data into ever growing parametric representations. We rather present an orthogonal, semi-parametric approach. We complement comparably small diffusion or autoregressive models with a separate image database and a retrieval strategy. During training we retrieve a set of nearest neighbors from this external database for each training instance and condition the generative model on these informative samples. While the retrieval approach is providing the (local) content, the model is focusing on learning the composition of scenes based on this content. As demonstrated by our experiments, simply swapping the database for one with different contents transfers a trained model post-hoc to a novel domain. The evaluation shows competitive performance on tasks which the generative model has not been trained on, such as class-conditional synthesis, zero-shot stylization or text-to-image synthesis without requiring paired text-image data. With negligible memory and computational overhead for the external database and retrieval we can significantly reduce the parameter count of the generative model and still outperform the state-of-the-art.",https://api.openreview.net/pdf/a15486040667eed798631d0def7fd4311e90b7af.pdf,zero_few-shot;representation;generative model;metric;transfer learning;augmentation;diffusion models,https://scholar.google.com/scholar?q=Retrieval-Augmented+Diffusion+Models
Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,2022,NIPS,"['Chitwan Saharia', 'William Chan', 'Saurabh Saxena', 'Lala Li', 'Jay Whang', 'Emily Denton', 'Seyed Kamyar Seyed Ghasemipour', 'Raphael Gontijo-Lopes', 'Burcu Karagol Ayan\u200e', 'Tim Salimans', 'Jonathan Ho', 'David J. Fleet', 'Mohammad Norouzi']",poster,"['text-to-image', 'generative models', 'diffusion models']","We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g., T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.",https://api.openreview.net/pdf/3c56da71deacc9029fd7714d9e2ecaccc0c38a37.pdf,graph;transformer;generative model;diffusion models;llm,https://scholar.google.com/scholar?q=Photorealistic+Text-to-Image+Diffusion+Models+with+Deep+Language+Understanding
Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models,2022,NIPS,"['Chen Henry Wu', 'Saman Motamed', 'Shaunak Srivastava', 'Fernando De la Torre']",poster,"['generative models', 'energy-based models', 'normalizing flows', 'generative adversarial networks', 'diffusion models', 'amortized inference']","Generative models (e.g., GANs, diffusion models) learn the underlying data distribution in an unsupervised manner. However, many applications of interest require sampling from a particular region of the output space or sampling evenly over a range of characteristics. For efficient sampling in these scenarios, we propose Generative Visual Prompt (PromptGen), a framework for distributional control over pre-trained generative models by incorporating knowledge of other off-the-shelf models. PromptGen defines control as energy-based models (EBMs) and samples images in a feed-forward manner by approximating the EBM with invertible neural networks, avoiding optimization at inference. Our experiments demonstrate how PromptGen can efficiently sample from several unconditional generative models (e.g., StyleGAN2, StyleNeRF, diffusion autoencoder, NVAE) in a controlled or/and de-biased manner using various off-the-shelf models: (1) with the CLIP model as control, PromptGen can sample images guided by text, (2) with image classifiers as control, PromptGen can de-bias generative models across a set of attributes or attribute combinations, and (3) with inverse graphics models as control, PromptGen can sample images of the same identity in different poses. (4) Finally, PromptGen reveals that the CLIP model shows a ""reporting bias"" when used as control, and PromptGen can further de-bias this controlled distribution in an iterative manner. The code is available at https://github.com/ChenWu98/Generative-Visual-Prompt.",https://api.openreview.net/pdf/a661a2f31f6dfc89508deaa6f64534cbcc46f69d.pdf,graph;optimization;vae;generative model;inference;diffusion models;llm,https://scholar.google.com/scholar?q=Generative+Visual+Prompt:+Unifying+Distributional+Control+of+Pre-Trained+Generative+Models
Video Diffusion Models,2022,NIPS,"['Jonathan Ho', 'Tim Salimans', 'Alexey A. Gritsenko', 'William Chan', 'Mohammad Norouzi', 'David J. Fleet']",poster,"['diffusion', 'score', 'video', 'generative', 'text-to-video']","Generating temporally coherent high fidelity video is an important milestone in generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results. Our model is a natural extension of the standard image diffusion architecture, and it enables jointly training from image and video data, which we find to reduce the variance of minibatch gradients and speed up optimization. To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. We present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation. Supplementary material is available at https://video-diffusion.github.io/.",https://api.openreview.net/pdf/18c5870889ea3c88015ded3092711233c64de8e2.pdf,optimization;generative model;diffusion models,https://scholar.google.com/scholar?q=Video+Diffusion+Models
Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models,2022,NIPS,"['Muyang Li', 'Ji Lin', 'Chenlin Meng', 'Stefano Ermon', 'song han', 'Jun-Yan Zhu']",poster,"['Sparse', 'Generative Models', 'Diffusion Models', 'GANs', 'Efficiency']","During image editing, existing deep generative models tend to re-synthesize the entire output from scratch, including the unedited regions. This leads to a significant waste of computation, especially for minor editing operations. In this work, we present Spatially Sparse Inference (SSI), a general-purpose technique that selectively performs computation for edited regions and accelerates various generative models, including both conditional GANs and diffusion models. Our key observation is that users tend to make gradual changes to the input image. This motivates us to cache and reuse the feature maps of the original image. Given an edited image, we sparsely apply the convolutional filters to the edited regions while reusing the cached features for the unedited regions. Based on our algorithm, we further propose Sparse Incremental Generative Engine (SIGE) to convert the computation reduction to latency reduction on off-the-shelf hardware. With 1.2%-area edited regions, our method reduces the computation of DDIM by $7.5\times$ and GauGAN by $18\times$  while preserving the visual fidelity. With SIGE, we accelerate the inference time of DDIM by $3.0\times$ on RTX 3090 and $6.6\times$ on Apple M1 Pro CPU, and GauGAN by $4.2\times$ on RTX 3090 and $14\times$ on Apple M1 Pro CPU.",https://api.openreview.net/pdf/3fcb8c077b7dcef758244270b7529d68d78212cf.pdf,graph;generative model;inference;sparse;diffusion models,https://scholar.google.com/scholar?q=Efficient+Spatially+Sparse+Inference+for+Conditional+GANs+and+Diffusion+Models
GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation,2022,ICLR,"['Minkai Xu', 'Lantao Yu', 'Yang Song', 'Chence Shi', 'Stefano Ermon', 'Jian Tang']",oral,"['molecular conformation generation', 'deep generative models', 'diffusion probabilistic models']","Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules. ",https://api.openreview.net/pdf/d6be0299d7f2d2bf947d450fffe98c8395458c75.pdf,graph;zero_few-shot;generative model;metric;diffusion models,https://scholar.google.com/scholar?q=GeoDiff:+A+Geometric+Diffusion+Model+for+Molecular+Conformation+Generation
Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme,2022,ICLR,"['Vadim Popov', 'Ivan Vovk', 'Vladimir Gogoryan', 'Tasnima Sadekova', 'Mikhail Sergeevich Kudinov', 'Jiansheng Wei']",oral,"['speech', 'voice conversion', 'diffusion models', 'stochastic differential equations']","Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset. We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level. As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis.",https://api.openreview.net/pdf/468145b46e459c5ba69e7017b6ef4eaece277e94.pdf,graph;generative model;diffusion models,https://scholar.google.com/scholar?q=Diffusion-Based+Voice+Conversion+with+Fast+Maximum+Likelihood+Sampling+Scheme
Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality,2022,ICLR,"['Daniel Watson', 'William Chan', 'Jonathan Ho', 'Mohammad Norouzi']",poster,[],"Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-fidelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any pre-trained diffusion model by differentiating through sample quality scores. We also present Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models. We show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. Our optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. DDSS achieves strong results on unconditional image generation across various datasets (e.g., FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any pre-trained diffusion model without fine-tuning or re-training required.",https://api.openreview.net/pdf/56f0145dd15f32bd53f6dba7efde74914a88f663.pdf,optimization;generative model;inference;diffusion models,https://scholar.google.com/scholar?q=Learning+Fast+Samplers+for+Diffusion+Models+by+Differentiating+Through+Sample+Quality
Autoregressive Diffusion Models,2022,ICLR,"['Emiel Hoogeboom', 'Alexey A. Gritsenko', 'Jasmijn Bastings', 'Ben Poole', 'Rianne van den Berg', 'Tim Salimans']",poster,"['diffusion', 'autoregressive models', 'lossless compression']","We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.",https://api.openreview.net/pdf/1e4e9d350d86450e306f14f662b9cdf718fce184.pdf,representation;generative model;diffusion models,https://scholar.google.com/scholar?q=Autoregressive+Diffusion+Models
PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior,2022,ICLR,"['Sang-gil Lee', 'Heeseung Kim', 'Chaehun Shin', 'Xu Tan', 'Chang Liu', 'Qi Meng', 'Tao Qin', 'Wei Chen', 'Sungroh Yoon', 'Tie-Yan Liu']",poster,"['diffusion-based model', 'generative model', 'speech synthesis']","Denoising diffusion probabilistic models have been recently proposed to generate high-quality samples by estimating the gradient of the data density. The framework assumes the prior noise as a standard Gaussian distribution, whereas the corresponding data distribution may be more complicated than the standard Gaussian distribution, which potentially introduces inefficiency in denoising the prior noise into the data sample because of the discrepancy between the data and the prior. In this paper, we propose PriorGrad to improve the efficiency of the conditional diffusion model (for example, a vocoder using a mel-spectrogram as the condition) by applying an adaptive prior derived from the data statistics based on the conditional information. We formulate the training and sampling procedures of PriorGrad and demonstrate the advantages of an adaptive prior through a theoretical analysis. Focusing on the audio domain, we consider the recently proposed diffusion-based audio generative models based on both the spectral and time domains and show that PriorGrad achieves faster convergence and superior performance, leading to an improved perceptual quality and tolerance to a smaller network capacity, and thereby demonstrating the efficiency of a data-dependent adaptive prior.",https://api.openreview.net/pdf/0e602657a19a26315b3e07434feb9b84217302e6.pdf,generative model;adaptive;diffusion models,https://scholar.google.com/scholar?q=PriorGrad:+Improving+Conditional+Denoising+Diffusion+Models+with+Data-Dependent+Adaptive+Prior
Label-Efficient Semantic Segmentation with Diffusion Models,2022,ICLR,"['Dmitry Baranchuk', 'Andrey Voynov', 'Ivan Rubachev', 'Valentin Khrulkov', 'Artem Babenko']",poster,[],"Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance. The superior performance of diffusion models has made them an appealing tool in several applications, including inpainting, super-resolution, and semantic editing. In this paper, we demonstrate that diffusion models can also serve as an instrument for semantic segmentation, especially in the setup when labeled data is scarce. In particular, for several pretrained diffusion models, we investigate the intermediate activations from the networks that perform the Markov step of the reverse diffusion process. We show that these activations effectively capture the semantic information from an input image and appear to be excellent pixel-level representations for the segmentation problem. Based on these observations, we describe a simple segmentation method, which can work even if only a few training images are provided. Our approach significantly outperforms the existing alternatives on several datasets for the same amount of human supervision. ",https://api.openreview.net/pdf/7f702e218df7a81da790cff07136c4f77297f473.pdf,transformer;representation;generative model;segmentation;diffusion models,https://scholar.google.com/scholar?q=Label-Efficient+Semantic+Segmentation+with+Diffusion+Models
SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations,2022,ICLR,"['Chenlin Meng', 'Yutong He', 'Yang Song', 'Jiaming Song', 'Jiajun Wu', 'Jun-Yan Zhu', 'Stefano Ermon']",poster,[],"Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user inputs (e.g., hand-drawn colored strokes) and realism of the synthesized images. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide in a form of manipulating RGB pixels, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing.",https://api.openreview.net/pdf/e3d44fdd105753b51dcb91a908082d6317713ae9.pdf,graph;generative model;diffusion models,https://scholar.google.com/scholar?q=SDEdit:+Guided+Image+Synthesis+and+Editing+with+Stochastic+Differential+Equations
BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis,2022,ICLR,"['Max W. Y. Lam', 'Jun Wang', 'Dan Su', 'Dong Yu']",poster,"['Speech Synthesis', 'Vocoder', 'Generative Model', 'Diffusion Model']","Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efficient sampling. We propose a new bilateral denoising diffusion model (BDDM) that parameterizes both the forward and reverse processes with a schedule network and a score network, which can train with a novel bilateral modeling objective. We show that the new surrogate objective can achieve a lower bound of the log marginal likelihood tighter than a conventional surrogate. We also find that BDDM allows inheriting pre-trained score network parameters from any DPMs and consequently enables speedy and stable learning of the schedule network and optimization of a noise schedule for sampling.
Our experiments demonstrate that BDDMs can generate high-fidelity audio samples with as few as three sampling steps. Moreover, compared to other state-of-the-art diffusion-based neural vocoders, BDDMs produce comparable or higher quality samples indistinguishable from human speech, notably with only seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave). We release our code at https://github.com/tencent-ailab/bddm.",https://api.openreview.net/pdf/8ac09c0c92b7fc2905b7bbd470f4a2a4607be605.pdf,graph;optimization;zero_few-shot;generative model;diffusion models,https://scholar.google.com/scholar?q=BDDM:+Bilateral+Denoising+Diffusion+Models+for+Fast+and+High-Quality+Speech+Synthesis
Pseudo Numerical Methods for Diffusion Models on Manifolds,2022,ICLR,"['Luping Liu', 'Yi Ren', 'Zhijie Lin', 'Zhou Zhao']",poster,"['diffusion model', 'generative model', 'numerical method', 'manifold']","Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples. However, DDPMs require hundreds to thousands of iterations to produce a sample. Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising equation (e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these acceleration methods cannot maintain the quality of samples and even introduce new noise at high speedup rate, which limit their practicability. To accelerate the inference process while keeping the sample quality, we provide a new perspective that DDPMs should be treated as solving differential equations on manifolds. Under such a perspective, we propose pseudo numerical methods for diffusion models (PNDMs). Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods. We change several classical numerical methods to corresponding pseudo numerical methods and find that pseudo linear multi-step method is the best method in most situations. According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules.",https://api.openreview.net/pdf/08c071f50f706076294158af529e4f1a2556df41.pdf,inference;diffusion models,https://scholar.google.com/scholar?q=Pseudo+Numerical+Methods+for+Diffusion+Models+on+Manifolds
Score-Based Generative Modeling with Critically-Damped Langevin Diffusion,2022,ICLR,"['Tim Dockhorn', 'Arash Vahdat', 'Karsten Kreis']",spotlight,"['Score-based generative modeling', 'denoising diffusion models', 'image synthesis']","Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered ""velocities"" that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models. We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD significantly outperforms solvers such as Euler–Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. Project page and code: https://nv-tlabs.github.io/CLD-SGM.",https://api.openreview.net/pdf/b0abf219a709e60170ee3d79e0068b682e8696dd.pdf,generative model;diffusion models,https://scholar.google.com/scholar?q=Score-Based+Generative+Modeling+with+Critically-Damped+Langevin+Diffusion
Progressive Distillation for Fast Sampling of Diffusion Models,2022,ICLR,"['Tim Salimans', 'Jonathan Ho']",spotlight,"['Diffusion Models', 'Generative Models', 'fast sampling']","Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps, compared to models in the literature. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with (near) state-of-the-art samplers taking 1024 or 8192 steps, and are able to distill down to models taking as little as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time.",https://api.openreview.net/pdf/3b30857a628099896b6123e85d6cf04c59abe77b.pdf,graph;zero_few-shot;generative model;distillation;diffusion models,https://scholar.google.com/scholar?q=Progressive+Distillation+for+Fast+Sampling+of+Diffusion+Models
Tackling the Generative Learning Trilemma with Denoising Diffusion GANs,2022,ICLR,"['Zhisheng Xiao', 'Karsten Kreis', 'Arash Vahdat']",spotlight,[],"A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000$\times$ faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively.",https://api.openreview.net/pdf/6570cfc5a990e77af23d8a4c6b934ac249ba4426.pdf,graph;zero_few-shot;generative model;multimodal;diffusion models,https://scholar.google.com/scholar?q=Tackling+the+Generative+Learning+Trilemma+with+Denoising+Diffusion+GANs
Protein Sequence and Structure Co-Design with Equivariant Translation,2023,ICLR,"['Chence Shi', 'Chuanrui Wang', 'Jiarui Lu', 'Bozitao Zhong', 'Jian Tang']",poster,"['protein design', 'sequence structure co-design', 'equivariant translation', 'geometric deep learning']","Proteins are macromolecules that perform essential functions in all living organisms. Designing novel proteins with specific structures and desired functions has been a long-standing challenge in the field of bioengineering. Existing approaches generate both protein sequence and structure using either autoregressive models or diffusion models, both of which suffer from high inference costs. In this paper, we propose a new approach capable of protein sequence and structure co-design, which iteratively translates both protein sequence and structure into the desired state from random initialization, based on context features given a priori. Our model consists of a trigonometry-aware encoder that reasons geometrical constraints and interactions from context features, and a roto-translation equivariant decoder that translates protein sequence and structure interdependently. Notably, all protein amino acids are updated in one shot in each translation step, which significantly accelerates the inference process. Experimental results across multiple tasks show that our model outperforms previous state-of-the-art baselines by a large margin, and is able to design proteins of high fidelity as regards both sequence and structure, with running time orders of magnitude less than sampling-based methods.",https://api.openreview.net/pdf/eb4c9a122dd34e10a2a9eb419a4ba937bd7b32e5.pdf,graph;optimization;inference;metric;diffusion models,https://scholar.google.com/scholar?q=Protein+Sequence+and+Structure+Co-Design+with+Equivariant+Translation
Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis,2023,ICLR,"['Weixi Feng', 'Xuehai He', 'Tsu-Jui Fu', 'Varun Jampani', 'Arjun Reddy Akula', 'Pradyumna Narayana', 'Sugato Basu', 'Xin Eric Wang', 'William Yang Wang']",poster,"['Text-to-Image Synthesis', 'Diffusion Models', 'Compositional Generation']","Large-scale diffusion models have achieved state-of-the-art results on text-to-image synthesis (T2I) tasks. Despite their ability to generate high-quality yet creative images, we observe that attribution-binding and compositional capabilities are still considered major challenging issues, especially when involving multiple objects. Attribute-binding requires the model to associate objects with the correct attribute descriptions, and compositional skills require the model to combine and generate multiple concepts into a single image. In this work, we improve these two aspects of T2I models to achieve more accurate image compositions. To do this, we incorporate linguistic structures with the diffusion guidance process based on the controllable properties of manipulating cross-attention layers in diffusion-based T2I models. We observe that keys and values in cross-attention layers have strong semantic meanings associated with object layouts and content. Therefore, by manipulating the cross-attention representations based on linguistic insights, we can better preserve the compositional semantics in the generated image. Built upon Stable Diffusion, a SOTA T2I model, our structured cross-attention design is efficient that requires no additional training samples. We achieve better compositional skills in qualitative and quantitative results, leading to a significant 5-8\% advantage in head-to-head user comparison studies. Lastly, we conduct an in-depth analysis to reveal potential causes of incorrect image compositions and justify the properties of cross-attention layers in the generation process. ",https://api.openreview.net/pdf/e1ae37e998417bc8a2fe61c08e82494f2db8b53e.pdf,graph;transformer;representation;generative model;diffusion models,https://scholar.google.com/scholar?q=Training-Free+Structured+Diffusion+Guidance+for+Compositional+Text-to-Image+Synthesis
Diffusion-based Image Translation using disentangled style and content representation,2023,ICLR,"['Gihyun Kwon', 'Jong Chul Ye']",poster,"['DDPM', 'CLIP', 'Image Translation', 'ViT']","Diffusion-based image translation guided by  semantic texts   or a single target image   has enabled flexible style transfer which is not limited to the specific domains. 
Unfortunately, due to the stochastic nature of diffusion models, it is often  difficult to maintain the original content of the image  during the reverse diffusion.
To address this, here we present a novel diffusion-based unsupervised image translation method, dubbed as DiffuseIT, using disentangled style and content representation.
 Specifically, inspired by the  slicing Vision Transformer, we extract intermediate keys of multihead self attention layer  from ViT model and used them as the content preservation loss. Then, an image guided style transfer is performed by matching the [CLS] classification token from the denoised samples and target image, whereas additional CLIP loss is used for the text-driven style transfer.
  To further accelerate the semantic change during the reverse  diffusion, we also propose a novel semantic divergence loss and resampling strategy. 
 Our experimental results show that the proposed method outperforms state-of-the-art baseline models in both text-guided and image-guided translation tasks. ",https://api.openreview.net/pdf/b3174c74984a2e90538982e09e40225a474d34e0.pdf,graph;transformer;representation;transfer learning;diffusion models;llm,https://scholar.google.com/scholar?q=Diffusion-based+Image+Translation+using+disentangled+style+and+content+representation
Discrete Predictor-Corrector Diffusion Models for Image Synthesis,2023,ICLR,"['Jose Lezama', 'Tim Salimans', 'Lu Jiang', 'Huiwen Chang', 'Jonathan Ho', 'Irfan Essa']",poster,"['discrete diffusion models', 'image synthesis']","We introduce Discrete Predictor-Corrector diffusion models (DPC), extending predictor-corrector samplers in Gaussian diffusion models to the discrete case. Predictor-corrector samplers are a class of samplers for diffusion models, which improve on ancestral samplers by correcting the sampling distribution of intermediate diffusion states using MCMC methods. In DPC, the Langevin corrector, which does not have a direct counterpart in discrete space, is replaced with a discrete MCMC transition defined by a learned corrector kernel. The corrector kernel is trained to make the correction steps achieve asymptotic convergence, in distribution, to the correct marginal of the intermediate diffusion states. Equipped with DPC, we revisit recent transformer-based  non-autoregressive generative models through the lens of discrete diffusion, and find that DPC can alleviate the compounding decoding error due to the parallel sampling of visual tokens. Our experiments show that DPC improves upon existing discrete latent space models for class-conditional image generation on ImageNet, and outperforms continuous diffusion models and GANs, according to standard metrics and user preference studies.",https://api.openreview.net/pdf/11def978889827cebd99ce8e154f1d7a8ba62dde.pdf,graph;transformer;generative model;metric;diffusion models,https://scholar.google.com/scholar?q=Discrete+Predictor-Corrector+Diffusion+Models+for+Image+Synthesis
Building Normalizing Flows with Stochastic Interpolants,2023,ICLR,"['Michael Samuel Albergo', 'Eric Vanden-Eijnden']",poster,"['normalizing flow', 'ODE', 'generative model']","A generative model based on a continuous-time normalizing flow between any pair of base and target probability densities is proposed. The velocity field of this flow is inferred from the probability current of a time-dependent density that interpolates between the base and the target in finite time. Unlike conventional normalizing flow inference methods based the maximum likelihood principle, which require costly backpropagation through ODE solvers, our interpolant approach leads to a simple quadratic loss for the velocity itself which is expressed in terms of expectations that are readily amenable to empirical estimation. The flow can be used to generate samples from either the base or target, and to estimate the likelihood at any time along the interpolant. In addition, the flow can be optimized to minimize the path length of the interpolant density, thereby paving the way for building optimal transport maps. In situations where the base is a Gaussian density, we also show that the velocity of our normalizing flow can also be used to construct a diffusion model to sample the target as well as estimate its score. However, our approach shows that we can bypass this diffusion completely and work at the level of the probability flow with greater simplicity, opening an avenue for methods based solely on ordinary differential equations as an alternative to those based on stochastic differential equations. Benchmarking on density estimation tasks illustrates that the learned flow can match and surpass conventional continuous flows at a fraction of the cost, and compares well with diffusions on image generation on CIFAR-10 and ImageNet $32 \times 32$. The method scales ab-initio ODE flows to previously unreachable image resolutions, demonstrated up to $128\times128$.",https://api.openreview.net/pdf/45d09331dca3b57160a64f0922ee4841585e15b5.pdf,zero_few-shot;generative model;inference;flow;diffusion models,https://scholar.google.com/scholar?q=Building+Normalizing+Flows+with+Stochastic+Interpolants
Information-Theoretic Diffusion,2023,ICLR,"['Xianghao Kong', 'Rob Brekelmans', 'Greg Ver Steeg']",poster,"['diffusion', 'density models', 'information theory']","Denoising diffusion models have spurred significant gains in density modeling and image generation, precipitating an industrial revolution in text-guided AI art generation.  We introduce a new mathematical foundation for diffusion models inspired by classic results in information theory that connect Information with Minimum Mean Square Error regression, the so-called I-MMSE relations. We generalize the I-MMSE relations to \emph{exactly} relate the data distribution to an optimal denoising regression problem, leading to an elegant refinement of existing diffusion bounds.  This new insight leads to several improvements for probability distribution estimation, including a theoretical justification for diffusion model ensembling. Remarkably, our framework shows how continuous and discrete probabilities can be learned with the same regression objective, avoiding domain-specific generative models used in variational methods.",https://api.openreview.net/pdf/b48d4f49696c822ea5644e2ba131c474e0f8c770.pdf,generative model;diffusion models,https://scholar.google.com/scholar?q=Information-Theoretic+Diffusion
Fast Sampling of Diffusion Models with Exponential Integrator,2023,ICLR,"['Qinsheng Zhang', 'Yongxin Chen']",poster,"['Fast diffusion model', 'generative model']","The past few years have witnessed the great success of Diffusion models~(DMs) in generating high-fidelity samples in generative modeling tasks. A major limitation of the DM is its notoriously slow sampling procedure which normally requires hundreds to thousands of time discretization steps of the learned diffusion process to reach the desired accuracy. Our goal is to develop a fast sampling method for DMs with a much less number of steps while retaining high sample quality. To this end, we systematically analyze the sampling procedure in DMs and identify key factors that affect the sample quality, among which the method of discretization is most crucial. By carefully examining the learned diffusion process, we propose Diffusion Exponential Integrator Sampler~(DEIS). It is based on the Exponential Integrator designed for discretizing ordinary differential equations (ODEs) and leverages a semilinear structure of the learned diffusion process to reduce the discretization error. The proposed method can be applied to any DMs and can generate high-fidelity samples in as few as 10 steps. Moreover, by directly using pre-trained DMs, we achieve state-of-art sampling performance when the number of score function evaluation~(NFE) is limited,  e.g., 4.17 FID with 10 NFEs, 2.86 FID with only 20 NFEs on CIFAR10.",https://api.openreview.net/pdf/0c36e66f4501ee45a0d7b31d2e7ed21e58c4c628.pdf,zero_few-shot;generative model;diffusion models,https://scholar.google.com/scholar?q=Fast+Sampling+of+Diffusion+Models+with+Exponential+Integrator
Denoising Diffusion Samplers,2023,ICLR,"['Francisco Vargas', 'Will Sussman Grathwohl', 'Arnaud Doucet']",poster,"['diffusion models', 'importance sampling', 'monte carlo', 'variational inference']","Denoising diffusion models are a popular class of generative models providing state-of-the-art results in many domains. One adds gradually noise to data using a diffusion to transform the data distribution into a Gaussian distribution. Samples from the generative model are then obtained by simulating an approximation of the time-reversal of this diffusion initialized by Gaussian samples. Practically, the intractable score terms appearing in the time-reversed process are approximated using score matching techniques. We explore here a similar idea to sample approximately from unnormalized probability density functions and estimate their normalizing constants. We consider a process where the target density diffuses towards a Gaussian. Denoising Diffusion Samplers (DDS) are obtained by approximating the corresponding time-reversal.  While score matching is not applicable in this context, we can leverage many of the ideas introduced in generative modeling for Monte Carlo sampling. Existing theoretical results from denoising diffusion models also provide theoretical guarantees for DDS. We discuss the connections between DDS, optimal control and Schr\""odinger bridges and finally demonstrate DDS experimentally on a variety of challenging sampling tasks.",https://api.openreview.net/pdf/0daf337159fe25b03ca3fa62e0e5b86b14cbd4b7.pdf,graph;generative model;diffusion models,https://scholar.google.com/scholar?q=Denoising+Diffusion+Samplers
f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation,2023,ICLR,"['Jiatao Gu', 'Shuangfei Zhai', 'Yizhe Zhang', 'Miguel Ángel Bautista', 'Joshua M. Susskind']",poster,"['diffusion models', 'progressive signal transformation']","Diffusion models (DMs) have recently emerged as SoTA tools for generative modeling in various domains. Standard DMs can be viewed as an instantiation of hierarchical variational autoencoders (VAEs) where the latent variables are inferred from input-centered Gaussian distributions with fixed scales and variances. Unlike VAEs, this formulation constrains DMs from changing the latent spaces and
learning abstract representations. In this work, we propose f-DM, a generalized family of DMs which allows progressive signal transformation. More precisely, we extend DMs to incorporate a set of (hand-designed or learned) transformations, where the transformed input is the mean of each diffusion step. We propose a generalized formulation and derive the corresponding de-noising objective with a modified sampling algorithm. As a demonstration, we apply f-DM in image generation tasks with a range of functions, including down-sampling, blurring, and learned transformations based on the encoder of pretrained VAEs. In addition, we identify the importance of adjusting the noise levels whenever the signal is sub-sampled and propose a simple rescaling recipe. f-DM can produce high-quality samples on standard image generation benchmarks like FFHQ, AFHQ, LSUN, and ImageNet with better efficiency and semantic interpretation.",https://api.openreview.net/pdf/b7219af3a9c6c0c0add8ae9fa74bae7fd3cc27f6.pdf,graph;optimization;zero_few-shot;transformer;representation;vae;generative model;diffusion models,https://scholar.google.com/scholar?q=f-DM:+A+Multi-stage+Diffusion+Model+via+Progressive+Signal+Transformation
Score-based Continuous-time Discrete Diffusion Models,2023,ICLR,"['Haoran Sun', 'Lijun Yu', 'Bo Dai', 'Dale Schuurmans', 'Hanjun Dai']",poster,"['discrete space diffusion', 'discrete score matching', 'continuous-time diffusion']","Score-based modeling through stochastic differential equations (SDEs) has provided a new perspective on diffusion models, and demonstrated superior performance on continuous data. However, the gradient of the log-likelihood function, \ie, the score function, is not properly defined for discrete spaces. This makes it non-trivial to adapt SDE with score functions to categorical data. In this paper, we extend diffusion models to discrete variables by introducing a stochastic jump process where the reverse process denoises via a continuous-time Markov chain. This formulation admits an analytical simulation during backward sampling. To learn the reverse process, we extend score matching to general categorical data, and show that an unbiased estimator can be obtained via simple matching of the conditional marginal distributions. We demonstrate the effectiveness of the proposed method on a set of synthetic and real-world music and image benchmarks.",https://api.openreview.net/pdf/d96e90543a3d04a3e6248eacc3088b15b0907078.pdf,graph;diffusion models,https://scholar.google.com/scholar?q=Score-based+Continuous-time+Discrete+Diffusion+Models
Images as Weight Matrices: Sequential Image Generation Through Synaptic Learning Rules,2023,ICLR,"['Kazuki Irie', 'Jürgen Schmidhuber']",poster,"['learning rules', 'Fast Weight Programmers', 'linear Transformers', 'image generation', 'GANs']","Work on fast weight programmers has demonstrated the effectiveness of key/value outer product-based learning rules for sequentially generating a weight matrix (WM) of a neural net (NN) by another NN or itself. However, the weight generation steps are typically not visually interpretable by humans, because the contents stored in the WM of an NN are not. Here we apply the same principle to generate natural images. The resulting fast weight painters (FPAs) learn to execute sequences of delta learning rules to sequentially generate images as sums of outer products of self-invented keys and values, one rank at a time, as if each image was a WM of an NN. We train our FPAs in the generative adversarial networks framework, and evaluate on various image datasets. We show how these generic learning rules can generate images with respectable visual quality without any explicit inductive bias for images. While the performance largely lags behind the one of specialised state-of-the-art image generators, our approach allows for visualising how synaptic learning rules iteratively produce complex connection patterns, yielding human-interpretable meaningful images. Finally, we also show that an additional convolutional U-Net (now popular in diffusion models) at the output of an FPA can learn one-step ""denoising"" of FPA-generated images to enhance their quality.
Our code is public.",https://api.openreview.net/pdf/aa76486a805137c091d1ab926caf23997d179f30.pdf,zero_few-shot;generative model;diffusion models,https://scholar.google.com/scholar?q=Images+as+Weight+Matrices:+Sequential+Image+Generation+Through+Synaptic+Learning+Rules
DensePure: Understanding Diffusion Models for Adversarial Robustness,2023,ICLR,"['Chaowei Xiao', 'Zhongzhu Chen', 'Kun Jin', 'Jiongxiao Wang', 'Weili Nie', 'Mingyan Liu', 'Anima Anandkumar', 'Bo Li', 'Dawn Song']",poster,"['adversarial robustness', 'certified robustness', 'diffusion model']","Diffusion models have been recently employed to  improve certified robustness through the process of denoising.  However, the theoretical understanding of why diffusion models are able to improve the certified robustness is still lacking, preventing from further improvement.  In this study, we close this gap by analyzing the fundamental properties of diffusion models and  establishing the conditions under which they can enhance certified robustness. This deeper understanding allows us to propose a new method   DensePure,  designed to improve the certified robustness of a pretrained model (i.e. classifier).   Given an (adversarial) input, DensePure consists of multiple runs of denoising via the reverse process of the diffusion model (with different random seeds) to get multiple reversed samples, which are then passed through the classifier, followed by majority voting of inferred labels to make the final prediction.  This design of using multiple runs of denoising is informed by our theoretical analysis of the conditional distribution of the reversed  sample. Specifically, when the data density of a clean sample is high, its conditional density under the reverse process in a diffusion model is also high;  thus sampling from the latter conditional distribution can purify the adversarial example and return the corresponding clean sample with a high probability. By using the highest density point in the conditional distribution as the reversed sample, we identify the robust region of a given instance under the diffusion model's reverse process. We show that this robust region is a union of multiple convex sets, and is potentially much larger than the robust regions identified in previous works. In practice, DensePure can approximate the label of the high density region in the conditional distribution so that it can enhance certified robustness. We conduct extensive experiments to demonstrate the effectiveness of DensePure by evaluating its certified robustness  given a standard model via randomized smoothing. We show that DensePure is consistently better than existing methods on ImageNet, with 7% improvement on average. ",https://api.openreview.net/pdf/d8526f8f386272bc4382c378f94069337bd8be63.pdf,zero_few-shot;diffusion models,https://scholar.google.com/scholar?q=DensePure:+Understanding+Diffusion+Models+for+Adversarial+Robustness
"Where to Diffuse, How to Diffuse, and How to Get Back: Automated Learning for Multivariate Diffusions",2023,ICLR,"['Raghav Singhal', 'Mark Goldstein', 'Rajesh Ranganath']",poster,"['Diffusion models', 'score based generative model', 'generative models', 'variational inference']","Diffusion-based generative models (DBGMs) perturb data to a target noise distribution and reverse this process to generate samples.  The choice of noising process, or inference diffusion process, affects both likelihoods and sample quality. For example, extending the inference process with auxiliary variables leads to improved sample quality. While there are many such multivariate diffusions to explore, each new one requires significant model-specific analysis, hindering rapid prototyping and evaluation. In this work, we study Multivariate Diffusion Models (MDMs). For any number of auxiliary variables, we provide a recipe for maximizing a lower-bound on the MDMs likelihood without requiring any model-specific analysis. We then demonstrate how to parameterize the diffusion for a specified target noise distribution; these two points together enable optimizing the inference diffusion process. Optimizing the diffusion expands easy experimentation from just a few well-known processes to an automatic search over all linear diffusions. To demonstrate these ideas, we introduce two new specific diffusions as well as learn a diffusion process on the MNIST, CIFAR10, and ImageNet32 datasets. We show learned MDMs match or surpass bits-per-dims (BPDs) relative to fixed choices of diffusions for a given dataset and model architecture.",https://api.openreview.net/pdf/f14263e154b403508bc9610cec0745bc42f8b9e7.pdf,zero_few-shot;generative model;inference;diffusion models,"https://scholar.google.com/scholar?q=Where+to+Diffuse,+How+to+Diffuse,+and+How+to+Get+Back:+Automated+Learning+for+Multivariate+Diffusions"
3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction,2023,ICLR,"['Jiaqi Guan', 'Wesley Wei Qian', 'Xingang Peng', 'Yufeng Su', 'Jian Peng', 'Jianzhu Ma']",poster,[],"Rich data and powerful machine learning models allow us to design drugs for a specific protein target <em>in silico</em>. Recently, the inclusion of 3D structures during targeted drug design shows superior performance to other target-free models as the atomic interaction in the 3D space is explicitly modeled. However, current 3D target-aware models either rely on the voxelized atom densities or the autoregressive sampling process, which are not equivariant to rotation or easily violate geometric constraints resulting in unrealistic structures. In this work, we develop a 3D equivariant diffusion model to solve the above challenges. To achieve target-aware molecule design, our method learns a joint generative process of both continuous atom coordinates and categorical atom types with a SE(3)-equivariant network. Moreover, we show that our model can serve as an unsupervised feature extractor to estimate the binding affinity under proper parameterization, which provides an effective way for drug screening. To evaluate our model, we propose a comprehensive framework to evaluate the quality of sampled molecules from different dimensions. Empirical studies show our model could generate molecules with more realistic 3D structures and better affinities towards the protein targets, and improve binding affinity ranking and prediction without retraining.",https://api.openreview.net/pdf/7d2f5804455f227aebb0b02ed88a71d50f3ad49a.pdf,optimization;zero_few-shot;generative model;metric;diffusion models;3d,https://scholar.google.com/scholar?q=3D+Equivariant+Diffusion+for+Target-Aware+Molecule+Generation+and+Affinity+Prediction
Pseudoinverse-Guided Diffusion Models for Inverse Problems,2023,ICLR,"['Jiaming Song', 'Arash Vahdat', 'Morteza Mardani', 'Jan Kautz']",poster,"['diffusion models', 'inverse problems']","Diffusion models have become competitive candidates for solving various inverse problems. Models trained for specific inverse problems work well but are limited to their particular use cases, whereas methods that use problem-agnostic models are general but often perform worse empirically. To address this dilemma, we introduce Pseudoinverse-guided Diffusion Models ($\Pi$GDM), an approach that uses problem-agnostic models to close the gap in performance. $\Pi$GDM directly estimates conditional scores from the measurement model of the inverse problem without additional training. It can address inverse problems with noisy, non-linear, or even non-differentiable measurements, in contrast to many existing approaches that are limited to noiseless linear ones. We illustrate the empirical effectiveness of $\Pi$GDM on several image restoration tasks, including super-resolution, inpainting and JPEG restoration. On ImageNet, $\Pi$GDM is competitive with state-of-the-art diffusion models trained on specific tasks, and is the first to achieve this with problem-agnostic diffusion models. $\Pi$GDM can also solve a wider set of inverse problems where the measurement processes are composed of several simpler ones.",https://api.openreview.net/pdf/210093330709030207aa90dbfe2a1f525ac5fb7d.pdf,diffusion models,https://scholar.google.com/scholar?q=Pseudoinverse-Guided+Diffusion+Models+for+Inverse+Problems
(Certified!!) Adversarial Robustness for Free!,2023,ICLR,"['Nicholas Carlini', 'Florian Tramer', 'Krishnamurthy Dj Dvijotham', 'Leslie Rice', 'Mingjie Sun', 'J Zico Kolter']",poster,[],"In this paper we show how to achieve state-of-the-art certified adversarial robustness to 2-norm bounded perturbations by relying exclusively on off-the-shelf pretrained models. To do so, we instantiate the denoised smoothing approach of Salman et al. by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This allows us to certify 71% accuracy on ImageNet under adversarial perturbations constrained to be within a 2-norm of 0.5, an improvement of 14 percentage points over the prior certified SoTA using any approach, or an improvement of 30 percentage points over denoised smoothing. We obtain these results using only pretrained diffusion models and image classifiers, without requiring any fine tuning or retraining of model parameters.",https://api.openreview.net/pdf/ffd8346924f2b5c29c98d9eebd37fde97ba3461a.pdf,optimization;zero_few-shot;diffusion models,https://scholar.google.com/scholar?q=(Certified!!)+Adversarial+Robustness+for+Free!
Understanding DDPM Latent Codes Through Optimal Transport,2023,ICLR,"['Valentin Khrulkov', 'Gleb Ryzhakov', 'Andrei Chertkov', 'Ivan Oseledets']",poster,"['diffusion models', 'ddpm', 'optimal transport', 'theory']","Diffusion models have recently outperformed alternative approaches to model the distribution of natural images. Such diffusion models allow for deterministic sampling via the probability flow ODE, giving rise to a latent space and an encoder map. While having important practical applications, such as the estimation of the likelihood, the theoretical properties of this map are not yet fully understood. In the present work, we partially address this question for the popular case of the VP-SDE (DDPM) approach. We show that, perhaps surprisingly, the DDPM encoder map coincides with the optimal transport map for common distributions; we support this claim by extensive numerical experiments using advanced tensor train solver for multidimensional Fokker-Planck equation. We provide additional theoretical evidence for the case of multivariate normal distributions.",https://api.openreview.net/pdf/05a5b6328c16d6e05741af0692ecdb4edd6955b9.pdf,zero_few-shot;flow;diffusion models,https://scholar.google.com/scholar?q=Understanding+DDPM+Latent+Codes+Through+Optimal+Transport
Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning,2023,ICLR,"['Zhendong Wang', 'Jonathan J Hunt', 'Mingyuan Zhou']",poster,"['offline RL', 'diffusion models', 'behavior cloning', 'policy regularization', 'Q-learning']","Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Q-learning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness of the diffusion model-based policy, and the coupling of the behavior cloning and policy improvement under the diffusion model both contribute to the outstanding performance of Diffusion-QL. We illustrate the superiority of our method compared to prior works in a simple 2D bandit example with a multimodal behavior policy. We then show that our method can achieve state-of-the-art performance on the majority of the D4RL benchmark tasks.",https://api.openreview.net/pdf/a55a26f03a11dcb7991b40527008d900c4a75044.pdf,reinforcement learning;offline reinforcement learning;optimization;generative model;multimodal;diffusion models,https://scholar.google.com/scholar?q=Diffusion+Policies+as+an+Expressive+Policy+Class+for+Offline+Reinforcement+Learning
Imitating Human Behaviour with Diffusion Models,2023,ICLR,"['Tim Pearce', 'Tabish Rashid', 'Anssi Kanervisto', 'Dave Bignell', 'Mingfei Sun', 'Raluca Georgescu', 'Sergio Valcarcel Macua', 'Shan Zheng Tan', 'Ida Momennejad', 'Katja Hofmann', 'Sam Devlin']",poster,"['imitation learning', 'behavioral cloning', 'behavioral cloning', 'diffusion models', 'generative models']","Diffusion models have emerged as powerful generative models in the text-to-image domain. This paper studies their application as observation-to-action models for imitating human behaviour in sequential environments. Human behaviour is stochastic and multimodal, with structured correlations between action dimensions. Meanwhile, standard modelling choices in behaviour cloning are limited in their expressiveness and may introduce bias into the cloned policy. We begin by pointing out the limitations of these choices. We then propose that diffusion models are an excellent fit for imitating human behaviour, since they learn an expressive distribution over the joint action space. We introduce several innovations to make diffusion models suitable for sequential environments; designing suitable architectures, investigating the role of guidance, and developing reliable sampling strategies. Experimentally, diffusion models closely match human demonstrations in a simulated robotic control task and a modern 3D gaming environment.",https://api.openreview.net/pdf/b6515fc810687c08e9e8cbc72cd22ef8e7cdc7b6.pdf,graph;generative model;multimodal;diffusion models;3d,https://scholar.google.com/scholar?q=Imitating+Human+Behaviour+with+Diffusion+Models
DiffusER: Diffusion via Edit-based Reconstruction,2023,ICLR,"['Machel Reid', 'Vincent Josua Hellendoorn', 'Graham Neubig']",poster,"['text generation', 'editing', 'denoising autoencoders']","In text generation, models that generate text from scratch one token at a time are currently the dominant paradigm. Despite being performant, these models lack the ability to revise existing text, which limits their usability in many practical scenarios. We look to address this, with DiffusER (Diffusion via Edit-based Reconstruction), a new edit-based generative model for text based on denoising diffusion models -- a class of models that use a Markov chain of denoising steps to incrementally generate data. DiffusER is not only a strong generative model in general, rivalling autoregressive models on several tasks spanning machine translation, summarization, and style transfer; it can also perform other varieties of generation that standard autoregressive models are not well-suited for. For instance, we demonstrate that DiffusER makes it possible for a user to condition generation on a prototype, or an incomplete sequence, and continue revising based on previous edit steps.",https://api.openreview.net/pdf/e35954a4ede0520dd7f8d02f039f79a60a4eb6f9.pdf,generative model;transfer learning;diffusion models,https://scholar.google.com/scholar?q=DiffusER:+Diffusion+via+Edit-based+Reconstruction
Defending against Adversarial Audio  via Diffusion Model,2023,ICLR,"['Shutong Wu', 'Jiongxiao Wang', 'Wei Ping', 'Weili Nie', 'Chaowei Xiao']",poster,"['Adversarial attack and defense', 'AI security', 'speech recognition', 'diffusion models']","Deep learning models have been widely used in commercial acoustic systems in recent years. However, adversarial audio examples can cause abnormal behaviors for those acoustic systems, while being hard for humans to perceive. Various methods, such as transformation-based defenses and adversarial training, have been proposed to protect acoustic systems from adversarial attacks, but they are less effective against adaptive attacks. Furthermore, directly applying the methods from the image domain can lead to suboptimal results because of the unique properties of audio data. In this paper, we propose an adversarial purification-based defense pipeline, AudioPure, for acoustic systems via off-the-shelf diffusion models. Taking advantage of the strong generation ability of diffusion models, AudioPure first adds a small amount of noise to the adversarial audio and then runs the reverse sampling step to purify the noisy audio and recover clean audio. AudioPure is a plug-and-play method that can be directly applied to any pretrained classifier without any fine-tuning or re-training. We conduct extensive experiments on the speech command recognition task to evaluate the robustness of AudioPure. Our method is effective against diverse adversarial attacks (e.g. L2 or L∞-norm). It outperforms the existing methods under both strong adaptive white-box and black-box attacks bounded by L2 or L∞-norm (up to +20% in robust accuracy). Besides, we also evaluate the certified robustness for perturbations bounded by L2-norm via randomized smoothing. Our pipeline achieves a higher certified accuracy than baselines.",https://api.openreview.net/pdf/f495c649b51716e4d5681e1db282a9df4c7ad03c.pdf,generative model;adaptive;diffusion models,https://scholar.google.com/scholar?q=Defending+against+Adversarial+Audio++via+Diffusion+Model
Quantized Compressed Sensing with Score-Based Generative Models,2023,ICLR,"['Xiangming Meng', 'Yoshiyuki Kabashima']",poster,"['generative models', 'compressed sensing', 'linear inverse problems', 'quantization']","We consider the general problem of recovering a high-dimensional signal from noisy quantized measurements. Quantization, especially coarse quantization such as 1-bit sign measurements, leads to severe information loss and thus a good prior knowledge of the unknown signal is helpful for accurate recovery. Motivated by the power of score-based generative models (SGM, also known as diffusion models) in capturing the rich structure of natural signals beyond simple sparsity, we propose an unsupervised data-driven approach called quantized compressed sensing with SGM (QCS-SGM), where the prior distribution is modeled by a pre-trained SGM. To perform posterior sampling, an annealed pseudo-likelihood score called ${\textit{noise perturbed pseudo-likelihood score}}$ is introduced and combined with the prior score of SGM. The proposed QCS-SGM applies to an arbitrary number of quantization bits. Experiments on a variety of baseline datasets demonstrate that the proposed QCS-SGM significantly outperforms existing state-of-the-art algorithms by a large margin for both in-distribution and out-of-distribution samples. Moreover, as a posterior sampling method, QCS-SGM can be easily used to obtain confidence intervals or uncertainty estimates of the reconstructed results. $\textit{The code  is available at}$ https://github.com/mengxiangming/QCS-SGM.",https://api.openreview.net/pdf/4f6f0e2347a3d6f9a88b39e445f77c1e7503064e.pdf,graph;generative model;diffusion models,https://scholar.google.com/scholar?q=Quantized+Compressed+Sensing+with+Score-Based+Generative+Models
Diffusion Models for Causal Discovery via Topological Ordering,2023,ICLR,"['Pedro Sanchez', 'Xiao Liu', ""Alison Q O'Neil"", 'Sotirios A. Tsaftaris']",poster,"['Diffusion Models', 'Causal Discovery', 'Topological Ordering', 'Score-based Methods']","Discovering causal relations from observational data becomes possible with additional assumptions such as considering the functional relations to be constrained as nonlinear with additive noise (ANM). Even with strong assumptions, causal discovery involves an expensive search problem over the space of directed acyclic graphs (DAGs). \emph{Topological ordering} approaches reduce the optimisation space of causal discovery by searching over a permutation rather than graph space.
For ANMs, the \emph{Hessian} of the data log-likelihood can be used for finding leaf nodes in a causal graph, allowing its topological ordering. However, existing computational methods for obtaining the Hessian still do not scale as the number of variables and the number of samples are increased. Therefore, inspired by recent innovations in diffusion probabilistic models (DPMs), we propose \emph{DiffAN}, a topological ordering algorithm that leverages DPMs for learning a Hessian function. We introduce theory for updating the learned Hessian without re-training the neural network, and we show that computing with a subset of samples gives an accurate approximation of the ordering, which allows scaling to datasets with more samples and variables. We show empirically that our method scales exceptionally well to datasets with up to $500$ nodes and up to $10^5$ samples while still performing on par over small datasets with state-of-the-art causal discovery methods.
Implementation is available at \url{https://github.com/vios-s/DiffAN} .",https://api.openreview.net/pdf/6507802ca7e95e60abfdd3910aaefa5450193400.pdf,graph;optimization;zero_few-shot;transformer;online learning;diffusion models,https://scholar.google.com/scholar?q=Diffusion+Models+for+Causal+Discovery+via+Topological+Ordering
DiGress: Discrete Denoising diffusion for graph generation,2023,ICLR,"['Clement Vignac', 'Igor Krawczuk', 'Antoine Siraudin', 'Bohan Wang', 'Volkan Cevher', 'Pascal Frossard']",poster,"['Graph generation', 'Denoising Diffusion Model', 'Molecule Generation', 'Permutation Equivariance', 'Discrete Diffusion']","This work introduces DiGress, a discrete denoising diffusion model for generating graphs with categorical node and edge attributes.
Our model utilizes a discrete diffusion process that progressively edits graphs with noise, through the process of adding or removing edges and changing the categories.
A graph transformer network is trained to revert this process, simplifying the problem of distribution learning over graphs into a sequence of node and edge classification tasks.
We further improve sample quality by introducing a Markovian noise model that preserves the marginal distribution of node and edge types during diffusion, and by incorporating auxiliary graph-theoretic features.
A procedure for conditioning the generation on graph-level features is also proposed.
DiGress achieves state-of-the-art performance on molecular and non-molecular datasets, with up to 3x validity improvement on a planar graph dataset. 
It is also the first model to scale to the large GuacaMol dataset containing 1.3M drug-like molecules without the use of molecule-specific representations.",https://api.openreview.net/pdf/ca5c988dd881d65b94fffbf35e0eaa5c5be23585.pdf,graph;transformer;representation;generative model;diffusion models,https://scholar.google.com/scholar?q=DiGress:+Discrete+Denoising+diffusion+for+graph+generation
DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models,2023,ICLR,"['Shansan Gong', 'Mukai Li', 'Jiangtao Feng', 'Zhiyong Wu', 'Lingpeng Kong']",poster,"['diffusion model', 'sequence to sequence', 'text generation', 'diveristy']","Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at https://github.com/Shark-NLP/DiffuSeq",https://api.openreview.net/pdf/60eecf7c181638fdfa60c671ca5d0b67644748cd.pdf,graph;generative model;diffusion models,https://scholar.google.com/scholar?q=DiffuSeq:+Sequence+to+Sequence+Text+Generation+with+Diffusion+Models
Equivariant Energy-Guided SDE for Inverse Molecular Design,2023,ICLR,"['Fan Bao', 'Min Zhao', 'Zhongkai Hao', 'Peiyao Li', 'Chongxuan Li', 'Jun Zhu']",poster,"['score-based model', 'diffusion model', 'inverse molecular design', 'energy guidance', 'molecule generation', 'equivariance']","Inverse molecular design is critical in material science and drug discovery, where the generated molecules should satisfy certain desirable properties. In this paper, we propose equivariant energy-guided stochastic differential equations (EEGSDE), a flexible framework for controllable 3D molecule generation under the guidance of an energy function in diffusion models. Formally, we show that EEGSDE naturally exploits the geometric symmetry in 3D molecular conformation, as long as the energy function is invariant to orthogonal transformations. Empirically, under the guidance of designed energy functions, EEGSDE significantly improves the baseline on QM9, in inverse molecular design targeted to quantum properties and molecular structures. Furthermore, EEGSDE is able to generate molecules with multiple target properties by combining the corresponding energy functions linearly.",https://api.openreview.net/pdf/7eccd68c8c19051056b77215ff617061615b0e5c.pdf,generative model;metric;diffusion models;3d,https://scholar.google.com/scholar?q=Equivariant+Energy-Guided+SDE+for+Inverse+Molecular+Design
Accelerating Guided Diffusion Sampling with Splitting Numerical Methods,2023,ICLR,"['Suttisak Wizadwongsa', 'Supasorn Suwajanakorn']",poster,"['Splitting Numerical Methods', 'Guided Diffusion Models']","Guided diffusion is a technique for conditioning the output of a diffusion model at sampling time without retraining the network for each specific task. However, one drawback of diffusion models, whether they are guided or unguided, is their slow sampling process. 
Recent techniques can accelerate unguided sampling by applying high-order numerical methods to the sampling process when viewed as differential equations. On the contrary, we discover that the same techniques do not work for guided sampling, and little has been explored about its acceleration. This paper explores the culprit of this problem and provides a solution based on operator splitting methods, motivated by our key finding that classical high-order numerical methods are unsuitable for the conditional function. Our proposed method can re-utilize the high-order methods for guided sampling and can generate images with the same quality as a 250-step DDIM baseline using 32-58% less sampling time on ImageNet256. 
We also demonstrate usage on a wide variety of conditional generation tasks, such as text-to-image generation, colorization, inpainting, and super-resolution.",https://api.openreview.net/pdf/b76de0f572f704575f927eccbbb6c0031f8b7427.pdf,zero_few-shot;generative model;diffusion models,https://scholar.google.com/scholar?q=Accelerating+Guided+Diffusion+Sampling+with+Splitting+Numerical+Methods
Blurring Diffusion Models,2023,ICLR,"['Emiel Hoogeboom', 'Tim Salimans']",poster,"['blurring', 'diffusion', 'generative model']","Recently, Rissanen et al., (2022) have presented a new type of diffusion process for generative modeling based on heat dissipation, or blurring, as an alternative to isotropic Gaussian diffusion. Here, we show that blurring can equivalently be defined through a Gaussian diffusion process with non-isotropic noise. In making this connection, we bridge the gap between inverse heat dissipation and denoising diffusion, and we shed light on the inductive bias that results from this modeling choice. Finally, we propose a generalized class of diffusion models that offers the best of both standard Gaussian denoising diffusion and inverse heat dissipation, which we call Blurring Diffusion Models. ",https://api.openreview.net/pdf/33c05b0bd300e12f24e2ac7a85f4716f5c4a9147.pdf,generative model;diffusion models,https://scholar.google.com/scholar?q=Blurring+Diffusion+Models
Generative Modelling with Inverse Heat Dissipation,2023,ICLR,"['Severi Rissanen', 'Markus Heinonen', 'Arno Solin']",poster,"['diffusion model', 'partial differential equation', 'inductive bias']","While diffusion models have shown great success in image generation, their noise-inverting generative process does not explicitly consider the structure of images, such as their inherent multi-scale nature. Inspired by diffusion models and the empirical success of coarse-to-fine modelling, we propose a new diffusion-like model that generates images through stochastically reversing the heat equation, a PDE that locally erases fine-scale information when run over the 2D plane of the image. We interpret the solution of the forward heat equation with constant additive noise as a variational approximation in the diffusion latent variable model. Our new model shows emergent qualitative properties not seen in standard diffusion models, such as disentanglement of overall colour and shape in images. Spectral analysis on natural images highlights connections to diffusion models and reveals an implicit coarse-to-fine inductive bias in them.",https://api.openreview.net/pdf/40788e42963a1dbec8dc274094381e704205a2be.pdf,generative model;diffusion models,https://scholar.google.com/scholar?q=Generative+Modelling+with+Inverse+Heat+Dissipation
Dual Diffusion Implicit Bridges for Image-to-Image Translation,2023,ICLR,"['Xuan Su', 'Jiaming Song', 'Chenlin Meng', 'Stefano Ermon']",poster,[],"Common image-to-image translation methods rely on joint training over data from both source and target domains. The training process requires concurrent access to both datasets, which hinders data separation and privacy protection; and existing models cannot be easily adapted for translation of new domain pairs. We present Dual Diffusion Implicit Bridges (DDIBs), an image translation method based on diffusion models, that circumvents training on domain pairs. Image translation with DDIBs relies on two diffusion models trained independently on each domain, and is a two-step process: DDIBs first obtain latent encodings for source images with the source diffusion model, and then decode such encodings using the target model to construct target images. Both steps are defined via ordinary differential equations (ODEs), thus the process is cycle consistent only up to discretization errors of the ODE solvers. Theoretically, we interpret DDIBs as concatenation of source to latent, and latent to target Schrodinger Bridges, a form of entropy-regularized optimal transport, to explain the efficacy of the method. Experimentally, we apply DDIBs on synthetic and high-resolution image datasets, to demonstrate their utility in a wide variety of translation tasks and their inherent optimal transport properties.",https://api.openreview.net/pdf/91f1c96de0279c81fb44166262ba54d69daf0fe4.pdf,diffusion models,https://scholar.google.com/scholar?q=Dual+Diffusion+Implicit+Bridges+for+Image-to-Image+Translation
Stable Target Field for Reduced Variance Score Estimation in Diffusion Models,2023,ICLR,"['Yilun Xu', 'Shangyuan Tong', 'Tommi S. Jaakkola']",poster,"['generative model', 'score-based models', 'diffusion models', 'variance reduction']","Diffusion models generate samples by reversing a fixed forward diffusion process. Despite already providing impressive empirical results, these diffusion models algorithms can be further improved by reducing the variance of the training targets in their denoising score-matching objective. We argue that the source of such variance lies in the handling of intermediate noise-variance scales, where multiple modes in the data affect the direction of reverse paths. We propose to remedy the problem by incorporating a reference batch which we use to calculate weighted conditional scores as more stable training targets. We show that the procedure indeed helps in the challenging intermediate regime by reducing (the trace of) the covariance of training targets. The new stable targets can be seen as trading bias for reduced variance, where the bias vanishes with increasing reference batch size. Empirically, we show that the new objective improves the image quality, stability, and training speed of various popular diffusion models across datasets with both general ODE and SDE solvers. When used in combination with EDM, our method yields a current SOTA FID of 1.90 with 35 network evaluations on the unconditional CIFAR-10 generation task. The code is available at https://github.com/Newbeeer/stf",https://api.openreview.net/pdf/bd21f9db32efdae84c04fffb45dc533162bb1809.pdf,graph;generative model;diffusion models,https://scholar.google.com/scholar?q=Stable+Target+Field+for+Reduced+Variance+Score+Estimation+in+Diffusion+Models
Markup-to-Image Diffusion Models with Scheduled Sampling,2023,ICLR,"['Yuntian Deng', 'Noriyuki Kojima', 'Alexander M Rush']",poster,[],"Building on recent advances in image generation, we present a fully data-driven approach to rendering markup into images. The approach is based on diffusion models, which parameterize the distribution of data using a sequence of denoising operations on top of a Gaussian noise distribution. We view the diffusion denoising process a sequential decision making process, and show that it exhibits compounding errors similar to exposure bias issues in imitation learning problems. To mitigate these issues, we adapt the scheduled sampling algorithm to diffusion training. We conduct experiments on four markup datasets: formulas (LaTeX), table layouts (HTML), sheet music (LilyPond), and molecular images (SMILES). These experiments each verify the effectiveness of diffusion and the use of scheduled sampling to fix generation issues. These results also show that the markup-to-image task presents a useful controlled compositional setting for diagnosing and analyzing generative image models.",https://api.openreview.net/pdf/f90b1cecbec20cb712499f3efdd915bf72fe7839.pdf,generative model;imitation learning;diffusion models,https://scholar.google.com/scholar?q=Markup-to-Image+Diffusion+Models+with+Scheduled+Sampling
Diffusion Probabilistic Fields,2023,ICLR,"['Peiye Zhuang', 'Samira Abnar', 'Jiatao Gu', 'Alex Schwing', 'Joshua M. Susskind', 'Miguel Ángel Bautista']",poster,"['Generative Models', 'Field Representation', 'Diffusion Models']","Diffusion probabilistic models have quickly become a major approach for generative modeling of images, 3D geometry, video and other domains. However, to adapt diffusion generative modeling to these domains the denoising network needs to be carefully designed for each domain independently, oftentimes under the assumption that data lives in a Euclidean grid. In this paper we introduce Diffusion Probabilistic Fields (DPF), a diffusion model that can learn distributions over continuous functions defined over metric spaces, commonly known as fields. We extend the formulation of diffusion probabilistic models to deal with this field parametrization in an explicit way, enabling us to define an end-to-end learning algorithm that side-steps the requirement of representing fields with latent vectors as in previous approaches (Dupont et al., 2022a; Du et al., 2021). We empirically show that, while using the same denoising network, DPF effectively deals with different modalities like 2D images and 3D geometry, in addition to modeling distributions over fields defined on non-Euclidean metric spaces.",https://api.openreview.net/pdf/73ebf3d9d08b88d3b59f24b690fe3f2584b6d628.pdf,generative model;metric;diffusion models;3d,https://scholar.google.com/scholar?q=Diffusion+Probabilistic+Fields
Novel View Synthesis with Diffusion Models,2023,ICLR,"['Daniel Watson', 'William Chan', 'Ricardo Martin Brualla', 'Jonathan Ho', 'Andrea Tagliasacchi', 'Mohammad Norouzi']",poster,"['3D', 'diffusion', 'ddpm', 'novel', 'view', 'synthesis', 'generative', 'models']","We present 3DiM (pronounced ""three-dim""), a diffusion model for 3D novel view synthesis from as few as a single image. The core of 3DiM is an image-to-image diffusion model -- 3DiM takes a single reference view and their poses as inputs, and generates a novel view via diffusion. 3DiM can then generate a full 3D consistent scene following our novel stochastic conditioning sampler: the output frames of the scene are generated autoregressively, and during the reverse diffusion process of each individual frame, we select a random conditioning frame from the set of previous frames at each denoising step. We demonstrate that stochastic conditioning yields much more 3D consistent results compared to the naive sampling process which only conditions on a single previous frame. We compare 3DiMs to prior work on the SRN ShapeNet dataset, demonstrating that 3DiM's generated videos from a single view achieve much higher fidelity while being approximately 3D consistent. We also introduce a new evaluation methodology, 3D consistency scoring, to measure the 3D consistency of a generated object by training a neural field on the model's output views. 3DiMs are geometry free, do not rely on hyper-networks or test-time optimization for novel view synthesis, and allow a single model to easily scale to a large number of scenes.",https://api.openreview.net/pdf/fa3c83b8ced74fdc2103d3de7774170ced113e61.pdf,optimization;zero_few-shot;diffusion models;3d,https://scholar.google.com/scholar?q=Novel+View+Synthesis+with+Diffusion+Models
Unified Discrete Diffusion for Simultaneous Vision-Language Generation,2023,ICLR,"['Minghui Hu', 'Chuanxia Zheng', 'Zuopeng Yang', 'Tat-Jen Cham', 'Heliang Zheng', 'Chaoyue Wang', 'Dacheng Tao', 'Ponnuthurai N. Suganthan']",poster,"['Multi-modal', 'Image generation', 'Image Caption.']","The recently developed discrete diffusion model performs extraordinarily well in generation tasks, especially in the text-to-image task, showing great potential for modeling multimodal signals. In this paper, we leverage these properties and present a unified multimodal generation model, which can perform text-based, image-based, and even vision-language simultaneous generation using a single model. Specifically, we unify the discrete diffusion process for multimodal signals by proposing a unified Markov transition matrix and a unified objective. Moreover, we design a multimodal mutual attention module to highlight the inter-modal linkages, which is vital for multimodal generation. Extensive experiments indicate that our proposed method can perform comparably to the state-of-the-art solutions in various generation tasks.",https://api.openreview.net/pdf/d386e35cf9491b25a9017b063c051ef1d750c527.pdf,transformer;generative model;multimodal;diffusion models;llm,https://scholar.google.com/scholar?q=Unified+Discrete+Diffusion+for+Simultaneous+Vision-Language+Generation
Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning,2023,ICLR,"['Ting Chen', 'Ruixiang ZHANG', 'Geoffrey Hinton']",poster,"['Diffusion Models', 'Discrete Data']","We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous state and continuous time diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.",https://api.openreview.net/pdf/a44a46ea8deb0fda30f91b5e7a0712d7544a462a.pdf,generative model;metric;diffusion models,https://scholar.google.com/scholar?q=Analog+Bits:+Generating+Discrete+Data+using+Diffusion+Models+with+Self-Conditioning
Discrete Contrastive Diffusion for Cross-Modal Music and Image Generation,2023,ICLR,"['Ye Zhu', 'Yu Wu', 'Kyle Olszewski', 'Jian Ren', 'Sergey Tulyakov', 'Yan Yan']",poster,"['Contrastive Diffusion', 'Conditioned Generations', 'Music Generation', 'Image Synthesis']","Diffusion probabilistic models (DPMs) have become a popular approach to conditional generation, due to their promising results and support for cross-modal synthesis. A key desideratum in conditional synthesis is to achieve high correspondence between the conditioning input and generated output. Most existing methods learn such relationships implicitly, by incorporating the prior into the variational lower bound. In this work, we take a different route---we explicitly enhance input-output connections by maximizing their mutual information. To this end, we introduce a Conditional Discrete Contrastive Diffusion (CDCD) loss and design two contrastive diffusion mechanisms to effectively incorporate it into the denoising process, combining the diffusion training and contrastive learning for the first time by connecting it with the conventional variational objectives. We demonstrate the efficacy of our approach in evaluations with diverse multimodal conditional synthesis tasks: dance-to-music generation, text-to-image synthesis, as well as class-conditioned image synthesis. On each, we enhance the input-output correspondence and achieve higher or competitive general synthesis quality. Furthermore, the proposed approach improves the convergence of diffusion models, reducing the number of required diffusion steps by more than 35% on two benchmarks, significantly increasing the inference speed.",https://api.openreview.net/pdf/d217aca9d2b4ac1be080c492bb9d4939061d6de7.pdf,zero_few-shot;generative model;contrastive learning;inference;multimodal;diffusion models,https://scholar.google.com/scholar?q=Discrete+Contrastive+Diffusion+for+Cross-Modal+Music+and+Image+Generation
Diffusion Probabilistic Modeling of Protein Backbones in 3D for the motif-scaffolding problem,2023,ICLR,"['Brian L. Trippe', 'Jason Yim', 'Doug Tischer', 'David Baker', 'Tamara Broderick', 'Regina Barzilay', 'Tommi S. Jaakkola']",poster,"['Diffusion Models', 'Sequential Monte Carlo', 'Protein Design', 'Geometric Deep Learning']","Construction of a scaffold structure that supports a desired motif, conferring protein function, shows promise for the design of vaccines and enzymes. But a general solution to this motif-scaffolding problem remains open. Current machine-learning techniques for scaffold design are either limited to unrealistically small scaffolds (up to length 20) or struggle to produce multiple diverse scaffolds. We propose to learn a distribution over diverse and longer protein backbone structures via an E(3)-equivariant graph neural network. We develop SMCDiff to efficiently sample scaffolds from this distribution conditioned on a given motif; our algorithm is the first to theoretically guarantee conditional samples from a diffusion model in the large-compute limit. We evaluate our designed backbones by how well they align with AlphaFold2-predicted structures. We show that our method can (1) sample scaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for a fixed motif.",https://api.openreview.net/pdf/9674e0c41114fe41488ab7b7ea23289d7b213496.pdf,graph;diffusion models;3d,https://scholar.google.com/scholar?q=Diffusion+Probabilistic+Modeling+of+Protein+Backbones+in+3D+for+the+motif-scaffolding+problem
DreamFusion: Text-to-3D using 2D Diffusion,2023,ICLR,"['Ben Poole', 'Ajay Jain', 'Jonathan T. Barron', 'Ben Mildenhall']",oral,"['diffusion models', 'score-based generative models', 'NeRF', 'neural rendering', '3d synthesis']","Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D or multiview data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.",https://api.openreview.net/pdf/fc5d88df1a06d30ae79fb23e87030f0fb2c8bd76.pdf,optimization;zero_few-shot;metric;distillation;diffusion models;3d,https://scholar.google.com/scholar?q=DreamFusion:+Text-to-3D+using+2D+Diffusion
Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions,2023,ICLR,"['Sitan Chen', 'Sinho Chewi', 'Jerry Li', 'Yuanzhi Li', 'Adil Salim', 'Anru Zhang']",oral,"['diffusion models', 'score-based generative models', 'sampling', 'score estimation', 'Langevin', 'stochastic differential equations']","We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does *not* reduce the complexity of SGMs.",https://api.openreview.net/pdf/f0dc173be132440952bd7d8221b096d0a0ecf2c7.pdf,generative model;diffusion models;llm,https://scholar.google.com/scholar?q=Sampling+is+as+easy+as+learning+the+score:+theory+for+diffusion+models+with+minimal+data+assumptions
Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models,2023,ICLR,['Dongzhuo Li'],poster,"['Deep generative models', 'inverse problems', 'Gaussianization']","Deep generative models such as GANs, normalizing flows, and diffusion models are powerful regularizers for inverse problems. They exhibit great potential for helping reduce ill-posedness and attain high-quality results. However, the latent tensors of such deep generative models can fall out of the desired high-dimensional standard Gaussian distribution during inversion, particularly in the presence of data noise and inaccurate forward models, leading to low-fidelity solutions. To address this issue, we propose to reparameterize and Gaussianize the latent tensors using novel differentiable data-dependent layers wherein custom operators are defined by solving optimization problems. These proposed layers constrain inverse problems to obtain high-fidelity in-distribution solutions. We validate our technique on three inversion tasks: compressive-sensing MRI, image deblurring, and eikonal tomography (a nonlinear PDE-constrained inverse problem) using two representative deep generative models: StyleGAN2 and Glow. Our approach achieves state-of-the-art performance in terms of accuracy and consistency.",https://api.openreview.net/pdf/0d918b07dda7aa6571c2c0ac1eff7f920d47bf7b.pdf,graph;optimization;zero_few-shot;generative model;online learning;flow;diffusion models,https://scholar.google.com/scholar?q=Differentiable+Gaussianization+Layers+for+Inverse+Problems+Regularized+by+Deep+Generative+Models
DDM$^2$: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models,2023,ICLR,"['Tiange Xiang', 'Mahmut Yurt', 'Ali B Syed', 'Kawin Setsompop', 'Akshay Chaudhari']",poster,"['Unsupervised MRI Denoising', 'Diffusion Models']","Magnetic resonance imaging (MRI) is a common and life-saving medical imaging technique. However, acquiring high signal-to-noise ratio MRI scans requires long scan times, resulting in increased costs and patient discomfort, and decreased throughput. Thus, there is great interest in denoising MRI scans, especially for the subtype of diffusion MRI scans that are severely SNR-limited. While most prior MRI denoising methods are supervised in nature, acquiring supervised training datasets for the multitude of anatomies, MRI scanners, and scan parameters proves impractical. Here, we propose Denoising Diffusion Models for Denoising Diffusion MRI (DDM^2), a self-supervised denoising method for MRI denoising using diffusion denoising generative models. Our three-stage framework integrates statistic-based denoising theory into diffusion models and performs denoising through conditional generation. During inference, we represent input noisy measurements as a sample from an intermediate posterior distribution within the diffusion Markov chain. We conduct experiments on 4 real-world in-vivo diffusion MRI datasets and show that our DDM^2 demonstrates superior denoising performances ascertained with clinically-relevant visual qualitative and quantitative metrics.",https://api.openreview.net/pdf/ba3ad1d1f754d30b7ce210a7b9c4d169705d5234.pdf,graph;zero_few-shot;generative model;inference;metric;diffusion models,https://scholar.google.com/scholar?q=DDM$^2$:+Self-Supervised+Diffusion+MRI+Denoising+with+Generative+Diffusion+Models
ChiroDiff: Modelling chirographic data with Diffusion Models,2023,ICLR,"['Ayan Das', 'Yongxin Yang', 'Timothy Hospedales', 'Tao Xiang', 'Yi-Zhe Song']",poster,"['chirographic data', 'continuous-time', 'diffusion model', 'generative model']","Generative modelling over continuous-time geometric constructs, a.k.a $chirographic\ data$ such as handwriting, sketches, drawings etc., have been accomplished through autoregressive distributions. Such strictly-ordered discrete factorization however falls short of capturing key properties of chirographic data -- it fails to build holistic understanding of the temporal concept due to one-way visibility (causality). Consequently, temporal data has been modelled as discrete token sequences of fixed sampling rate instead of capturing the true underlying concept. In this paper, we introduce a powerful model-class namely Denoising\ Diffusion\ Probabilistic\ Models or DDPMs for chirographic data that specifically addresses these flaws. Our model named ""ChiroDiff"", being non-autoregressive, learns to capture holistic concepts and therefore remains resilient to higher temporal sampling rate up to a good extent. Moreover, we show that many important downstream utilities (e.g. conditional sampling, creative mixing) can be flexibly implemented using ChiroDiff. We further show some unique use-cases like stochastic vectorization, de-noising/healing, abstraction are also possible with this model-class. We perform quantitative and qualitative evaluation of our framework on relevant datasets and found it to be better or on par with competing approaches.",https://api.openreview.net/pdf/2df012fbedfa6bf4cafce1218732561500298315.pdf,graph;generative model;metric;diffusion models,https://scholar.google.com/scholar?q=ChiroDiff:+Modelling+chirographic+data+with+Diffusion+Models
kNN-Diffusion: Image Generation via Large-Scale Retrieval,2023,ICLR,"['Shelly Sheynin', 'Oron Ashual', 'Adam Polyak', 'Uriel Singer', 'Oran Gafni', 'Eliya Nachmani', 'Yaniv Taigman']",poster,[],"Recent text-to-image models have achieved impressive results. However, since they require large-scale datasets of text-image pairs, it is impractical to train them on new domains where data is scarce or not labeled.
In this work, we propose using large-scale retrieval methods, in particular, efficient k-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a substantially small and efficient text-to-image diffusion model using only pre-trained multi-modal embeddings, but without an explicit text-image dataset, (2) generating out-of-distribution images by simply swapping the retrieval database at inference time, and (3) performing text-driven local semantic manipulations while preserving object identity. To demonstrate the robustness of our method, we apply our kNN approach on two state-of-the-art diffusion backbones, and show results on several different datasets. As evaluated by human studies and automatic metrics, our method achieves state-of-the-art results compared to existing approaches that train text-to-image generation models using images-only dataset.",https://api.openreview.net/pdf/b8975b72f550b7061f9e42248a7f29257a2c71fc.pdf,generative model;inference;metric;multimodal;diffusion models,https://scholar.google.com/scholar?q=kNN-Diffusion:+Image+Generation+via+Large-Scale+Retrieval
gDDIM: Generalized denoising diffusion implicit models,2023,ICLR,"['Qinsheng Zhang', 'Molei Tao', 'Yongxin Chen']",spotlight,"['Fast sampling', 'diffusion model']","Our goal is to extend the denoising diffusion implicit model (DDIM) to general diffusion models~(DMs) besides isotropic diffusions. Instead of constructing a non-Markov noising process as in the original DDIM, we examine the mechanism of DDIM from a numerical perspective.  We discover that the DDIM can be obtained by using some specific approximations of the score when solving the corresponding stochastic differential equation. We present an interpretation of the accelerating effects of DDIM that also explains the advantages of a deterministic sampling scheme over the stochastic one for fast sampling. Building on this insight, we extend DDIM to general DMs, coined generalized DDIM (gDDIM), with a small but delicate modification in parameterizing the score network. We validate gDDIM in two non-isotropic DMs: Blurring diffusion model (BDM) and Critically-damped Langevin diffusion model (CLD). We observe more than 20 times acceleration in BDM. In the CLD, a diffusion model by augmenting the diffusion process with velocity, our algorithm achieves an FID score of 2.26, on CIFAR10, with only 50 number of score function evaluations~(NFEs) and an FID score of 2.86 with only 27 NFEs.",https://api.openreview.net/pdf/101656f96f6c22373cb9bf570b89250c966aedd5.pdf,graph;diffusion models,https://scholar.google.com/scholar?q=gDDIM:+Generalized+denoising+diffusion+implicit+models
Learning Diffusion Bridges on Constrained Domains,2023,ICLR,"['Xingchao Liu', 'Lemeng Wu', 'Mao Ye', 'qiang liu']",spotlight,[],"Diffusion models have achieved promising results on generative learning recently. However, because diffusion processes are most naturally applied  on the unconstrained Euclidean space $\mathrm{R}^d$, key challenges arise for developing diffusion based models for learning data on constrained and structured domains. We present a simple and unified framework to achieve this that can be easily adopted to various types of domains, including product spaces of any type (be it bounded/unbounded, continuous/discrete, categorical/ordinal, or  their mix). In our model, the diffusion process is driven by a drift force that is a sum of two terms: one singular force designed by $Doob's~ h$-$transform$ that ensures all outcomes of the process to belong to the desirable domain, and one non-singular neural force field that is trained to make sure the outcome follows the data distribution statistically. Experiments show that our methods perform superbly on generating tabular data, images, semantic segments and 3D point clouds. ",https://api.openreview.net/pdf/8cfc259b7bd52dd062abbfdf800fc63779766d37.pdf,optimization;zero_few-shot;generative model;diffusion models;3d,https://scholar.google.com/scholar?q=Learning+Diffusion+Bridges+on+Constrained+Domains
Flow Matching for Generative Modeling,2023,ICLR,"['Yaron Lipman', 'Ricky T. Q. Chen', 'Heli Ben-Hamu', 'Maximilian Nickel', 'Matthew Le']",spotlight,"['continuous normalizing flows', 'generative models']","We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples---which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.",https://api.openreview.net/pdf/e99034416acd1ca82991f5d63735e77130fc06a7.pdf,zero_few-shot;generative model;flow;diffusion models,https://scholar.google.com/scholar?q=Flow+Matching+for+Generative+Modeling
MeshDiffusion: Score-based Generative 3D Mesh Modeling,2023,ICLR,"['Zhen Liu', 'Yao Feng', 'Michael J. Black', 'Derek Nowrouzezahrai', 'Liam Paull', 'Weiyang Liu']",spotlight,"['generative model', 'diffusion model', '3D mesh', 'shape generation']","We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parameterization. We demonstrate the effectiveness of our model on multiple generative tasks.",https://api.openreview.net/pdf/f4b27531cf7771c608830f2a184f9c5ef06eab1c.pdf,graph;representation;generative model;metric;diffusion models;3d,https://scholar.google.com/scholar?q=MeshDiffusion:+Score-based+Generative+3D+Mesh+Modeling
DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion,2023,ICLR,"['Qitian Wu', 'Chenxiao Yang', 'Wentao Zhao', 'Yixuan He', 'David Wipf', 'Junchi Yan']",spotlight,"['structured representation learning', 'diffusion model', 'optimization-induced model', 'node prediction']","Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t. a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instance numbers, and an advanced version for learning complex structures. Experiments highlight the wide applicability of our model as a general-purpose encoder backbone with superior performance in various tasks, such as node classification on large graphs, semi-supervised image/text classification, and spatial-temporal dynamics prediction. The codes are available at https://github.com/qitianwu/DIFFormer.",https://api.openreview.net/pdf/2c274286ca9d89f558de1d9abc67d9b0a429bc4d.pdf,graph;optimization;transformer;representation;generative model;metric;diffusion models,https://scholar.google.com/scholar?q=DIFFormer:+Scalable+(Graph)+Transformers+Induced+by+Energy+Constrained+Diffusion
"Neural Lagrangian Schr\""{o}dinger Bridge: Diffusion Modeling for Population Dynamics",2023,ICLR,"['Takeshi Koshizuka', 'Issei Sato']",spotlight,"['Population Dynamics', 'Trajectory Inference', 'Neural SDEs', 'Stochastic Optimal Transport', 'Schrödinger Bridge']","Population dynamics is the study of temporal and spatial variation in the size of populations of organisms and is a major part of population ecology. One of the main difficulties in analyzing population dynamics is that we can only obtain observation data with coarse time intervals from fixed-point observations due to experimental costs or measurement constraints. Recently, modeling population dynamics by using continuous normalizing flows (CNFs) and dynamic optimal transport has been proposed to infer the sample trajectories from a fixed-point observed population. While the sample behavior in CNFs is deterministic, the actual sample in biological systems moves in an essentially random yet directional manner. Moreover, when a sample moves from point A to point B in dynamical systems, its trajectory typically follows the principle of least action in which the corresponding action has the smallest possible value. To satisfy these requirements of the sample trajectories, we formulate the Lagrangian Schrödinger bridge (LSB) problem and propose to solve it approximately by modeling the advection-diffusion process with regularized neural SDE. We also develop a model architecture that enables faster computation of the loss function. Experimental results show that the proposed method can efficiently approximate the population-level dynamics even for high-dimensional data and that using the prior knowledge introduced by the Lagrangian enables us to estimate the sample-level dynamics with stochastic behavior.",https://api.openreview.net/pdf/4f5dfd7d5e9825029e736d0ace01eda002efdcb8.pdf,optimization;zero_few-shot;flow;diffusion models,"https://scholar.google.com/scholar?q=Neural+Lagrangian+Schr\""{o}dinger+Bridge:+Diffusion+Modeling+for+Population+Dynamics"
Prompt-to-Prompt Image Editing with Cross-Attention Control,2023,ICLR,"['Amir Hertz', 'Ron Mokady', 'Jay Tenenbaum', 'Kfir Aberman', 'Yael Pritch', 'Daniel Cohen-or']",spotlight,"['Image generation', 'Image editing', 'Diffusion models', 'Attention layer', 'Computer vision', 'Machine learning']","Recent large-scale text-driven synthesis diffusion models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Therefore, it is only natural to build upon these synthesis models to provide text-driven image editing capabilities. However, Editing is challenging for these generative models, since an innate property of an editing technique is to preserve some content from the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. We analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we propose to control the attention maps along the diffusion process. Our approach enables us to monitor the synthesis process by editing the textual prompt only, paving the way to a myriad of caption-based editing applications such as localized editing by replacing a word, global editing by adding a specification, and even controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts with different text-to-image models, demonstrating high-quality synthesis and fidelity to the edited prompts.",https://api.openreview.net/pdf/a6e78444f28f4790c2b8eb24364ced3ce736feb0.pdf,graph;zero_few-shot;transformer;generative model;diffusion models;llm,https://scholar.google.com/scholar?q=Prompt-to-Prompt+Image+Editing+with+Cross-Attention+Control
DiffEdit: Diffusion-based semantic image editing with mask guidance,2023,ICLR,"['Guillaume Couairon', 'Jakob Verbeek', 'Holger Schwenk', 'Matthieu Cord']",spotlight,"['computer vision', 'image editing', 'diffusion models']","Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. 
Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. 
DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images.",https://api.openreview.net/pdf/3d837329e3740d349726e77482e1be2f69278a1b.pdf,graph;optimization;zero_few-shot;generative model;inference;diffusion models;llm,https://scholar.google.com/scholar?q=DiffEdit:+Diffusion-based+semantic+image+editing+with+mask+guidance
Distilling Model Failures as Directions in Latent Space,2023,ICLR,"['Saachi Jain', 'Hannah Lawrence', 'Ankur Moitra', 'Aleksander Madry']",spotlight,"['datasets', 'biases', 'subpopulations']","Existing methods for isolating hard subpopulations and spurious correlations in datasets often require human intervention. This can make these methods labor-intensive and dataset-specific. To address these shortcomings, we present a scalable method for automatically distilling a model's failure modes. Specifically, we harness linear classifiers to identify consistent error patterns, and, in turn, induce a natural representation of these failure modes as directions within the feature space. We demonstrate that this framework allows us to discover and automatically caption challenging subpopulations within the training dataset. Moreover, by combining our framework with off-the-shelf diffusion models, we can generate images that are especially challenging for the analyzed model, and thus can be used to perform synthetic data augmentation that helps remedy the model's failure modes.",https://api.openreview.net/pdf/c9daa261ea96d95a6dee52da157a59e14333cf07.pdf,graph;zero_few-shot;representation;augmentation;distillation;diffusion models,https://scholar.google.com/scholar?q=Distilling+Model+Failures+as+Directions+in+Latent+Space
Stochastic Multi-Person 3D Motion Forecasting,2023,ICLR,"['Sirui Xu', 'Yu-Xiong Wang', 'Liangyan Gui']",spotlight,"['stochastic forecasting', 'multi-person 3D motion', 'dual-level generative modeling']","This paper aims to deal with the ignored real-world complexities in prior work on human motion forecasting, emphasizing the social properties of multi-person motion, the diversity of motion and social interactions, and the complexity of articulated motion. To this end, we introduce a novel task of stochastic multi-person 3D motion forecasting. We propose a dual-level generative modeling framework that separately models independent individual motion at the local level and social interactions at the global level. Notably, this dual-level modeling mechanism can be achieved within a shared generative model, through introducing learnable latent codes that represent intents of future motion and switching the codes' modes of operation at different levels. Our framework is general; we instantiate it with different generative models, including generative adversarial networks and diffusion models, and various multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D, and SoMoF benchmarks show that our approach produces diverse and accurate multi-person predictions, significantly outperforming the state of the art. ",https://api.openreview.net/pdf/cd1fe46c26063b4d564a5f4fa721d062014dd432.pdf,generative model;diffusion models;3d,https://scholar.google.com/scholar?q=Stochastic+Multi-Person+3D+Motion+Forecasting
SketchKnitter: Vectorized Sketch Generation with Diffusion Models,2023,ICLR,"['Qiang Wang', 'Haoge Deng', 'Yonggang Qi', 'Da Li', 'Yi-Zhe Song']",spotlight,[],"We show vectorized sketch generation can be identified as a reversal of the stroke deformation process. This relationship was established by means of a diffusion model that learns data distributions over the stroke-point locations and pen states of real human sketches. Given randomly scattered stroke-points, sketch generation becomes a process of deformation-based denoising, where the generator rectifies positions of stroke points at each timestep to converge at a recognizable sketch. A key innovation was to embed recognizability into the reverse time diffusion process. It was observed that the estimated noise during the reversal process is strongly correlated with sketch classification accuracy. An auxiliary recurrent neural network (RNN) was consequently used to quantify recognizability during data sampling. It follows that, based on the recognizability scores, a sampling shortcut function can also be devised that renders better quality sketches with fewer sampling steps. Finally it is shown that the model can be easily extended to a conditional generation framework, where given incomplete and unfaithful sketches, it yields one that is more visually appealing and with higher recognizability.",https://api.openreview.net/pdf/aa51f28767b5d95ceced7af0c79780b06d2fd1e0.pdf,zero_few-shot;generative model;diffusion models,https://scholar.google.com/scholar?q=SketchKnitter:+Vectorized+Sketch+Generation+with+Diffusion+Models
Diffusion Posterior Sampling for General Noisy Inverse Problems,2023,ICLR,"['Hyungjin Chung', 'Jeongsol Kim', 'Michael Thompson Mccann', 'Marc Louis Klasky', 'Jong Chul Ye']",spotlight,"['Diffusion model', 'Inverse problem', 'Posterior sampling']","Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via the Laplace approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring.",https://api.openreview.net/pdf/dd7f2e1f5581d91eb4c1ff34fec78b93d3dfa599.pdf,optimization;generative model;online learning;diffusion models,https://scholar.google.com/scholar?q=Diffusion+Posterior+Sampling+for+General+Noisy+Inverse+Problems
Diffusion Models Already Have A Semantic Latent Space,2023,ICLR,"['Mingi Kwon', 'Jaeseok Jeong', 'Youngjung Uh']",spotlight,"['diffusion models', 'semantic image editing']","Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we measure editing strength and quality deficiency of a generative process at timesteps to provide a principled design of the process for versatility and quality improvements. Our method is applicable to various architectures (DDPM++, iDDPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN-bedroom, and METFACES).",https://api.openreview.net/pdf/0d48a82a332a9c1fbc68f65e41a9b16eb9efa537.pdf,generative model;metric;diffusion models,https://scholar.google.com/scholar?q=Diffusion+Models+Already+Have+A+Semantic+Latent+Space
Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model,2023,ICLR,"['Yinhuai Wang', 'Jiwen Yu', 'Jian Zhang']",spotlight,"['Zero-Shot', 'Inverse Problems', 'Super-Resolution', 'Diffusion Models', 'Range-Null Space', 'Image Restoration', 'Colorization', 'Compressed Sensing', 'Inpainting', 'Deblur', 'Old Photo Restoration', 'Blind Restoration']","Most existing Image Restoration (IR) models are task-specific, which can not be generalized to different degradation operators. In this work, we propose the Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for arbitrary linear IR problems, including but not limited to image super-resolution, colorization, inpainting, compressed sensing, and deblurring. DDNM only needs a pre-trained off-the-shelf diffusion model as the generative prior, without any extra training or network modifications. By refining only the null-space contents during the reverse diffusion process, we can yield diverse results satisfying both data consistency and realness. We further propose an enhanced and robust version, dubbed DDNM+, to support noisy restoration and improve restoration quality for hard tasks. Our experiments on several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot IR methods. We also demonstrate that DDNM+ can solve complex real-world applications, e.g., old photo restoration. ",https://api.openreview.net/pdf/e31de23cacc50c8cddef5c6e559520cdd3a62b0c.pdf,zero_few-shot;generative model;diffusion models,https://scholar.google.com/scholar?q=Zero-Shot+Image+Restoration+Using+Denoising+Diffusion+Null-Space+Model
Denoising Diffusion Error Correction Codes,2023,ICLR,"['Yoni Choukroun', 'Lior Wolf']",spotlight,"['ECC', 'Deep Learning', 'Diffusion Models']","Error correction code (ECC) is an integral part of the physical communication layer, ensuring reliable data transfer over noisy channels. 
Recently, neural decoders have demonstrated their advantage over classical decoding techniques. 
However, recent state-of-the-art neural decoders suffer from high complexity and lack the important iterative scheme characteristic of many legacy decoders. 
In this work, we propose to employ denoising diffusion models for the soft decoding of linear codes at arbitrary block lengths. 
Our framework models the forward channel corruption as a series of diffusion steps that can be reversed iteratively. 
Three contributions are made: (i) a diffusion process suitable for the decoding setting is introduced, (ii) the neural diffusion decoder is conditioned on the number of parity errors, which indicates the level of corruption at a given step, (iii) a line search procedure based on the code's syndrome obtains the optimal reverse diffusion step size. 
The proposed approach demonstrates the power of diffusion models for ECC and is able to achieve state-of-the-art accuracy, outperforming the other neural decoders by sizable margins, even for a single reverse diffusion step. ",https://api.openreview.net/pdf/1ba7d8f5e235d93b8db4a40b633bb42c9494223e.pdf,graph;transfer learning;diffusion models,https://scholar.google.com/scholar?q=Denoising+Diffusion+Error+Correction+Codes
Human Motion Diffusion Model,2023,ICLR,"['Guy Tevet', 'Sigal Raab', 'Brian Gordon', 'Yoni Shafir', 'Daniel Cohen-or', 'Amit Haim Bermano']",spotlight,[],"Natural and expressive human motion generation is the holy grail of computer animation.
It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. 
Diffusion models are promising candidates for the human motion domain since they
have already shown remarkable generative capabilities in other domains, and their many-to-many nature. 
In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for human motion data.  MDM is transformer-based, combining insights from motion generation literature. 
A notable design-choice is that it predicts the sample itself rather than the noise in each step to facilitate the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion, action-to-motion, and unconditioned motion generation. ",https://api.openreview.net/pdf/f0e30bdff6d93fdd5a01526aaea18c2fec384fc0.pdf,graph;zero_few-shot;transformer;generative model;metric;diffusion models;llm,https://scholar.google.com/scholar?q=Human+Motion+Diffusion+Model
