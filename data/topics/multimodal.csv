title,year,source,authors,class,keywords,abstract,pdf_link,topic,google_scholar_link
Pixel-Aligned Volumetric Avatars,2021,CVPR,Amit Raj;Michael Zollhofer;Tomas Simon;Jason Saragih;Shunsuke Saito;James Hays;Stephen Lombardi,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Raj_Pixel-Aligned_Volumetric_Avatars_CVPR_2021_paper.pdf,metric;multimodal,https://scholar.google.com/scholar?q=Pixel-Aligned+Volumetric+Avatars
UC2: Universal Cross-Lingual Cross-Modal Vision-and-Language Pre-Training,2021,CVPR,Mingyang Zhou;Luowei Zhou;Shuohang Wang;Yu Cheng;Linjie Li;Zhou Yu;Jingjing Liu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_UC2_Universal_Cross-Lingual_Cross-Modal_Vision-and-Language_Pre-Training_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=UC2:+Universal+Cross-Lingual+Cross-Modal+Vision-and-Language+Pre-Training
DRANet: Disentangling Representation and Adaptation Networks for Unsupervised Cross-Domain Adaptation,2021,CVPR,Seunghun Lee;Sunghyun Cho;Sunghoon Im,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_DRANet_Disentangling_Representation_and_Adaptation_Networks_for_Unsupervised_Cross-Domain_Adaptation_CVPR_2021_paper.pdf,representation;multimodal,https://scholar.google.com/scholar?q=DRANet:+Disentangling+Representation+and+Adaptation+Networks+for+Unsupervised+Cross-Domain+Adaptation
Can Audio-Visual Integration Strengthen Robustness Under Multimodal Attacks?,2021,CVPR,Yapeng Tian;Chenliang Xu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Tian_Can_Audio-Visual_Integration_Strengthen_Robustness_Under_Multimodal_Attacks_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Can+Audio-Visual+Integration+Strengthen+Robustness+Under+Multimodal+Attacks?
Informative and Consistent Correspondence Mining for Cross-Domain Weakly Supervised Object Detection,2021,CVPR,Luwei Hou;Yu Zhang;Kui Fu;Jia Li,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Hou_Informative_and_Consistent_Correspondence_Mining_for_Cross-Domain_Weakly_Supervised_Object_CVPR_2021_paper.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Informative+and+Consistent+Correspondence+Mining+for+Cross-Domain+Weakly+Supervised+Object+Detection
Probabilistic Embeddings for Cross-Modal Retrieval,2021,CVPR,Sanghyuk Chun;Seong Joon Oh;Rafael Sampaio de Rezende;Yannis Kalantidis;Diane Larlus,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Chun_Probabilistic_Embeddings_for_Cross-Modal_Retrieval_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Probabilistic+Embeddings+for+Cross-Modal+Retrieval
Skeleton Merger: An Unsupervised Aligned Keypoint Detector,2021,CVPR,Ruoxi Shi;Zhengrong Xue;Yang You;Cewu Lu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Skeleton_Merger_An_Unsupervised_Aligned_Keypoint_Detector_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Skeleton+Merger:+An+Unsupervised+Aligned+Keypoint+Detector
Deep RGB-D Saliency Detection With Depth-Sensitive Attention and Automatic Multi-Modal Fusion,2021,CVPR,Peng Sun;Wenhu Zhang;Huanyu Wang;Songyuan Li;Xi Li,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_Deep_RGB-D_Saliency_Detection_With_Depth-Sensitive_Attention_and_Automatic_Multi-Modal_CVPR_2021_paper.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Deep+RGB-D+Saliency+Detection+With+Depth-Sensitive+Attention+and+Automatic+Multi-Modal+Fusion
Farewell to Mutual Information: Variational Distillation for Cross-Modal Person Re-Identification,2021,CVPR,Xudong Tian;Zhizhong Zhang;Shaohui Lin;Yanyun Qu;Yuan Xie;Lizhuang Ma,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Tian_Farewell_to_Mutual_Information_Variational_Distillation_for_Cross-Modal_Person_Re-Identification_CVPR_2021_paper.pdf,distillation;multimodal,https://scholar.google.com/scholar?q=Farewell+to+Mutual+Information:+Variational+Distillation+for+Cross-Modal+Person+Re-Identification
ChallenCap: Monocular 3D Capture of Challenging Human Performances Using Multi-Modal References,2021,CVPR,Yannan He;Anqi Pang;Xin Chen;Han Liang;Minye Wu;Yuexin Ma;Lan Xu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/He_ChallenCap_Monocular_3D_Capture_of_Challenging_Human_Performances_Using_Multi-Modal_CVPR_2021_paper.pdf,graph;multimodal;3d,https://scholar.google.com/scholar?q=ChallenCap:+Monocular+3D+Capture+of+Challenging+Human+Performances+Using+Multi-Modal+References
Prototypical Cross-Domain Self-Supervised Learning for Few-Shot Unsupervised Domain Adaptation,2021,CVPR,Xiangyu Yue;Zangwei Zheng;Shanghang Zhang;Yang Gao;Trevor Darrell;Kurt Keutzer;Alberto Sangiovanni Vincentelli,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Yue_Prototypical_Cross-Domain_Self-Supervised_Learning_for_Few-Shot_Unsupervised_Domain_Adaptation_CVPR_2021_paper.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Prototypical+Cross-Domain+Self-Supervised+Learning+for+Few-Shot+Unsupervised+Domain+Adaptation
Deep Lucas-Kanade Homography for Multimodal Image Alignment,2021,CVPR,Yiming Zhao;Xinming Huang;Ziming Zhang,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Deep_Lucas-Kanade_Homography_for_Multimodal_Image_Alignment_CVPR_2021_paper.pdf,graph;multimodal,https://scholar.google.com/scholar?q=Deep+Lucas-Kanade+Homography+for+Multimodal+Image+Alignment
Audio-Visual Instance Discrimination with Cross-Modal Agreement,2021,CVPR,Pedro Morgado;Nuno Vasconcelos;Ishan Misra,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Morgado_Audio-Visual_Instance_Discrimination_with_Cross-Modal_Agreement_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Audio-Visual+Instance+Discrimination+with+Cross-Modal+Agreement
Vx2Text: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs,2021,CVPR,Xudong Lin;Gedas Bertasius;Jue Wang;Shih-Fu Chang;Devi Parikh;Lorenzo Torresani,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Vx2Text_End-to-End_Learning_of_Video-Based_Text_Generation_From_Multimodal_Inputs_CVPR_2021_paper.pdf,generative model;multimodal,https://scholar.google.com/scholar?q=Vx2Text:+End-to-End+Learning+of+Video-Based+Text+Generation+From+Multimodal+Inputs
Multi-Modal Relational Graph for Cross-Modal Video Moment Retrieval,2021,CVPR,Yawen Zeng;Da Cao;Xiaochi Wei;Meng Liu;Zhou Zhao;Zheng Qin,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Zeng_Multi-Modal_Relational_Graph_for_Cross-Modal_Video_Moment_Retrieval_CVPR_2021_paper.pdf,graph;multimodal,https://scholar.google.com/scholar?q=Multi-Modal+Relational+Graph+for+Cross-Modal+Video+Moment+Retrieval
Cross-Domain Similarity Learning for Face Recognition in Unseen Domains,2021,CVPR,Masoud Faraki;Xiang Yu;Yi-Hsuan Tsai;Yumin Suh;Manmohan Chandraker,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Faraki_Cross-Domain_Similarity_Learning_for_Face_Recognition_in_Unseen_Domains_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Cross-Domain+Similarity+Learning+for+Face+Recognition+in+Unseen+Domains
Few-Shot Image Generation via Cross-Domain Correspondence,2021,CVPR,Utkarsh Ojha;Yijun Li;Jingwan Lu;Alexei A. Efros;Yong Jae Lee;Eli Shechtman;Richard Zhang,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Ojha_Few-Shot_Image_Generation_via_Cross-Domain_Correspondence_CVPR_2021_paper.pdf,generative model;multimodal,https://scholar.google.com/scholar?q=Few-Shot+Image+Generation+via+Cross-Domain+Correspondence
Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT Benchmark for Crowd Counting,2021,CVPR,Lingbo Liu;Jiaqi Chen;Hefeng Wu;Guanbin Li;Chenglong Li;Liang Lin,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Cross-Modal_Collaborative_Representation_Learning_and_a_Large-Scale_RGBT_Benchmark_for_CVPR_2021_paper.pdf,representation;multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Collaborative+Representation+Learning+and+a+Large-Scale+RGBT+Benchmark+for+Crowd+Counting
Multimodal Contrastive Training for Visual Representation Learning,2021,CVPR,Xin Yuan;Zhe Lin;Jason Kuen;Jianming Zhang;Yilin Wang;Michael Maire;Ajinkya Kale;Baldo Faieta,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Multimodal_Contrastive_Training_for_Visual_Representation_Learning_CVPR_2021_paper.pdf,representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Multimodal+Contrastive+Training+for+Visual+Representation+Learning
Learning From the Master: Distilling Cross-Modal Advanced Knowledge for Lip Reading,2021,CVPR,Sucheng Ren;Yong Du;Jianming Lv;Guoqiang Han;Shengfeng He,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Learning_From_the_Master_Distilling_Cross-Modal_Advanced_Knowledge_for_Lip_CVPR_2021_paper.pdf,distillation;multimodal,https://scholar.google.com/scholar?q=Learning+From+the+Master:+Distilling+Cross-Modal+Advanced+Knowledge+for+Lip+Reading
Multi-Modal Fusion Transformer for End-to-End Autonomous Driving,2021,CVPR,Aditya Prakash;Kashyap Chitta;Andreas Geiger,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Prakash_Multi-Modal_Fusion_Transformer_for_End-to-End_Autonomous_Driving_CVPR_2021_paper.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Multi-Modal+Fusion+Transformer+for+End-to-End+Autonomous+Driving
Multi-Scale Aligned Distillation for Low-Resolution Detection,2021,CVPR,Lu Qi;Jason Kuen;Jiuxiang Gu;Zhe Lin;Yi Wang;Yukang Chen;Yanwei Li;Jiaya Jia,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Qi_Multi-Scale_Aligned_Distillation_for_Low-Resolution_Detection_CVPR_2021_paper.pdf,zero_few-shot;distillation;multimodal,https://scholar.google.com/scholar?q=Multi-Scale+Aligned+Distillation+for+Low-Resolution+Detection
Adaptive Cross-Modal Prototypes for Cross-Domain Visual-Language Retrieval,2021,CVPR,Yang Liu;Qingchao Chen;Samuel Albanie,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Adaptive_Cross-Modal_Prototypes_for_Cross-Domain_Visual-Language_Retrieval_CVPR_2021_paper.pdf,adaptive;multimodal,https://scholar.google.com/scholar?q=Adaptive+Cross-Modal+Prototypes+for+Cross-Domain+Visual-Language+Retrieval
Self-Aligned Video Deraining With Transmission-Depth Consistency,2021,CVPR,Wending Yan;Robby T. Tan;Wenhan Yang;Dengxin Dai,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_Self-Aligned_Video_Deraining_With_Transmission-Depth_Consistency_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Self-Aligned+Video+Deraining+With+Transmission-Depth+Consistency
Progressive Modality Reinforcement for Human Multimodal Emotion Recognition From Unaligned Multimodal Sequences,2021,CVPR,Fengmao Lv;Xiang Chen;Yanyong Huang;Lixin Duan;Guosheng Lin,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Lv_Progressive_Modality_Reinforcement_for_Human_Multimodal_Emotion_Recognition_From_Unaligned_CVPR_2021_paper.pdf,reinforcement learning;multimodal,https://scholar.google.com/scholar?q=Progressive+Modality+Reinforcement+for+Human+Multimodal+Emotion+Recognition+From+Unaligned+Multimodal+Sequences
Single Pair Cross-Modality Super Resolution,2021,CVPR,Guy Shacht;Dov Danon;Sharon Fogel;Daniel Cohen-Or,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Shacht_Single_Pair_Cross-Modality_Super_Resolution_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Single+Pair+Cross-Modality+Super+Resolution
Learning Cross-Modal Retrieval With Noisy Labels,2021,CVPR,Peng Hu;Xi Peng;Hongyuan Zhu;Liangli Zhen;Jie Lin,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Learning_Cross-Modal_Retrieval_With_Noisy_Labels_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Learning+Cross-Modal+Retrieval+With+Noisy+Labels
Cross-Domain Adaptive Clustering for Semi-Supervised Domain Adaptation,2021,CVPR,Jichang Li;Guanbin Li;Yemin Shi;Yizhou Yu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Cross-Domain_Adaptive_Clustering_for_Semi-Supervised_Domain_Adaptation_CVPR_2021_paper.pdf,adaptive;multimodal,https://scholar.google.com/scholar?q=Cross-Domain+Adaptive+Clustering+for+Semi-Supervised+Domain+Adaptation
OTCE: A Transferability Metric for Cross-Domain Cross-Task Representations,2021,CVPR,Yang Tan;Yang Li;Shao-Lun Huang,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Tan_OTCE_A_Transferability_Metric_for_Cross-Domain_Cross-Task_Representations_CVPR_2021_paper.pdf,representation;metric;transfer learning;multimodal,https://scholar.google.com/scholar?q=OTCE:+A+Transferability+Metric+for+Cross-Domain+Cross-Task+Representations
Cross-Modal Center Loss for 3D Cross-Modal Retrieval,2021,CVPR,Longlong Jing;Elahe Vahdani;Jiaxing Tan;Yingli Tian,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf,multimodal;3d,https://scholar.google.com/scholar?q=Cross-Modal+Center+Loss+for+3D+Cross-Modal+Retrieval
Cross-Modal Contrastive Learning for Text-to-Image Generation,2021,CVPR,Han Zhang;Jing Yu Koh;Jason Baldridge;Honglak Lee;Yinfei Yang,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Cross-Modal_Contrastive_Learning_for_Text-to-Image_Generation_CVPR_2021_paper.pdf,generative model;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Contrastive+Learning+for+Text-to-Image+Generation
Defending Multimodal Fusion Models Against Single-Source Adversaries,2021,CVPR,Karren Yang;Wan-Yi Lin;Manash Barman;Filipe Condessa;Zico Kolter,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Defending_Multimodal_Fusion_Models_Against_Single-Source_Adversaries_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Defending+Multimodal+Fusion+Models+Against+Single-Source+Adversaries
PointAugmenting: Cross-Modal Augmentation for 3D Object Detection,2021,CVPR,Chunwei Wang;Chao Ma;Ming Zhu;Xiaokang Yang,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_PointAugmenting_Cross-Modal_Augmentation_for_3D_Object_Detection_CVPR_2021_paper.pdf,augmentation;multimodal;3d,https://scholar.google.com/scholar?q=PointAugmenting:+Cross-Modal+Augmentation+for+3D+Object+Detection
There Is More Than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking With Sound by Distilling Multimodal Knowledge,2021,CVPR,Francisco Rivera Valverde;Juana Valeria Hurtado;Abhinav Valada,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Valverde_There_Is_More_Than_Meets_the_Eye_Self-Supervised_Multi-Object_Detection_CVPR_2021_paper.pdf,zero_few-shot;distillation;multimodal,https://scholar.google.com/scholar?q=There+Is+More+Than+Meets+the+Eye:+Self-Supervised+Multi-Object+Detection+and+Tracking+With+Sound+by+Distilling+Multimodal+Knowledge
Discover Cross-Modality Nuances for Visible-Infrared Person Re-Identification,2021,CVPR,Qiong Wu;Pingyang Dai;Jie Chen;Chia-Wen Lin;Yongjian Wu;Feiyue Huang;Bineng Zhong;Rongrong Ji,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Discover_Cross-Modality_Nuances_for_Visible-Infrared_Person_Re-Identification_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Discover+Cross-Modality+Nuances+for+Visible-Infrared+Person+Re-Identification
Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation,2021,CVPR,Zhekai Du;Jingjing Li;Hongzu Su;Lei Zhu;Ke Lu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Du_Cross-Domain_Gradient_Discrepancy_Minimization_for_Unsupervised_Domain_Adaptation_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Cross-Domain+Gradient+Discrepancy+Minimization+for+Unsupervised+Domain+Adaptation
Robust Multimodal Vehicle Detection in Foggy Weather Using Complementary Lidar and Radar Signals,2021,CVPR,Kun Qian;Shilin Zhu;Xinyu Zhang;Li Erran Li,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Qian_Robust_Multimodal_Vehicle_Detection_in_Foggy_Weather_Using_Complementary_Lidar_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Robust+Multimodal+Vehicle+Detection+in+Foggy+Weather+Using+Complementary+Lidar+and+Radar+Signals
M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-Training,2021,CVPR,Minheng Ni;Haoyang Huang;Lin Su;Edward Cui;Taroon Bharti;Lijuan Wang;Dongdong Zhang;Nan Duan,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Ni_M3P_Learning_Universal_Representations_via_Multitask_Multilingual_Multimodal_Pre-Training_CVPR_2021_paper.pdf,representation;multimodal,https://scholar.google.com/scholar?q=M3P:+Learning+Universal+Representations+via+Multitask+Multilingual+Multimodal+Pre-Training
VisualVoice: Audio-Visual Speech Separation With Cross-Modal Consistency,2021,CVPR,Ruohan Gao;Kristen Grauman,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_VisualVoice_Audio-Visual_Speech_Separation_With_Cross-Modal_Consistency_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=VisualVoice:+Audio-Visual+Speech+Separation+With+Cross-Modal+Consistency
Partially View-Aligned Representation Learning With Noise-Robust Contrastive Loss,2021,CVPR,Mouxing Yang;Yunfan Li;Zhenyu Huang;Zitao Liu;Peng Hu;Xi Peng,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Partially_View-Aligned_Representation_Learning_With_Noise-Robust_Contrastive_Loss_CVPR_2021_paper.pdf,representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Partially+View-Aligned+Representation+Learning+With+Noise-Robust+Contrastive+Loss
StEP: Style-Based Encoder Pre-Training for Multi-Modal Image Synthesis,2021,CVPR,Moustafa Meshry;Yixuan Ren;Larry S. Davis;Abhinav Shrivastava,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Meshry_StEP_Style-Based_Encoder_Pre-Training_for_Multi-Modal_Image_Synthesis_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=StEP:+Style-Based+Encoder+Pre-Training+for+Multi-Modal+Image+Synthesis
Multimodal Motion Prediction With Stacked Transformers,2021,CVPR,Yicheng Liu;Jinghuai Zhang;Liangji Fang;Qinhong Jiang;Bolei Zhou,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Multimodal_Motion_Prediction_With_Stacked_Transformers_CVPR_2021_paper.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Multimodal+Motion+Prediction+With+Stacked+Transformers
EvDistill: Asynchronous Events To End-Task Learning via Bidirectional Reconstruction-Guided Cross-Modal Knowledge Distillation,2021,CVPR,Lin Wang;Yujeong Chae;Sung-Hoon Yoon;Tae-Kyun Kim;Kuk-Jin Yoon,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_EvDistill_Asynchronous_Events_To_End-Task_Learning_via_Bidirectional_Reconstruction-Guided_Cross-Modal_CVPR_2021_paper.pdf,distillation;multimodal,https://scholar.google.com/scholar?q=EvDistill:+Asynchronous+Events+To+End-Task+Learning+via+Bidirectional+Reconstruction-Guided+Cross-Modal+Knowledge+Distillation
LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents,2021,CVPR,ByeoungDo Kim;Seong Hyeon Park;Seokhwan Lee;Elbek Khoshimjonov;Dongsuk Kum;Junsoo Kim;Jeong Soo Kim;Jun Won Choi,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Kim_LaPred_Lane-Aware_Prediction_of_Multi-Modal_Future_Trajectories_of_Dynamic_Agents_CVPR_2021_paper.pdf,reinforcement learning;multimodal,https://scholar.google.com/scholar?q=LaPred:+Lane-Aware+Prediction+of+Multi-Modal+Future+Trajectories+of+Dynamic+Agents
Unbiased Mean Teacher for Cross-Domain Object Detection,2021,CVPR,Jinhong Deng;Wen Li;Yuhua Chen;Lixin Duan,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_Unbiased_Mean_Teacher_for_Cross-Domain_Object_Detection_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Unbiased+Mean+Teacher+for+Cross-Domain+Object+Detection
How2Sign: A Large-Scale Multimodal Dataset for Continuous American Sign Language,2021,CVPR,Amanda Duarte;Shruti Palaskar;Lucas Ventura;Deepti Ghadiyaram;Kenneth DeHaan;Florian Metze;Jordi Torres;Xavier Giro-i-Nieto,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Duarte_How2Sign_A_Large-Scale_Multimodal_Dataset_for_Continuous_American_Sign_Language_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=How2Sign:+A+Large-Scale+Multimodal+Dataset+for+Continuous+American+Sign+Language
Shared Cross-Modal Trajectory Prediction for Autonomous Driving,2021,CVPR,Chiho Choi;Joon Hee Choi;Jiachen Li;Srikanth Malla,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Choi_Shared_Cross-Modal_Trajectory_Prediction_for_Autonomous_Driving_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Shared+Cross-Modal+Trajectory+Prediction+for+Autonomous+Driving
Revamping Cross-Modal Recipe Retrieval With Hierarchical Transformers and Self-Supervised Learning,2021,CVPR,Amaia Salvador;Erhan Gundogdu;Loris Bazzani;Michael Donoser,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Salvador_Revamping_Cross-Modal_Recipe_Retrieval_With_Hierarchical_Transformers_and_Self-Supervised_Learning_CVPR_2021_paper.pdf,zero_few-shot;transformer;multimodal,https://scholar.google.com/scholar?q=Revamping+Cross-Modal+Recipe+Retrieval+With+Hierarchical+Transformers+and+Self-Supervised+Learning
Geo-FARM: Geodesic Factor Regression Model for Misaligned Pre-Shape Responses in Statistical Shape Analysis,2021,CVPR,Chao Huang;Anuj Srivastava;Rongjie Liu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Geo-FARM_Geodesic_Factor_Regression_Model_for_Misaligned_Pre-Shape_Responses_in_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Geo-FARM:+Geodesic+Factor+Regression+Model+for+Misaligned+Pre-Shape+Responses+in+Statistical+Shape+Analysis
Looking Into Your Speech: Learning Cross-Modal Affinity for Audio-Visual Speech Separation,2021,CVPR,Jiyoung Lee;Soo-Whan Chung;Sunok Kim;Hong-Goo Kang;Kwanghoon Sohn,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Looking_Into_Your_Speech_Learning_Cross-Modal_Affinity_for_Audio-Visual_Speech_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Looking+Into+Your+Speech:+Learning+Cross-Modal+Affinity+for+Audio-Visual+Speech+Separation
Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning,2021,ICLR,"['Namyeong Kwon', 'Hwidong Na', 'Gabriel Huang', 'Simon Lacoste-Julien']",poster,"['Meta-learning', 'Few-shot learning', 'Out-of-domain', 'Uncertainty', 'Ensemble', 'Adversarial training', 'Stepsize optimization']","Model-agnostic meta-learning (MAML) is a popular method for few-shot learning but assumes that we have access to the meta-training set. In practice, training on the meta-training set may not always be an option due to data privacy concerns, intellectual property issues, or merely lack of computing resources. In this paper, we consider the novel problem of repurposing pretrained MAML checkpoints to solve new few-shot classification tasks. Because of the potential distribution mismatch, the original MAML steps may no longer be optimal. Therefore we propose an alternative meta-testing procedure and combine MAML gradient steps with adversarial training and uncertainty-based stepsize adaptation. Our method outperforms ""vanilla"" MAML on same-domain and cross-domains benchmarks using both SGD and Adam optimizers and shows improved robustness to the choice of base stepsize.",/pdf/d7cdb4aa01e48fb3d0a82cabe99b8fe0b1c57f47.pdf,graph;zero_few-shot;meta-learning;multimodal,https://scholar.google.com/scholar?q=Repurposing+Pretrained+Models+for+Robust+Out-of-domain+Few-Shot+Learning
A Distributional Perspective on Actor-Critic Framework,2021,ICLR,"['Daniel Wontae Nam', 'Younghoon Kim', 'Chan Youn Park']",poster,"['Value distribution learning', 'reinforcement learning', 'deep learning', 'distributional reinforcement learning', 'distributional actor-critic']","Recent distributional reinforcement learning methods, despite their successes, still contain fundamental problems that can lead to inaccurate representations of value distributions, such as distributional instability, action type restriction, and conflation between samples and statistics. In this paper, we present a novel distributional actor-critic framework, GMAC, to address such problems. Adopting a stochastic policy removes the first two problems, and the conflation in the approximation is alleviated by minimizing the Crame ́r distance between the value distribution and its Bellman target distribution. In addition, GMAC improves data efficiency by generating the Bellman target distribution through the Sample-Replacement algorithm, denoted by SR(λ), which provides a distributional generalization of multi-step policy evaluation algorithms. We empirically show that our method captures the multimodality of value distributions and improves the performance of a conventional actor-critic method with low computational cost in both discrete and continuous action spaces, using Arcade Learning Environment (ALE) and PyBullet environment.",/pdf/a75d1906a44178f2a5a092af52c2c522af21faec.pdf,reinforcement learning;zero_few-shot;representation;multimodal,https://scholar.google.com/scholar?q=A+Distributional+Perspective+on+Actor-Critic+Framework
Cross-Probe BERT for Efficient and Effective Cross-Modal Search,2021,ICLR,"['TAN YU', 'Hongliang Fei', 'Ping Li']",poster,[],"Inspired by the great success of BERT in NLP tasks, many text-vision BERT models emerged recently. Benefited from cross-modal attention,  text-vision BERT models have achieved excellent performance in many language-vision tasks including text-image retrieval.   Nevertheless,  cross-modal attentions used in text-vision BERT models require too expensive computation cost when solving text-vision retrieval, which is impractical for large-scale search. In this work, we develop a novel architecture, Cross-Probe BERT.  It relies on devised text and vision probes, and cross-modal attentions are conducted on text and vision probes.  It takes lightweight computation cost, and meanwhile effectively exploits cross-modal attention.  Systematic experiments conducted on two public benchmarks demonstrate state-of-the-art effectiveness and efficiency of the proposed method.",/pdf/bfe1d62b8bfc84e8b127b5d895a48c66d4de5d0d.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Cross-Probe+BERT+for+Efficient+and+Effective+Cross-Modal+Search
When does preconditioning help or hurt generalization?,2021,ICLR,"['Shun-ichi Amari', 'Jimmy Ba', 'Roger Baker Grosse', 'Xuechen Li', 'Atsushi Nitanda', 'Taiji Suzuki', 'Denny Wu', 'Ji Xu']",poster,"['generalization', 'second-order optimization', 'natural gradient descent', 'high-dimensional asymptotics']","While second order optimizers such as natural gradient descent (NGD) often speed up optimization, their effect on generalization has been called into question. This work presents a more nuanced view on how the \textit{implicit bias} of optimizers affects the comparison of generalization properties. 
We provide an exact asymptotic bias-variance decomposition of the generalization error of preconditioned ridgeless regression in the overparameterized regime, and consider the inverse population Fisher information matrix (used in NGD) as a particular example. We determine the optimal preconditioner $\boldsymbol{P}$ for both the bias and variance, and find that the relative generalization performance of different optimizers depends on label noise and ``shape'' of the signal (true parameters): when the labels are noisy, the model is misspecified, or the signal is misaligned with the features, NGD can achieve lower risk; conversely, GD generalizes better under clean labels, a well-specified model, or aligned signal. 
Based on this analysis, we discuss several approaches to manage the bias-variance tradeoff, and the potential benefit of interpolating between first- and second-order updates. We then extend our analysis to regression in the reproducing kernel Hilbert space and demonstrate that preconditioning can lead to more efficient decrease in the population risk. Lastly, we empirically compare the generalization error of first- and second-order optimizers in neural network experiments, and observe robust trends matching our theoretical analysis. ",/pdf/f3a56e608245c40252059cbf936c8e2ef23f8c8c.pdf,optimization;zero_few-shot;transformer;multimodal,https://scholar.google.com/scholar?q=When+does+preconditioning+help+or+hurt+generalization?
A Good Image Generator Is What You Need for High-Resolution Video Synthesis,2021,ICLR,"['Yu Tian', 'Jian Ren', 'Menglei Chai', 'Kyle Olszewski', 'Xi Peng', 'Dimitris N. Metaxas', 'Sergey Tulyakov']",poster,"['high-resolution video generation', 'contrastive learning', 'cross-domain video generation']","Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques. Code will be released at https://github.com/snap-research/MoCoGAN-HD.",/pdf/08bf1c319723defae9a4e04ca258811da08d2ed3.pdf,graph;zero_few-shot;representation;generative model;multimodal,https://scholar.google.com/scholar?q=A+Good+Image+Generator+Is+What+You+Need+for+High-Resolution+Video+Synthesis
Towards Principled Representation Learning for Entity Alignment,2021,ICLR,"['Lingbing Guo', 'Zequn Sun', 'Mingyang Chen', 'Wei Hu', 'Huajun Chen']",poster,"['Representation Learning', 'Knowledge Graph', 'Entity Alignment', 'Knowledge Graph Embedding']","Knowledge graph (KG) representation learning for entity alignment has recently received great attention. Compared with conventional methods, these embedding-based ones are considered to be robuster for highly-heterogeneous and cross-lingual entity alignment scenarios as they do not rely on the quality of  machine translation or feature extraction. Despite the significant improvement that has been made, there is little understanding of how the embedding-based entity alignment methods actually work. Most existing methods rest on the foundation that a small number of pre-aligned entities can serve as anchors to connect the embedding spaces of two KGs. But no one investigates the rationality of such foundation. In this paper, we define a typical paradigm abstracted from the existing methods, and analyze how the representation discrepancy between two potentially-aligned entities is implicitly bounded by a predefined margin in the scoring function for embedding learning. However, such a margin cannot guarantee to be tight enough for alignment learning. We mitigate this problem by proposing a new approach that explicitly learns KG-invariant and principled entity representations, meanwhile preserves the original infrastructure of existing methods. In this sense, the model not only pursues the closeness of aligned entities on geometric distance, but also aligns the neural ontologies of two KGs to eliminate the discrepancy in feature distribution and underlying ontology knowledge. Our experiments demonstrate consistent and significant improvement in performance against the existing embedding-based entity alignment methods, including several state-of-the-art ones.",/pdf/3300b127d7c97226f47fe6588d36135df3a9e283.pdf,graph;transformer;representation;metric;multimodal,https://scholar.google.com/scholar?q=Towards+Principled+Representation+Learning+for+Entity+Alignment
Multi-modal Self-Supervision from Generalized Data Transformations,2021,ICLR,"['Mandela Patrick', 'Yuki Asano', 'Polina Kuznetsova', 'Ruth Fong', 'Joao F. Henriques', 'Geoffrey Zweig', 'Andrea Vedaldi']",poster,"['video representation learning', 'multi-modal learning', 'self-supervised learning', 'audio-visual learning', 'noise-contrastive learning']","In the image domain, excellent representation can be learned by inducing invariance to content-preserving transformations, such as image distortions. In this paper, we show that, for videos, the answer is more complex, and that better results can be obtained by accounting for the interplay between invariance, distinctiveness, multiple modalities and time.  We introduce Generalized Data Transformations (GDTs) as a way to capture this interplay. GDTs reduce most previous self-supervised approaches to a choice of data transformations, even when this was not the case in the original formulations. They also allow to choose whether the representation should be invariant or distinctive w.r.t. each effect and tell which combinations are valid, thus allowing us to explore the space of combinations systematically. We show in this manner that being invariant to certain transformations and distinctive to others is critical to learning effective video representations, improving the state-of-the-art by a large margin, and even surpassing supervised pretraining. We demonstrate results on a variety of downstream video and audio classification and retrieval tasks, on datasets such as HMDB-51, UCF-101, DCASE2014, ESC-50 and VGG-Sound. In particular, we achieve new state-of-the-art accuracies of 72.8% on HMDB-51 and 95.2% on UCF-101.",/pdf/925a3ab98e402eba7e2993a85a57969dfdf4ca6b.pdf,graph;zero_few-shot;representation;multimodal;self-supervision,https://scholar.google.com/scholar?q=Multi-modal+Self-Supervision+from+Generalized+Data+Transformations
Explainable Subgraph Reasoning for Forecasting on Temporal Knowledge Graphs,2021,ICLR,"['Zhen Han', 'Peng Chen', 'Yunpu Ma', 'Volker Tresp']",poster,"['Temporal knowledge graph', 'future link prediction', 'graph neural network', 'subgraph reasoning.']","Modeling time-evolving knowledge graphs (KGs) has recently gained increasing interest. Here, graph representation learning has become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to interpret their predictions. This paper provides a link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the structural dependencies and the temporal dynamics. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and by attention propagation. Our approach provides human-understandable evidence explaining the forecast. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model obtains a relative improvement of up to 20 $\%$ on Hits@1 compared to the previous best temporal KG forecasting method. We also conduct a survey with 53 respondents, and the results show that the evidence extracted by the model for link forecasting is aligned with human understanding. ",/pdf/0ab0ca1b52f6655da73e49f5bd22facb0665152b.pdf,graph;transformer;representation;multimodal,https://scholar.google.com/scholar?q=Explainable+Subgraph+Reasoning+for+Forecasting+on+Temporal+Knowledge+Graphs
Grounded Language Learning Fast and Slow,2021,ICLR,"['Felix Hill', 'Olivier Tieleman', 'Tamara von Glehn', 'Nathaniel Wong', 'Hamza Merzic', 'Stephen Clark']",poster,"['language', 'cognition', 'fast-mapping', 'grounding', 'word-learning', 'memory', 'meta-learning']","Recent work has shown that large text-based neural language models acquire a surprising propensity for one-shot learning. Here, we show that an agent situated in a simulated 3D world, and endowed with a novel dual-coding external memory, can exhibit similar one-shot word learning when trained with conventional RL algorithms. After a single introduction to a novel object via visual perception and language (""This is a dax""), the agent can manipulate the object as instructed (""Put the dax on the bed""), combining short-term, within-episode knowledge of the nonsense word with long-term lexical and motor knowledge. We find that, under certain training conditions and with a particular memory writing mechanism, the agent's one-shot word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. We further show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful later. Together, the results demonstrate that deep neural networks can exploit meta-learning, episodic memory and an explicitly multi-modal environment to account for 'fast-mapping', a fundamental pillar of human cognitive development and a potentially transformative capacity for artificial agents.   ",/pdf/e357c41d68e8a24bfdaba368a3b2baa867fa25e2.pdf,reinforcement learning;zero_few-shot;meta-learning;multimodal;3d,https://scholar.google.com/scholar?q=Grounded+Language+Learning+Fast+and+Slow
Learning Spatiotemporal Features via Video and Text Pair Discrimination,2021,ICLR,"['Tianhao Li', 'Limin Wang']",poster,"['Spatiotemporal Feature Learning', 'Video and Text Pair Discrimination', 'Self-/Weakly Supervised Learning']","Current video representations heavily rely on learning from manually annotated video datasets which are time-consuming and expensive to acquire. We observe videos are naturally accompanied by abundant text information such as YouTube titles and Instagram captions. In this paper, we leverage this visual-textual connection to learn spatiotemporal features in an efficient weakly-supervised manner. We present a general cross-modal pair discrimination (CPD) framework to capture this correlation between a video and its associated text. We train our CPD models on both standard video dataset (Kinetics-210k) and uncurated web video dataset (Instagram-300k) to demonstrate its effectiveness. Without further fine-tuning, the learnt models obtain competitive results for action classification on Kinetics under the linear classification protocol. Moreover, our visual model provides an effective initialization to fine-tune on downstream tasks, which yields a remarkable performance gain for action recognition on UCF101 and HMDB51, compared with the existing state-of-the-art self-supervised training methods. In addition, our CPD demonstrates that pre-training on a relatively small dataset is able to yield a comparable performance to those methods of using order magnitude more data, which is meaningful and practicable for the scenarios with limited computational facilities.",/pdf/d2c133ea67d988b7aeb8e4d1b7d522cbf04bfeaf.pdf,zero_few-shot;representation;multimodal,https://scholar.google.com/scholar?q=Learning+Spatiotemporal+Features+via+Video+and+Text+Pair+Discrimination
Unsupervised Cross-lingual Representation Learning for Speech Recognition,2021,ICLR,"['Alexis Conneau', 'Alexei Baevski', 'Ronan Collobert', 'Abdelrahman Mohamed', 'Michael Auli']",poster,"['Deep learning', 'speech processing', 'multilingual modeling', 'cross-lingual']","This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72% compared to the best known results. On BABEL, our approach improves word error rate by 16% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages.",/pdf/29371e08bbc339bfa5d49ff4c9e15ed5ae57b5d0.pdf,representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Unsupervised+Cross-lingual+Representation+Learning+for+Speech+Recognition
DINO: A Conditional Energy-Based GAN for Domain Translation,2021,ICLR,"['Konstantinos Vougioukas', 'Stavros Petridis', 'Maja Pantic']",poster,"['Generative Modelling', 'Domain Translation', 'Conditional GANs', 'Energy-Based GANs']","Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.",/pdf/1770fc1a0716d2fde0cefb49d59d540311331789.pdf,graph;generative model;multimodal,https://scholar.google.com/scholar?q=DINO:+A+Conditional+Energy-Based+GAN+for+Domain+Translation
Cross-Modal Retrieval Augmentation for Multi-Modal Classification,2021,ICLR,"['Shir Gur', 'Natalia Neverova', 'Chris Stauffer', 'Ser-Nam Lim', 'Douwe Kiela', 'Austin Reiter']",poster,"['Multi-Modal', 'VQA', 'Retrieval']","Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves state-of-the-art image-caption retrieval performance w.r.t. similar methods.
Second, we show that retrieval-augmented multi-modal transformers using the trained alignment model
significantly improve results on VQA over strong baselines.
We further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.",/pdf/81da21846e7fa5586bc34982e8a31e85cb0ee0d2.pdf,transformer;inference;augmentation;multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Retrieval+Augmentation+for+Multi-Modal+Classification
Reusing Preprocessing Data as Auxiliary Supervision in Conversational Analysis,2021,ICLR,"['Joshua Yee Kim', 'Kalina Yacef']",poster,"['Multitask Learning', 'Multimodal Conversational Analysis']","Conversational analysis systems are trained using noisy human labels and often require heavy preprocessing during multi-modal feature extraction. Using noisy labels in single-task learning increases the risk of over-fitting. However, auxiliary tasks could improve the performance of the primary task learning. This approach is known as Primary Multi-Task Learning (MTL). A challenge of MTL is the selection of beneficial auxiliary tasks that avoid negative transfer. In this paper, we explore how the preprocessed data used for feature engineering can be re-used as auxiliary tasks in Primary MTL, thereby promoting the productive use of data in the form of auxiliary supervision learning. Our main contributions are: (1) the identification of sixteen beneficially auxiliary tasks, (2) the method of distributing learning capacity between the primary and auxiliary tasks, and (3) the relative supervision hierarchy between the primary and auxiliary tasks. Extensive experiments on IEMOCAP and SEMAINE data validate the improvements over single-task approaches, and suggest that it may generalize across multiple primary tasks.",/pdf/da2a1adc3badad1b7601fdfc2d1972c6b1843a75.pdf,graph;transfer learning;multi-task;multimodal,https://scholar.google.com/scholar?q=Reusing+Preprocessing+Data+as+Auxiliary+Supervision+in+Conversational+Analysis
Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets,2021,ICLR,"['Hayeon Lee', 'Eunyoung Hyung', 'Sung Ju Hwang']",poster,"['Machine Learning', 'Neural Architecture Search', 'Meta-learning']","Despite the success of recent Neural Architecture Search (NAS) methods on various tasks which have shown to output networks that largely outperform human-designed networks, conventional NAS methods have mostly tackled the optimization of searching for the network architecture for a single task (dataset), which does not generalize well across multiple tasks (datasets). Moreover, since such task-specific methods search for a neural architecture from scratch for every given task, they incur a large computational cost, which is problematic when the time and monetary budget are limited. In this paper, we propose an efficient NAS framework that is trained once on a database consisting of datasets and pretrained networks and can rapidly search for a neural architecture for a novel dataset. The proposed MetaD2A (Meta Dataset-to-Architecture) model can stochastically generate graphs (architectures) from a given set (dataset) via a cross-modal latent space learned with amortized meta-learning. Moreover, we also propose a meta-performance predictor to estimate and select the best architecture without direct training on target datasets. The experimental results demonstrate that our model meta-learned on subsets of ImageNet-1K and architectures from NAS-Bench 201 search space successfully generalizes to multiple unseen datasets including CIFAR-10 and CIFAR-100, with an average search time of 33 GPU seconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than NSGANetV2, a transferable NAS method, with comparable performance. We believe that the MetaD2A proposes a new research direction for rapid NAS as well as ways to utilize the knowledge from rich databases of datasets and architectures accumulated over the past years. Code is available at https://github.com/HayeonLee/MetaD2A.",/pdf/fdbe572e5160119399f6de757ed8a528ebdd78b1.pdf,graph;optimization;meta-learning;transfer learning;multimodal,https://scholar.google.com/scholar?q=Rapid+Neural+Architecture+Search+by+Learning+to+Generate+Graphs+from+Datasets
A Universal Representation Transformer Layer for Few-Shot Image Classification,2021,ICLR,"['Lu Liu', 'William L. Hamilton', 'Guodong Long', 'Jing Jiang', 'Hugo Larochelle']",poster,[],"Few-shot classification aims to recognize unseen classes when presented with only a small number of samples. We consider the problem of multi-domain few-shot image classification, where unseen classes and examples come from diverse data sources. This problem has seen growing interest and has inspired the development of benchmarks such as Meta-Dataset. A key challenge in this multi-domain setting is to effectively integrate the feature representations from the diverse set of training domains. Here, we propose a Universal Representation Transformer (URT) layer, that meta-learns to leverage universal features for few-shot classification by dynamically re-weighting and composing the most appropriate domain-specific representations. In experiments, we show that URT sets a new state-of-the-art result on Meta-Dataset. Specifically, it achieves top-performance on the highest number of data sources compared to competing methods. We analyze variants of URT and present a visualization of the attention score heatmaps that sheds light on how the model performs cross-domain generalization.",/pdf/82c74f9d1bbe056efab8db3ab6e90c45142d11f3.pdf,transformer;representation;meta-learning;multimodal,https://scholar.google.com/scholar?q=A+Universal+Representation+Transformer+Layer+for+Few-Shot+Image+Classification
On Learning Universal Representations Across Languages,2021,ICLR,"['Xiangpeng Wei', 'Rongxiang Weng', 'Yue Hu', 'Luxi Xing', 'Heng Yu', 'Weihua Luo']",poster,"['universal representation learning', 'cross-lingual pretraining', 'hierarchical contrastive learning']","Recent studies have demonstrated the overwhelming advantage of cross-lingual pre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual NLP tasks. However, existing approaches essentially capture the co-occurrence among tokens through involving the masked language model (MLM) objective with token-level cross entropy. In this work, we extend these approaches to learn sentence-level representations and show the effectiveness on cross-lingual understanding and generation. Specifically, we propose a Hierarchical Contrastive Learning (HiCTL) method to (1) learn universal representations for parallel sentences distributed in one or multiple languages and (2) distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence. We conduct evaluations on two challenging cross-lingual tasks, XTREME and machine translation. Experimental results show that the HiCTL outperforms the state-of-the-art XLM-R by an absolute gain of 4.2% accuracy on the XTREME benchmark as well as achieves substantial improvements on both of the high resource and low-resource English$\rightarrow$X translation tasks over strong baselines.",/pdf/24e87f8b61ab2261652760587470c14f2fef8366.pdf,graph;zero_few-shot;transformer;representation;generative model;contrastive learning;multimodal,https://scholar.google.com/scholar?q=On+Learning+Universal+Representations+Across+Languages
A Benchmark for Voice-Face Cross-Modal Matching and  Retrieval,2021,ICLR,"['Chuyuan Xiong', 'Deyuan Zhang', 'Tao Liu', 'Xiaoyong Du', 'Jiankun Tian', 'Songyan Xue']",poster,"['Cross-Modal Learning', 'Voice-Face Matching', 'Voice-Face Retrieval']","Cross-modal associations between a person's voice and face can be learned algorithmically, and this is a useful functionality in many audio and visual applications. The problem can be defined as two tasks: voice-face matching and retrieval. Recently, this topic has attracted much research attention, but it is still in its early stages of development, and evaluation protocols and test schemes need to be more standardized. Performance metrics for different subtasks are also scarce, and a benchmark for this problem needs to be established. In this paper, a baseline evaluation framework is proposed for voice-face matching and retrieval tasks. Test confidence is analyzed, and a confidence interval for estimated accuracy is proposed. Various state-of-the-art performances with high test confidence are achieved on a series of subtasks using the baseline method  (called TriNet) included in this framework. The source code will be published along with the paper. The results of this study can provide a basis for future research on voice-face cross-modal learning.",/pdf/ce8c756f6b1ad07c5135032cfdf32f1166604520.pdf,transformer;metric;multimodal,https://scholar.google.com/scholar?q=A+Benchmark+for+Voice-Face+Cross-Modal+Matching+and++Retrieval
Cross-Modal Domain Adaptation for Reinforcement Learning,2021,ICLR,"['Xiong-Hui Chen', 'Shengyi Jiang', 'Feng Xu', 'Yang Yu']",poster,"['Domain Adaptation', 'Reinforcement Learning']","Domain adaptation is a promising direction for deploying RL agents in real-world applications, where vision-based robotics tasks constitute an important part. Cur-rent methods that train polices on simulated images not only require a delicately crafted simulator, but also add extra burdens to the training process. In this paper, we propose a method that can learn a mapping from high-dimensional images to low-level simulator states, allowing agents trained on the source domain of state input to transfer well to the target domain of image input. By fully leveraging the sequential information in the trajectories and incorporating the policy to guide the training process, our method overcomes the intrinsic ill-posedness in cross-modal domain adaptation when structural constraints from the same modality are unavailable. Experiments on MuJoCo environments show that the policy, once combined with the mapping function,  can be deployed directly in the target domain with only a small performance gap, while current methods designed for same-modal domain adaptation fail on this problem.",/pdf/0ea377563030f6529a6cbcd5db3f6e4a23a9044d.pdf,reinforcement learning;graph;optimization;zero_few-shot;transfer learning;multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Domain+Adaptation+for+Reinforcement+Learning
Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings,2021,ICLR,"['Linlin Liu', 'Thien Hai Nguyen', 'Shafiq Joty', 'Lidong Bing', 'Luo Si']",poster,[],"Cross-lingual word embeddings (CLWE) have been proven useful in many cross-lingual tasks. However, most existing approaches to learn CLWE including the ones with contextual embeddings are sense agnostic. In this work, we propose a novel framework to align contextual embeddings at the sense level by leveraging cross-lingual signal from bilingual dictionaries only. We operationalize our framework by first proposing a novel sense-aware cross entropy loss to model word senses explicitly. The monolingual ELMo and BERT models pretrained with our sense-aware cross entropy loss demonstrate significant performance improvement for word sense disambiguation tasks. We then propose a sense alignment objective on top of the sense-aware cross entropy loss for cross-lingual model pretraining, and pretrain cross-lingual models for several language pairs (English to German/Spanish/Japanese/Chinese). Compared with the best baseline results, our cross-lingual models achieve 0.52%, 2.09% and 1.29% average performance improvements on zero-shot cross-lingual NER, sentiment classification and XNLI tasks, respectively. We will release our code.",/pdf/46c8460908e4686b4f6b2e45383c7c9eff12dae6.pdf,graph;zero_few-shot;transformer;multimodal,https://scholar.google.com/scholar?q=Towards+Multi-Sense+Cross-Lingual+Alignment+of+Contextual+Embeddings
VECO: Variable Encoder-decoder Pre-training for Cross-lingual Understanding and Generation,2021,ICLR,"['Fuli Luo', 'Wei Wang', 'Jiahao Liu', 'Yijia Liu', 'Bin Bi', 'Songfang Huang', 'Fei Huang', 'Luo Si']",poster,"['Natural Language Processing', 'Representation Learning']","Recent studies about learning multilingual representations have achieved significant performance gains across a wide range of downstream cross-lingual tasks. They train either an encoder-only Transformer mainly for understanding tasks, or an encoder-decoder Transformer specifically for generation tasks, ignoring the correlation between the two tasks and frameworks. In contrast, this paper presents a variable encoder-decoder (VECO) pre-training approach to unify the two mainstreams in both model architectures and pre-training tasks. VECO splits the standard Transformer block into several sub-modules trained with both inner-sequence and cross-sequence masked language modeling, and correspondingly reorganizes certain sub-modules for understanding and generation tasks during inference. Such a workflow not only ensures to train the most streamlined parameters necessary for two kinds of tasks, but also enables them to boost each other via sharing common sub-modules. As a result, VECO delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark covering text classification, sequence labeling, question answering, and sentence retrieval. For generation tasks, VECO also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to 1~2 BLEU.",/pdf/fb3bb3c54fd99596eaae6ee0bfe3eb214f2081c7.pdf,zero_few-shot;transformer;representation;generative model;inference;flow;multimodal,https://scholar.google.com/scholar?q=VECO:+Variable+Encoder-decoder+Pre-training+for+Cross-lingual+Understanding+and+Generation
Grounded Compositional Generalization with Environment Interactions,2021,ICLR,['Yuanpeng Li'],poster,"['compositional generalization', 'grounding']","In this paper, we present a compositional generalization approach in grounded agent instruction learning. Compositional generalization is an important part of human intelligence, but current neural network models do not have such ability. This is more complicated in multi-modal problems with grounding. Our proposed approach has two main ideas. First, we use interactions between agent and the environment to find components in the output. Second, we apply entropy regularization to learn corresponding input components for each output component. The results show the proposed approach significantly outperforms baselines in most tasks, with more than 25% absolute average accuracy increase. We also investigate the impact of entropy regularization and other changes with ablation study. We hope this work is the first step to address grounded compositional generalization, and it will be helpful in advancing artificial intelligence research.
",/pdf/669be6b7068636016fdbe5ccbd133407c852d4c1.pdf,reinforcement learning;multimodal,https://scholar.google.com/scholar?q=Grounded+Compositional+Generalization+with+Environment+Interactions
Modal Uncertainty Estimation via Discrete Latent Representations,2021,ICLR,"['Di Qiu', 'Zhanghan Ke', 'Peng Su', 'Lok Ming Lui']",poster,"['uncertainty estimation', 'one -to-many mapping', 'conditional generative model', 'discrete latent space', 'medical image segmentation']","Many important problems in the real world don't have unique solutions. It is thus important for machine learning models to be capable of proposing different plausible solutions with meaningful probability measures.
In this work we propose a novel deep learning based framework, named {\it modal uncertainty estimation}  (MUE), to learn the one-to-many mappings between the inputs and outputs, together with faithful uncertainty estimation.
Motivated by the multi-modal posterior collapse problem in current conditional generative models, MUE uses a set of discrete latent variables, each representing a latent mode hypothesis that explains one type of input-output relationship, to generate the one-to-many mappings. Benefit from the discrete nature of the latent representations, MUE can estimate any input the conditional probability distribution of the outputs effectively. Moreover, MUE is efficient during training since the discrete latent space and its uncertainty estimation are jointly learned.
We also develop the theoretical background of MUE and extensively validate it on both synthetic and realistic tasks. MUE demonstrates (1) significantly more accurate uncertainty estimation than the current state-of-the-art, and (2) its informativeness for practical use.

",/pdf/33557352a7ba4632d3afc9b06de3be30f2eab419.pdf,representation;generative model;multimodal,https://scholar.google.com/scholar?q=Modal+Uncertainty+Estimation+via+Discrete+Latent+Representations
Towards Defending Multiple Adversarial Perturbations via Gated Batch Normalization,2021,ICLR,"['Aishan Liu', 'Shiyu Tang', 'Xianglong Liu', 'Xinyun Chen', 'Lei Huang', 'Zhuozhuo Tu', 'Dawn Song', 'Dacheng Tao']",poster,"['adversarial examples', 'multiple adversarial peturbation types', 'adversarial robustness']","There is now extensive evidence demonstrating that deep neural networks are vulnerable to adversarial examples, motivating the development of defenses against adversarial attacks. However, existing adversarial defenses typically improve model robustness against individual specific perturbation types. Some recent methods improve model robustness against adversarial attacks in multiple $\ell_p$ balls, but their performance against each perturbation type is still far from satisfactory. To better understand this phenomenon, we propose the \emph{multi-domain} hypothesis, stating that different types of adversarial perturbations are drawn from different domains. Guided by the multi-domain hypothesis, we propose~\emph{Gated Batch Normalization (GBN)}, a novel building block for deep neural networks that improves robustness against multiple perturbation types. GBN consists of a gated sub-network and a multi-branch batch normalization (BN) layer, where the gated sub-network separates different perturbation types, and each BN branch is in charge of a single perturbation type and learns domain-specific statistics for input transformation. Then, features from different branches are aligned as domain-invariant representations for the subsequent layers. We perform extensive evaluations of our approach on MNIST, CIFAR-10, and Tiny-ImageNet, and in doing so demonstrate that GBN outperforms previous defense proposals against multiple perturbation types, \ie, $\ell_1$, $\ell_2$, and $\ell_{\infty}$ perturbations, by large margins of 10-20\%.",/pdf/0e952681c8686c14af9a92c49e7d8bab4dde2b5a.pdf,graph;representation;multimodal,https://scholar.google.com/scholar?q=Towards+Defending+Multiple+Adversarial+Perturbations+via+Gated+Batch+Normalization
A Hypergradient Approach to Robust Regression without Correspondence,2021,ICLR,"['Yujia Xie', 'Yixiu Mao', 'Simiao Zuo', 'Hongteng Xu', 'Xiaojing Ye', 'Tuo Zhao', 'Hongyuan Zha']",poster,"['Regression without correspondence', 'differentiable programming', 'first-order optimization', 'Sinkhorn algorithm']","We consider a regression problem, where the correspondence between the input and output data is not available. Such shuffled data are commonly observed in many real world problems. Take flow cytometry as an example: the measuring instruments are unable to preserve the correspondence between the samples and the measurements. Due to the combinatorial nature of the problem, most of the existing methods are only applicable when the sample size is small, and are limited to linear regression models. To overcome such bottlenecks, we propose a new computational framework --- ROBOT --- for the shuffled regression problem, which is applicable to large data and complex models. Specifically, we propose to formulate regression without correspondence as a continuous optimization problem. Then by exploiting the interaction between the regression model and the data correspondence, we propose to develop a hypergradient approach based on differentiable programming techniques. Such a hypergradient approach essentially views the data correspondence as an operator of the regression model, and therefore it allows us to find a better descent direction for the model parameters by differentiating through the data correspondence. ROBOT is quite general, and can be further extended to an inexact correspondence setting, where the input and output data are not necessarily exactly aligned. Thorough numerical experiments show that ROBOT achieves better performance than existing methods in both linear and nonlinear regression tasks, including real-world applications such as flow cytometry and multi-object tracking.  ",/pdf/01f4f60af211bcbeb9691824fbd8f92211e50098.pdf,optimization;zero_few-shot;online learning;flow;multimodal,https://scholar.google.com/scholar?q=A+Hypergradient+Approach+to+Robust+Regression+without+Correspondence
Learning to Infer Run-Time Invariants from Source code,2021,ICLR,"['Vincent Josua Hellendoorn', 'Premkumar Devanbu', 'Alex Polozov', 'Mark Marron']",poster,"['Invariants', 'Software Engineering', 'Programming Languages']","Source code is notably different from natural language in that it is meant to be executed. Experienced developers infer complex ""invariants"" about run-time state while reading code, which helps them to constrain and predict program behavior. Knowing these invariants can be helpful; yet developers rarely encode these explicitly, so machine-learning methods don't have much aligned data to learn from. We propose an approach that adapts cues within existing if-statements  regarding explicit run-time expectations to generate aligned datasets of code and implicit invariants. We also propose a contrastive loss to inhibit generation of illogical invariants. Our model learns to infer a wide vocabulary of invariants for arbitrary code, which can be used to detect and repair real bugs. This is entirely complementary to established approaches, which either use logical engines that scale poorly, or run-time traces that are expensive to obtain; when present, that data can complement our tool, as we demonstrate in conjunction with Daikon, an existing tool. Our results show that neural models can derive useful representations of run-time behavior directly from source code.",/pdf/9a8b40a23dafc62eba2855dbb30fde364d8fa4ef.pdf,graph;optimization;representation;generative model;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Learning+to+Infer+Run-Time+Invariants+from+Source+code
Grounding Language to Entities for Generalization in Reinforcement Learning,2021,ICLR,"['H. J. Austin Wang', 'Karthik R Narasimhan']",poster,"['reinforcement learning', 'language grounding']","In this paper, we consider the problem of leveraging textual descriptions to improve generalization of control policies to new scenarios. Unlike prior work in this space, we do not assume access to any form of prior knowledge connecting text and state observations, and learn both symbol grounding and control policy simultaneously. This is challenging due to a lack of concrete supervision, and incorrect groundings can result in worse performance than policies that do not use the text at all. 
We develop a new model, EMMA (Entity Mapper with Multi-modal Attention) which uses a multi-modal entity-conditioned attention module that allows for selective focus over relevant sentences in the manual for each entity in the environment. EMMA is end-to-end differentiable and can learn a latent grounding of entities and dynamics from text to observations using environment rewards as the only source of supervision.
To empirically test our model, we design a new framework of 1320 games and collect text manuals with free-form natural language via crowd-sourcing. We demonstrate that EMMA achieves successful zero-shot generalization to unseen games with new dynamics, obtaining significantly higher rewards compared to multiple baselines. The grounding acquired by EMMA is also robust to noisy descriptions and linguistic variation.",/pdf/b623f87a2e8f4603ca80b916a8dbb1ec49618fab.pdf,reinforcement learning;graph;zero_few-shot;transformer;multimodal,https://scholar.google.com/scholar?q=Grounding+Language+to+Entities+for+Generalization+in+Reinforcement+Learning
Non-robust Features through the Lens of Universal Perturbations,2021,ICLR,"['Sung Min Park', 'Kuo-An Wei', 'Kai Yuanqing Xiao', 'Jerry Li', 'Aleksander Madry']",poster,"['adversarial examples', 'robustness', 'non-robust features']","Recent work ties adversarial examples to existence of non-robust features: features which are susceptible to small perturbations and believed to be unintelligible to humans, but still useful for prediction. We study universal adversarial perturbations and demonstrate that the above picture is more nuanced. Specifically, even though universal perturbations---similarly to standard adversarial perturbations---do leverage non-robust features, these features tend to be fundamentally different from the ``standard'' ones and, in particular, non-trivially human-aligned. Namely, universal perturbations have more human-aligned locality and spatial invariance properties. However, we also show that these human-aligned non-robust features have much less predictive signal than general non-robust features. Our findings thus take a step towards improving our understanding of these previously unintelligible features.",/pdf/b01ee46313ef48630aa6d3cb38400866db43ec8b.pdf,multimodal,https://scholar.google.com/scholar?q=Non-robust+Features+through+the+Lens+of+Universal+Perturbations
On Learning Read-once DNFs With Neural Networks,2021,ICLR,"['Ido Bronstein', 'Alon Brutzkus', 'Amir Globerson']",poster,"['neural network', 'DNF', 'read-once', 'inductive bias', 'reconstruction', 'alignment']","Learning functions over Boolean variables is a fundamental problem in machine learning. But not much is known about learning such functions by neural networks. Because learning these functions in the distribution free setting is NP-Hard, they are unlikely to be efficiently learnable by networks in this case. However, assuming the inputs are sampled from the uniform distribution, an important subset of functions that are known to be efficiently learnable is read-once DNFs. Here we focus on this setting where the functions are learned by a convex neural network and gradient descent. 
We first observe empirically that the learned neurons are aligned with the terms of the DNF, despite the fact that there are many zero-error networks that do not have this property. Thus, the learning process has a clear inductive bias towards such logical formulas. To gain a better theoretical understanding of this phenomenon we focus on minimizing the population risk. We show that this risk can be minimized by multiple networks: from ones that memorize data to ones that compactly represent the DNF. We then set out to understand why gradient descent ``""chooses"" the compact representation. 
We use a computer assisted proof to prove the inductive bias for relatively small DNFs, and use it to design a process for reconstructing the DNF from the learned network. We then continue to provide theoretical insights on the learning process and the loss surface to better understand the resulting inductive bias. For example, we show that the neurons in solutions with minimum $l_2$-norm of the weights are also aligned with the terms of the DNF. Finally, we empirically show that our results are validated in the empirical case for high dimensional DNFs, more general network architectures and tabular datasets.",/pdf/b582e8c072b5e37b4ed35523bea1b2eb36f0b54b.pdf,representation;multimodal,https://scholar.google.com/scholar?q=On+Learning+Read-once+DNFs+With+Neural+Networks
On Alignment in Deep Linear Neural Networks,2021,ICLR,"['Adityanarayanan Radhakrishnan', 'Eshaan Nichani', 'Daniel Bernstein', 'Caroline Uhler']",poster,"['Alignment', 'Linear Neural Networks', 'Implicit Regularization']","    We study the properties of alignment, a form of implicit regularization, in linear neural networks under gradient descent.  We define alignment for fully connected networks with multidimensional outputs and show that it is a natural extension of alignment in networks with 1-dimensional outputs as defined by Ji and Telgarsky, 2018.  While in fully connected networks, there always exists a global minimum corresponding to an aligned solution, we analyze alignment as it relates to the training process.  Namely, we characterize when alignment is an invariant of training under gradient descent by providing necessary and sufficient conditions for this invariant to hold. In such settings, the dynamics of gradient descent simplify, thereby allowing us to provide an explicit learning rate under which the network converges linearly to a global minimum.  We then analyze networks with layer constraints such as convolutional networks. In this setting, we prove that gradient descent is equivalent to projected gradient descent, and that alignment is impossible with sufficiently large datasets.  
",/pdf/1d640d09b4b14153e9215e013fb0bda05974459d.pdf,optimization;zero_few-shot;multimodal,https://scholar.google.com/scholar?q=On+Alignment+in+Deep+Linear+Neural+Networks
Multimodal Attention for Layout Synthesis in Diverse Domains,2021,ICLR,"['Kamal Gupta', 'Vijay Mahadevan', 'Alessandro Achille', 'Justin Lazarow', 'Larry S. Davis', 'Abhinav Shrivastava']",poster,"['layout generation', 'layout synthesis', 'multimodal attention', 'transformers', 'document layouts', 'generative model', '3D']","We address the problem of scene layout generation for diverse domains such as images, mobile applications, documents and 3D objects. Most complex scenes, natural or human-designed, can be expressed as a meaningful arrangement of simpler compositional graphical primitives. Generating a new layout or extending an existing layout requires understanding the relationships between these primitives. To do this, we propose a multimodal attention framework, MMA, that leverages self-attention to learn contextual relationships between layout elements and generate novel layouts in a given domain. Our framework allows us to generate a new layout either from an empty set or from an initial seed set of primitives, and can easily scale to support an arbitrary of primitives per layout. Further, our analyses show that the model is able to automatically capture the semantic properties of the primitives. We propose simple improvements in both representation of layout primitives, as well as training methods to demonstrate competitive performance in very diverse data domains such as object bounding boxes in natural images (COCO bounding boxes), documents (PubLayNet), mobile applications (RICO dataset) as well as 3D shapes (PartNet).",/pdf/7148445c3c9718b886b2066e8d5fed0f5fcb2e6b.pdf,graph;zero_few-shot;transformer;representation;generative model;multimodal;3d,https://scholar.google.com/scholar?q=Multimodal+Attention+for+Layout+Synthesis+in+Diverse+Domains
Aligning AI With Shared Human Values,2021,ICLR,"['Dan Hendrycks', 'Collin Burns', 'Steven Basart', 'Andrew Critch', 'Jerry Li', 'Dawn Song', 'Jacob Steinhardt']",poster,"['value learning', 'human preferences', 'alignment']","We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",/pdf/d05fd010cf3982444373e7567a61c45c18c0f61f.pdf,reinforcement learning;multimodal,https://scholar.google.com/scholar?q=Aligning+AI+With+Shared+Human+Values
Spectral Synthesis for Satellite-to-Satellite Translation,2021,ICLR,"['Thomas Vandal', 'Daniel McDuff', 'Weile Wang', 'Andrew Michaelis', 'Ramakrishna Nemani']",poster,[],"Earth observing satellites carrying multi-spectral sensors are widely used to monitor the physical and biological states of the atmosphere, land, and oceans. These satellites have different vantage points above the earth and different spectral imaging bands resulting in inconsistent imagery from one to another. This presents challenges in building downstream applications. What if we could generate synthetic bands for existing satellites from the union of all domains? We tackle the problem of generating synthetic spectral imagery for multispectral sensors as an unsupervised image-to-image translation problem with partial labels and introduce a novel shared spectral reconstruction loss. Simulated experiments performed by dropping one or more spectral bands show that cross-domain reconstruction outperforms measurements obtained from a second vantage point. On a downstream cloud detection task, we show that generating synthetic bands with our model improves segmentation performance beyond our baseline. Our proposed approach enables synchronization of multispectral data and provides a basis for more homogeneous remote sensing datasets.",/pdf/33b6aa5ecd9698d8d438a01f1eb206607af88c54.pdf,graph;segmentation;multimodal,https://scholar.google.com/scholar?q=Spectral+Synthesis+for+Satellite-to-Satellite+Translation
AdaDGS: An adaptive black-box optimization method with a nonlocal directional Gaussian smoothing gradient,2021,ICLR,"['Hoang A Tran', 'Guannan Zhang']",poster,[],"The local gradient points to the direction of the steepest slope in an infinitesimal neighborhood. An optimizer guided by the local gradient is often trapped in local optima when the loss landscape is multi-modal. A directional Gaussian smoothing (DGS) approach was recently proposed in (Zhang et al., 2020) and used to define a truly nonlocal gradient, referred to as the DGS gradient, for high-dimensional black-box optimization.  Promising results show that replacing the traditional local gradient with the DGS gradient can significantly improve the performance of gradient-based methods in optimizing highly multi-modal loss functions.  However, the optimal performance of the DGS gradient may rely on fine tuning of two important hyper-parameters, i.e., the smoothing radius and the learning rate.  In this paper, we present a simple, yet ingenious and efficient adaptive approach for optimization with the DGS gradient, which removes the need of hyper-parameter fine tuning.  Since the DGS gradient generally points to a good search direction, we perform a line search along the DGS direction to determine the step size at each iteration. The learned step size in turn will inform us of the scale of function landscape in the surrounding area, based on which we adjust the smoothing radius accordingly for the next iteration. We present experimental results on high-dimensional benchmark functions, an airfoil design problem and a game content generation problem. The AdaDGS method has shown superior performance over several the state-of-the-art black-box optimization methods.",/pdf/ee389da57b7b95da9ec8c37bee732aed581144bb.pdf,optimization;generative model;adaptive;multimodal,https://scholar.google.com/scholar?q=AdaDGS:+An+adaptive+black-box+optimization+method+with+a+nonlocal+directional+Gaussian+smoothing+gradient
Importance-based Multimodal Autoencoder,2021,ICLR,"['Sayan Ghosh', 'Eugene Laksana', 'Louis-Philippe Morency', 'Stefan Scherer']",poster,"['Neural networks', 'Speech analysis', 'Multimodal', 'Autoencoders', 'Representation Learning']","Integrating information from multiple modalities (e.g.,  verbal,  acoustic and visual data) into meaningful representations has seen great progress in recent years. However, two challenges are not sufficiently addressed by current approaches: (1) computationally efficient training of multimodal autoencoder networks which are robust in the absence of modalities, and (2) unsupervised learning of important subspaces in each modality which are correlated with other modalities.   In this paper we propose the IMA (Importance-based Multimodal Autoencoder) model, a scalable model that learns modality importances and robust multimodal representations through a novel cross-covariance based loss function.  We conduct experiments on MNIST-TIDIGITS a multimodal dataset of spoken and image digits,and on IEMOCAP, a multimodal emotion corpus. The IMA model is able to distinguish  digits  from  uncorrelated  noise,  and  word-level  importances  learnt  that correspond to the separation between function and emotional words.  The multimodal representations learnt by IMA are also competitive with state-of-the-art baseline approaches on downstream tasks.",/pdf/a5d1fba75c2e2b8167d3d44d9c55ae5c8b87ba7f.pdf,representation;multimodal,https://scholar.google.com/scholar?q=Importance-based+Multimodal+Autoencoder
A Flexible Framework for Discovering Novel Categories with Contrastive Learning,2021,ICLR,"['Xuhui Jia', 'Kai Han', 'Yukun Zhu', 'Bradley Green']",poster,"['deep learning', 'novel classes', 'clustering', 'self-supervised learning', 'unsupervised learning']","This paper studies the problem of novel category discovery on single- and multi-modal data with labels from different but relevant categories. We present a generic, end-to-end framework to jointly learn a reliable representation and assign clusters to unlabelled data. To avoid over-fitting the learnt embedding to labelled data, we take inspiration from self-supervised representation learning by noise-contrastive estimation and extend it to jointly handle labelled and unlabelled data. In particular, we proposed using category discrimination on labelled data and cross-modal discrimination on multi-modal data to augment instance discrimination used in conventional contrastive learning approaches. We further introduce Winner-Take-All (WTA) hashing algorithm on the shared representation space to generate pairwise pseudo labels for unlabelled data to better predict cluster assignments. We thoroughly evaluate our framework on large-scale multi-modal video benchmarks Kinetics-400 and VGG-Sound, and image benchmarks CIFAR10, CIFAR100 and ImageNet, obtaining state-of-the-art results.",/pdf/8414b8db7dd08447ef51961809911ca96f541635.pdf,zero_few-shot;representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=A+Flexible+Framework+for+Discovering+Novel+Categories+with+Contrastive+Learning
How to Design Sample and Computationally Efficient VQA Models,2021,ICLR,"['Karan Samel', 'Zelin Zhao', 'Kuan Wang', 'Robin Luo', 'Binghong Chen', 'Le Song']",poster,"['vqa', 'visual question answering', 'neural modules', 'probabilistic logic']","In multi-modal reasoning tasks, such as visual question answering (VQA), there have been many modeling and training paradigms tested. Previous models propose different methods for the vision and language tasks, but which ones perform the best while being sample and computationally efficient? Based on our experiments, we find that representing the text as probabilistic programs and images as object-level scene graphs best satisfy these desiderata. We extend existing models to leverage these soft programs and scene graphs to train on question answer pairs in an end-to-end manner. Empirical results demonstrate that this differentiable end-to-end program executor is able to maintain state-of-the-art accuracy while being sample and computationally efficient.",/pdf/993b247d28594eb01131e24f59a1eea94bbfa5f8.pdf,graph;multimodal,https://scholar.google.com/scholar?q=How+to+Design+Sample+and+Computationally+Efficient+VQA+Models
Cross-model Back-translated Distillation for Unsupervised Machine Translation,2021,ICLR,"['Phi Xuan Nguyen', 'Shafiq Joty', 'Kui Wu', 'AiTi Aw']",poster,"['unsupervised machine translation', 'NMT', 'machine translation']","Recent unsupervised machine translation (UMT) systems usually employ three main principles: initialization, language modeling and iterative back-translation, though they may apply them differently. Crucially, iterative back-translation and denoising auto-encoding for language modeling provide data diversity to train the UMT systems. However, these diversification processes may have reached their limit. We introduce a novel component to the standard UMT framework called Cross-model Back-translated Distillation (CBD), that is aimed to induce another level of data diversification that existing principles lack. CBD is applicable to all previous UMT approaches. In our experiments, it boosts the performance of the standard UMT methods by 1.5-2.0 BLEU. In particular, in WMT'14 English-French, WMT'16 German-English and English-Romanian, CBD outperforms cross-lingual masked language model (XLM) by 2.3, 2.2 and 1.6 BLEU, respectively. It also yields 1.5-3.3 BLEU improvements in IWSLT English-French and English-German tasks. Through extensive experimental analyses, we show that CBD is effective because it embraces data diversity while other similar variants do not.",/pdf/7188c1b2b5af74e92001927d2f7750e0e22447b5.pdf,distillation;multimodal,https://scholar.google.com/scholar?q=Cross-model+Back-translated+Distillation+for+Unsupervised+Machine+Translation
Learning Neural Generative Dynamics for Molecular Conformation Generation,2021,ICLR,"['Minkai Xu', 'Shitong Luo', 'Yoshua Bengio', 'Jian Peng', 'Jian Tang']",poster,"['Molecular conformation generation', 'deep generative models', 'continuous normalizing flow', 'energy-based models']","We study how to generate molecule conformations (i.e., 3D structures) from a molecular graph. Traditional methods, such as molecular dynamics, sample conformations via computationally expensive simulations. Recently, machine learning methods have shown great potential by training on a large collection of conformation data. Challenges arise from the limited model capacity for capturing complex distributions of conformations and the difficulty in modeling long-range dependencies between atoms. Inspired by the recent progress in deep generative models, in this paper, we propose a novel probabilistic framework to generate valid and diverse conformations given a molecular graph. We propose a method combining the advantages of both flow-based and energy-based models, enjoying: (1) a high model capacity to estimate the multimodal conformation distribution; (2) explicitly capturing the complex long-range dependencies between atoms in the observation space. Extensive experiments demonstrate the superior performance of the proposed method on several benchmarks, including conformation generation and distance modeling tasks, with a significant improvement over existing generative models for molecular conformation sampling.",/pdf/90d50e0ca739c22ebb906023d446dc8c8f98a7e0.pdf,graph;zero_few-shot;generative model;flow;multimodal;3d,https://scholar.google.com/scholar?q=Learning+Neural+Generative+Dynamics+for+Molecular+Conformation+Generation
Improving robustness of softmax corss-entropy loss via inference information,2021,ICLR,"['Bingbing Song', 'Wei He', 'Renyang Liu', 'Shui Yu', 'Ruxin Wang', 'Mingming Gong', 'Tongliang Liu', 'Wei Zhou']",poster,"['Adversarial defense', 'Loss function', 'Neural networks robustness']","Adversarial examples easily mislead the vision systems based on deep neural networks (DNNs) trained with the softmax cross entropy (SCE) loss. Such a vulnerability of DNN comes from the fact that SCE drives DNNs to fit on the training samples, whereas the resultant feature distributions between the training and adversarial examples are unfortunately misaligned. Several state-of-the-arts start from improving the inter-class separability of training samples by modifying loss functions, where we argue that the adversarial examples are ignored and thus limited robustness to adversarial attacks is resulted. In this paper, we exploit inference region which inspires us to involve a margin-like inference information to SCE, resulting in a novel inference-softmax cross entropy (I-SCE) loss, which is intuitively appealing and interpretable. The inference information is a guarantee to both the inter-class separability and the improved generalization to adversarial examples, which is furthermore demonstrated under the min-max framework. Extensive experiments show that under strong adaptive attacks, the DNN models trained with the proposed I-SCE loss achieve superior performance and robustness over the state-of-the-arts.",/pdf/edd8c6db7a847da99b8d3251740e82df70bec1f2.pdf,graph;adaptive;inference;multimodal,https://scholar.google.com/scholar?q=Improving+robustness+of+softmax+corss-entropy+loss+via+inference+information
Relating by Contrasting: A Data-efficient Framework for Multimodal Generative Models,2021,ICLR,"['Yuge Shi', 'Brooks Paige', 'Philip Torr', 'Siddharth N']",poster,"['Deep generative model', 'multi-modal learning', 'representation learning']","Multimodal learning for generative models often refers to the learning of abstract concepts from the commonality of information in multiple modalities, such as vision and language. While it has proven effective for learning generalisable representations, the training of such models often requires a large amount of related multimodal data that shares commonality, which can be expensive to come by. To mitigate this, we develop a novel contrastive framework for generative model learning, allowing us to train the model not just by the commonality between modalities, but by the distinction between ""related"" and ""unrelated"" multimodal data. We show in experiments that our method enables data-efficient multimodal learning on challenging datasets for various multimodal VAE models. We also show that under our proposed framework, the generative model can accurately identify related samples from unrelated ones, making it possible to make use of the plentiful unlabeled, unpaired multimodal data.",/pdf/9f989526ad09a1935c06a52631aac027c4f90633.pdf,graph;zero_few-shot;representation;vae;generative model;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Relating+by+Contrasting:+A+Data-efficient+Framework+for+Multimodal+Generative+Models
Variational Invariant Learning for Bayesian Domain Generalization,2021,ICLR,"['Zehao Xiao', 'Jiayi Shen', 'Xiantong Zhen', 'Ling Shao', 'Cees G. M. Snoek']",poster,"['domain generalization', 'variational invariant learning', 'Bayesian inference']","Domain generalization addresses the out-of-distribution problem, which is challenging due to the domain shift and the uncertainty caused by the inaccessibility to data from the target domains. In this paper, we propose variational invariant learning, a probabilistic inference framework that jointly models domain invariance and uncertainty. We introduce variational Bayesian approximation into both the feature representation and classifier layers to facilitate invariant learning for better generalization across domains. In the probabilistic modeling framework, we introduce a domain-invariant principle to explore invariance across domains in a unified way. We incorporate the principle into the variational Bayesian layers in neural networks, achieving domain-invariant representations and classifier. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies demonstrate the benefits of our proposal and on all benchmarks our variational invariant learning consistently delivers state-of-the-art performance. ",/pdf/4964403e8dbac758c6a7c684ffdf59b5fef70e7a.pdf,graph;representation;bayesian;inference;multimodal,https://scholar.google.com/scholar?q=Variational+Invariant+Learning+for+Bayesian+Domain+Generalization
BOIL: Towards Representation Change for Few-shot Learning,2021,ICLR,"['Jaehoon Oh', 'Hyungjun Yoo', 'ChangHwan Kim', 'Se-Young Yun']",poster,[],"Model Agnostic Meta-Learning (MAML) is one of the most representative of gradient-based meta-learning algorithms. MAML learns new tasks with a few data samples using inner updates from a meta-initialization point and learns the meta-initialization parameters with outer updates. It has recently been hypothesized that representation reuse, which makes little change in efficient representations, is the dominant factor in the performance of the meta-initialized model through MAML in contrast to representation change, which causes a significant change in representations. In this study, we investigate the necessity of representation change for the ultimate goal of few-shot learning, which is solving domain-agnostic tasks. To this aim, we propose a novel meta-learning algorithm, called BOIL (Body Only update in Inner Loop), which updates only the body (extractor) of the model and freezes the head (classifier) during inner loop updates. BOIL leverages representation change rather than representation reuse. A frozen head cannot achieve better results than even a random guessing classifier at the initial point of new tasks, and feature vectors (representations) have to move quickly to their corresponding frozen head vectors. We visualize this property using cosine similarity, CKA, and empirical results without the head. Although the inner loop updates purely hinge on representation change, BOIL empirically shows significant performance improvement over MAML, particularly on cross-domain tasks. The results imply that representation change in gradient-based meta-learning approaches is a critical component.",/pdf/1a8c0d92bdde5a60d7b354b43c36e22abacfc99a.pdf,zero_few-shot;representation;meta-learning;multimodal,https://scholar.google.com/scholar?q=BOIL:+Towards+Representation+Change+for+Few-shot+Learning
"MultiModalQA: complex question answering over text, tables and images",2021,ICLR,"['Alon Talmor', 'Ori Yoran', 'Amnon Catav', 'Dan Lahav', 'Yizhong Wang', 'Akari Asai', 'Gabriel Ilharco', 'Hannaneh Hajishirzi', 'Jonathan Berant']",poster,"['NLP', 'Question Answering', 'Dataset', 'Multi-Modal', 'Multi-Hop']","When answering complex questions, people can seamlessly combine information from visual, textual and tabular sources. 
While interest in models that reason over multiple pieces of evidence has surged in recent years, there has been relatively little work on question answering models that reason across multiple modalities.
In this paper, we present MultiModalQA (MMQA): a challenging question answering dataset that requires joint reasoning over text, tables and images. 
We create MMQA using a new framework for generating complex multi-modal questions at scale, harvesting tables from Wikipedia, and attaching images and text paragraphs using entities that appear in each table. We then define a formal language that allows us to take questions that can be answered from a single modality, and combine them to generate cross-modal questions. Last, crowdsourcing workers take these automatically generated questions and rephrase them into more fluent language.
We create 29,918 questions through this procedure, and empirically demonstrate the necessity of a multi-modal multi-hop approach to solve our task: our multi-hop model, ImplicitDecomp, achieves an average F1 of 51.7  over cross-modal questions, substantially outperforming a strong baseline that achieves 38.2 F1, but still lags significantly behind human performance, which is at 90.1 F1.",/pdf/f3dad930cb55abce99a229e35cc131a2db791b66.pdf,graph;zero_few-shot;multimodal,"https://scholar.google.com/scholar?q=MultiModalQA:+complex+question+answering+over+text,+tables+and+images"
K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters,2021,ICLR,"['Ruize Wang', 'Duyu Tang', 'Nan Duan', 'zhongyu wei', 'Xuanjing Huang', 'Jianshu Ji', 'Guihong Cao', 'Daxin Jiang', 'Ming Zhou']",poster,[],"We study the problem of injecting knowledge into large pre-trained models like BERT and RoBERTa. Existing methods typically update the original parameters of pre-trained models when injecting knowledge. However, when multiple kinds of knowledge are injected, they may suffer from catastrophic forgetting.  To address this, we propose K-Adapter, which remains the original parameters of the pre-trained model fixed and supports continual knowledge infusion. Taking RoBERTa as the pre-trained model, K-Adapter has a neural adapter for each kind of infused knowledge, like a plug-in connected to RoBERTa. There is no information flow between different adapters, thus different adapters are efficiently trained in a distributed way. We inject two kinds of knowledge, including factual knowledge obtained from automatically aligned text-triplets on Wikipedia and Wikidata, and linguistic knowledge obtained from dependency parsing. Results on three knowledge-driven tasks (total six datasets) including relation classification, entity typing and question answering demonstrate that each adapter improves the performance, and the combination of both adapters brings further improvements. Probing experiments further indicate that K-Adapter captures richer factual and commonsense knowledge than RoBERTa.",/pdf/d61bb4c56ead10012fe2e4f86b4c402af4653b6a.pdf,graph;zero_few-shot;transformer;flow;multimodal,https://scholar.google.com/scholar?q=K-Adapter:+Infusing+Knowledge+into+Pre-Trained+Models+with+Adapters
Switching-Aligned-Words Data Augmentation for Neural Machine Translation,2021,ICLR,"['Fengshun Xiao', 'Zuchao Li', 'hai zhao']",poster,"['Machine Translation', 'Data augmentation']","In neural machine translation (NMT), data augmentation methods such as back-translation make it possible to use extra monolingual data to help improve translation performance, while it needs extra training data and the in-domain monolingual data is not always available. In this paper, we present a novel data augmentation method for neural machine translation by using only the original training data without extra data. More accurately, we randomly replace words or mixup with their aligned alternatives in another language when training neural machine translation models. Since aligned word pairs appear in the same position of each other during training, it is helpful to form bilingual embeddings which are proved useful to provide a performance boost \citep{liu2019shared}. Experiments on both small and large scale datasets show that our method significantly outperforms the baseline models.",/pdf/2dda90597d761c6c4eb3ec487711fbe704bbcd21.pdf,graph;augmentation;multimodal,https://scholar.google.com/scholar?q=Switching-Aligned-Words+Data+Augmentation+for+Neural+Machine+Translation
Unbiased Learning with State-Conditioned Rewards in Adversarial Imitation Learning,2021,ICLR,"['Dong-Sig Han', 'Hyunseo Kim', 'Hyundo Lee', 'Je-Hwan Ryu', 'Byoung-Tak Zhang']",poster,"['Adversarial Learning', 'Imitation Learning', 'Inverse Reinforcement Learning', 'Reinforcement Learning', 'Transfer Learning']","Adversarial imitation learning has emerged as a general and scalable framework for automatic reward acquisition. However, we point out that previous methods commonly exploited occupancy-dependent reward learning formulation—which hinders the reconstruction of optimal decision as an energy-based model. Despite the theoretical justification, the occupancy measures tend to cause issues in practice because of high variance and low vulnerability to domain shifts. Another reported problem is termination biases induced by provided rewarding and regularization schemes around terminal states. In order to deal with these issues, this work presents a novel algorithm called causal adversarial inverse reinforcement learning. Our formulation draws a strong connection between adversarial learning and energy-based reinforcement learning; thus, the architecture is capable of recovering a reward function that induces a multi-modal policy. In experiments, we demonstrate that our approach outperforms prior methods in challenging continuous control tasks, even under significant variation in the environments.",/pdf/ea713baf7de08de4cc79b3b88ea21bf1482a7260.pdf,reinforcement learning;graph;zero_few-shot;multimodal;imitation learning,https://scholar.google.com/scholar?q=Unbiased+Learning+with+State-Conditioned+Rewards+in+Adversarial+Imitation+Learning
Probabilistic Mixture-of-Experts for Efficient Deep Reinforcement Learning,2021,ICLR,"['Jie Ren', 'Yewen Li', 'Zihan Ding', 'Wei Pan', 'Hao Dong']",poster,"['Deep Reinforcement Learning', 'Sample Efficiency', 'Gaussian Mixture Models', 'Mixture-of-Experts']","Deep reinforcement learning (DRL) has successfully solved various problems recently, typically with a unimodal policy representation. However, grasping the decomposable and hierarchical structures within a complex task can be essential for further improving its learning efficiency and performance, which may lead to a multimodal policy or a mixture-of-experts (MOE). To our best knowledge, present DRL algorithms for general utility do not deploy MOE methods as policy function approximators due to the lack of differentiability, or without explicit probabilistic representation. In this work, we propose a differentiable probabilistic mixture-of-experts (PMOE) embedded in the end-to-end training scheme for generic off-policy and on-policy algorithms using stochastic policies, e.g., Soft Actor-Critic (SAC) and Proximal Policy Optimisation (PPO). Experimental results testify the advantageous performance of our method over unimodal polices and three different MOE methods, as well as a method of option frameworks, based on two types of DRL algorithms. We also demonstrate the distinguishable primitives learned with PMOE in different environments.",/pdf/7908c48efdf3950042d552df0317bde9c1233fa5.pdf,reinforcement learning;representation;multimodal,https://scholar.google.com/scholar?q=Probabilistic+Mixture-of-Experts+for+Efficient+Deep+Reinforcement+Learning
XLA: A Robust Unsupervised Data Augmentation Framework for Cross-Lingual NLP,2021,ICLR,"['M Saiful Bari', 'Tasnim Mohiuddin', 'Shafiq Joty']",poster,"['multi-lingual', 'cross-lingual', 'data-augmentation', 'nlp']","Transfer learning has yielded state-of-the-art (SoTA) results in many supervised NLP tasks. However,  annotated data for every target task in every target language is rare, especially for low-resource languages. We propose XLA, a novel data augmentation framework for self-supervised learning in zero-resource transfer learning scenarios. In particular, XLA aims to solve cross-lingual adaptation problems from a source language task distribution to an unknown target language task distribution, assuming no training label in the target language task. At its core, XLA performs simultaneous self-training with data augmentation and unsupervised sample selection. To show its effectiveness, we conduct extensive experiments on zero-resource cross-lingual transfer tasks for Named Entity Recognition (NER), Natural Language Inference (NLI) and paraphrase identification on Paraphrase Adversaries from Word Scrambling (PAWS). XLA achieves SoTA results in all the tasks, outperforming the baselines by a good margin. With an in-depth framework dissection, we demonstrate the cumulative contributions of different components to XLA's success. ",/pdf/c576a5a0c1b73cfecd4854da7cbcf0e012355281.pdf,graph;zero_few-shot;inference;transfer learning;augmentation;multimodal,https://scholar.google.com/scholar?q=XLA:+A+Robust+Unsupervised+Data+Augmentation+Framework+for+Cross-Lingual+NLP
Cross-Domain Few-Shot Learning by Representation Fusion,2021,ICLR,"['Thomas Adler', 'Johannes Brandstetter', 'Michael Widrich', 'Andreas Mayr', 'David Kreil', 'Michael K Kopp', 'Günter Klambauer', 'Sepp Hochreiter']",poster,"['cross-domain learning', 'few-shot learning', 'Hebbian learning', 'ensemble learning', 'domain shift', 'domain adaptation', 'representation fusion']","In order to quickly adapt to new data, few-shot learning aims at learning from few examples, often by using already acquired knowledge. The new data often differs from the previously seen data due to a domain shift, that is, a change of the input-target distribution. While several methods perform well on small domain shifts like new target classes with similar inputs, larger domain shifts are still challenging. Large domain shifts may result in abstract concepts that are not shared between the original and the new domain. However, low-level concepts like edges in images might still be shared and useful. For cross-domain few-shot learning, we suggest representation fusion to unify different abstraction levels of a deep neural network into one representation. We propose Cross-domain Hebbian Ensemble Few-shot learning (CHEF), which consists of representation fusion by an ensemble of Hebbian learners acting on different layers of a deep neural network that was trained on the original domain. On the few-shot datasets miniImagenet and tieredImagenet, where the domain shift is small, CHEF is competitive with state-of-the-art methods. On cross-domain few-shot benchmark challenges with larger domain shifts, CHEF obtains state-of-the-art results in all categories. We further apply CHEF on a real-world cross-domain application in drug discovery. We consider a domain shift from bioactive molecules to environmental chemicals and drugs with twelve associated toxicity prediction tasks. On these tasks that are highly relevant for computational drug discovery, CHEF significantly outperforms all its competitors. ",/pdf/d70200698f93df70ba4727908ec9f1c7b7f8a6ed.pdf,graph;zero_few-shot;representation;active learning;multimodal,https://scholar.google.com/scholar?q=Cross-Domain+Few-Shot+Learning+by+Representation+Fusion
PABI: A Unified PAC-Bayesian Informativeness Measure for Incidental Supervision Signals,2021,ICLR,"['Hangfeng He', 'Mingyuan Zhang', 'Qiang Ning', 'Dan Roth']",poster,"['informativeness measure', 'incidental supervision', 'natural language processing']","Real-world applications often require making use of {\em a range of incidental supervision signals}. However, we currently lack a principled way to measure the benefit an incidental training dataset can bring, and the common practice of using indirect, weaker signals is through exhaustive experiments with various models and hyper-parameters. This paper studies whether we can, {\em in a single framework, quantify the benefit of various types of incidental signals for one's target task without going through combinatorial experiments}. We propose PABI, a unified informativeness measure motivated by PAC-Bayesian theory, characterizing the reduction in uncertainty that indirect, weak signals provide. We demonstrate PABI's use in quantifying various types of incidental signals including partial labels, noisy labels, constraints, cross-domain signals, and combinations of these. Experiments with various setups on two natural language processing (NLP) tasks, named entity recognition (NER) and question answering (QA), show that PABI correlates well with learning performance, providing a promising way to determine, ahead of learning, which supervision signals would be beneficial.",/pdf/372d209a33d11701f6e69ef4dcd894d5dd78f4f9.pdf,optimization;zero_few-shot;bayesian;multimodal,https://scholar.google.com/scholar?q=PABI:+A+Unified+PAC-Bayesian+Informativeness+Measure+for+Incidental+Supervision+Signals
Learning from Demonstration with Weakly Supervised Disentanglement,2021,ICLR,"['Yordan Hristov', 'Subramanian Ramamoorthy']",poster,"['representation learning for robotics', 'physical symbol grounding', 'semi-supervised learning']","Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teach- ing robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot – that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the tasks and data available - see supplementary materials at: https://sites.google.com/view/weak-label-lfd.",/pdf/49356704899c9011f84ed5a7f4cb44bbe43ae737.pdf,zero_few-shot;generative model;multimodal,https://scholar.google.com/scholar?q=Learning+from+Demonstration+with+Weakly+Supervised+Disentanglement
Parameter Efficient Multimodal Transformers for Video Representation Learning,2021,ICLR,"['Sangho Lee', 'Youngjae Yu', 'Gunhee Kim', 'Thomas Breuel', 'Jan Kautz', 'Yale Song']",poster,"['Self-supervised learning', 'audio-visual representation learning', 'video representation learning']","The recent success of Transformers in the language domain has motivated adapting it to a multimodal setting, where a new visual model is trained in tandem with an already pretrained language model. However, due to the excessive memory requirements from Transformers, existing work typically fixes the language model and train only the vision module, which limits its ability to learn cross-modal information in an end-to-end manner. In this work, we focus on reducing the parameters of multimodal Transformers in the context of audio-visual video representation learning. We alleviate the high memory requirement by sharing the parameters of Transformers across layers and modalities; we decompose the Transformer into modality-specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. We show that our approach reduces parameters of the Transformers up to 97%, allowing us to train our model end-to-end from scratch. We also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. To demonstrate our approach, we pretrain our model on 30-second clips (480 frames) from Kinetics-700 and transfer it to audio-visual classification tasks.",/pdf/2cccd8332d73cd6ac7e33027594d30f19af464d5.pdf,zero_few-shot;transformer;representation;transfer learning;multimodal;low-rank,https://scholar.google.com/scholar?q=Parameter+Efficient+Multimodal+Transformers+for+Video+Representation+Learning
Analogical Reasoning for Visually Grounded Compositional Generalization,2021,ICLR,"['Bo Wu', 'Haoyu Qin', 'Alireza Zareian', 'Carl Vondrick', 'Shih-Fu Chang']",poster,[],"Children acquire language subconsciously by observing the surrounding world and listening to descriptions. They can discover the meaning of words even without explicit language knowledge, and generalize to novel compositions effortlessly. In this paper, we bring this ability to AI, by studying the task of multimodal compositional generalization within the context of visually grounded language acquisition. We propose a multimodal transformer model augmented with a novel mechanism for analogical reasoning, which approximates novel compositions by learning semantic mapping and reasoning operations from previously seen compositions.  Our proposed method, Analogical Reasoning Transformer Networks (ARTNet), is trained on raw multimedia data (video frames and transcripts), and after observing a set of compositions such as ""washing apple"" or ""cutting carrot"", it can generalize and recognize new compositions in new video frames, such as ""washing carrot"" or ""cutting apple"". To this end, ARTNet refers to relevant instances in the training data and uses their visual features and captions to establish analogies with the query image. Then it chooses a suitable verb and noun to create a new composition that describes the new image best. Extensive experiments on an instructional video dataset demonstrate that the proposed method achieves significantly better generalization capability and recognition accuracy compared to state-of-the-art transformer models.",/pdf/977466c215834d865ac53b4286576a0f4880dd1e.pdf,transformer;augmentation;multimodal,https://scholar.google.com/scholar?q=Analogical+Reasoning+for+Visually+Grounded+Compositional+Generalization
Federated Learning Based on Dynamic Regularization,2021,ICLR,"['Durmus Alp Emre Acar', 'Yue Zhao', 'Ramon Matas', 'Matthew Mattina', 'Paul Whatmough', 'Venkatesh Saligrama']",poster,"['Federated Learning', 'Deep Neural Networks', 'Distributed Optimization']","We propose a novel federated learning method for distributively training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. We view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. We point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss. Different from recent prior works, that either attempt inexact minimization or utilize devices for parallelizing gradient computation, we propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. We demonstrate both through empirical results on real and synthetic data as well as analytical results that our scheme leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data.",/pdf/d5d8224518e951c4f83d7ee7338ee5862ea09a04.pdf,zero_few-shot;federated learning;multimodal,https://scholar.google.com/scholar?q=Federated+Learning+Based+on+Dynamic+Regularization
Watching the World Go By: Representation Learning from Unlabeled Videos,2021,ICLR,"['Daniel Gordon', 'Kiana Ehsani', 'Dieter Fox', 'Ali Farhadi']",poster,"['Representation Learning', 'Unsupervised Learning', 'Video Analytics']","Recent unsupervised representation learning techniques show remarkable success on many single image tasks by using instance discrimination:  learning to differentiate between two augmented versions of the same image and a large batch of unrelated images.  Prior work uses artificial data augmentation techniques such as cropping, and color jitter which can only affect the image in superficial waysand are not aligned with how objects actually change e.g. occlusion, deformation,viewpoint change.  We argue that videos offer this natural augmentation for free.Videos can provide entirely new views of objects,  show deformation,  and even connect semantically similar but visually distinct concepts.We propose Video Noise Contrastive Estimation,  a method for using unlabeled video to learnstrong, transferable, single image representations.We demonstrate improvements over recent unsupervised single image techniques,as well as over fully supervised ImageNet pretraining, across temporal and non-temporal tasks.",/pdf/4ffe4e853c95333eeab980c0e58d33cc89776a18.pdf,representation;contrastive learning;transfer learning;augmentation;multimodal,https://scholar.google.com/scholar?q=Watching+the+World+Go+By:+Representation+Learning+from+Unlabeled+Videos
Self-Supervised Learning of Compressed Video Representations,2021,ICLR,"['Youngjae Yu', 'Sangho Lee', 'Gunhee Kim', 'Yale Song']",poster,"['Compressed videos', 'self-supervised learning']","Self-supervised learning of video representations has received great attention. Existing methods typically require frames to be decoded before being processed, which increases compute and storage requirements and ultimately hinders large-scale training. In this work, we propose an efficient self-supervised approach to learn video representations by eliminating the expensive decoding step. We use a three-stream video architecture that encodes I-frames and P-frames of a compressed video. Unlike existing approaches that encode I-frames and P-frames individually, we propose to jointly encode them by establishing bidirectional dynamic connections across streams. To enable self-supervised learning, we propose two pretext tasks that leverage the multimodal nature (RGB, motion vector, residuals) and the internal GOP structure of compressed videos. The first task asks our network to predict zeroth-order motion statistics in a spatio-temporal pyramid; the second task asks correspondence types between I-frames and P-frames after applying temporal transformations. We show that our approach achieves competitive performance on compressed video recognition both in supervised and self-supervised regimes. 
",/pdf/32a283d43ef9e69a3039580759776862664a9146.pdf,zero_few-shot;transformer;representation;multimodal,https://scholar.google.com/scholar?q=Self-Supervised+Learning+of+Compressed+Video+Representations
Discriminative Cross-Modal Data Augmentation for Medical Imaging Applications,2021,ICLR,"['Yue Yang', 'Pengtao Xie']",poster,"['Deep learning', 'Medical imaging', 'Cross-Modal Learning']","While deep learning methods have shown great success in medical image analysis, they require a number of medical images to train. Due to data privacy concerns and unavailability of medical annotators, it is oftentimes very difficult to obtain a lot of labeled medical images for model training. In this paper, we study cross-modality data augmentation to mitigate the data deficiency issue in medical imaging domain. We propose a discriminative unpaired image-to-image translation model which translate images in source modality into images in target modality where the translation task is conducted jointly with the downstream prediction task and the translation is guided by the prediction. Experiments on two applications demonstrate the effectiveness of our method. ",/pdf/8fb6967f0b15eb8138cdacade03b26fe04371ae8.pdf,graph;augmentation;multimodal,https://scholar.google.com/scholar?q=Discriminative+Cross-Modal+Data+Augmentation+for+Medical+Imaging+Applications
Structural Landmarking and Interaction Modelling: on Resolution Dilemmas in Graph Classification,2021,ICLR,"['Kai Zhang', 'Yaokang Zhu', 'Jun Wang', 'Haibin Ling', 'Jie Zhang', 'Hongyuan Zha']",poster,"['Graph Pooling', 'Graph Classiciation', 'Interaction Preserving Graph Pooling', 'Structure Landmarking']","Graph neural networks are promising architecture for learning and inference with graph-structured data. However, generating informative graph level features has long been a challenge.  Current practice of  graph-pooling typically summarizes a graph by squeezing it into a single vector. This may lead to significant loss of predictive, iterpretable structural information, because properties of a complex system are believed to arise largely from the interaction among its components. In this paper, we analyze the intrinsic difficulty in graph classification under the unified concept of ``""resolution dilemmas"" and propose `SLIM, an inductive neural network model for Structural Landmarking and Interaction Modelling, to remedy the information loss in graph pooling. We show that, by projecting graphs onto end-to-end optimizable, and well-aligned substructure landmarks (representatives), the resolution dilemmas can be resolved effectively, so that explicit interacting relation between component parts of a graph can be leveraged directly in explaining its complexity and predicting its property. Empirical evaluations, in comparison with state-of-the-art, demonstrate promising results of our approach on a number of benchmark datasets for graph classification.
",/pdf/7469d918ea04138141acfd7b4b2e7f8fd3df8a65.pdf,graph;inference;multimodal,https://scholar.google.com/scholar?q=Structural+Landmarking+and+Interaction+Modelling:+on+Resolution+Dilemmas+in+Graph+Classification
Variational Dynamic Mixtures,2021,ICLR,"['Chen Qiu', 'Stephan Mandt', 'Maja Rudolph']",poster,['sequential latent variable models'],"Deep probabilistic time series forecasting models have become an integral part of machine learning. While several powerful generative models have been proposed, we provide evidence that their associated inference models are oftentimes too limited and cause the generative model to predict mode-averaged dynamics. Mode-averaging is problematic since many real-world sequences are highly multi-modal, and their averaged dynamics are unphysical (e.g., predicted taxi trajectories might run through buildings on the street map). To better capture multi-modality, we develop variational dynamic mixtures (VDM): a new variational family to infer sequential latent variables. The VDM approximate posterior at each time step is a mixture density network, whose parameters come from propagating multiple samples through a recurrent architecture. This results in an expressive  multi-modal posterior approximation. In an empirical study, we show that VDM outperforms competing approaches on highly multi-modal datasets from different domains. ",/pdf/294d0de17b0d93c0ab68551335362535f2c5816e.pdf,graph;generative model;inference;multimodal,https://scholar.google.com/scholar?q=Variational+Dynamic+Mixtures
Explicit Connection Distillation,2021,ICLR,"['Lujun Li', 'Yikai Wang', 'Anbang Yao', 'Yi Qian', 'Xiao Zhou', 'Ke He']",poster,[],"One effective way to ease the deployment of deep neural networks on resource constrained devices is Knowledge Distillation (KD), which boosts the accuracy of a low-capacity student model by mimicking the learnt information of a high-capacity teacher (either a single model or a multi-model ensemble). Although great progress has been attained on KD research, existing efforts are primarily invested to design better distillation losses by using soft logits or intermediate feature representations of the teacher as the extra supervision. In this paper, we present Explicit Connection Distillation (ECD), a new KD framework, which addresses the knowledge distillation problem in a novel perspective of bridging dense intermediate feature connections between a student network and its corresponding teacher generated automatically in the training, achieving knowledge transfer goal via direct cross-network layer-to-layer gradients propagation. ECD has two interdependent modules. In the first module, given a student network, an auxiliary teacher architecture is temporarily generated conditioned on strengthening feature representations of basic convolutions of the student network via replacing them with dynamic additive convolutions and keeping the other layers unchanged in structure. The teacher generated in this way guarantees its superior capacity and makes a perfect feature alignment (both in input and output dimensions) to the student at every convolutional layer. In the second module, dense feature connections between the aligned convolutional layers from the student to its auxiliary teacher are introduced, which allows explicit layer-to-layer gradients propagation from the teacher to the student via the merged model training from scratch. Intriguingly, as feature connection direction is one-way, all feature connections together with the auxiliary teacher merely exist during training phase. Experiments on popular image classification tasks validate the effectiveness of our method. Code will be made publicly available.",/pdf/ab053bf673bacba208f889cf35ee87a0b83616bb.pdf,graph;optimization;zero_few-shot;representation;transfer learning;distillation;multimodal,https://scholar.google.com/scholar?q=Explicit+Connection+Distillation
ADIS-GAN: Affine Disentangled GAN,2021,ICLR,"['Letao Liu', 'Martin Saerbeck', 'Justin Dauwels']",poster,"['Deep Learning', 'Disentangled Representation', 'Generative Adversarial Network', 'Computer Vision']","This paper proposes Affine Disentangled GAN (ADIS-GAN), which is a Generative Adversarial Network that can explicitly disentangle affine transformations in a self-supervised and rigorous manner.  The objective is inspired by InfoGAN, where an additional affine regularizer acts as the inductive bias.  The affine regularizer is rooted in the affine transformation properties of images, changing some properties of the underlying images, while leaving all other properties invariant. We derive the affine regularizer by decomposing the affine matrix into separate transformation matrices and inferring the transformation parameters by maximum-likelihood estimation.  Unlike the disentangled representations learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable, where transformations such as rotation, horizontal and vertical zoom, horizontal and vertical skew,  horizontal and vertical translation can be explicitly selected and learned. ADIS-GAN successfully disentangles these features on the MNIST, CelebA, and dSprites datasets.",/pdf/7fdabffbb7873150d6097dac467869b53b2e118c.pdf,graph;zero_few-shot;representation;generative model;multimodal,https://scholar.google.com/scholar?q=ADIS-GAN:+Affine+Disentangled+GAN
Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency,2021,ICLR,"['Qiang Zhang', 'Tete Xiao', 'Alexei A Efros', 'Lerrel Pinto', 'Xiaolong Wang']",poster,"['self-supervised learning', 'robotics']","At the heart of many robotics problems is the challenge of learning correspondences across domains. For instance, imitation learning requires obtaining correspondence between humans and robots; sim-to-real requires correspondence between physics simulators and real hardware; transfer learning requires correspondences between different robot environments. In this paper, we propose to learn correspondence across such domains emphasizing on differing modalities (vision and internal state), physics parameters (mass and friction), and morphologies (number of limbs). Importantly, correspondences are learned using unpaired and randomly collected data from the two domains. We propose dynamics cycles that align dynamic robotic behavior across two domains using a cycle consistency constraint. Once this correspondence is found, we can directly transfer the policy trained on one domain to the other, without needing any additional fine-tuning on the second domain. We perform experiments across a variety of problem domains, both in simulation and on real robots. Our framework is able to align uncalibrated monocular video of a real robot arm to dynamic state-action trajectories of a simulated arm without paired data. Video demonstrations of our results are available at: https://sites.google.com/view/cycledynamics .",/pdf/2d895dd1e0137048768cefacc5fe9d81daa35d58.pdf,optimization;transfer learning;multimodal;imitation learning,https://scholar.google.com/scholar?q=Learning+Cross-Domain+Correspondence+for+Control+with+Dynamics+Cycle-Consistency
Diverse Video Generation using a Gaussian Process Trigger,2021,ICLR,"['Gaurav Shrivastava', 'Abhinav Shrivastava']",poster,"['video synthesis', 'future frame generation', 'video generation', 'gaussian process priors', 'diverse video generation']","Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.",/pdf/fa17e9ba0b11fb7c0b424feda300e370d82df3eb.pdf,graph;generative model;multimodal,https://scholar.google.com/scholar?q=Diverse+Video+Generation+using+a+Gaussian+Process+Trigger
Non-Attentive Tacotron: Robust and controllable neural TTS synthesis including unsupervised duration modeling,2021,ICLR,"['Jonathan Shen', 'Ye Jia', 'Mike Chrzanowski', 'Yu Zhang', 'Isaac Elias', 'Heiga Zen', 'Yonghui Wu']",poster,"['tts', 'text-to-speech']","This paper presents Non-Attentive Tacotron based on the Tacotron 2 text-to-speech model, replacing the attention mechanism with an explicit duration predictor. This improves robustness significantly as measured by unaligned duration ratio and word deletion rate, two metrics introduced in this paper for large-scale robustness evaluation using a pre-trained speech recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron achieves a 5-scale mean opinion score for naturalness of 4.41, slightly outperforming Tacotron 2. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, we propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training.",/pdf/1868713d01e66c6f590aeb4eeb35dff5848ec601.pdf,transformer;inference;metric;multimodal,https://scholar.google.com/scholar?q=Non-Attentive+Tacotron:+Robust+and+controllable+neural+TTS+synthesis+including+unsupervised+duration+modeling
Language Controls More Than Top-Down Attention: Modulating Bottom-Up Visual Processing with Referring Expressions,2021,ICLR,"['Ozan Arkan Can', 'Ilker Kesen', 'Deniz Yuret']",poster,"['Referring Expression Understanding', 'Language-Vision Problems', 'Grounded Language Understanding']","How to best integrate linguistic and perceptual processing in multimodal tasks is an important open problem. In this work we argue that the common technique of using language to direct visual attention over high-level visual features may not be optimal. Using language throughout the bottom-up visual pathway, going from pixels to high-level features, may be necessary. Our experiments on several English referring expression datasets show significant improvements when language is used to control the filters for bottom-up visual processing in addition to top-down attention.",/pdf/4ab86a642f612cffb87fd273aa47be6991f83cc0.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Language+Controls+More+Than+Top-Down+Attention:+Modulating+Bottom-Up+Visual+Processing+with+Referring+Expressions
Single Layers of Attention Suffice to Predict Protein Contacts,2021,ICLR,"['Nick Bhattacharya', 'Neil Thomas', 'Roshan Rao', 'Justas Daupras', 'Peter K Koo', 'David Baker', 'Yun S. Song', 'Sergey Ovchinnikov']",poster,"['Protein Structure', 'Proteins', 'Contact Prediction', 'Representation Learning', 'Language Modeling', 'Attention', 'Transformer', 'BERT', 'Markov Random Fields', 'Potts Models', 'Self-supervised learning']","The established approach to unsupervised protein contact prediction estimates coevolving positions using undirected graphical models. This approach trains a Potts model on a Multiple Sequence Alignment, then predicts that the edges with highest weight correspond to contacts in the 3D structure. On the other hand, increasingly large Transformers are being pretrained on protein sequence databases but have demonstrated mixed results for downstream tasks, including contact prediction. This has sparked discussion about the role of scale and attention-based models in unsupervised protein representation learning. We argue that attention is a principled model of protein interactions, grounded in real properties of protein family data. We introduce a simplified attention layer, factored attention, and show that it achieves comparable performance to Potts models, while sharing parameters both within and across families. Further, we extract contacts from the attention maps of a pretrained Transformer and show they perform competitively with the other two approaches. This provides evidence that large-scale pretraining can learn meaningful protein features when presented with unlabeled and unaligned data. We contrast factored attention with the Transformer to indicate that the Transformer leverages hierarchical signal in protein family databases not captured by our single-layer models. This raises the exciting possibility for the development of powerful structured models of protein family databases.",/pdf/31cd8f234cb9c568d12cebd3756d42c58197276a.pdf,graph;transformer;representation;multimodal;3d,https://scholar.google.com/scholar?q=Single+Layers+of+Attention+Suffice+to+Predict+Protein+Contacts
Deep Gated Canonical Correlation Analysis,2021,ICLR,"['Ofir Lindenbaum', 'Moshe Salhov', 'Amir Averbuch', 'Yuval Kluger']",poster,[],"Canonical Correlation Analysis (CCA) models can extract informative correlated representations from multimodal unlabelled data. Despite their success, CCA models may break if the number of variables exceeds the number of samples. We propose Deep Gated-CCA, a method for learning correlated representations based on a sparse subset of variables from two observed modalities. The proposed procedure learns two non-linear transformations and simultaneously gates the input variables to identify a subset of most correlated variables. The non-linear transformations are learned by training two neural networks to maximize a shared correlation loss defined based on their outputs. Gating is obtained by adding an approximate $\ell_0$ regularization term applied to the input variables. This approximation relies on a recently proposed continuous Gaussian based relaxation for Bernoulli variables which act as gates. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, the method outperforms other linear and non-linear CCA models.",/pdf/aff1b325a1bd037b901d645892a8d84204e8085f.pdf,representation;sparse;multimodal,https://scholar.google.com/scholar?q=Deep+Gated+Canonical+Correlation+Analysis
Generalized Multimodal ELBO,2021,ICLR,"['Thomas M. Sutter', 'Imant Daunhawer', 'Julia E Vogt']",poster,"['Multimodal', 'VAE', 'ELBO', 'self-supervised', 'generative learning']","Multiple data types naturally co-occur when describing real-world phenomena and learning from them is a long-standing goal in machine learning research. However, existing self-supervised generative models approximating an ELBO are not able to fulfill all desired requirements of multimodal models: their posterior approximation functions lead to a trade-off between the semantic coherence and the ability to learn the joint data distribution. We propose a new, generalized ELBO formulation for multimodal data that overcomes these limitations. The new objective encompasses two previous methods as special cases and combines their benefits without compromises. In extensive experiments, we demonstrate the advantage of the proposed method compared to state-of-the-art models in self-supervised, generative learning tasks.",/pdf/2cfd5fea6a35d4586487da796743d75dacc7118c.pdf,zero_few-shot;generative model;multimodal,https://scholar.google.com/scholar?q=Generalized+Multimodal+ELBO
Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?,2021,ICLR,"['Balázs Kégl', 'Gabriel Hurtado', 'Albert Thomas']",poster,"['model-based reinforcement learning', 'generative models', 'mixture density nets', 'dynamic systems', 'heteroscedasticity']","We contribute to micro-data model-based reinforcement learning (MBRL) by rigorously comparing popular generative models using a fixed (random shooting) control agent. We find that on an environment that requires multimodal posterior predictives, mixture density nets outperform all other models by a large margin. When multimodality is not required, our surprising finding is that we do not need probabilistic posterior predictives: deterministic models are on par, in fact they consistently (although non-significantly) outperform their probabilistic counterparts. We also found that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons. At the methodological side, we design metrics and an experimental protocol which can be used to evaluate the various models, predicting their asymptotic performance when using them on the control problem. Using this framework, we improve the state-of-the-art sample complexity of MBRL on Acrobot by two to four folds, using an aggressive training schedule which is outside of the hyperparameter interval usually considered.",/pdf/04313ea0678f51bf6e97525219f5b92003b041b9.pdf,reinforcement learning;graph;generative model;metric;multimodal,https://scholar.google.com/scholar?q=Model-based+micro-data+reinforcement+learning:+what+are+the+crucial+model+properties+and+which+model+to+choose?
Set Prediction without Imposing Structure as Conditional Density Estimation,2021,ICLR,"['David W Zhang', 'Gertjan J. Burghouts', 'Cees G. M. Snoek']",poster,"['set prediction', 'energy based models']","Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.",/pdf/04c489674227569994e57717321c907597b1355c.pdf,metric;augmentation;multimodal,https://scholar.google.com/scholar?q=Set+Prediction+without+Imposing+Structure+as+Conditional+Density+Estimation
Coverage as a Principle for Discovering Transferable Behavior in Reinforcement Learning,2021,ICLR,"['Víctor Campos', 'Pablo Sprechmann', 'Steven Stenberg Hansen', 'Andre Barreto', 'Charles Blundell', 'Alex Vitvitskyi', 'Steven Kapturowski', 'Adria Puigdomenech Badia']",poster,"['deep reinforcement learning', 'transfer learning', 'unsupervised learning', 'exploration']","Designing agents that acquire knowledge autonomously and use it to solve new tasks efficiently is an important challenge in reinforcement learning. Unsupervised learning provides a useful paradigm for autonomous acquisition of task-agnostic knowledge. In supervised settings, representations discovered through unsupervised pre-training offer important benefits when transferred to downstream tasks. Given the nature of the reinforcement learning problem, we explore how to transfer knowledge through behavior instead of representations. The behavior of pre-trained policies may be used for solving the task at hand (exploitation), as well as for collecting useful data to solve the problem (exploration). We argue that pre-training policies to maximize coverage will result in behavior that is useful for both strategies. When using these policies for both exploitation and exploration, our agents discover solutions that lead to larger returns. The largest gains are generally observed in domains requiring structured exploration, including settings where the behavior of the pre-trained policies is misaligned with the downstream task.",/pdf/f61a0bd1bf18a7a4188cd8f8c9b04e5224b515ab.pdf,reinforcement learning;representation;transfer learning;multimodal,https://scholar.google.com/scholar?q=Coverage+as+a+Principle+for+Discovering+Transferable+Behavior+in+Reinforcement+Learning
Learning without Forgetting: Task Aware Multitask Learning for Multi-Modality Tasks,2021,ICLR,"['Sathish Reddy Indurthi', 'Mohd Abbas Zaidi', 'Nikhil Kumar Lakumarapu', 'Beomseok Lee', 'HyoJung Han', 'Sangha Kim', 'Inchul Hwang']",poster,"['Deep Learning', 'Joint Learning', 'Meta Learning', 'Multi-task Learning']","Existing joint learning strategies like multi-task or meta-learning focus more on shared learning and have little to no scope for task-specific learning. This creates the need for a distinct shared pretraining phase and a task-specific finetuning phase. The fine-tuning phase creates separate systems for each task, where improving the performance of a particular task necessitates forgetting some of the knowledge garnered in other tasks. Humans, on the other hand, perform task-specific learning in synergy with general domain-based learning. Inspired by these learning patterns in humans, we suggest a simple yet generic task aware framework to incorporate into existing joint learning strategies. The proposed framework computes task-specific representations to modulate model parameters during joint learning. Hence, it performs both shared and task-specific learning in a single-phase resulting in a single model for all the tasks. The single model itself achieves significant performance gains over the existing joint learning strategies.  For example, we train a model on Speech Translation (ST), Automatic Speech Recognition (ASR), and Machine Translation (MT) tasks using the proposed task aware joint learning approach. This single model achieves a performance of 28.64 BLEU score on ST MuST-C English-German, WER of 11.61 on ASR TEDLium v3, and BLEU score of 23.35 on MT WMT14 English-German tasks. This sets a new state-of-the-art performance (SOTA) on the ST task while outperforming the existing end-to-end ASR systems with a competitive performance on the MT task. ",/pdf/326b033b762bc36fb57cadabf56a9e2532de96b7.pdf,representation;meta-learning;multi-task;multimodal,https://scholar.google.com/scholar?q=Learning+without+Forgetting:+Task+Aware+Multitask+Learning+for+Multi-Modality+Tasks
Multimodal Variational Autoencoders for Semi-Supervised Learning: In Defense of Product-of-Experts,2021,ICLR,"['Svetlana Kutuzova', 'Oswin Krause', 'Douglas McCloskey', 'Mads Nielsen', 'Christian Igel']",poster,"['variational autoencoder', 'multimodal data', 'product-of-experts', 'semi-supervised learning']","Multimodal generative models should be able to learn a meaningful latent representation that enables a coherent joint generation of all modalities (e.g., images and text). Many applications also require the ability to accurately sample modalities conditioned on observations of a subset of the modalities. Often not all modalities may be observed for all training data points, so  semi-supervised learning should be possible.
In this study, we evaluate a family of product-of-experts (PoE) based variational autoencoders that have these desired properties. We include a novel PoE based architecture and training procedure. An empirical evaluation shows that the PoE based models can outperform an additive mixture-of-experts (MoE) approach.
Our experiments support the intuition that PoE models are more suited for a conjunctive combination of modalities while MoEs are more suited for a disjunctive fusion. ",/pdf/f5df283b136768f85d39a9bc755da5c9798f05f3.pdf,representation;vae;generative model;multimodal,https://scholar.google.com/scholar?q=Multimodal+Variational+Autoencoders+for+Semi-Supervised+Learning:+In+Defense+of+Product-of-Experts
Calibrated Adversarial Refinement for Stochastic Semantic Segmentation,2021,ICLR,"['Elias Kassapis', 'Georgi Dikov', 'Deepak Gupta', 'Cedric Nugteren']",poster,"['stochastic semantic segmentation', 'conditional generative models', 'adversarial training', 'calibration', 'uncertainty']","Ambiguities in images or unsystematic annotation can lead to multiple valid solutions in semantic segmentation. To learn a distribution over predictions, recent work has explored the use of probabilistic networks. However, these do not necessarily capture the empirical distribution accurately. In this work, we aim to learn a calibrated multimodal predictive distribution, where the empirical frequency of the sampled predictions closely reflects that of the corresponding labels in the training set. To this end, we propose a novel two-stage, cascaded strategy for calibrated adversarial refinement. In the first stage, we explicitly model the data with a categorical likelihood. In the second, we train an adversarial network to sample from it an arbitrary number of coherent predictions. The model can be used independently or integrated into any black-box segmentation framework to facilitate learning of calibrated stochastic mappings. We demonstrate the utility and versatility of the approach by attaining state-of-the-art results on the multigrader LIDC dataset and a modified Cityscapes dataset. In addition, we use a toy regression dataset to show that our framework is not confined to semantic segmentation, and the core design can be adapted to other tasks requiring learning a calibrated predictive distribution.",/pdf/b1bdb0206f69f90685fd408c18777686459b0ad0.pdf,segmentation;multimodal,https://scholar.google.com/scholar?q=Calibrated+Adversarial+Refinement+for+Stochastic+Semantic+Segmentation
Multi-View Disentangled Representation,2021,ICLR,"['Zongbo Han', 'Changqing Zhang', 'Huazhu Fu', 'Qinghua Hu', 'Joey Tianyi Zhou']",poster,[],"Learning effective representations for data with multiple views is crucial in machine learning and pattern recognition. Recently great efforts have focused on learning unified or latent representations to integrate information from different views for specific tasks. These approaches generally assume simple or implicit relationships between different views and as a result are not able to flexibly and explicitly depict the correlations among these views. To address this, we firstly propose the definition and conditions for multi-view disentanglement providing general instructions for disentangling representations between different views. Furthermore, a novel objective function is derived to explicitly disentangle the multi-view data into a shared part across different views and a (private) exclusive part within each view. Experiments on a variety of multi-modal datasets demonstrate that our objective can effectively disentangle information from different views while satisfying the disentangling conditions.",/pdf/408a3026f62529c7dbcff93c4f2ce77a5c4744d4.pdf,representation;multimodal;multi-view,https://scholar.google.com/scholar?q=Multi-View+Disentangled+Representation
Sequence Metric Learning as Synchronization of Recurrent Neural Networks,2021,ICLR,"['Paul Compagnon', 'Grégoire Lefebvre', 'Stefan Duffner', 'Christophe Garcia']",poster,"['Metric learning', 'sequence processing', 'siamese recurrent neural network', 'dynamical systems']","Sequence metric learning is becoming a widely adopted approach for various applications dealing with sequential multi-variate data such as activity recognition or natural language processing and is most of the time tackled with sequence alignment approaches or representation learning. 
In this paper, we propose to study this subject from the point of view of dynamical system theory by drawing the analogy between synchronized trajectories produced by dynamical systems and the distance between similar sequences processed by a siamese recurrent neural network. 
Indeed, a siamese recurrent network comprises two identical sub-networks, two identical dynamical systems which can theoretically achieve complete synchronization if a coupling is introduced between them. 
We therefore propose a new neural network model that implements this coupling with a new gate integrated into the classical Gated Recurrent Unit architecture. This model is thus able to simultaneously learn a similarity metric and the synchronization of unaligned multi-variate sequences in a weakly supervised way.
Our experiments show that introducing such a coupling improves the performance of the siamese Gated Recurrent Unit architecture on an activity recognition dataset. ",/pdf/8bf2d1729e4cad904fdd23b2e92ceeea2ef8f903.pdf,zero_few-shot;representation;metric;multimodal;llm,https://scholar.google.com/scholar?q=Sequence+Metric+Learning+as+Synchronization+of+Recurrent+Neural+Networks
You Only Need Adversarial Supervision for Semantic Image Synthesis,2021,ICLR,"['Edgar Schönfeld', 'Vadim Sushko', 'Dan Zhang', 'Juergen Gall', 'Bernt Schiele', 'Anna Khoreva']",poster,"['Semantic Image Synthesis', 'GANs', 'Image Generation', 'Deep Learning']","Despite their recent successes, GAN models for semantic image synthesis still suffer from poor image quality when trained with only adversarial supervision. Historically, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limiting the progress of GAN models for semantic image synthesis. In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially- and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity with better alignment to their input label maps, making the use of the perceptual loss superfluous. Moreover, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image change. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve an average improvement of $6$ FID and $5$ mIoU points over the state of the art across different datasets using only adversarial supervision.",/pdf/296a08e6901d8e9191af10b50555200a0efb3fc4.pdf,zero_few-shot;segmentation;multimodal;3d,https://scholar.google.com/scholar?q=You+Only+Need+Adversarial+Supervision+for+Semantic+Image+Synthesis
Learning a Max-Margin Classifier for Cross-Domain Sentiment Analysis,2021,ICLR,"['Mohammad Rostami', 'Aram Galstyan']",poster,"['natural language processing', 'sentiment analysis', 'cross-domain data representation', 'distribution alignment']"," Sentiment analysis is a costly yet necessary task for enterprises to study the opinions of their costumers to improve their products and services and to determine optimal   marketing strategies. Due to existence of a wide range of domains across different products and services, cross-domain sentiment analysis methods have received significant attention in recent years. These methods mitigate the domain gap between different applications by training cross-domain generalizable classifiers which help to relax the need for individual data annotation per each domain. Most existing methods focus on learning domain-agnostic representations that are invariant with respect to both the source and the target domains. As a result, a classifier that is trained using  annotated data in a source domain, would generalize well in a related target domain. In this work, we introduce a new domain adaptation method which induces large margins between different classes in an embedding space based on the notion of prototypical distribution. This embedding space is trained to be domain-agnostic by matching the data distributions across the domains. Large margins in the source domain help to reduce the effect of ``domain shift'' on the performance of a trained classifier in the target domain. Theoreticaland empirical analysis are provided to demonstrate that the method is effective. ",/pdf/855c64d55a3f8ccd8976ca89c74dfe29da0ccbaa.pdf,graph;transformer;representation;multimodal,https://scholar.google.com/scholar?q=Learning+a+Max-Margin+Classifier+for+Cross-Domain+Sentiment+Analysis
A Multi-Modal and Multitask Benchmark in the Clinical Domain,2021,ICLR,"['Yong Huang', 'Edgar Mariano Marroquin', 'Volodymyr Kuleshov']",poster,"['multi-modal', 'multitask', 'machine learning in healthcare', 'benchmark']","Healthcare represents one of the most promising application areas for machine learning algorithms,  including modern methods based on deep learning.   Modern deep learning algorithms perform best on large datasets and on unstructured modalities such as text or image data; advances in deep learning have often been driven by the availability of such large datasets. Here, we introduce Multi-Modal Multitask MIMIC-III (M3) — a dataset and benchmark for evaluating machine learning algorithms in the healthcare domain.  This dataset contains multi-modal patient data collected from intensive care units — including physiological time series, clinical notes, ECG waveforms, and tabular inputs — and defines six clinical tasks — including predicting mortality, decompensation, readmission, and other outcomes — which serve as benchmarks for comparing algorithms. We introduce new multi-modal and multitask models for this dataset, and show that they outperform previous state-of-the-art results that only rely on a subset of all tasks and modalities. This highlights the potential of multitask and multi-modal learning to improve the performance of algorithms in the healthcare domain. More generally, we envision M3 as a general resource that will help accelerate research in applying machine learning to healthcare.",/pdf/106229493bb858b25794f5f3ac77ecab060af02d.pdf,multimodal,https://scholar.google.com/scholar?q=A+Multi-Modal+and+Multitask+Benchmark+in+the+Clinical+Domain
Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines,2021,ICLR,"['Matthew A Wright', 'Joseph E. Gonzalez']",poster,"['Transformer models', 'attention models', 'kernel methods', 'reproducing kernel Banach spaces']","Despite their ubiquity in core AI fields like natural language processing, the mechanics of deep attention-based neural networks like the ``Transformer'' model are not fully understood. In this article, we present a new perspective towards understanding how Transformers work. In particular, we show that the ``""dot-product attention"" that is the core of the Transformer's operation can be characterized as a kernel learning method on a pair of Banach spaces. In particular, the Transformer's kernel is characterized as having an infinite feature dimension. Along the way we generalize the standard kernel learning problem to what we term a ""binary"" kernel learning problem, where data come from two input domains and a response is defined for every cross-domain pair. We prove a new representer theorem for these binary kernel machines with non-Mercer (indefinite, asymmetric) kernels (implying that the functions learned are elements of reproducing kernel Banach spaces rather than Hilbert spaces), and also prove a new universal approximation theorem showing that the Transformer calculation can learn any binary non-Mercer reproducing kernel Banach space pair. We experiment with new kernels in Transformers, and obtain results that suggest the infinite dimensionality of the standard Transformer kernel is partially responsible for its performance. This paper's results provide a new theoretical understanding of a very important but poorly understood model in modern machine learning.",/pdf/f74f64a37a8da695d70254ba4711835a68801485.pdf,transformer;metric;multimodal,https://scholar.google.com/scholar?q=Transformers+are+Deep+Infinite-Dimensional+Non-Mercer+Binary+Kernel+Machines
Conditional Generative Modeling via Learning the Latent Space,2021,ICLR,"['Sameera Ramasinghe', 'Kanchana Nisal Ranasinghe', 'Salman Khan', 'Nick Barnes', 'Stephen Gould']",poster,"['Multimodal Spaces', 'Conditional Generation', 'Generative Modeling']","Although deep learning has achieved appealing results on several machine learning tasks, most of the models are deterministic at inference, limiting their application to single-modal settings. We propose a novel general-purpose framework for conditional generation in multimodal spaces, that uses latent variables to model generalizable learning patterns while minimizing a family of regression cost functions. At inference, the latent variables are optimized to find solutions corresponding to multiple output modes.  Compared to existing generative solutions, our approach demonstrates faster and more stable convergence, and can learn better representations for downstream tasks. Importantly, it provides a simple generic model that can perform better than highly engineered pipelines tailored using domain expertise on a variety of tasks, while generating diverse outputs. Code available at https://github.com/samgregoost/cGML.",/pdf/ad10b1238b8c96783d156228bbe0a955123a991c.pdf,graph;representation;generative model;inference;multimodal,https://scholar.google.com/scholar?q=Conditional+Generative+Modeling+via+Learning+the+Latent+Space
Motif-Driven Contrastive Learning of Graph Representations,2021,ICLR,"['Shichang Zhang', 'Ziniu Hu', 'Arjun Subramonian', 'Yizhou Sun']",poster,"['graph neural network', 'self-supervised learning', 'contrastive learning', 'graph motif learning']","Graph motifs are significant subgraph patterns occurring frequently in graphs, and they play important roles in representing the whole graph characteristics. For example, in the chemical domain, functional groups are motifs that can determine molecule properties. Mining and utilizing motifs, however, is a non-trivial task for large graph datasets. Traditional motif discovery approaches mostly rely on exact counting or statistical estimation, which are hard to scale for a large number of graphs with continuous and high-dimension features. In light of the significance and challenges of motif mining, we propose : MICRO-Graph: a framework for \underline{M}ot\underline{I}f-driven \underline{C}ontrastive lea\underline{R}ning \underline{O}f \underline{G}raph representations to: 1) pre-train Graph Neural Networks (GNNs) in a self-supervised manner to automatically extract graph motifs from large graph datasets; 2) leverage learned motifs to guide the contrastive learning of graph representations, which further benefit various graph downstream tasks. Specifically, given a graph dataset, a motif learner cluster similar and significant subgraphs into corresponding motif slots. Based on the learned motifs, a motif-guided subgraph segmenter is trained to generate more informative subgraphs, which are used to conduct graph-to-subgraph contrastive learning of GNNs. Our discovering strategy is to simutaneously do clustering and contrastive learning on dynamically sampled subgraphs. The clustering part pull together similar subgraphs across different whole graphs, as the contrastive part push away dissimilar ones. Meanwhile, our learnable sampler will generate subgraph samples better aligned with the discoverying procedure. By pre-training on ogbn-molhiv molecule dataset with our proposed MICRO-Graph, the pre-trained GNN model can enhance various chemical property prediction downstream tasks with scarce label by 2.0%, and significantly higher than other state-of-the-art self-supervised learning baselines. 
",/pdf/938edae56f95e180d491633988f2c03bb7077dae.pdf,graph;zero_few-shot;representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Motif-Driven+Contrastive+Learning+of+Graph+Representations
Contrastive Self-Supervised Learning of Global-Local Audio-Visual Representations,2021,ICLR,"['Shuang Ma', 'Zhaoyang Zeng', 'Daniel McDuff', 'Yale Song']",poster,"['Contrastive learning', 'self-supervised learning', 'video representation learning', 'audio-visual representation learning', 'multimodal representation learning']","Contrastive self-supervised learning has delivered impressive results in many audio-visual recognition tasks. However, existing approaches optimize for learning either global representations useful for high-level understanding tasks such as classification, or local representations useful for tasks such as audio-visual source localization and separation. While they produce satisfactory results in their intended downstream scenarios, they often fail to generalize to tasks that they were not originally designed for. In this work, we propose a versatile self-supervised approach to learn audio-visual representations that can generalize to both the tasks which require global semantic information (e.g., classification) and the tasks that require fine-grained spatio-temporal information (e.g. localization). We achieve this by optimizing two cross-modal contrastive objectives that together encourage our model to learn discriminative global-local visual information given audio signals. To show that our approach learns generalizable video representations, we evaluate it on various downstream scenarios including action/sound classification, lip reading, deepfake detection, and sound source localization. ",/pdf/b7faea319b1db13e622046a97f6120367f01c302.pdf,graph;zero_few-shot;representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Contrastive+Self-Supervised+Learning+of+Global-Local+Audio-Visual+Representations
Optimal Neural Program Synthesis from Multimodal Specifications,2021,ICLR,"['Xi Ye', 'Qiaochu Chen', 'Isil Dillig', 'Greg Durrett']",poster,['program synthesis'],"Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.",/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf,graph;optimization;zero_few-shot;active learning;multimodal,https://scholar.google.com/scholar?q=Optimal+Neural+Program+Synthesis+from+Multimodal+Specifications
XMixup: Efficient Transfer Learning with Auxiliary Samples by Cross-Domain Mixup,2021,ICLR,"['Xingjian Li', 'Haoyi Xiong', 'Haozhe An', 'Cheng-zhong Xu', 'Dejing Dou']",poster,"['transfer learning', 'deep learning']","Transferring knowledge from large source datasets is an effective way to fine-tune the deep neural networks of the target task with a small sample size. A great number of algorithms have been proposed to facilitate deep transfer learning, and these techniques could be generally categorized into two groups – Regularized Learning of the target task using models that have been pre-trained from source datasets, and Multitask Learning with both source and target datasets to train a shared backbone neural network. In this work, we aim to improve the multitask paradigm for deep transfer learning via Cross-domain Mixup (XMixup). While the existing multitask learning algorithms need to run backpropagation over both the source and target datasets and usually consume a higher gradient complexity, XMixup transfers the knowledge from source to target tasks more efficiently: for every class of the target task, XMixup selects the auxiliary samples from the source dataset and augments training samples via the simple mixup strategy. We evaluate XMixup over six real world transfer learning datasets. Experiment results show that XMixup improves the accuracy by 1.9% on average. Compared with other state-of-the-art transfer learning approaches, XMixup costs much less training time while still obtains higher accuracy.",/pdf/1a88444fb3cd58103a150ec21b79c2bbfd9960d6.pdf,transfer learning;multimodal,https://scholar.google.com/scholar?q=XMixup:+Efficient+Transfer+Learning+with+Auxiliary+Samples+by+Cross-Domain+Mixup
Batch Inverse-Variance Weighting: Deep Heteroscedastic Regression,2021,ICLR,"['Vincent Mai', 'Waleed Khamies', 'Liam Paull']",poster,"['Regression', 'Noisy labels', 'Supervised Learning', 'Uncertainty', 'Variance', 'Heteroscedastic', 'Privileged Information']","In model learning, when the training dataset on which the parameters are optimized and the testing dataset on which the model is evaluated are not sampled from identical distributions, we say that the datasets are misaligned. It is well-known that this misalignment can negatively impact model performance. A common source of misalignment is that the inputs are sampled from different distributions. Another source for this misalignment is that the label generating process used to create the training dataset is imperfect. In this work, we consider this setting and additionally assume that the label generating process is able to provide us with a quantity for the role of each label in the misalignment between the datasets, which we consider to be privileged information. Specifically, we consider the task of regression with labels corrupted by heteroscedastic noise and we assume that we have access to an estimate of the variance over each sample. We propose a general approach to include this privileged information in the loss function together with dataset statistics inferred from the mini-batch to mitigate the impact of the dataset misalignment. Subsequently, we propose a specific algorithm for the heteroscedastic regression case, called Batch Inverse-Variance weighting, which adapts inverse-variance weighting for linear regression to the case of neural network function approximation.  We demonstrate that this approach achieves a significant improvement in network training performances compared to baselines when confronted with high, input-independent noise.",/pdf/3b8feb4acb73a876bf25c4c28aea3d5d339d376d.pdf,multimodal,https://scholar.google.com/scholar?q=Batch+Inverse-Variance+Weighting:+Deep+Heteroscedastic+Regression
Model Selection for Cross-Lingual Transfer using a Learned Scoring Function,2021,ICLR,"['Yang Chen', 'Alan Ritter']",poster,[],"Transformers that are pre-trained on multilingual text corpora, such as, mBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer learning results.  In the zero-shot cross-lingual transfer setting, only English training data is assumed, and the fine-tuned model is evaluated on another target language.  No target-language validation data is assumed in this setting, however substantial variance has been observed in target language performance between different fine-tuning runs.  Prior work has relied on English validation/development data to select among models that are fine-tuned with different learning rates, number of steps and other hyperparameters, often resulting in suboptimal choices.  In this paper, we show that it is possible to select consistently better models when small amounts of annotated data are available in an auxiliary pivot language.
We propose a machine learning approach to model selection that uses the fine-tuned model's own internal representations to predict its cross-lingual capabilities.  In extensive experiments we find that our approach consistently selects better models than English validation data across five languages and five well-studied NLP tasks, achieving results that are comparable to small amounts of target language development data.",/pdf/b83f1b0b280b72926fcd4a77b17b1582221da650.pdf,zero_few-shot;transformer;representation;transfer learning;multimodal,https://scholar.google.com/scholar?q=Model+Selection+for+Cross-Lingual+Transfer+using+a+Learned+Scoring+Function
Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models,2021,ICLR,"['Zirui Wang', 'Yulia Tsvetkov', 'Orhan Firat', 'Yuan Cao']",poster,"['Multi-task Learning', 'Multilingual Modeling']","Massively multilingual models subsuming tens or even hundreds of languages pose great challenges to multi-task optimization. While it is a common practice to apply a language-agnostic procedure optimizing a joint multilingual task objective, how to properly characterize and take advantage of its underlying problem structure for improving optimization efficiency remains under-explored. In this paper, we attempt to peek into the black-box of multilingual optimization through the lens of loss function geometry. We find that gradient similarity measured along the optimization trajectory is an important signal, which correlates well with not only language proximity but also the overall model performance. Such observation helps us to identify a critical limitation of existing gradient-based multi-task learning methods, and thus we derive a simple and scalable optimization procedure, named Gradient Vaccine, which encourages more geometrically aligned parameter updates for close tasks. Empirically, our method obtains significant model performance gains on multilingual machine translation and XTREME benchmark tasks for multilingual language models. Our work reveals the importance of properly measuring and utilizing language proximity in multilingual optimization, and has broader implications for multi-task learning beyond multilingual modeling.",/pdf/4958372042631716242a4b3f1a10231548614522.pdf,optimization;metric;multi-task;multimodal,https://scholar.google.com/scholar?q=Gradient+Vaccine:+Investigating+and+Improving+Multi-task+Optimization+in+Massively+Multilingual+Models
Globetrotter: Unsupervised Multilingual Translation from Visual Alignment,2021,ICLR,"['Didac Suris Coll-Vinent', 'Dave Epstein', 'Carl Vondrick']",poster,"['cross-modal', 'multilingual', 'unsupervised translation', 'visual similarity']","Machine translation in a multi-language scenario requires large-scale parallel corpora  for  every  language  pair.   Unsupervised  translation  is  challenging  because there is no explicit connection between languages, and the existing methods have to rely on topological properties of the language representations.  We introduce a framework that leverages visual similarity to align multiple languages, using images as the bridge between them. We estimate the cross-modal alignment between language and images, and use this estimate to guide the learning of cross-lingual representations.   Our  language  representations  are  trained  jointly  in  one  model with a single stage.  Experiments with fifty-two languages show that our method outperforms prior work on unsupervised word-level and sentence-level translation using retrieval.",/pdf/e4b8f3d103c2e0e02613c5c826d2406665386cec.pdf,graph;representation;multimodal,https://scholar.google.com/scholar?q=Globetrotter:+Unsupervised+Multilingual+Translation+from+Visual+Alignment
Neural Nonnegative CP Decomposition for Hierarchical Tensor Analysis,2021,ICLR,"['Joshua Vendrow', 'Jamie Haddock', 'Deanna Needell']",poster,"['nonnegative tensor decompositions', 'topic modeling', 'hierarchical model', 'CP decomposition', 'neural network', 'backpropagation']","There is a significant demand for topic modeling on large-scale data with complex multi-modal structure in applications such as multi-layer network analysis, temporal document classification, and video data analysis; frequently this multi-modal data has latent hierarchical structure. We propose a new hierarchical nonnegative CANDECOMP/PARAFAC (CP) decomposition (hierarchical NCPD) model and a training method, Neural NCPD, for performing hierarchical topic modeling on multi-modal tensor data. Neural NCPD utilizes a neural network architecture and backpropagation to mitigate error propagation through hierarchical NCPD.  ",/pdf/5ff44b48db470aff2d5a68a8ad106eee6fb065a6.pdf,multimodal,https://scholar.google.com/scholar?q=Neural+Nonnegative+CP+Decomposition+for+Hierarchical+Tensor+Analysis
HalentNet: Multimodal Trajectory Forecasting with Hallucinative Intents,2021,ICLR,"['Deyao Zhu', 'Mohamed Zahran', 'Li Erran Li', 'Mohamed Elhoseiny']",poster,[],"Motion forecasting is essential for making intelligent decisions in robotic navigation. As a result, the multi-agent behavioral prediction has become a core component of modern human-robot interaction applications such as autonomous driving. Due to various intentions and interactions among agents, agent trajectories can have multiple possible futures. Hence, the motion forecasting model's ability to cover possible modes becomes essential to enable accurate prediction. Towards this goal, we introduce HalentNet to better model the future motion distribution in addition to a traditional trajectory regression learning objective by incorporating generative augmentation losses. We model intents with unsupervised discrete random variables whose training is guided by a collaboration between two key signals: A discriminative loss that encourages intents' diversity and a hallucinative loss that explores intent transitions (i.e., mixed intents) and encourages their smoothness. This regulates the neural network behavior to be more accurately predictive on uncertain scenarios due to the active yet careful exploration of possible future agent behavior. Our model's learned representation leads to better and more semantically meaningful coverage of the trajectory distribution. Our experiments show that our method can improve over the state-of-the-art trajectory forecasting benchmarks, including vehicles and pedestrians, for about 20% on average FDE and 50% on road boundary violation rate when predicting 6 seconds future. We also conducted human experiments to show that our predicted trajectories received 39.6% more votes than the runner-up approach and 32.2% more votes than our variant without hallucinative mixed intent loss. The code will be released soon. ",/pdf/9f7a72a2ef90cdbf07b3067bf2417e59741a2483.pdf,reinforcement learning;representation;generative model;active learning;augmentation;multi-agent;multimodal,https://scholar.google.com/scholar?q=HalentNet:+Multimodal+Trajectory+Forecasting+with+Hallucinative+Intents
Recurrent Flow Networks: A Recurrent Latent Variable Model for Density Modelling of Urban Mobility,2021,ICML,"['Daniele Gammelli', 'Filipe Rodrigues']",poster,"['Urban Mobility', 'Latent variable models', 'amortized inference', 'normalizing flows']","Mobility-on-demand (MoD) systems represent a rapidly developing mode of transportation wherein travel requests are dynamically handled by a coordinated fleet of vehicles. Crucially, the efficiency of an MoD system highly depends on how well supply and demand distributions are aligned in spatio-temporal space (i.e., to satisfy user demand, cars have to be available in the correct place and at the desired time). When modelling urban mobility as temporal sequences, current approaches typically rely on either (i) a spatial discretization (e.g. ConvLSTMs), or (ii) a Gaussian mixture model to describe the conditional output distribution.
In this paper, we argue that both of these approaches could exhibit structural limitations when faced with highly complex data distributions such as for urban mobility densities. To address this issue, we introduce recurrent flow networks which combine deterministic and stochastic recurrent hidden states with conditional normalizing flows and show how the added flexibility allows our model to generate distributions matching potentially complex urban topologies. ",https://api.openreview.net/pdf/d356ee091b0384b0365659bc9b96c425f8344945.pdf,zero_few-shot;flow;multimodal,https://scholar.google.com/scholar?q=Recurrent+Flow+Networks:+A+Recurrent+Latent+Variable+Model+for+Density+Modelling+of+Urban+Mobility
Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning,2021,ICML,"['Víctor Campos', 'Pablo Sprechmann', 'Steven Stenberg Hansen', 'Andre Barreto', 'Steven Kapturowski', 'Alex Vitvitskyi', 'Adria Puigdomenech Badia', 'Charles Blundell']",poster,"['deep reinforcement learning', 'transfer learning', 'unsupervised learning', 'exploration']","Designing agents that acquire knowledge autonomously and use it to solve new tasks efficiently is an important challenge in reinforcement learning. Knowledge acquired during an unsupervised pre-training phase is often transferred by fine-tuning neural network weights once rewards are exposed, as is common practice in supervised domains. Given the nature of the reinforcement learning problem, we argue that standard fine-tuning strategies alone are not enough for efficient transfer in challenging domains. We introduce Behavior Transfer (BT), a technique that leverages pre-trained policies for exploration and that is complementary to transferring neural network weights. Our experiments show that, when combined with large-scale pre-training in the absence of rewards, existing intrinsic motivation objectives can lead to the emergence of complex behaviors. These pre-trained policies can then be leveraged by BT to discover better solutions than without pre-training, and combining BT with standard fine-tuning strategies results in additional benefits. The largest gains are generally observed in domains requiring structured exploration, including settings where the behavior of the pre-trained policies is misaligned with the downstream task. ",https://api.openreview.net/pdf/2ddb1a16f61fdbdcccb3576e5ffc6d39959fc939.pdf,reinforcement learning;graph;transfer learning;multimodal,https://scholar.google.com/scholar?q=Beyond+Fine-Tuning:+Transferring+Behavior+in+Reinforcement+Learning
Why be adversarial? Let's cooperate!: Cooperative Dataset Alignment via JSD Upper Bound,2021,ICML,"['Wonwoong Cho', 'Ziyu Gong', 'David I. Inouye']",spotlight,"['Unsupervised dataset alignment', 'Invertible flows']","Unsupervised dataset alignment estimates a transformation that maps two or more source domains to a shared aligned domain given only the domain datasets. This task has many applications including generative modeling, unsupervised domain adaptation, and socially aware learning. Most prior works use adversarial learning (i.e., min-max optimization), which can be challenging to optimize and evaluate. A few recent works explore non-adversarial flow-based (i.e., invertible) approaches, but they lack a unified perspective. Therefore, we propose to unify and generalize previous flow-based approaches under a single non-adversarial framework, which we prove is equivalent to minimizing an upper bound on the Jensen-Shannon Divergence (JSD). Importantly, our problem reduces to a min-min, i.e., cooperative, problem and can provide a natural evaluation metric for unsupervised dataset alignment.",https://api.openreview.net/pdf/e2a8743e99cc15246724d0f695b58bb2d8e7a3be.pdf,graph;optimization;zero_few-shot;generative model;metric;flow;multimodal,https://scholar.google.com/scholar?q=Why+be+adversarial?+Let's+cooperate!:+Cooperative+Dataset+Alignment+via+JSD+Upper+Bound
Multimodal AutoML on Structured Tables with Text Fields,2021,ICML,"['Xingjian Shi', 'Jonas Mueller', 'Nick Erickson', 'Mu Li', 'Alex Smola']",oral,"['multimodal', 'automl', 'transformer', 'text']","We  design  automated  supervised  learning  systems  for  data  tables  that  not  only  contain numeric/categorical columns,  but  text  fields  as  well.  Here  we  assemble  15  multimodal data tables that each contain some text fields and stem from a real business application. Over this benchmark, we evaluate numerous multimodal AutoML strategies, including a standard two-stage approach where NLP is used to featurize the text such that AutoML for tabular data can then be applied. We propose various practically superior strategies based on multimodal adaptations of Transformer networks and stack ensembling of these networks with classical tabular models.  Beyond performing the best in our benchmark, our proposed (fully automated) methodology manages to rank 1st place (against human data scientists) when fit to the raw tabular/text data in two MachineHack prediction competitions  and 2nd place (out of 2380 teams) in Kaggle’s Mercari Price Suggestion Challenge.",https://api.openreview.net/pdf/aba28b8aed8429cf3e9e792a64b5e2597ca4075e.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Multimodal+AutoML+on+Structured+Tables+with+Text+Fields
Towards Achieving Adversarial Robustness Beyond Perceptual Limits,2021,ICML,"['Sravanti Addepalli', 'Samyak Jain', 'Gaurang Sriramanan', 'Shivangi Khare', 'Venkatesh Babu Radhakrishnan']",poster,"['Adversarial Robustness', 'Adversarial Defense', 'Adversarial Training', 'Large perturbations']","The vulnerability of Deep Neural Networks to Adversarial Attacks has fuelled research towards building robust models. While most existing Adversarial Training algorithms aim towards defending against imperceptible attacks, real-world adversaries are not limited by such constraints. In this work, we aim to achieve adversarial robustness at larger epsilon bounds. We first discuss the ideal goals of an adversarial defense algorithm beyond perceptual limits, and further highlight the shortcomings of naively extending existing training algorithms to higher perturbation bounds. In order to overcome these shortcomings, we propose a novel defense, Oracle-Aligned Adversarial Training (OA-AT), that attempts to align the predictions of the network with that of an Oracle during adversarial training. The proposed approach achieves state-of-the-art performance at large epsilon bounds ($\ell_\infty$ bound of $16/255$) while outperforming adversarial training algorithms such as AWP, TRADES and PGD-AT at standard perturbation bounds ($\ell_\infty$ bound of $8/255$) as well.",https://api.openreview.net/pdf/df6f35e642e924085b24fa5adea2b278e82067a7.pdf,optimization;multimodal,https://scholar.google.com/scholar?q=Towards+Achieving+Adversarial+Robustness+Beyond+Perceptual+Limits
SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Adversarial Robustness,2021,ICML,"['Jongheon Jeong', 'Sejun Park', 'Minkyu Kim', 'Heung-Chang Lee', 'Doguk Kim', 'Jinwoo Shin']",poster,"['randomized smoothing', 'mixup', 'adversarial robustness', 'certified defense', 'adversarial defense', 'confidence calibration']","Randomized smoothing is currently a state-of-the-art method to construct a certifiably robust classifier from neural networks against $\ell_2$-adversarial perturbations. Under the paradigm, the robustness of a classifier is aligned with the prediction confidence, i.e., the higher confidence from a smoothed classifier implies the better robustness. This motivates us to rethink the fundamental trade-off between accuracy and robustness in terms of calibrating confidences of smoothed classifier. In this paper, we propose a simple training scheme, coined SmoothMix, to control the robustness of smoothed classifiers via self-mixup: it trains convex combinations of samples along the direction of adversarial perturbation for each input. The proposed procedure effectively identifies over-confident, near off-class samples as a cause of limited robustness in case of smoothed classifiers, and offers an intuitive way to adaptively set a new decision boundary between these samples for better robustness. Our experiments show that the proposed method can significantly improve the certified $\ell_2$-robustness of smoothed classifiers compared to state-of-the-art robust training methods.",https://api.openreview.net/pdf/ba362967915a9bb2249f40cd5b12288e671dff2b.pdf,adaptive;multimodal,https://scholar.google.com/scholar?q=SmoothMix:+Training+Confidence-calibrated+Smoothed+Classifiers+for+Certified+Adversarial+Robustness
Skipping the Frame-Level: Event-Based Piano Transcription With Neural Semi-CRFs,2021,NIPS,"['Yujia Yan', 'Frank Cwitkowitz', 'Zhiyao Duan']",poster,"['Music', 'Audio', 'Piano Transcrition', 'Music Transcription', 'Semi-Markov', 'CRFs', 'Sound Event Detection', 'Music Information Retrieval']","Piano transcription systems are typically optimized to estimate pitch activity at each frame of audio. They are often followed by carefully designed heuristics and post-processing algorithms to estimate note events from the frame-level predictions. Recent methods have also framed piano transcription as a multi-task learning problem, where the activation of different stages of a note event are estimated independently. These practices are not well aligned with the desired outcome of the task, which is the specification of note intervals as holistic events, rather than the aggregation of disjoint observations. In this work, we propose a novel formulation of piano transcription, which is optimized to directly predict note events. Our method is based on Semi-Markov Conditional Random Fields (semi-CRF), which produce scores for intervals rather than individual frames. When formulating piano transcription in this way, we eliminate the need to rely on disjoint frame-level estimates for different stages of a note event. We conduct experiments on the MAESTRO dataset and demonstrate that the proposed model surpasses the current state-of-the-art for piano transcription. Our results suggest that the semi-CRF output layer, while still quadratic in complexity, is a simple, fast and well-performing solution for event-based prediction, and may lead to similar success in other areas which currently rely on frame-level estimates.",https://api.openreview.net/pdf/240883eb11d2e89ef2d879ce55c8a2e92919fc47.pdf,zero_few-shot;multi-task;multimodal;llm,https://scholar.google.com/scholar?q=Skipping+the+Frame-Level:+Event-Based+Piano+Transcription+With+Neural+Semi-CRFs
Nested Variational Inference,2021,NIPS,"['Heiko Zimmermann', 'Hao Wu', 'Babak Esmaeili', 'Jan-Willem van de Meent']",poster,"['Variational Inference', 'Importance Sampling', 'Monte Carlo methods']","We develop nested variational inference (NVI), a family of methods that learn proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristics to guide the sampler. Our experiments apply NVI to (a) sample from a multimodal distribution using a learned annealing path (b) learn heuristics that approximate the likelihood of future observations in a hidden Markov model and (c) to perform amortized inference in hierarchical deep generative models. We observe that optimizing nested objectives leads to improved sample quality in terms of log average weight and effective sample size.",https://api.openreview.net/pdf/b6dd2b680243d7183c310d7ee9966616be351a20.pdf,generative model;inference;multimodal,https://scholar.google.com/scholar?q=Nested+Variational+Inference
Class-agnostic Reconstruction of Dynamic Objects from Videos,2021,NIPS,"['Zhongzheng Ren', 'Xiaoming Zhao', 'Alex Schwing']",poster,"['Dynamic reconstruction', 'implicit network', '3D vision']","We introduce REDO, a class-agnostic framework to REconstruct the Dynamic Objects from RGBD or calibrated videos. Compared to prior work, our problem setting is more realistic yet more challenging for three reasons: 1) due to occlusion or camera settings an object of interest may never be entirely visible, but we aim to reconstruct the complete shape; 2) we aim to handle different object dynamics including rigid motion, non-rigid motion, and articulation; 3) we aim to reconstruct different  categories  of  objects  with  one  unified  framework. To  address  these challenges, we develop two novel modules.  First, we introduce a canonical 4D implicit function which is pixel-aligned with aggregated temporal visual cues. Second, we develop a 4D transformation module which captures object dynamics to support temporal propagation and aggregation. We study the efficacy of REDO in extensive experiments on synthetic RGBD video datasets SAIL-VOS 3D and DeformingThings4D++,  and on real-world video data 3DPW. We find REDO outperforms state-of-the-art dynamic reconstruction methods by a margin. In ablation studies we validate each developed component.",https://api.openreview.net/pdf/730f18b8b5d9e7a1239eeb5c37fb107cc7e525e6.pdf,graph;multimodal;3d,https://scholar.google.com/scholar?q=Class-agnostic+Reconstruction+of+Dynamic+Objects+from+Videos
Grounding Spatio-Temporal Language with Transformers,2021,NIPS,"['Tristan Karch', 'Laetitia Teodorescu', 'Katja Hofmann', 'Clément Moulin-Frier', 'Pierre-Yves Oudeyer']",poster,"['Language Grounding', 'Embodied Autonomous Agents']","Language is an interface to the outside world. In order for embodied agents to use it, language must be grounded in other, sensorimotor modalities. While there is an extended literature studying how machines can learn grounded language, the topic of how to learn spatio-temporal linguistic concepts is still largely uncharted. To make progress in this direction, we here introduce a novel spatio-temporal language grounding task where the goal is to learn the meaning of spatio-temporal descriptions of behavioral traces of an embodied agent. This is achieved by training a truth function that predicts if a description matches a given history of observations. The descriptions involve time-extended predicates in past and present tense as well as spatio-temporal references to objects in the scene. To study the role of architectural biases in this task, we train several models including multimodal Transformer architectures; the latter implement different attention computations between words and objects across space and time. We test models on two classes of generalization: 1) generalization to new sentences, 2) generalization to grammar primitives. We observe that maintaining object identity in the attention computation of our Transformers is instrumental to achieving good performance on generalization overall, and that summarizing object traces in a single token has little influence on performance. We then discuss how this opens new perspectives for language-guided autonomous embodied agents.
",https://api.openreview.net/pdf/cabaa909389fa044ad12e8368f845efc537d3453.pdf,reinforcement learning;transformer;multimodal,https://scholar.google.com/scholar?q=Grounding+Spatio-Temporal+Language+with+Transformers
SADGA: Structure-Aware Dual Graph Aggregation Network for Text-to-SQL,2021,NIPS,"['Ruichu Cai', 'Jinjie Yuan', 'Boyan Xu', 'Zhifeng Hao']",poster,"['Graph Aggregation Network', 'Graph Neural Network', 'Text-to-SQL']","The Text-to-SQL task, aiming to translate the natural language of the questions into SQL queries, has drawn much attention recently.  One of the most challenging problems of Text-to-SQL is how to generalize the trained model to the unseen database schemas, also known as the cross-domain Text-to-SQL task. The key lies in the generalizability of (i) the encoding method to model the question and the database schema and (ii) the question-schema linking method to learn the mapping between words in the question and tables/columns in the database schema. Focusing on the above two key issues, we propose a \emph{Structure-Aware Dual Graph Aggregation Network} (SADGA) for cross-domain Text-to-SQL. In SADGA, we adopt the graph structure to provide a unified encoding model for both the natural language question and database schema. Based on the proposed unified modeling, we further devise a structure-aware aggregation method to learn the mapping between the question-graph and schema-graph. The structure-aware aggregation method is featured with \emph{Global Graph Linking}, \emph{Local Graph Linking} and \emph{Dual-Graph Aggregation Mechanism}. We not only study the performance of our proposal empirically but also achieved 3rd place on the challenging Text-to-SQL benchmark Spider at the time of writing.",https://api.openreview.net/pdf/191c891a324d1f194b3665c373d5d82d5bfe8a6b.pdf,graph;transformer;multimodal,https://scholar.google.com/scholar?q=SADGA:+Structure-Aware+Dual+Graph+Aggregation+Network+for+Text-to-SQL
Learning from Inside: Self-driven Siamese Sampling and Reasoning for Video Question Answering,2021,NIPS,"['Weijiang Yu', 'Haoteng Zheng', 'Mengfei Li', 'Lei Ji', 'Lijun Wu', 'Nong Xiao', 'Nan Duan']",poster,"['Video Question Answering', 'Multi-view Learning', 'Data-effeciency Reasoning']","Recent advances in the video question answering (i.e., VideoQA) task have achieved strong success by following the paradigm of fine-tuning each clip-text pair independently on the pretrained transformer-based model via supervised learning. Intuitively, multiple samples (i.e., clips) should be interdependent to capture similar visual and key semantic information in the same video. To consider the interdependent knowledge between contextual clips into the network inference, we propose a Siamese Sampling and Reasoning (SiaSamRea) approach, which consists of a siamese sampling mechanism to generate sparse and similar clips (i.e., siamese clips) from the same video, and a novel reasoning strategy for integrating the interdependent knowledge between contextual clips into the network. The reasoning strategy contains two modules: (1) siamese knowledge generation to learn the inter-relationship among clips; (2) siamese knowledge reasoning to produce the refined soft label by propagating the weights of inter-relationship to the predicted candidates of all clips. Finally, our SiaSamRea can endow the current multimodal reasoning paradigm with the ability of learning from inside via the guidance of soft labels. Extensive experiments demonstrate our SiaSamRea achieves state-of-the-art performance on five VideoQA benchmarks, e.g., a significant +2.1% gain on MSRVTT-QA, +2.9% on MSVD-QA, +1.0% on ActivityNet-QA, +1.8% on How2QA and +4.3% (action) on TGIF-QA.",https://api.openreview.net/pdf/025bb6f0c72492bc1348ca9f4b0f0bba7f54509a.pdf,zero_few-shot;transformer;generative model;inference;sparse;multimodal;llm,https://scholar.google.com/scholar?q=Learning+from+Inside:+Self-driven+Siamese+Sampling+and+Reasoning+for+Video+Question+Answering
How does a Neural Network's Architecture Impact its Robustness to Noisy Labels?,2021,NIPS,"['Jingling Li', 'Mozhi Zhang', 'Keyulu Xu', 'John P Dickerson', 'Jimmy Ba']",poster,"['noisy labels', 'architectural inductive bias', 'algorithmic alignment', 'graph neural networks']","Noisy labels are inevitable in large real-world datasets. In this work, we explore an area understudied by previous works --- how the network's architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network's robustness via the predictive power in its representations --- the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also find that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels.",https://api.openreview.net/pdf/ad3cf56af1a92f68bdb99333ab2058d0bf47b6f0.pdf,representation;multimodal;llm,https://scholar.google.com/scholar?q=How+does+a+Neural+Network's+Architecture+Impact+its+Robustness+to+Noisy+Labels?
NEO: Non Equilibrium Sampling on the Orbits of a Deterministic Transform,2021,NIPS,"['Achille Thin', 'Yazid Janati El Idrissi', 'Sylvain Le Corff', 'Charles Ollion', 'Eric Moulines', 'Arnaud Doucet', 'Alain Durmus', 'Christian P Robert']",poster,"['Importance Sampling', 'Monte Carlo', 'Markov chain Monte Carlo']","Sampling from a complex distribution $\pi$ and approximating its intractable normalizing constant $\mathrm{Z}$ are challenging problems. 
In this paper, a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) samplers is derived. 
Given an invertible map $\mathrm{T}$, these schemes combine (with weights) elements from the forward and backward Orbits   through points sampled from a proposal distribution $\rho$. The map $\mathrm{T}$ does not leave the target $\pi$ invariant, hence the name NEO, standing for Non-Equilibrium Orbits. 
NEO-IS provides unbiased estimators of the normalizing constant and self-normalized IS estimators of expectations under $\pi$ while NEO-MCMC combines multiple NEO-IS estimates of the normalizing constant and an iterated sampling-importance resampling mechanism to sample from $\pi$. 
For $\mathrm{T}$ chosen as a discrete-time integrator of a conformal Hamiltonian system, NEO-IS achieves state-of-the art performance on difficult benchmarks and NEO-MCMC is able to explore highly multimodal targets. Additionally, we provide detailed theoretical results for both methods. In particular, we show that NEO-MCMC is uniformly geometrically ergodic and establish explicit mixing time estimates under mild conditions.
",https://api.openreview.net/pdf/9f996fb9431c51d8d6033acbba449155bc7a5bd7.pdf,graph;metric;multimodal,https://scholar.google.com/scholar?q=NEO:+Non+Equilibrium+Sampling+on+the+Orbits+of+a+Deterministic+Transform
Multilingual Pre-training with Universal Dependency Learning,2021,NIPS,"['Kailai Sun', 'Zuchao Li', 'hai zhao']",poster,"['natural language processing', 'multilingual pre-trained language model', 'universal syntactic dependency parsing']","The pre-trained language model (PrLM) demonstrates domination in downstream natural language processing tasks, in which multilingual PrLM takes advantage of language universality to alleviate the issue of limited resources for low-resource languages. Despite its successes, the performance of multilingual PrLM is still unsatisfactory, when multilingual PrLMs only focus on plain text and ignore obvious universal linguistic structure clues. Existing PrLMs have shown that monolingual linguistic structure knowledge may bring about better performance. Thus we propose a novel multilingual PrLM that supports both explicit universal dependency parsing and implicit language modeling. Syntax in terms of universal dependency parse serves as not only pre-training objective but also learned representation in our model, which brings unprecedented PrLM interpretability and convenience in downstream task use. Our model outperforms two popular multilingual PrLM, multilingual-BERT and XLM-R, on cross-lingual natural language understanding (NLU) benchmarks and linguistic structure parsing datasets, demonstrating the effectiveness and stronger cross-lingual modeling capabilities of our approach.",https://api.openreview.net/pdf/73d55d17d605a497a8e20e2d9259667b177c00a7.pdf,zero_few-shot;transformer;representation;multimodal,https://scholar.google.com/scholar?q=Multilingual+Pre-training+with+Universal+Dependency+Learning
Scalable Thompson Sampling using Sparse Gaussian Process Models,2021,NIPS,"['Sattar Vakili', 'Henry Moss', 'Artem Artemev', 'Vincent Dutordoir', 'Victor Picheny']",poster,"['Thomsson sampling', 'Gaussian processes', 'posterior sampling', 'regret']","Thompson Sampling (TS) from Gaussian Process (GP) models is a powerful tool for the optimization of black-box functions. Although TS enjoys strong theoretical guarantees and convincing empirical performance, it incurs a large computational overhead that scales polynomially with the optimization budget. Recently, scalable TS methods based on sparse GP models have been proposed to increase the scope of TS, enabling its application to problems that are sufficiently multi-modal, noisy or combinatorial to require more than a few hundred evaluations to be solved. However, the approximation error introduced by sparse GPs invalidates all existing regret bounds. In this work, we perform a theoretical and empirical analysis of scalable TS. We provide theoretical guarantees and show that the drastic reduction in computational complexity of scalable TS can be enjoyed without loss in the regret performance over the standard TS. These conceptual claims are validated for practical implementations of scalable TS on synthetic benchmarks and as part of a real-world high-throughput molecular design task.",https://api.openreview.net/pdf/3ea3c355b40037fdc4f85d817f110eff6b509bf7.pdf,optimization;sparse;multimodal,https://scholar.google.com/scholar?q=Scalable+Thompson+Sampling+using+Sparse+Gaussian+Process+Models
Exploring Cross-Video and Cross-Modality Signals for Weakly-Supervised Audio-Visual Video Parsing,2021,NIPS,"['Yan-Bo Lin', 'Hung-Yu Tseng', 'Hsin-Ying Lee', 'Yen-Yu Lin', 'Ming-Hsuan Yang']",poster,"['Audio-visual Video Parsing', 'Weakly-supervised Learning', 'Vision Applications and Systems']","The audio-visual video parsing task aims to temporally parse a video into audio or visual event categories. However, it is labor intensive to temporally annotate audio and visual events and thus hampers the learning of a parsing model. To this end, we propose to explore additional cross-video and cross-modality supervisory signals to facilitate weakly-supervised audio-visual video parsing. The proposed method exploits both the common and diverse event semantics across videos to identify audio or visual events. In addition, our method explores event co-occurrence across audio, visual, and audio-visual streams. We leverage the explored cross-modality co-occurrence to localize segments of target events while excluding irrelevant ones. The discovered supervisory signals across different videos and modalities can greatly facilitate the training with only video-level annotations. Quantitative and qualitative results demonstrate that the proposed method performs favorably against existing methods on weakly-supervised audio-visual video parsing.",https://api.openreview.net/pdf/073ff2331abe5b4c2068a9c78a1d225a9afa2694.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Exploring+Cross-Video+and+Cross-Modality+Signals+for+Weakly-Supervised+Audio-Visual+Video+Parsing
Dual Progressive Prototype Network for Generalized Zero-Shot Learning,2021,NIPS,"['Chaoqun Wang', 'Shaobo Min', 'Xuejin Chen', 'Xiaoyan Sun', 'Houqiang Li']",poster,"['generalized zero-shot learning', 'progressive prototypes']","Generalized Zero-Shot Learning (GZSL) aims to recognize new categories with auxiliary semantic information, e.g., category attributes. In this paper, we handle the critical issue of domain shift problem, i.e., confusion between seen and unseen categories, by progressively improving cross-domain transferability and category discriminability of visual representations. Our approach, named Dual Progressive Prototype Network (DPPN), constructs two types of prototypes that record prototypical visual patterns for attributes and categories, respectively. With attribute prototypes, DPPN alternately searches attribute-related local regions and updates corresponding attribute prototypes to progressively explore accurate attribute-region correspondence. This enables DPPN to produce visual representations with accurate attribute localization ability, which benefits the semantic-visual alignment and representation transferability. Besides, along with progressive attribute localization, DPPN further projects category prototypes into multiple spaces to progressively repel visual representations from different categories, which boosts category discriminability. Both attribute and category prototypes are collaboratively learned in a unified framework, which makes visual representations of DPPN transferable and distinctive.	Experiments on four benchmarks prove that DPPN effectively alleviates the domain shift problem in GZSL.",https://api.openreview.net/pdf/ee6a0e9bd4eb4dcdb48ddc8263c38626897675ec.pdf,zero_few-shot;representation;transfer learning;multimodal,https://scholar.google.com/scholar?q=Dual+Progressive+Prototype+Network+for+Generalized+Zero-Shot+Learning
"VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",2021,NIPS,"['Hassan Akbari', 'Liangzhe Yuan', 'Rui Qian', 'Wei-Hong Chuang', 'Shih-Fu Chang', 'Yin Cui', 'Boqing Gong']",poster,"['Self-supervised Learning', 'Multimodal Understanding', 'Transformer', 'Contrastive Learning', 'Video Recognition', 'Audio Recognition']","We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training.",https://api.openreview.net/pdf/0389bbed39becb624a7f6a7295bc1f59bbfecedd.pdf,zero_few-shot;transformer;representation;contrastive learning;transfer learning;multimodal,"https://scholar.google.com/scholar?q=VATT:+Transformers+for+Multimodal+Self-Supervised+Learning+from+Raw+Video,+Audio+and+Text"
Attention Bottlenecks for Multimodal Fusion,2021,NIPS,"['Arsha Nagrani', 'Shan Yang', 'Anurag Arnab', 'Aren Jansen', 'Cordelia Schmid', 'Chen Sun']",poster,"['multimodal', 'fusion', 'attention', 'audiovisual', 'transformers', 'video']","Humans perceive the world by concurrently processing and fusing high-dimensional inputs from multiple modalities such as vision and audio.  Machine perception models, in stark contrast, are typically modality-specific and optimised for unimodal benchmarks.
A common approach for building multimodal models is to simply combine multiple of these modality-specific architectures using late-stage fusion of final representations or predictions ('late-fusion').
Instead, we introduce a novel transformer based architecture that uses 'attention bottlenecks' for modality fusion at multiple layers. Compared to traditional pairwise self-attention,  these bottlenecks force information between different modalities to pass through a small number of '`bottleneck' latent units, requiring the model to collate and condense the most relevant information in each modality and only share what is necessary. We find that such a strategy improves fusion performance, at the same time reducing computational cost. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple audio-visual classification benchmarks including Audioset, Epic-Kitchens and VGGSound. All code and models will be released.",https://api.openreview.net/pdf/9cb270ebf0bfc44ccb16fd1c3c96ba2fb80691e7.pdf,transformer;representation;multimodal,https://scholar.google.com/scholar?q=Attention+Bottlenecks+for+Multimodal+Fusion
Bubblewrap: Online tiling and real-time flow prediction on neural manifolds,2021,NIPS,"['Anne Draelos', 'Pranjal Gupta', 'Na Young Jun', 'Chaichontat Sriworarat', 'John Pearson']",poster,"['neuroscience', 'neural populations', 'online', 'real-time']","While most classic studies of function in experimental neuroscience have focused on the coding properties of individual neurons, recent developments in recording technologies have resulted in an increasing emphasis on the dynamics of neural populations. This has given rise to a wide variety of models for analyzing population activity in relation to experimental variables, but direct testing of many neural population hypotheses requires intervening in the system based on current neural state, necessitating models capable of inferring neural state online. Existing approaches, primarily based on dynamical systems, require strong parametric assumptions that are easily violated in the noise-dominated regime and do not scale well to the thousands of data channels in modern experiments. To address this problem, we propose a method that combines fast, stable dimensionality reduction with a soft tiling of the resulting neural manifold, allowing dynamics to be approximated as a probability flow between tiles. This method can be fit efficiently using online expectation maximization, scales to tens of thousands of tiles, and outperforms existing methods when dynamics are noise-dominated or feature multi-modal transition probabilities. The resulting model can be trained at kiloHertz data rates, produces accurate approximations of neural dynamics within minutes, and generates predictions on submillisecond time scales. It retains predictive performance throughout many time steps into the future and is fast enough to serve as a component of closed-loop causal experiments.",https://api.openreview.net/pdf/e26b21dcf1686126fdb1ef08a43b8c2ad96dd0da.pdf,zero_few-shot;online learning;metric;flow;multimodal;llm,https://scholar.google.com/scholar?q=Bubblewrap:+Online+tiling+and+real-time+flow+prediction+on+neural+manifolds
Adversarial Robustness without Adversarial Training: A Teacher-Guided Curriculum Learning Approach,2021,NIPS,"['Anindya Sarkar', 'Anirban Sarkar', 'Sowrya Gali', 'Vineeth N. Balasubramanian']",poster,"['Adversarial Robustness', 'Inner Maximization', 'Outer Minimization', 'Attribution Map', 'Curriculum Learning']","Current SOTA adversarially robust models are mostly based on adversarial training (AT) and differ only by some regularizers either at inner maximization or outer minimization steps. Being repetitive in nature during the inner maximization step, they take a huge time to train. We propose a non-iterative method that enforces the following ideas during training. Attribution maps are more aligned to the actual object in the image for adversarially robust models compared to naturally trained models. Also, the allowed set of pixels to perturb an image (that changes model decision) should be restricted to the object pixels only, which reduces the attack strength by limiting the attack space. Our method achieves significant performance gains with a little extra effort (10-20%) over existing AT models and outperforms all other methods in terms of adversarial as well as natural accuracy. We have performed extensive experimentation with CIFAR-10, CIFAR-100, and TinyImageNet datasets and reported results against many popular strong adversarial attacks to prove the effectiveness of our method.",https://api.openreview.net/pdf/8418677e0587cd6dcc81dd4e8a67c699c752e154.pdf,zero_few-shot;multimodal;curriculum learning,https://scholar.google.com/scholar?q=Adversarial+Robustness+without+Adversarial+Training:+A+Teacher-Guided+Curriculum+Learning+Approach
Multimodal Few-Shot Learning with Frozen Language Models,2021,NIPS,"['Maria Tsimpoukelli', 'Jacob Menick', 'Serkan Cabi', 'S. M. Ali Eslami', 'Oriol Vinyals', 'Felix Hill']",poster,"['Language Modeling', 'Multimodal', 'Few-shot Learning']","When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model presented with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of any number of interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks.",https://api.openreview.net/pdf/ec87f429a486beb17ac4c8a1a1f0e9107d39fb49.pdf,zero_few-shot;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=Multimodal+Few-Shot+Learning+with+Frozen+Language+Models
Convergence and Alignment of Gradient Descent with Random Backpropagation Weights,2021,NIPS,"['Ganlin Song', 'Ruitu Xu', 'John Lafferty']",poster,"['feedback alignment', 'deep learning']","Stochastic gradient descent with backpropagation is the workhorse of artificial neural networks. It has long been recognized that backpropagation fails to be a biologically plausible algorithm. Fundamentally, it is a non-local procedure---updating one neuron's synaptic weights requires knowledge of synaptic weights or receptive fields of downstream neurons. This limits the use of artificial neural networks as a tool for understanding the biological principles of information processing in the brain. Lillicrap et al. (2016) propose a more biologically plausible ""feedback alignment"" algorithm that uses random and fixed backpropagation weights, and show promising simulations. In this paper we study the mathematical properties of the feedback alignment procedure by analyzing convergence and alignment for two-layer networks under squared error loss. In the overparameterized setting, we prove that the error converges to zero exponentially fast, and also that regularization is necessary in order for the  parameters to become aligned with the random backpropagation weights. Simulations are given that are consistent with this analysis and suggest further generalizations. These results contribute to our understanding of how biologically plausible algorithms might carry out weight learning in a manner different from Hebbian learning, with performance that is comparable with the full non-local backpropagation algorithm.",https://api.openreview.net/pdf/1e4da80921a23cef6b1fc28bab06070662a8377c.pdf,multimodal,https://scholar.google.com/scholar?q=Convergence+and+Alignment+of+Gradient+Descent+with+Random+Backpropagation+Weights
Implicit Semantic Response Alignment for Partial Domain Adaptation,2021,NIPS,"['Wenxiao Xiao', 'Zhengming Ding', 'Hongfu Liu']",poster,"['Partial Domain Adaptation', 'Transfer Learning', 'Implicit Semantic Discovery', 'Feature-Level Weighting Schema', 'Semantic Alignment']","Partial Domain Adaptation (PDA) addresses the unsupervised domain adaptation problem where the target label space is a subset of the source label space. Most state-of-art PDA methods tackle the inconsistent label space by assigning weights to classes or individual samples, in an attempt to discard the source data that belongs to the irrelevant classes. However, we believe samples from those extra categories would still contain valuable information to promote positive transfer. In this paper, we propose the Implicit Semantic Response Alignment to explore the intrinsic relationships among different categories by applying a weighted schema on the feature level. Specifically, we design a class2vec module to extract the implicit semantic topics from the visual features. With an attention layer, we calculate the semantic response according to each implicit semantic topic. Then semantic responses of source and target data are aligned to retain the relevant information contained in multiple categories by weighting the features, instead of samples. Experiments on several cross-domain benchmark datasets demonstrate the effectiveness of our method over the state-of-the-art PDA methods. Moreover, we elaborate in-depth analyses to further explore implicit semantic alignment. ",https://api.openreview.net/pdf/97f107b5d727716b7fd78f33d09642c88845441a.pdf,transformer;transfer learning;multimodal,https://scholar.google.com/scholar?q=Implicit+Semantic+Response+Alignment+for+Partial+Domain+Adaptation
TriBERT: Human-centric Audio-visual Representation Learning,2021,NIPS,"['Tanzila Rahman', 'Mengyu Yang', 'Leonid Sigal']",poster,"['BERT', 'Transformer', 'multi-modal', 'joint representation']","The recent success of transformer models in language, such as BERT, has motivated the use of such architectures for multi-modal feature learning and tasks. However, most multi-modal variants (e.g., ViLBERT) have limited themselves to visual-linguistic data. Relatively few have explored its use in audio-visual modalities, and none, to our knowledge, illustrate them in the context of granular audio-visual detection or segmentation tasks such as sound source separation and localization. In this work, we introduce TriBERT -- a transformer-based architecture, inspired by ViLBERT, which enables contextual feature learning across three modalities: vision, pose, and audio, with the use of flexible co-attention. The use of pose keypoints is inspired by recent works that illustrate that such representations can significantly boost performance in many audio-visual scenarios where often one or more persons are responsible for the sound explicitly (e.g., talking) or implicitly (e.g., sound produced as a function of human manipulating an object). From a technical perspective, as part of the TriBERT architecture, we introduce a learned visual tokenization scheme based on spatial attention and leverage weak-supervision to allow granular cross-modal interactions for visual and pose modalities. Further, we supplement learning with sound-source separation loss formulated across all three streams. We pre-train our model on the large MUSIC21 dataset and demonstrate improved performance in audio-visual sound source separation on that dataset as well as other datasets through fine-tuning. In addition, we show that the learned TriBERT representations are generic and significantly improve performance on other audio-visual tasks such as cross-modal audio-visual-pose retrieval by as much as 66.7% in top-1 accuracy. ",https://api.openreview.net/pdf/d1319019f277f31ed11f395d658db7c33115e0d6.pdf,zero_few-shot;transformer;representation;segmentation;multimodal,https://scholar.google.com/scholar?q=TriBERT:+Human-centric+Audio-visual+Representation+Learning
Shape your Space: A Gaussian Mixture Regularization Approach to Deterministic Autoencoders,2021,NIPS,"['Amrutha Saseendran', 'Kathrin Skubch', 'Stefan Falkner', 'Margret Keuper']",poster,"['Generative Model', 'Variational Autoencoder', 'Gaussian mixture model', 'Regularization']","Variational Autoencoders (VAEs) are powerful probabilistic models to learn representations of complex data distributions. One important limitation of VAEs is the strong prior assumption that latent representations learned by the model follow a simple uni-modal Gaussian distribution. Further, the variational training procedure poses considerable practical challenges. Recently proposed regularized autoencoders offer a deterministic autoencoding framework, that simplifies the original VAE objective and is significantly easier to train. Since these models only provide weak control over the learned latent distribution, they require an ex-post density estimation step to generate samples comparable to those of VAEs. In this paper, we propose a simple and end-to-end trainable deterministic autoencoding framework, that efficiently shapes the latent space of the model during training and utilizes the capacity of expressive multi-modal latent distributions. The proposed training procedure provides direct evidence if the latent distribution adequately captures complex aspects of the encoded data. We show in experiments the expressiveness and sample quality of our model in various challenging continuous and discrete domains. An implementation is available at https://github.com/boschresearch/GMM_DAE.",https://api.openreview.net/pdf/caa6e263390336534966c5eb121bbafbfc122984.pdf,graph;zero_few-shot;representation;vae;multimodal,https://scholar.google.com/scholar?q=Shape+your+Space:+A+Gaussian+Mixture+Regularization+Approach+to+Deterministic+Autoencoders
Marginalised Gaussian Processes with Nested Sampling,2021,NIPS,"['Fergus Simpson', 'Vidhi Lalchand', 'Carl Edward Rasmussen']",poster,"['Gaussian Processes', 'nested sampling', 'Bayesian inference']","Gaussian Process models are a rich distribution over functions with inductive biases controlled by a kernel function. Learning occurs through optimisation of the kernel hyperparameters using the marginal likelihood as the objective. This work proposes nested sampling as a means of marginalising kernel hyperparameters,  because it is a technique that is well-suited to exploring complex, multi-modal distributions. We benchmark against Hamiltonian Monte Carlo on time-series and two-dimensional regression tasks, finding that a principled approach to quantifying hyperparameter uncertainty substantially improves the quality of prediction intervals.
",https://api.openreview.net/pdf/740d824a1b60969979fd8f8e31b18693681e1074.pdf,graph;multimodal,https://scholar.google.com/scholar?q=Marginalised+Gaussian+Processes+with+Nested+Sampling
Landmark-RxR: Solving Vision-and-Language Navigation with Fine-Grained Alignment Supervision,2021,NIPS,"['Keji He', 'Yan Huang', 'Qi Wu', 'Jianhua Yang', 'Dong An', 'Shuanglin Sima', 'Liang Wang']",poster,"['Vision-and-Language Navigation', 'Fine-grained dataset', 'Cross-modal alignment', 'Reward Shaping', 'Re-initialization mechanism']","In Vision-and-Language Navigation (VLN) task, an agent is asked to navigate inside 3D indoor environments following given instructions. Cross-modal alignment is one of the most critical challenges in VLN because the predicted trajectory needs to match the given instruction accurately. In this paper, we address the cross-modal alignment challenge from the perspective of fine-grain. Firstly, to alleviate weak cross-modal alignment supervision from coarse-grained data, we introduce a human-annotated fine-grained VLN dataset, namely Landmark-RxR. Secondly, to further enhance local cross-modal alignment under fine-grained supervision, we investigate the focal-oriented rewards with soft and hard forms, by focusing on the critical points sampled from fine-grained Landmark-RxR. Moreover, to fully evaluate the navigation process, we also propose a re-initialization mechanism that makes metrics insensitive to difficult points, which can cause the agent to deviate from the correct trajectories. Experimental results show that our agent has superior navigation performance on Landmark-RxR, en-RxR and R2R. Our dataset and code are available at https://github.com/hekj/Landmark-RxR.",https://api.openreview.net/pdf/e50105eb82a000c5709cda05c642b18dfad18c4c.pdf,reinforcement learning;zero_few-shot;metric;multimodal;3d,https://scholar.google.com/scholar?q=Landmark-RxR:+Solving+Vision-and-Language+Navigation+with+Fine-Grained+Alignment+Supervision
Distilling Meta Knowledge on Heterogeneous Graph for Illicit Drug Trafficker Detection on Social Media,2021,NIPS,"['Yiyue Qian', 'Yiming Zhang', 'Yanfang Ye', 'Chuxu Zhang']",poster,"['graph representation learning', 'few-shot learning', 'drug trafficker detection']","Driven by the considerable profits, the crime of drug trafficking (a.k.a. illicit drug trading) has co-evolved with modern technologies, e.g., social media such as Instagram has become a popular platform for marketing and selling illicit drugs. The activities of online drug trafficking are nimble and resilient, which call for novel techniques to effectively detect, disrupt, and dismantle illicit drug trades. In this paper, we propose a holistic framework named MetaHG to automatically detect illicit drug traffickers on social media (i.e., Instagram), by tackling the following two new challenges: (1) different from existing works which merely focus on analyzing post content, MetaHG is capable of jointly modeling multi-modal content and relational structured information on social media for illicit drug trafficker detection; (2) in addition, through the proposed meta-learning technique, MetaHG addresses the issue of requiring sufficient data for model training. More specifically, in our proposed MetaHG, we first build a heterogeneous graph (HG) to comprehensively characterize the complex ecosystem of drug trafficking on social media.  Then, we employ a relation-based graph convolutional neural network to learn node (i.e., user) representations over the built HG, in which we introduce graph structure refinement to compensate the sparse connection among entities in the HG for more robust node representation learning. Afterwards, we propose a meta-learning algorithm for model optimization. A self-supervised module and a knowledge distillation module are further designed to exploit unlabeled data for improving the model. Extensive experiments based on the real-world data collected from Instagram demonstrate that the proposed MetaHG outperforms state-of-the-art methods.",https://api.openreview.net/pdf/a177ec106a4d29160d088477c4532d7ebf9ff427.pdf,graph;optimization;zero_few-shot;representation;online learning;meta-learning;sparse;distillation;multimodal;llm,https://scholar.google.com/scholar?q=Distilling+Meta+Knowledge+on+Heterogeneous+Graph+for+Illicit+Drug+Trafficker+Detection+on+Social+Media
Revisit Multimodal Meta-Learning through the Lens of Multi-Task Learning,2021,NIPS,"['Milad Abdollahzadeh', 'Touba Malekzadeh', 'Ngai-man Cheung']",poster,"['Meta-Learning', 'Few-Shot Learning', 'Multi-Task Learning', 'Transference Analysis']","Multimodal meta-learning is a recent problem that extends conventional few-shot meta-learning by generalizing its setup to diverse multimodal task distributions. This setup makes a step towards mimicking how humans make use of a diverse set of prior skills to learn new skills. Previous work has achieved encouraging performance. In particular, in spite of the diversity of the multimodal tasks, previous work claims that a single meta-learner trained on a multimodal distribution can sometimes outperform multiple specialized meta-learners trained on individual unimodal distributions. The improvement is attributed to knowledge transfer between different modes of task distributions. However, there is no deep investigation to verify and understand the knowledge transfer between multimodal tasks. Our work makes two contributions to multimodal meta-learning. First, we propose a method to quantify knowledge transfer between tasks of different modes at a micro-level. Our quantitative, task-level analysis is inspired by the recent transference idea from multi-task learning. Second, inspired by hard parameter sharing in multi-task learning and a new interpretation of related work, we propose a new multimodal meta-learner that outperforms existing work by considerable margins. While the major focus is on multimodal meta-learning, our work also attempts to shed light on task interaction in conventional meta-learning. The code for this project is available at https://miladabd.github.io/KML.",https://api.openreview.net/pdf/93eee7f7f008acf9d4844c4c2d1fcbe5635a9f0d.pdf,graph;meta-learning;transfer learning;multi-task;multimodal,https://scholar.google.com/scholar?q=Revisit+Multimodal+Meta-Learning+through+the+Lens+of+Multi-Task+Learning
What Makes Multi-Modal Learning Better than Single (Provably),2021,NIPS,"['Yu Huang', 'Chenzhuang Du', 'Zihui Xue', 'Xuanyao Chen', 'Hang Zhao', 'Longbo Huang']",poster,['Multi-modal learning theory'],"The world provides us with data of multiple modalities. Intuitively, models fusing data from different modalities outperform their uni-modal counterparts, since more information is aggregated. Recently, joining the success of deep learning, there is an influential line of work on deep multi-modal learning, which has remarkable empirical results on various applications. However, theoretical justifications in this field are notably lacking.
                        Can multi-modal learning provably perform better than uni-modal?
In this paper, we answer this question under a most popular multi-modal fusion framework, which firstly encodes features from different modalities into a common latent space and seamlessly maps the latent representations into the task space. We prove that learning with multiple modalities achieves a  smaller population risk than only using its subset of modalities. The main intuition is that the former has a more accurate estimate of the latent space representation. To the best of our knowledge, this is the first theoretical treatment to capture important qualitative phenomena observed in real multi-modal applications from the generalization perspective. Combining with experiment results, we show that multi-modal learning does possess an appealing formal guarantee.",https://api.openreview.net/pdf/6f923fb3658fd0b4e28b32005693e42fdc0f8b19.pdf,representation;multimodal,https://scholar.google.com/scholar?q=What+Makes+Multi-Modal+Learning+Better+than+Single+(Provably)
GRIN: Generative Relation and Intention Network for Multi-agent Trajectory Prediction,2021,NIPS,"['Longyuan Li', 'Jian Yao', 'Li Kevin Wenliang', 'Tong He', 'Tianjun Xiao', 'Junchi Yan', 'David Wipf', 'Zheng Zhang']",poster,"['Trajectory prediction', 'variational Autoencoder', 'deep learning', 'probabilistic']","Learning the distribution of future trajectories conditioned on the past is a crucial problem for understanding multi-agent systems. This is challenging because humans make decisions based on complex social relations and personal intents, resulting in highly complex uncertainties over trajectories. To address this problem, we propose a conditional deep generative model that combines advances in graph neural networks. The prior and recognition model encodes two types of latent codes for each agent: an inter-agent latent code to represent social relations and an intra-agent latent code to represent agent intentions. The decoder is carefully devised to leverage the codes in a disentangled way to predict multi-modal future trajectory distribution. Specifically, a graph attention network built upon inter-agent latent code is used to learn continuous pair-wise relations, and an agent's motion is controlled by its latent intents and its observations of all other agents. Through experiments on both synthetic and real-world datasets, we show that our model outperforms previous work in multiple performance metrics. We also show that our model generates realistic multi-modal trajectories.",https://api.openreview.net/pdf/a712408147577efb64ca28995452697f1c0fcaad.pdf,reinforcement learning;graph;transformer;generative model;metric;multi-agent;multimodal,https://scholar.google.com/scholar?q=GRIN:+Generative+Relation+and+Intention+Network+for+Multi-agent+Trajectory+Prediction
Non-Gaussian Gaussian Processes for Few-Shot Regression,2021,NIPS,"['Marcin Sendera', 'Jacek Tabor', 'Aleksandra Nowak', 'Andrzej Bedychaj', 'Massimiliano Patacchiola', 'Tomasz Trzcinski', 'Przemysław Spurek', 'Maciej Zieba']",poster,"['Meta-Learning', 'Few-Shot Learning', 'Few-Shot Regression', 'Normalizing Flows', 'Gaussian Processes']","Gaussian Processes (GPs) have been widely used in machine learning to model distributions over functions, with applications including multi-modal regression, time-series prediction, and few-shot learning. GPs are particularly useful in the last application since they rely on Normal distributions and enable closed-form computation of the posterior probability function. Unfortunately, because the resulting posterior is not flexible enough to capture complex distributions, GPs assume high similarity between subsequent tasks - a requirement rarely met in real-world conditions. In this work, we address this limitation by leveraging the flexibility of Normalizing Flows to modulate the posterior predictive distribution of the GP. This makes the GP posterior locally non-Gaussian, therefore we name our method Non-Gaussian Gaussian Processes (NGGPs). More precisely, we propose an invertible ODE-based mapping that operates on each component of the random variable vectors and shares the parameters across all of them. We empirically tested the flexibility of NGGPs on various few-shot learning regression datasets, showing that the mapping can incorporate context embedding information to model different noise levels for periodic functions. As a result, our method shares the structure of the problem between subsequent tasks, but the contextualization allows for adaptation to dissimilarities. NGGPs outperform the competing state-of-the-art approaches on a diversified set of benchmarks and applications.",https://api.openreview.net/pdf/2864c2415dd1ee661adecab892f2fb1c613b69cd.pdf,graph;zero_few-shot;flow;multimodal,https://scholar.google.com/scholar?q=Non-Gaussian+Gaussian+Processes+for+Few-Shot+Regression
Robust Visual Reasoning via Language Guided Neural Module Networks,2021,NIPS,"['Arjun Reddy Akula', 'Varun Jampani', 'Soravit Changpinyo', 'Song-Chun Zhu']",poster,"['Neural Module Networks', 'Adaptive Convolutions']","Neural module networks (NMN) are a popular approach for solving multi-modal tasks such as visual question answering (VQA) and visual referring expression recognition (REF). A key limitation in prior implementations of NMN is that the neural modules do not effectively capture the association between the visual input and the relevant neighbourhood context of the textual input. This limits their generalizability. For instance, NMN fail to understand new concepts such as “yellow sphere to the left"" even when it is a combination of known concepts from train data: “blue sphere"", “yellow cube"", and “metallic cube to the left"". In this paper, we address this limitation by introducing a language-guided adaptive convolution layer (LG-Conv) into NMN, in which the filter weights of convolutions are explicitly multiplied with a spatially varying language-guided kernel. Our model allows the neural module to adaptively co-attend over potential objects of interest from the visual and textual inputs. Extensive experiments on VQA and REF tasks demonstrate the effectiveness of our approach. Additionally, we propose a new challenging out-of-distribution test split for REF task, which we call C3-Ref+, for explicitly evaluating the NMN’s ability to generalize well to adversarial perturbations and unseen combinations of known concepts. Experiments on C3-Ref+ further demonstrate the generalization capabilities of our approach.",https://api.openreview.net/pdf/8b090a650b6f1b08378470764784bc350b8f2f10.pdf,graph;zero_few-shot;adaptive;meta-learning;multimodal,https://scholar.google.com/scholar?q=Robust+Visual+Reasoning+via+Language+Guided+Neural+Module+Networks
Exploring the Limits of Out-of-Distribution Detection,2021,NIPS,"['Stanislav Fort', 'Jie Ren', 'Balaji Lakshminarayanan']",poster,"['out-of-distribution detection', 'outlier exposure']","Near out-of-distribution detection (OOD) is a major challenge for deep neural networks. We demonstrate that large-scale pre-trained transformers can significantly improve the state-of-the-art (SOTA) on a range of near OOD tasks across different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD detection, we improve the AUROC from 85% (current SOTA) to more than 96% using Vision Transformers pre-trained on ImageNet21k. On a challenging genomics OOD detection benchmark, we improve the AUROC from 66% to 77% using transformer and unsupervised pre-training.  To further improve performance, we explore the few-shot outlier exposure setting where a few examples from outlier classes may be available; we show that  pre-trained transformers are particularly well-suited for outlier exposure, and that the AUROC of OOD detection on CIFAR-100 vs CIFAR-10  can be improved to 98.7% with just 1 image per OOD class, and 99.46% with 10 images per OOD class. For multi-modal image-text pre-trained transformers such as CLIP, we explore a new way of using just the names of outlier classes as a sole source of information without any accompanying images, and show that this outperforms previous SOTA on standard OOD benchmark tasks. ",https://api.openreview.net/pdf/c48a5142b64f4951dcffa4be2d890d919cfc695e.pdf,graph;transformer;multimodal,https://scholar.google.com/scholar?q=Exploring+the+Limits+of+Out-of-Distribution+Detection
Conflict-Averse Gradient Descent for Multi-task learning,2021,NIPS,"['Bo Liu', 'Xingchao Liu', 'Xiaojie Jin', 'Peter Stone', 'qiang liu']",poster,['multi-task learning'],"The goal of multi-task learning is to enable more efficient learning than single task learning by sharing model structures for a diverse set of tasks. A standard multi-task learning objective is to minimize the average loss across all tasks. While straightforward, using this objective often results in much worse final performance for each task than learning them independently. A major challenge in optimizing a multi-task model is the conflicting gradients, where gradients of different task objectives are not well aligned so that following the average gradient direction can be detrimental to specific tasks' performance. Previous work has proposed several heuristics to manipulate the task gradients for mitigating this problem. But most of them lack convergence guarantee and/or could converge to any Pareto-stationary point.
In this paper, we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the average loss function, while leveraging the worst local improvement of individual tasks to regularize the algorithm trajectory. CAGrad balances the objectives automatically and still provably converges to a minimum over the average loss. It includes the regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA) in the multi-objective optimization (MOO) literature as special cases. On a series of challenging multi-task supervised learning and reinforcement learning tasks, CAGrad achieves improved performance over prior state-of-the-art multi-objective gradient manipulation methods.",https://api.openreview.net/pdf/c63d1faf98117c718633529585b7d5e8c4b491bb.pdf,reinforcement learning;graph;optimization;zero_few-shot;multi-task;multimodal,https://scholar.google.com/scholar?q=Conflict-Averse+Gradient+Descent+for+Multi-task+learning
Predicting Event Memorability from Contextual Visual Semantics,2021,NIPS,"['Qianli Xu', 'Fen Fang', 'Ana Garcia del Molino', 'Vigneshwaran Subbaraju', 'Joo Hwee Lim']",poster,"['Event memory', 'image memorability', 'visual semantics', 'episodic memory', 'lifelog']","Episodic event memory is a key component of human cognition. Predicting event memorability,i.e., to what extent an event is recalled, is a tough challenge in memory research and has profound implications for artificial intelligence. In this study, we investigate factors that affect event memorability according to a cued recall process. Specifically, we explore whether event memorability is contingent on the event context, as well as the intrinsic visual attributes of image cues. We design a novel experiment protocol and conduct a large-scale experiment with 47 elder subjects over 3 months.  Subjects’ memory of life events is tested in a cued recall process. Using advanced visual analytics methods, we build a first-of-its-kind event memorability dataset (called R3) with rich information about event context and visual semantic features. Furthermore, we propose a contextual event memory network (CEMNet) that tackles multi-modal input to predict item-wise event memorability, which outperforms competitive benchmarks.  The findings inform deeper understanding of episodic event memory, and open up a new avenue for prediction of human episodic memory.  Source code is available at https://github.com/ffzzy840304/Predicting-Event-Memorability.",https://api.openreview.net/pdf/5b7671f40a81b5ee4256a53fa672499fef577a50.pdf,multimodal,https://scholar.google.com/scholar?q=Predicting+Event+Memorability+from+Contextual+Visual+Semantics
The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations,2021,NIPS,"['Peter Hase', 'Harry Xie', 'Mohit Bansal']",poster,"['feature importance', 'interpretability', 'explainability', 'social alignment', 'search methods', 'feature attribution', 'salience methods']","Feature importance (FI) estimates are a popular form of explanation, and they are commonly created and evaluated by computing the change in model confidence caused by removing certain input features at test time. For example, in the standard Sufficiency metric, only the top-k most important tokens are kept. In this paper, we study several under-explored dimensions of FI explanations, providing conceptual and empirical improvements for this form of explanation. First, we advance a new argument for why it can be problematic to remove features from an input when creating or evaluating explanations: the fact that these counterfactual inputs are out-of-distribution (OOD) to models implies that the resulting explanations are socially misaligned. The crux of the problem is that the model prior and random weight initialization influence the explanations (and explanation metrics) in unintended ways. To resolve this issue, we propose a simple alteration to the model training process, which results in more socially aligned explanations and metrics. Second, we compare among five approaches for removing features from model inputs. We find that some methods produce more OOD counterfactuals than others, and we make recommendations for selecting a feature-replacement function. Finally, we introduce four search-based methods for identifying FI explanations and compare them to strong baselines, including LIME, Anchors, and Integrated Gradients. Through experiments with six diverse text classification datasets, we find that the only method that consistently outperforms random search is a Parallel Local Search (PLS) that we introduce. Improvements over the second best method are as large as 5.4 points for Sufficiency and 17 points for Comprehensiveness.",https://api.openreview.net/pdf/cbdc780532f1382b58d6f66ce8639c462c9c8024.pdf,metric;multimodal,https://scholar.google.com/scholar?q=The+Out-of-Distribution+Problem+in+Explainability+and+Search+Methods+for+Feature+Importance+Explanations
Aligned Structured Sparsity Learning for Efficient Image Super-Resolution,2021,NIPS,"['Yulun Zhang', 'Huan Wang', 'Can Qin', 'Yun Fu']",spotlight,"['Neural Network Pruning', 'Lightweight Image Super-Resolution', 'Aligned Structured Sparsity Learning']","Lightweight image super-resolution (SR) networks have obtained promising results with moderate model size. Many SR methods have focused on designing lightweight architectures, which neglect to further reduce the redundancy of network parameters. On the other hand, model compression techniques, like neural architecture search and knowledge distillation, typically consume considerable memory and computation resources. In contrast, network pruning is a cheap and effective model compression technique. However, it is hard to be applied to SR networks directly, because filter pruning for residual blocks is well-known tricky. To address the above issues, we propose aligned structured sparsity learning (ASSL), which introduces a weight normalization layer and applies $L_2$ regularization to the scale parameters for sparsity. To align the pruned locations across different layers, we propose a \emph{sparsity structure alignment} penalty term, which minimizes the norm of soft mask gram matrix. We apply aligned structured sparsity learning strategy to train efficient image SR network, named as ASSLN, with smaller model size and lower computation than state-of-the-art methods. We conduct extensive comparisons with lightweight SR networks. Our ASSLN achieves superior performance gains over recent methods quantitatively and visually.",https://api.openreview.net/pdf/e913bbc7caee9b11a9a70c1d1094df9563ffaa17.pdf,zero_few-shot;distillation;multimodal,https://scholar.google.com/scholar?q=Aligned+Structured+Sparsity+Learning+for+Efficient+Image+Super-Resolution
Multimodal and Multilingual Embeddings for Large-Scale Speech Mining,2021,NIPS,"['Paul-Ambroise Duquenne', 'Hongyu Gong', 'Holger Schwenk']",spotlight,"['Speech Mining', 'Large-scale mining', 'Speech Translation']","We present an approach to encode a speech signal into a fixed-size representation which minimizes the cosine loss with the existing massively multilingual LASER text embedding space. Sentences are close in this embedding space, independently of their language and modality, either text or audio. Using a similarity metric in that multimodal embedding space, we perform mining of audio in German, French, Spanish and English from Librivox against billions of sentences from Common Crawl. This yielded more than twenty thousand hours of aligned speech translations.  To evaluate the automatically mined speech/text corpora, we train neural speech translation systems for several languages pairs. Adding the mined data, achieves significant improvements in the BLEU score on the CoVoST2 and the MUST-C test sets with respect to a very competitive baseline. Our approach can also be used to directly perform speech-to-speech mining, without the need to first transcribe or translate the data. We obtain more than one thousand three hundred hours of aligned speech in French, German, Spanish and English. This speech corpus has the potential to boost research in speech-to-speech translation which suffers from scarcity of natural end-to-end training data. All the mined multimodal corpora will be made freely available.",https://api.openreview.net/pdf/323b06b17e9d64aff536c9cfd341f591d7f21139.pdf,representation;metric;multimodal,https://scholar.google.com/scholar?q=Multimodal+and+Multilingual+Embeddings+for+Large-Scale+Speech+Mining
Look at What I’m Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos,2021,NIPS,"['Reuben Tan', 'Bryan A. Plummer', 'Kate Saenko', 'Hailin Jin', 'Bryan Russell']",spotlight,"['Self-supervision', 'video understanding', 'natural language']","We introduce the task of spatially localizing narrated interactions in videos. Key to our approach is the ability to learn to spatially localize interactions with self-supervision on a large corpus of videos with accompanying transcribed narrations. 
To achieve this goal, we propose a multilayer cross-modal attention network that enables effective optimization of a contrastive loss during training. We introduce a divided strategy that alternates between computing inter- and intra-modal attention across the visual and natural language modalities, which allows effective training via directly contrasting the two modalities' representations. We demonstrate the effectiveness of our approach by self-training on the HowTo100M instructional video dataset and evaluating on a newly collected dataset of localized described interactions in the YouCook2 dataset. We show that our approach outperforms alternative baselines, including shallow co-attention and full cross-modal attention. We also apply our approach to grounding phrases in images with weak supervision on Flickr30K and show that stacking multiple attention layers is effective and, when combined with a word-to-region loss, achieves state of the art on recall-at-one and pointing hand accuracies.",https://api.openreview.net/pdf/85b05c73c8fd9d818f0ae4ffc03f1cf7c64848ef.pdf,optimization;zero_few-shot;transformer;representation;contrastive learning;multimodal;self-supervision,https://scholar.google.com/scholar?q=Look+at+What+I’m+Doing:+Self-Supervised+Spatial+Grounding+of+Narrations+in+Instructional+Videos
Batch Normalization Orthogonalizes Representations in Deep Random Networks,2021,NIPS,"['Hadi Daneshmand', 'Amir Joudaki', 'Francis Bach']",spotlight,"['Batch normalization', 'Theory of deep neural networks', 'Markov chains', 'Random Neural Networks', 'Optimization for neural networks']","This paper underlines an elegant property of batch-normalization (BN): Successive batch normalizations with random linear updates make samples increasingly orthogonal. We establish a non-asymptotic characterization of the interplay between depth, width, and the orthogonality of deep representations. More precisely, we prove, under a mild assumption, the deviation of the representations from orthogonality rapidly decays with depth up to a term inversely proportional to the network width. This result has two main theoretical and practical implications: 1) Theoretically, as the depth grows, the distribution of the outputs contracts to a Wasserstein-2 ball around an isotropic normal distribution. Furthermore, the radius of this Wasserstein ball shrinks with the width of the network. 2) Practically, the orthogonality of the representations directly influences the performance of stochastic gradient descent (SGD). When representations are initially aligned, we observe SGD wastes many iterations to disentangle representations before the classification. Nevertheless, we experimentally show that starting optimization from orthogonal representations is sufficient to accelerate SGD, with no need for BN.",https://api.openreview.net/pdf/25c304f8a27f5a32f91c5cc504b510508b1142d4.pdf,optimization;representation;multimodal,https://scholar.google.com/scholar?q=Batch+Normalization+Orthogonalizes+Representations+in+Deep+Random+Networks
Neural Human Performer: Learning Generalizable Radiance Fields for Human Performance Rendering,2021,NIPS,"['YoungJoong Kwon', 'Dahun Kim', 'Duygu Ceylan', 'Henry Fuchs']",spotlight,"['Novel view synthesis', 'Neural radiance fields', 'Human performance capture', 'Generalizable Neural radiance fields']","In this paper, we aim at synthesizing a free-viewpoint video of an arbitrary human performance using sparse multi-view cameras. Recently, several works have addressed this problem by learning person-specific neural radiance fields (NeRF) to capture the appearance of a particular human. In parallel, some work proposed to use pixel-aligned features to generalize radiance fields to arbitrary new scenes and objects. Adopting such generalization approaches to humans, however, is highly challenging due to the heavy occlusions and dynamic articulations of body parts. To tackle this, we propose Neural Human Performer, a novel approach that learns generalizable neural radiance fields based on a parametric human body model for robust performance capture. Specifically, we first introduce a temporal transformer that aggregates tracked visual features based on the skeletal body motion over time. Moreover, a multi-view transformer is proposed to perform cross-attention between the temporally-fused features and the pixel-aligned features at each time step to integrate observations on the fly from multiple views. Experiments on the ZJU-MoCap and AIST datasets show that our method significantly outperforms recent generalizable NeRF methods on unseen identities and poses.",https://api.openreview.net/pdf/d64d45c80d395213a756be07e4bb15893f371d48.pdf,graph;transformer;metric;sparse;multimodal;multi-view,https://scholar.google.com/scholar?q=Neural+Human+Performer:+Learning+Generalizable+Radiance+Fields+for+Human+Performance+Rendering
"PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition",2021,NIPS,"['Cheng-I Lai', 'Yang Zhang', 'Alexander H. Liu', 'Shiyu Chang', 'Yi-Lun Liao', 'Yung-Sung Chuang', 'Kaizhi Qian', 'Sameer Khurana', 'David Daniel Cox', 'James R. Glass']",spotlight,"['Speech Recognition', 'Self-Supervised Learning', 'Lottery Ticket Hypothesis', 'Pruning']","Self-supervised speech representation learning (speech SSL) has demonstrated the benefit of scale in learning rich representations for Automatic Speech Recognition (ASR) with limited paired data, such as wav2vec 2.0. We investigate the existence of sparse subnetworks in pre-trained speech SSL models that achieve even better low-resource ASR results. However, directly applying widely adopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost needed. Moreover, we show that the discovered subnetworks yield minimal performance gain compared to the original dense network.
We present Prune-Adjust-Re-Prune (PARP), which discovers and finetunes subnetworks for much better performance, while only requiring a single downstream ASR finetuning run. PARP is inspired by our surprising observation that subnetworks pruned for pre-training tasks need merely a slight adjustment to achieve a sizeable performance boost in downstream ASR tasks. Extensive experiments on low-resource ASR verify (1) sparse subnetworks exist in mono-lingual/multi-lingual pre-trained speech SSL, and (2) the computational advantage and performance gain of PARP over baseline pruning methods.
In particular, on the 10min Librispeech split without LM decoding, PARP discovers subnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full model. We further demonstrate the effectiveness of PARP via: cross-lingual pruning without any phone recognition degradation, the discovery of a multi-lingual subnetwork for 10 spoken languages in 1 finetuning run, and its applicability to pre-trained BERT/XLNet for natural language tasks1.",https://api.openreview.net/pdf/9b0c9a372e97938a5e4648cdbf51404084dc617d.pdf,graph;zero_few-shot;transformer;representation;sparse;multimodal,"https://scholar.google.com/scholar?q=PARP:+Prune,+Adjust+and+Re-Prune+for+Self-Supervised+Speech+Recognition"
Breaking the Dilemma of Medical Image-to-image Translation,2021,NIPS,"['Lingke Kong', 'Chenyu Lian', 'Detian Huang', 'ZhenJiang Li', 'Yanle Hu', 'Qichao Zhou']",spotlight,['Image-to-image Translation'],"Supervised Pix2Pix and unsupervised Cycle-consistency are two modes that dominate the field of medical image-to-image translation. However, neither modes are ideal. The Pix2Pix mode has excellent performance. But it requires paired and well pixel-wise aligned images, which may not always be achievable due to respiratory motion or anatomy change between times that paired images are acquired. The Cycle-consistency mode is less stringent with training data and works well on unpaired or misaligned images. But its performance may not be optimal. In order to break the dilemma of the existing modes, we propose a new unsupervised mode called RegGAN for medical image-to-image translation. It is based on the theory of ""loss-correction"". In RegGAN, the misaligned target images are considered as noisy labels and the generator is trained with an additional registration network to fit the misaligned noise distribution adaptively. The goal is to search for the common optimal solution to both image-to-image translation and registration tasks. We incorporated RegGAN into a few state-of-the-art image-to-image translation methods and demonstrated that RegGAN could be easily combined with these methods to improve their performances. Such as a simple CycleGAN in our mode surpasses latest NICEGAN even though using less network parameters. Based on our results, RegGAN outperformed both Pix2Pix on aligned data and Cycle-consistency on misaligned or unpaired data. RegGAN is insensitive to noises which makes it a better choice for a wide range of scenarios, especially for medical image-to-image translation tasks in which well pixel-wise aligned data are not available. Code and dataset are available at https://github.com/Kid-Liet/Reg-GAN.",https://api.openreview.net/pdf/635fd0e10fce7c374460fca897ce1fdf83793642.pdf,adaptive;multimodal,https://scholar.google.com/scholar?q=Breaking+the+Dilemma+of+Medical+Image-to-image+Translation
Align before Fuse: Vision and Language Representation Learning with Momentum Distillation,2021,NIPS,"['Junnan Li', 'Ramprasaath R. Selvaraju', 'Akhilesh Deepak Gotmare', 'Shafiq Joty', 'Caiming Xiong', 'Steven Hoi']",spotlight,"['vision and language pre-training', 'representation learning', 'image-text retrieval', 'visual question answering', 'vision-language reasoning']","Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves absolute improvements of 2.37% and 3.84% compared to the state-of-the-art, while enjoying faster inference speed. Code and models are available at https://github.com/salesforce/ALBEF.",https://api.openreview.net/pdf/3871e256115f20fc403eb8052c4244a4ca63ac0c.pdf,graph;transformer;representation;contrastive learning;inference;distillation;multimodal,https://scholar.google.com/scholar?q=Align+before+Fuse:+Vision+and+Language+Representation+Learning+with+Momentum+Distillation
Task-Adaptive Neural Network Search with Meta-Contrastive Learning,2021,NIPS,"['Wonyong Jeong', 'Hayeon Lee', 'Geon Park', 'Eunyoung Hyung', 'Jinheon Baek', 'Sung Ju Hwang']",spotlight,"['AutoML', 'Meta-Learning', 'Transfer Learning']","Most conventional Neural Architecture Search (NAS) approaches are limited in that they only generate architectures without searching for the optimal parameters. While some NAS methods handle this issue by utilizing a supernet trained on a large-scale dataset such as ImageNet, they may be suboptimal if the target tasks are highly dissimilar from the dataset the supernet is trained on. To address such limitations, we introduce a novel problem of Neural Network Search (NNS), whose goal is to search for the optimal pretrained network for a novel dataset and constraints (e.g. number of parameters), from a model zoo. Then, we propose a novel framework to tackle the problem, namely Task-Adaptive Neural Network Search (TANS). Given a model-zoo that consists of network pretrained on diverse datasets, we use a novel amortized meta-learning framework to learn a cross-modal latent space with contrastive loss, to maximize the similarity between a dataset and a high-performing network on it, and minimize the similarity between irrelevant dataset-network pairs. We validate the effectiveness and efficiency of our method on ten real-world datasets, against existing NAS/AutoML baselines. The results show that our method instantly retrieves networks that outperform models obtained with the baselines with significantly fewer training steps to reach the target performance, thus minimizing the total cost of obtaining a task-optimal network. Our code and the model-zoo are available at https://anonymous.4open.science/r/TANS-33D6",https://api.openreview.net/pdf/512008b33b35bd3eb08b2d70791e2a4c7d064194.pdf,optimization;adaptive;contrastive learning;meta-learning;multimodal;3d,https://scholar.google.com/scholar?q=Task-Adaptive+Neural+Network+Search+with+Meta-Contrastive+Learning
Raw Nav-merge Seismic Data to Subsurface Properties with MLP based Multi-Modal Information Unscrambler,2021,NIPS,"['Aditya Desai', 'Zhaozhuo Xu', 'Menal Gupta', 'Anu Chandran', 'Antoine Vial-Aussavy', 'Anshumali Shrivastava']",poster,"['seismic inversion', 'sustainability', 'seismic data ingestion']","Traditional seismic inversion (SI) maps the hundreds of terabytes of raw-field data to subsurface properties in gigabytes.  This inversion process is expensive, requiring over a year of human and computational effort. Recently, data-driven approaches equipped with Deep learning (DL) are envisioned to improve SI efficiency.  However, these improvements are restricted to data with highly reduced scale and complexity. To extend these approaches to real-scale seismic data, researchers need to process raw nav-merge seismic data into an image and perform convolution. We argue that this convolution-based way of SI is not only computationally expensive but also conceptually problematic. Seismic data is not naturally an image and need not be processed as images. In this work, we go beyond convolution and propose a novel SI method. We solve the scalability of SI by proposing a new auxiliary learning paradigm for SI (Aux-SI). This paradigm breaks the SI into local inversion tasks, which predicts each small chunk of subsurface properties using surrounding seismic data. Aux-SI combines these local predictions to obtain the entire subsurface model. However, even this local inversion is still challenging due to: (1) high-dimensional, spatially irregular multi-modal seismic data, (2) there is no concrete spatial mapping (or alignment) between subsurface properties and raw data. To handle these challenges, we propose an all-MLP architecture,  Multi-Modal Information Unscrambler (MMI-Unscrambler), that unscrambles seismic information by ingesting all available multi-modal data. The experiment shows that MMI-Unscrambler outperforms both SOTA U-Net and Transformer models on simulation data. We also scale MMI-Unscrambler to raw-field nav-merge data on Gulf-of-Mexico to obtain a geologically sound velocity model with an SSIM score of 0.8. To the best of our knowledge, this is the first successful demonstration of the DL approach on SI for real, large-scale, and complicated raw field data. ",https://api.openreview.net/pdf/e8555e87979ad5bcb3aa50feaa56dda548814095.pdf,graph;transformer;multimodal,https://scholar.google.com/scholar?q=Raw+Nav-merge+Seismic+Data+to+Subsurface+Properties+with+MLP+based+Multi-Modal+Information+Unscrambler
Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning,2021,NIPS,"['Yizhen Zhang', 'Minkyu Choi', 'Kuan Han', 'Zhongming Liu']",poster,"['language learning', 'visual grounding', 'visual relational reasoning', 'cross-modal contrastive learning', 'grounded cognition']","In natural language processing, most models try to learn semantic representations merely from texts. The learned representations encode the “distributional semantics” but fail to connect to any knowledge about the physical world. In contrast, humans learn language by grounding concepts in perception and action and the brain encodes “grounded semantics” for cognition. Inspired by this notion and recent work in vision-language learning, we design a two-stream model for grounding language learning in vision. The model includes a VGG-based visual stream and a Bert-based language stream. The two streams merge into a joint representational space. Through cross-modal contrastive learning, the model first learns to align visual and language representations with the MS COCO dataset. The model further learns to retrieve visual objects with language queries through a cross-modal attention module and to infer the visual relations between the retrieved objects through a bilinear operator with the Visual Genome dataset. After training, the model’s language stream is a stand-alone language model capable of embedding concepts in a visually grounded semantic space. This semantic space manifests principal dimensions explainable with human intuition and neurobiological knowledge. Word embeddings in this semantic space are predictive of human-defined norms of semantic features and are segregated into perceptually distinctive clusters. Furthermore, the visually grounded language model also enables compositional language understanding based on visual knowledge and multimodal image search with queries based on images, texts, or their combinations.",https://api.openreview.net/pdf/df89469efbc27e97c89a9cc00967559b52cfe92a.pdf,transformer;representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Explainable+Semantic+Space+by+Grounding+Language+to+Vision+with+Cross-Modal+Contrastive+Learning
Neural Dubber: Dubbing for Videos According to Scripts,2021,NIPS,"['Chenxu Hu', 'Qiao Tian', 'Tingle Li', 'Wang Yuping', 'Yuxuan Wang', 'Hang Zhao']",poster,"['text to speech', 'speech', 'audio-visual', 'synchronization', 'multi-modal']","Dubbing is a post-production process of re-recording actors’ dialogues, which is extensively used in filmmaking and video production. It is usually performed manually by professional voice actors who read lines with proper prosody, and in synchronization with the pre-recorded videos. In this work, we propose Neural Dubber, the first neural network model to solve a novel automatic video dubbing (AVD) task: synthesizing human speech synchronized with the given video from the text. Neural Dubber is a multi-modal text-to-speech (TTS) model that utilizes the lip movement in the video to control the prosody of the generated speech. Furthermore, an image-based speaker embedding (ISE) module is developed for the multi-speaker setting, which enables Neural Dubber to generate speech with a reasonable timbre according to the speaker’s face. Experiments on the chemistry lecture single-speaker dataset and LRS2 multi-speaker dataset show that Neural Dubber can generate speech audios on par with state-of-the-art TTS models in terms of speech quality. Most importantly, both qualitative and quantitative evaluations show that Neural Dubber can control the prosody of synthesized speech by the video, and generate high-fidelity speech temporally synchronized with the video.",https://api.openreview.net/pdf/26620b90db87b2cde6cbaa93c5886ce819821421.pdf,multimodal,https://scholar.google.com/scholar?q=Neural+Dubber:+Dubbing+for+Videos+According+to+Scripts
Understanding the Limits of Unsupervised Domain Adaptation via Data Poisoning,2021,NIPS,"['Akshay Mehra', 'Bhavya Kailkhura', 'Pin-Yu Chen', 'Jihun Hamm']",poster,"['data poisoning', 'unsupervised domain adaptation', 'domain adaptation']","Unsupervised domain adaptation (UDA) enables cross-domain learning without target domain labels by transferring knowledge from a labeled source domain whose distribution differs from that of the target. However, UDA is not always successful and several accounts of `negative transfer' have been reported in the literature. In this work, we prove a simple lower bound on the target domain error that complements the existing upper bound. Our bound shows the insufficiency of minimizing source domain error and marginal distribution mismatch for a guaranteed reduction in the target domain error, due to the possible increase of induced labeling function mismatch. This insufficiency is further illustrated through simple distributions for which the same UDA approach succeeds, fails, and may succeed or fail with an equal chance. Motivated from this, we propose novel data poisoning attacks to fool UDA methods into learning representations that produce large target domain errors. We evaluate the effect of these attacks on popular UDA methods using benchmark datasets where they have been previously shown to be successful. Our results show that poisoning can significantly decrease the target domain accuracy, dropping it to almost 0% in some cases, with the addition of only 10% poisoned data in the source domain. The failure of these UDA methods demonstrates their limitations at guaranteeing cross-domain generalization consistent with our lower bound. Thus, evaluating UDA methods in adversarial settings such as data poisoning provides a better sense of their robustness to data distributions unfavorable for UDA.",https://api.openreview.net/pdf/280bc110b14adad7156f2c6d476360716922b95c.pdf,graph;zero_few-shot;representation;transfer learning;multimodal,https://scholar.google.com/scholar?q=Understanding+the+Limits+of+Unsupervised+Domain+Adaptation+via+Data+Poisoning
Learning 3D Dense Correspondence via Canonical Point Autoencoder,2021,NIPS,"['Anchieh Cheng', 'Xueting Li', 'Min Sun', 'Ming-Hsuan Yang', 'Sifei Liu']",poster,"['3D dense correspondence', 'point autoencoder']","We propose a canonical point autoencoder (CPAE) that predicts dense correspondences between 3D shapes of the same category. The autoencoder performs two key functions: (a) encoding an arbitrarily ordered point cloud to a canonical primitive, e.g., a sphere, and (b) decoding the primitive back to the original input instance shape. As being placed in the bottleneck, this primitive plays a key role to map all the unordered point clouds on the canonical surface, and to be reconstructed in an ordered fashion. Once trained, points from different shape instances that are mapped to the same locations on the primitive surface are determined to be a pair of correspondence. Our method does not require any form of annotation or self-supervised part segmentation network and can handle unaligned input point clouds within a certain rotation range. Experimental results on 3D semantic keypoint transfer and part segmentation transfer show that our model performs favorably against state-of-the-art correspondence learning methods.",https://api.openreview.net/pdf/bb872cb123f4f2718cdaa6ddac2098715be1a0fe.pdf,graph;zero_few-shot;transfer learning;segmentation;multimodal;3d,https://scholar.google.com/scholar?q=Learning+3D+Dense+Correspondence+via+Canonical+Point+Autoencoder
Learning Signal-Agnostic Manifolds of Neural Fields,2021,NIPS,"['Yilun Du', 'Katherine M. Collins', 'Joshua B. Tenenbaum', 'Vincent Sitzmann']",poster,"['Neural Fields', 'Coordinate-based', 'Implicit Function', 'Manifold', 'Multimodal', 'Generative Model', 'Cross-Modal']","Deep neural networks have been used widely to learn the latent structure of datasets, across modalities such as images, shapes, and audio signals. However, existing models are generally modality-dependent, requiring custom architectures and objectives to process different classes of signals. We leverage neural fields to capture the underlying structure in image, shape, audio and cross-modal audiovisual domains in a modality-independent manner. We cast our task as one of learning a manifold, where we aim to infer a low-dimensional, locally linear subspace in which our data resides. By enforcing coverage of the manifold, local linearity, and local isometry, our model -- dubbed GEM -- learns to capture the underlying structure of datasets across modalities. 
We can then travel along linear regions of our manifold to obtain perceptually consistent interpolations between samples, and can further use GEM to recover points on our manifold and glean not only diverse completions of input images, but cross-modal hallucinations of audio or image signals.  Finally, we show that by walking across the underlying manifold of GEM, we may generate new samples in our signal domains.",https://api.openreview.net/pdf/c8bdadc4eb46780f10261ecafbfc4253c033a5ac.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Learning+Signal-Agnostic+Manifolds+of+Neural+Fields
VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer,2021,NIPS,"['Zineng Tang', 'Jaemin Cho', 'Hao Tan', 'Mohit Bansal']",poster,"['video-language knowledge distillation', 'visual grounding', 'video-text transfer']","Since visual perception can give rich information beyond text descriptions for world understanding, there has been increasing interest in leveraging visual grounding for language learning. Recently, vokenization (Tan and Bansal, 2020) has attracted attention by using the predictions of a text-to-image retrieval model as labels for language model supervision. Despite its success, the method suffers from approximation error of using finite image labels and the lack of vocabulary diversity of a small image-text dataset. To overcome these limitations, we present VidLanKD, a video-language knowledge distillation method for improving language understanding. We train a multi-modal teacher model on a video-text dataset, and then transfer its knowledge to a student language model with a text dataset. To avoid approximation error, we propose to use different knowledge distillation objectives. In addition, the use of a large-scale video-text dataset helps learn diverse and richer vocabularies. In our experiments, VidLanKD achieves consistent improvements over text-only language models and vokenization models, on several downstream language understanding tasks including GLUE, SQuAD, and SWAG. We also demonstrate the improved world knowledge, physical reasoning, and temporal reasoning capabilities of our model by evaluating on the GLUE-diagnostics, PIQA, and TRACIE datasets. Lastly, we present comprehensive ablation studies as well as visualizations of the learned text-to-video grounding results of our teacher and student language models.",https://api.openreview.net/pdf/057de3182a5a5cd7c53511f3718513372965c46c.pdf,graph;transformer;transfer learning;distillation;multimodal,https://scholar.google.com/scholar?q=VidLanKD:+Improving+Language+Understanding+via+Video-Distilled+Knowledge+Transfer
EvoGrad: Efficient Gradient-Based Meta-Learning and Hyperparameter Optimization,2021,NIPS,"['Ondrej Bohdal', 'Yongxin Yang', 'Timothy Hospedales']",poster,"['Meta-learning', 'Hyperparameter optimization', 'Evolution']","Gradient-based meta-learning and hyperparameter optimization have seen significant progress recently, enabling practical end-to-end training of neural networks together with many hyperparameters. Nevertheless, existing approaches are relatively expensive as they need to compute second-order derivatives and store a longer computational graph. This cost prevents scaling them to larger network architectures. We present EvoGrad, a new approach to meta-learning that draws upon evolutionary techniques to more efficiently compute hypergradients. EvoGrad estimates hypergradient with respect to hyperparameters without calculating second-order gradients, or storing a longer computational graph, leading to significant improvements in  efficiency. We evaluate EvoGrad on three substantial recent meta-learning applications, namely cross-domain few-shot learning with feature-wise transformations, noisy label learning with Meta-Weight-Net and low-resource cross-lingual learning with meta representation transformation. The results show that EvoGrad significantly improves efficiency and enables scaling meta-learning to bigger architectures such as from ResNet10 to ResNet34.",https://api.openreview.net/pdf/c38280528ab24ede6a7d25bea7d6b4de4b60d02f.pdf,graph;optimization;zero_few-shot;transformer;representation;meta-learning;multimodal,https://scholar.google.com/scholar?q=EvoGrad:+Efficient+Gradient-Based+Meta-Learning+and+Hyperparameter+Optimization
Neural Hybrid Automata: Learning Dynamics With Multiple Modes and Stochastic Transitions,2021,NIPS,"['Michael Poli', 'Stefano Massaroli', 'Luca Scimeca', 'Sanghyuk Chun', 'Seong Joon Oh', 'Atsushi Yamashita', 'Hajime Asama', 'Jinkyoo Park', 'Animesh Garg']",poster,"['dynamical systems', 'time series', 'neural differential equations', 'control', 'stochastic hybrid systems']","Effective control and prediction of dynamical systems require appropriate handling of continuous-time and discrete, event-triggered processes. Stochastic hybrid systems (SHSs), common across engineering domains, provide a formalism for dynamical systems subject to discrete, possibly stochastic, state jumps and multi-modal continuous-time flows. Despite the versatility and importance of SHSs across applications, a general procedure for the explicit learning of both discrete events and multi-mode continuous dynamics remains an open problem. This work introduces Neural Hybrid Automata (NHAs), a recipe for learning SHS dynamics without a priori knowledge on the number, mode parameters, and inter-modal transition dynamics. NHAs provide a systematic inference method based on normalizing flows, neural differential equations, and self-supervision. We showcase NHAs on several tasks, including mode recovery and flow learning in systems with stochastic transitions, and end-to-end learning of hierarchical robot controllers.",https://api.openreview.net/pdf/cd6a9a087f1ec380d78087fba05bb91bc8e54f35.pdf,graph;zero_few-shot;inference;flow;multimodal;self-supervision,https://scholar.google.com/scholar?q=Neural+Hybrid+Automata:+Learning+Dynamics+With+Multiple+Modes+and+Stochastic+Transitions
Probing Inter-modality: Visual Parsing with Self-Attention for Vision-and-Language Pre-training,2021,NIPS,"['Hongwei Xue', 'Yupan Huang', 'Bei Liu', 'Houwen Peng', 'Jianlong Fu', 'Houqiang Li', 'Jiebo Luo']",poster,"['vision-language pre-training', 'Transformer', 'inter-modality']","Vision-Language Pre-training (VLP) aims to learn multi-modal representations from image-text pairs and serves for downstream vision-language tasks in a fine-tuning fashion. The dominant VLP models adopt a CNN-Transformer architecture, which embeds images with a CNN, and then aligns images and text with a Transformer.  Visual relationship between visual contents plays an important role in image understanding and is the basic for inter-modal alignment learning. However, CNNs have limitations in visual relation learning due to local receptive field's weakness in modeling long-range dependencies. Thus the two objectives of learning visual relation and inter-modal alignment are encapsulated in the same Transformer network. Such design might restrict the inter-modal alignment learning in the Transformer by ignoring the specialized characteristic of each objective. To tackle this, we propose a fully Transformer visual embedding for VLP to better learn visual relation and further promote inter-modal alignment. Specifically, we propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language modalities (i.e., inter-modality). We also design a novel masking optimization mechanism named Masked Feature Regression (MFR) in Transformer to further promote the inter-modality learning. To the best of our knowledge, this is the first study to explore the benefit of Transformer for visual feature learning in VLP.  We verify our method on a wide range of vision-language tasks, including Visual Question Answering (VQA), Visual Entailment and Visual Reasoning. Our approach not only outperforms the state-of-the-art VLP performance, but also shows benefits on the IMF metric.",https://api.openreview.net/pdf/c09e21b76be62dab97d93b6166f0eea86e779135.pdf,optimization;zero_few-shot;transformer;representation;metric;flow;multimodal,https://scholar.google.com/scholar?q=Probing+Inter-modality:+Visual+Parsing+with+Self-Attention+for+Vision-and-Language+Pre-training
Learning Space Partitions for Path Planning,2021,NIPS,"['Kevin Yang', 'Tianjun Zhang', 'Chris Cummins', 'Brandon Cui', 'Benoit Steiner', 'Linnan Wang', 'Joseph E. Gonzalez', 'Dan Klein', 'Yuandong Tian']",poster,"['Search', 'Path Planning', 'Optimization']","Path planning, the problem of efficiently discovering high-reward trajectories, often requires optimizing a high-dimensional and multimodal reward function. Popular approaches like CEM and CMA-ES greedily focus on promising regions of the search space and may get trapped in local maxima. DOO and VOOT balance exploration and exploitation, but use space partitioning strategies independent of the reward function to be optimized. Recently, LaMCTS empirically learns to partition the search space in a reward-sensitive manner for black-box optimization. In this paper, we develop a novel formal regret analysis for when and why such an adaptive region partitioning scheme works. We also propose a new path planning method LaP3 which improves the function value estimation within each sub-region, and uses a latent representation of the search space. Empirically, LaP3 outperforms existing path planning methods in 2D navigation tasks, especially in the presence of difficult-to-escape local optima, and shows benefits when plugged into the planning components of model-based RL such as PETS. These gains transfer to highly multimodal real-world tasks, where we outperform strong baselines in compiler phase ordering by up to 39% on average across 9 tasks, and in molecular design by up to 0.4 on properties on a 0-1 scale. Code is available at https://github.com/yangkevin2/neurips2021-lap3.",https://api.openreview.net/pdf/78c785bf7825e18a36586f1aceec7707940a355a.pdf,optimization;representation;adaptive;transfer learning;multimodal,https://scholar.google.com/scholar?q=Learning+Space+Partitions+for+Path+Planning
Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with Unlabeled Data,2021,NIPS,"['Ashraful Islam', 'Chun-Fu Chen', 'Rameswar Panda', 'Leonid Karlinsky', 'Rogerio Feris', 'Richard Radke']",poster,"['Few-shot-learning', 'Knowledge distillation']","Most existing works in few-shot learning rely on meta-learning the network on a large base dataset which is typically from the same domain as the target dataset. We tackle the problem of cross-domain few-shot learning where there is a large shift between the base and target domain. The problem of cross-domain few-shot recognition with unlabeled target data is largely unaddressed in the literature. STARTUP was the first method that tackles this problem using self-training. However, it uses a fixed teacher pretrained on a labeled base dataset to create soft labels for the unlabeled target samples. As the base dataset and unlabeled dataset are from different domains, projecting the target images in the class-domain of the base dataset with a fixed pretrained model might be sub-optimal. We propose a simple dynamic distillation-based approach to facilitate unlabeled images from the novel/base dataset. We impose consistency regularization by calculating predictions from the weakly-augmented versions of the unlabeled images from a teacher network and matching it with the strongly augmented versions of the same images from a student network. The parameters of the teacher network are updated as exponential moving average of the parameters of the student network. We show that the proposed network learns representation that can be easily adapted to the target domain even though it has not been trained with target-specific classes during the pretraining phase. Our model outperforms the current state-of-the art method by 4.4% for 1-shot and 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows competitive performance on traditional in-domain few-shot learning task.",https://api.openreview.net/pdf/c7556ace76ebb6cc28c81e0b3bab33f55a9861f1.pdf,zero_few-shot;representation;meta-learning;augmentation;distillation;multimodal,https://scholar.google.com/scholar?q=Dynamic+Distillation+Network+for+Cross-Domain+Few-Shot+Recognition+with+Unlabeled+Data
Causal Abstractions of Neural Networks,2021,NIPS,"['Atticus Geiger', 'Hanson Lu', 'Thomas F Icard', 'Christopher Potts']",poster,"['neural network analysis', 'interpretability', 'explainability', 'causality']","Structural analysis methods (e.g., probing and feature attribution) are increasingly important tools for neural network analysis. We propose a new structural analysis method grounded in a formal theory of causal abstraction that provides rich characterizations of model-internal representations and their roles in input/output behavior. In this method, neural representations are aligned with variables in interpretable causal models, and then interchange interventions are used to experimentally verify that the neural representations have the causal properties of their aligned variables. We apply this method in a case study to analyze neural models trained on Multiply Quantified Natural Language Inference (MQNLI) corpus, a highly complex NLI dataset that was constructed with a tree-structured natural logic causal model. We discover that a BERT-based model with state-of-the-art performance successfully realizes parts of the natural logic model’s causal structure, whereas a simpler baseline model fails to show any such structure, demonstrating that neural representations encode the compositional structure of MQNLI examples.",https://api.openreview.net/pdf/a5dbcfc9697d1b3593b8efff682f73efe741c93e.pdf,transformer;representation;inference;multimodal,https://scholar.google.com/scholar?q=Causal+Abstractions+of+Neural+Networks
SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Robustness,2021,NIPS,"['Jongheon Jeong', 'Sejun Park', 'Minkyu Kim', 'Heung-Chang Lee', 'Doguk Kim', 'Jinwoo Shin']",poster,"['randomized smoothing', 'mixup', 'adversarial robustness', 'certified defense', 'adversarial defense', 'confidence calibration']","Randomized smoothing is currently a state-of-the-art method to construct a certifiably robust classifier from neural networks against $\ell_2$-adversarial perturbations. Under the paradigm, the robustness of a classifier is aligned with the prediction confidence, i.e., the higher confidence from a smoothed classifier implies the better robustness. This motivates us to rethink the fundamental trade-off between accuracy and robustness in terms of calibrating confidences of a smoothed classifier. In this paper, we propose a simple training scheme, coined SmoothMix, to control the robustness of smoothed classifiers via self-mixup: it trains on convex combinations of samples along the direction of adversarial perturbation for each input. The proposed procedure effectively identifies over-confident, near off-class samples as a cause of limited robustness in case of smoothed classifiers, and offers an intuitive way to adaptively set a new decision boundary between these samples for better robustness. Our experimental results demonstrate that the proposed method can significantly improve the certified $\ell_2$-robustness of smoothed classifiers compared to existing state-of-the-art robust training methods.",https://api.openreview.net/pdf/a049fa77690bfea4501a8701524c7866135dfbb6.pdf,adaptive;multimodal,https://scholar.google.com/scholar?q=SmoothMix:+Training+Confidence-calibrated+Smoothed+Classifiers+for+Certified+Robustness
UFC-BERT: Unifying Multi-Modal Controls for Conditional Image Synthesis,2021,NIPS,"['Zhu Zhang', 'Jianxin Ma', 'Chang Zhou', 'Rui Men', 'Zhikang Li', 'Ming Ding', 'Jie Tang', 'Jingren Zhou', 'Hongxia Yang']",poster,"['conditional image synthesis', 'BERT', 'multi-modal control']","Conditional image synthesis aims to create an image according to some multi-modal guidance in the forms of textual descriptions, reference images, and image blocks to preserve, as well as their combinations. In this paper, instead of investigating these control signals separately, we propose a new two-stage architecture, UFC-BERT, to unify any number of multi-modal controls. In UFC-BERT, both the diverse control signals and the synthesized image are uniformly represented as a sequence of discrete tokens to be processed by Transformer. Different from existing two-stage autoregressive approaches such as DALL-E and VQGAN, UFC-BERT adopts non-autoregressive generation (NAR) at the second stage to enhance the holistic consistency of the synthesized image, to support preserving specified image blocks, and to improve the synthesis speed. Further, we design a progressive algorithm that iteratively improves the non-autoregressively generated image, with the help of two estimators developed for evaluating the compliance with the controls and evaluating the fidelity of the synthesized image, respectively. Extensive experiments on a newly collected large-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal CelebA-HQ verify that UFC-BERT can synthesize high-fidelity images that comply with flexible multi-modal controls.
",https://api.openreview.net/pdf/45c82f4613e5ec66c0ee0c90634842ec7c667558.pdf,transformer;generative model;multimodal,https://scholar.google.com/scholar?q=UFC-BERT:+Unifying+Multi-Modal+Controls+for+Conditional+Image+Synthesis
On the Sample Complexity of Privately Learning Axis-Aligned Rectangles,2021,NIPS,"['Menachem Sadigurschi', 'Uri Stemmer']",poster,"['differential privacy', 'statistical learning', 'axis-aligned rectangles']","We revisit the fundamental problem of learning Axis-Aligned-Rectangles over a finite grid $X^d\subseteq\mathbb{R}^d$ with differential privacy. Existing results show that the sample complexity of this problem is at most $\min\left\{ d{\cdot}\log|X| \;,\; d^{1.5}{\cdot}\left(\log^*|X| \right)^{1.5}\right\}$. That is, existing constructions either require sample complexity that grows linearly with $\log|X|$, or else it grows super linearly with the dimension $d$.  We present a novel algorithm that reduces the sample complexity to only $\tilde{O}\left\{d{\cdot}\left(\log^*|X|\right)^{1.5}\right\}$,  attaining a dimensionality optimal dependency without requiring the sample complexity to grow with $\log|X|$. The technique used in order to attain this improvement involves the deletion of ""exposed"" data-points on the go, in a fashion designed to avoid the cost of the adaptive composition theorems.
The core of this technique may be of individual interest, introducing a new method for constructing statistically-efficient private algorithms.
",https://api.openreview.net/pdf/16c21b0111080a3ef0da1705c0f174124f02bc09.pdf,adaptive;multimodal,https://scholar.google.com/scholar?q=On+the+Sample+Complexity+of+Privately+Learning+Axis-Aligned+Rectangles
Residual Relaxation for Multi-view Representation Learning,2021,NIPS,"['Yifei Wang', 'Zhengyang Geng', 'Feng Jiang', 'Chuming Li', 'Yisen Wang', 'Jiansheng Yang', 'Zhouchen Lin']",poster,"['Self-supervised Learning', 'Representation Learning', 'Multi-view Learning']","Multi-view methods learn representations by aligning multiple views of the same image and their performance largely depends on the choice of data augmentation. In this paper, we notice that some other useful augmentations, such as image rotation, are harmful for multi-view methods because they cause a semantic shift that is too large to be aligned well. This observation motivates us to relax the exact alignment objective to better cultivate stronger augmentations. Taking image rotation as a case study, we develop a generic approach, Pretext-aware Residual Relaxation (Prelax), that relaxes the exact alignment by allowing an adaptive residual vector between different views and encoding the semantic shift through pretext-aware learning. Extensive experiments on different backbones show that our method can not only improve multi-view methods with existing augmentations, but also benefit from stronger image augmentations like rotation.",https://api.openreview.net/pdf/15fcfa57c961ae8156e6dcfb6d0b5addda6616fa.pdf,zero_few-shot;representation;adaptive;augmentation;multimodal;multi-view,https://scholar.google.com/scholar?q=Residual+Relaxation+for+Multi-view+Representation+Learning
Leveraging Distribution Alignment via Stein Path for Cross-Domain Cold-Start Recommendation,2021,NIPS,"['Weiming Liu', 'Jiajie Su', 'Chaochao Chen', 'Xiaolin Zheng']",poster,"['Recommendation', 'Domain Adaptation', 'Stein Path']","Cross-Domain Recommendation (CDR) has been popularly studied to utilize different domain knowledge to solve the cold-start problem in recommender systems. In this paper, we focus on the Cross-Domain Cold-Start Recommendation (CDCSR) problem. That is, how to leverage the information from a source domain, where items are 'warm', to improve the recommendation performance of a target domain, where items are 'cold'. Unfortunately, previous approaches on cold-start and CDR cannot reduce the latent embedding discrepancy across domains efficiently and lead to model degradation. To address this issue, we propose DisAlign, a cross-domain recommendation framework for the CDCSR problem, which utilizes both rating and auxiliary representations from the source domain to improve the recommendation performance of the target domain. Specifically, we first propose Stein path alignment for aligning the latent embedding distributions across domains, and then further propose its improved version, i.e., proxy Stein path, which can reduce the operation consumption and improve efficiency. Our empirical study on Douban and Amazon datasets demonstrate that DisAlign significantly outperforms the state-of-the-art models under the CDCSR setting.
",https://api.openreview.net/pdf/1f049a42d1bbc4228d19e7bef9ec7ae548e7f7ab.pdf,graph;representation;multimodal,https://scholar.google.com/scholar?q=Leveraging+Distribution+Alignment+via+Stein+Path+for+Cross-Domain+Cold-Start+Recommendation
History Aware Multimodal Transformer for Vision-and-Language Navigation,2021,NIPS,"['Shizhe Chen', 'Pierre-Louis Guhur', 'Cordelia Schmid', 'Ivan Laptev']",poster,"['vision-and-language navigation', 'transformer']","Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories. ",https://api.openreview.net/pdf/7a98b07d97723ec77f2aec51cbb7b20521aeab98.pdf,reinforcement learning;zero_few-shot;transformer;multimodal;llm,https://scholar.google.com/scholar?q=History+Aware+Multimodal+Transformer+for+Vision-and-Language+Navigation
Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction,2021,NIPS,"['Dominik Stöger', 'Mahdi Soltanolkotabi']",poster,"['low-rank matrix recovery', 'overparameterized learning', 'non-convex optimization']","Recently there has been significant theoretical progress on understanding the convergence and generalization of gradient-based methods on nonconvex losses with overparameterized models. Nevertheless, many aspects of optimization and generalization and in particular the critical role of small random initialization are not fully understood. In this paper, we take a step towards demystifying this role by proving that small random initialization followed by a few iterations of gradient descent behaves akin to popular spectral methods. We also show that this implicit spectral bias from small random initialization, which is provably more prominent for overparameterized models, also puts the gradient descent iterations on a particular trajectory towards solutions that are not only globally optimal but also generalize well. Concretely, we focus on the problem of reconstructing a low-rank matrix from a few measurements via a natural nonconvex formulation. In this setting, we show that the trajectory of the gradient descent iterations from small random initialization can be approximately decomposed into three phases: (I) a spectral or alignment phase where we show that that the iterates have an implicit spectral bias akin to spectral initialization allowing us to show that at the end of this phase the column space of the iterates and the underlying low-rank matrix are sufficiently aligned, (II) a saddle avoidance/refinement phase where we show that the trajectory of the gradient iterates moves away from certain degenerate saddle points, and (III) a local refinement phase where we show that after avoiding the saddles the iterates converge quickly to the underlying low-rank matrix. Underlying our analysis are insights for the analysis of overparameterized nonconvex optimization schemes that may have implications for computational problems beyond low-rank reconstruction.",https://api.openreview.net/pdf/914ba2ca6946046323e23cd508c6cb729aa5d7c4.pdf,optimization;zero_few-shot;multimodal;low-rank,https://scholar.google.com/scholar?q=Small+random+initialization+is+akin+to+spectral+learning:+Optimization+and+generalization+guarantees+for+overparameterized+low-rank+matrix+reconstruction
One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval,2021,NIPS,"['Akari Asai', 'Xinyan Yu', 'Jungo Kasai', 'Hannaneh Hajishirzi']",poster,"['Question answering', 'Multilingual NLP', 'Open-domain QA', 'Cross-lingual Information Retrieval']","We present Cross-lingual Open-Retrieval Answer Generation (CORA), the first unified many-to-many question answering (QA) model that can answer questions across many languages, even for ones without language-specific annotated data or knowledge sources.
We introduce a new dense passage retrieval algorithm that is trained to retrieve documents across languages for a question.
Combined with a  multilingual autoregressive generation model, CORA answers directly in the target language without any translation or in-language retrieval modules as used in prior work. We propose an iterative training method that automatically extends annotated data available only in high-resource languages to low-resource ones. Our results show that CORA substantially outperforms the previous state of the art on multilingual open QA benchmarks across 26 languages, 9 of which are unseen during training. Our analyses show the significance of cross-lingual retrieval and generation in many languages, particularly under low-resource settings. ",https://api.openreview.net/pdf/65b93bac792cf4b2f68db029b9eef2a897036f68.pdf,zero_few-shot;generative model;multimodal,https://scholar.google.com/scholar?q=One+Question+Answering+Model+for+Many+Languages+with+Cross-lingual+Dense+Passage+Retrieval
Multimodal Virtual Point 3D Detection,2021,NIPS,"['Tianwei Yin', 'Xingyi Zhou', 'Philipp Kraehenbuehl']",poster,"['3D detection', 'multi-modal', 'autonomous driving', 'fusion', 'Lidar']","Lidar-based sensing drives current autonomous vehicles. Despite rapid progress, current Lidar sensors still lag two decades behind traditional color cameras in terms of resolution and cost. For autonomous driving, this means that large objects close to the sensors are easily visible, but far-away or small objects comprise only one measurement or two. This is an issue, especially when these objects turn out to be driving hazards. On the other hand, these same objects are clearly visible in onboard RGB sensors. In this work, we present an approach to seamlessly fuse RGB sensors into Lidar-based 3D recognition. Our approach takes a set of 2D detections to generate dense 3D virtual points to augment an otherwise sparse 3D point cloud. These virtual points naturally integrate into any standard Lidar-based 3D detectors along with regular Lidar measurements. The resulting multi-modal detector is simple and effective. Experimental results on the large-scale nuScenes dataset show that our framework improves a strong CenterPoint baseline by a significant $6.6$ mAP, and outperforms competing fusion approaches. Code and more visualizations are available at https://tianweiy.github.io/mvp/
",https://api.openreview.net/pdf/f31b1d12da046cfa77cb40ca72a910fc3caad711.pdf,sparse;multimodal;3d,https://scholar.google.com/scholar?q=Multimodal+Virtual+Point+3D+Detection
Pretraining Representations for Data-Efficient Reinforcement Learning,2021,NIPS,"['Max Schwarzer', 'Nitarshan Rajkumar', 'Michael Noukhovitch', 'Ankesh Anand', 'Laurent Charlin', 'R Devon Hjelm', 'Philip Bachman', 'Aaron Courville']",poster,"['Pretraining', 'Sample Efficiency', 'Reinforcement Learning', 'Self-Supervised Learning']","Data efficiency is a key challenge for deep reinforcement learning. We address this problem by using unlabeled data to pretrain an encoder which is then finetuned on a small amount of task-specific data. To encourage learning representations which capture diverse aspects of the underlying MDP, we employ a combination of latent dynamics modelling and unsupervised goal-conditioned RL. When limited to 100k steps of interaction on Atari games (equivalent to two hours of human experience), our approach significantly surpasses prior work combining offline representation pretraining with task-specific finetuning, and compares favourably with other pretraining methods that require orders of magnitude more data. Our approach shows particular promise when combined with larger models as well as more diverse, task-aligned observational data -- approaching human-level performance and data-efficiency on Atari in our best setting.",https://api.openreview.net/pdf/d67995b3a69f7a250ca80511e3b4eaf9bf8aa7a4.pdf,reinforcement learning;offline reinforcement learning;representation;multimodal,https://scholar.google.com/scholar?q=Pretraining+Representations+for+Data-Efficient+Reinforcement+Learning
UniDoc: Unified Pretraining Framework for Document Understanding,2021,NIPS,"['Jiuxiang Gu', 'Jason Kuen', 'Vlad I Morariu', 'Handong Zhao', 'Rajiv Jain', 'Nikolaos Barmpalios', 'Ani Nenkova', 'Tong Sun']",poster,"['BERT', 'Self-Supervised Learning', 'Document Understanding']","Document intelligence automates the extraction of information from documents and supports many business applications. Recent self-supervised learning methods on large-scale unlabeled document datasets have opened up promising directions towards reducing annotation efforts by training models with self-supervised objectives. However, most of the existing document pretraining methods are still language-dominated. We present UDoc, a new unified pretraining framework for document understanding. UDoc is designed to support most document understanding tasks, extending the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image. An important feature of UDoc is that it learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities. Extensive empirical analysis demonstrates that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks.",https://api.openreview.net/pdf/dc202e1b46577d2b842b8a08240e5633a8801497.pdf,graph;zero_few-shot;transformer;representation;multimodal,https://scholar.google.com/scholar?q=UniDoc:+Unified+Pretraining+Framework+for+Document+Understanding
CogView: Mastering Text-to-Image Generation via Transformers,2021,NIPS,"['Ming Ding', 'Zhuoyi Yang', 'Wenyi Hong', 'Wendi Zheng', 'Chang Zhou', 'Da Yin', 'Junyang Lin', 'Xu Zou', 'Zhou Shao', 'Hongxia Yang', 'Jie Tang']",poster,"['Transformer', 'pretraining', 'generative model', 'cross-modality']","Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.
",https://api.openreview.net/pdf/28c1260b263facf56041d213b06c9492447512b9.pdf,transformer;vae;generative model;multimodal,https://scholar.google.com/scholar?q=CogView:+Mastering+Text-to-Image+Generation+via+Transformers
PartialFed: Cross-Domain Personalized Federated Learning via Partial Initialization,2021,NIPS,"['Benyuan Sun', 'Hongxing Huo', 'Yi Yang', 'Bo Bai']",poster,"['Federated Learning', 'Non i.i.d.', 'Personalization']","The burst of applications empowered by massive data have aroused unprecedented privacy concerns in AI society. Currently, data confidentiality protection has been one core issue during deep model training. Federated Learning (FL), which enables privacy-preserving training across multiple silos, gained rising popularity for its parameter-only communication. However, previous works have shown that FL revealed a significant performance drop if the data distributions are heterogeneous among different clients, especially when the clients have cross-domain characteristic, such as traffic, aerial and in-door. To address this challenging problem, we propose a novel idea, PartialFed, which loads a subset of the global model’s parameters rather than loading the entire model used in most previous works. We first validate our algorithm with manually decided loading strategies inspired by various expert priors, named PartialFed-Fix. Then we develop PartialFed-Adaptive, which automatically selects personalized loading strategy for each client. The superiority of our algorithm is proved by demonstrating the new state-of-the-art results on cross-domain federated classification and detection. In particular, solely by initializing a small fraction of layers locally, we improve the performance of FedAvg on Office-Home and UODB by 4.88% and 2.65%, respectively. Further studies show that the adaptive strategy performs significantly better on domains with large deviation, e.g. improves AP50 by 4.03% and 4.89% on aerial and medical image detection compared to FedAvg. ",https://api.openreview.net/pdf/e57606b7a7101a7db09fc3a2dd3a3485682aa472.pdf,graph;adaptive;federated learning;multimodal,https://scholar.google.com/scholar?q=PartialFed:+Cross-Domain+Personalized+Federated+Learning+via+Partial+Initialization
Evidential Softmax for Sparse Multimodal Distributions in Deep Generative Models,2021,NIPS,"['Phil Chen', 'Masha Itkina', 'Ransalu Senanayake', 'Mykel Kochenderfer']",poster,"['Deep Learning or Neural Networks', 'Sparsity and Feature Selection', 'Variational Inference', '(Application) Natural Language and Text Processing']","Many applications of generative models rely on the marginalization of their high-dimensional output probability distributions. Normalization functions that yield sparse probability distributions can make exact marginalization more computationally tractable. However, sparse normalization functions usually require alternative loss functions for training since the log-likelihood is undefined for sparse probability distributions. Furthermore, many sparse normalization functions often collapse the multimodality of distributions. In this work, we present ev-softmax, a sparse normalization function that preserves the multimodality of probability distributions. We derive its properties, including its gradient in closed-form, and introduce a continuous family of approximations to ev-softmax that have full support and can be trained with probabilistic loss functions such as negative log-likelihood and Kullback-Leibler divergence. We evaluate our method on a variety of generative models, including variational autoencoders and auto-regressive architectures. Our method outperforms existing dense and sparse normalization techniques in distributional accuracy. We demonstrate that ev-softmax successfully reduces the dimensionality of probability distributions while maintaining multimodality.",https://api.openreview.net/pdf/411eeb2536a2ccf2c8fd2d51d4bbca86ce6ab0cc.pdf,graph;vae;generative model;sparse;multimodal,https://scholar.google.com/scholar?q=Evidential+Softmax+for+Sparse+Multimodal+Distributions+in+Deep+Generative+Models
Generalizable Multi-linear Attention Network,2021,NIPS,"['Tao Jin', 'Zhou Zhao']",poster,"['Multimodal', 'Attention Mechanism', 'Tensor Decomposition']","The majority of existing multimodal sequential learning methods focus on how to obtain effective representations and ignore the importance of multimodal fusion. Bilinear attention network (BAN) is a commonly used fusion method, which leverages tensor operations to associate the features of different modalities. However, BAN has a poor compatibility for more modalities, since the computational complexity of the attention map increases exponentially with the number of modalities. Based on this concern, we propose a new method called generalizable multi-linear attention network (MAN), which can associate as many modalities as possible in linear complexity with hierarchical approximation decomposition (HAD). Besides, considering the fact that softmax attention kernels cannot be decomposed as linear operation directly, we adopt the addition random features (ARF) mechanism to approximate the non-linear softmax functions with enough theoretical analysis. We conduct extensive experiments on four datasets of three tasks (multimodal sentiment analysis, multimodal speaker traits recognition, and video retrieval), the experimental results show that MAN could achieve competitive results compared with the state-of-the-art methods, showcasing the effectiveness of the approximation decomposition and addition random features mechanism.",https://api.openreview.net/pdf/9f55ac3abba2ff83287000bac83fbd395e5c2819.pdf,transformer;representation;multimodal,https://scholar.google.com/scholar?q=Generalizable+Multi-linear+Attention+Network
Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model,2021,NIPS,"['Jiangning Zhang', 'Chao Xu', 'Jian Li', 'Wenzhou Chen', 'Yabiao Wang', 'Ying Tai', 'Shuo Chen', 'Chengjie Wang', 'Feiyue Huang', 'Yong Liu']",poster,"['Evolutionary Algorithm', 'Vision Transformer', 'Spatial-Filling Curve', 'ImageNet Classification']","Inspired by biological evolution, we explain the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derive that both of them have consistent mathematical representation. Analogous to the dynamic local population in EA, we improve the existing transformer structure and propose a more efficient EAT model, and design task-related heads to deal with different tasks more flexibly. Moreover, we introduce the spatial-filling curve into the current vision transformer to sequence image data into a uniform sequential format. Thus we can design a unified EAT framework to address multi-modal tasks, separating the network architecture from the data format adaptation. Our approach achieves state-of-the-art results on the ImageNet classification task compared with recent vision transformer works while having smaller parameters and greater throughput. We further conduct multi-modal tasks to demonstrate the superiority of the unified EAT, \eg, Text-Based Image Retrieval, and our approach improves the rank-1 by +3.7 points over the baseline on the CSS dataset.",https://api.openreview.net/pdf/9bb810f672fe5247b29dd023c998b44425d04389.pdf,transformer;representation;multimodal,https://scholar.google.com/scholar?q=Analogous+to+Evolutionary+Algorithm:+Designing+a+Unified+Sequence+Model
CLIP-It! Language-Guided Video Summarization,2021,NIPS,"['Medhini Narasimhan', 'Anna Rohrbach', 'Trevor Darrell']",poster,"['video summarization', 'language-guided video summarization', 'query-focused video summarization', 'multimodal video summarization']","A generic video summary is an abridged version of a video that conveys the whole story and features the most important scenes. Yet the importance of scenes in a video is often subjective, and users should have the option of customizing the summary by using natural language to specify what is important to them. Further, existing models for fully automatic generic summarization have not exploited available language models, which can serve as an effective prior for saliency. This work introduces CLIP-It, a single framework for addressing both generic and query-focused video summarization, typically approached separately in the literature. We propose a language-guided multimodal transformer that learns to score frames in a video based on their importance relative to one another and their correlation with a user-defined query (for query-focused summarization) or an automatically generated dense video caption (for generic video summarization). Our model can be extended to the unsupervised setting by training without ground-truth supervision. We outperform baselines and prior work by a significant margin on both standard video summarization datasets (TVSum and SumMe) and a query-focused video summarization dataset (QFVS). Particularly, we achieve large improvements in the transfer setting, attesting to our method's strong generalization capabilities.",https://api.openreview.net/pdf/9132614951b489d3aee11cc20de3d48d9057534d.pdf,graph;transformer;transfer learning;multimodal,https://scholar.google.com/scholar?q=CLIP-It!+Language-Guided+Video+Summarization
Privately Learning Mixtures of Axis-Aligned Gaussians,2021,NIPS,"['Ishaq Aden-Ali', 'Hassan Ashtiani', 'Christopher Liaw']",poster,"['differential privacy', 'distribution learning', 'mixtures of Gaussians']","We consider the problem of learning multivariate Gaussians under the constraint of approximate differential privacy. We prove that $\widetilde{O}(k^2 d \log^{3/2}(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to learn a mixture of $k$ axis-aligned Gaussians in $\mathbb{R}^d$ to within total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-differential privacy. This is the first result for privately learning mixtures of unbounded axis-aligned (or even unbounded univariate) Gaussians. If the covariance matrices of each of the Gaussians is the identity matrix, we show that $\widetilde{O}(kd/\alpha^2 + kd \log(1/\delta) / \alpha \varepsilon)$ samples are sufficient.
To prove our results, we design a new technique for privately learning mixture distributions.  A class of distributions $\mathcal{F}$ is said to be list-decodable if there is an algorithm that, given ""heavily corrupted"" samples from $f \in \mathcal{F}$, outputs a list of distributions one of which approximates $f$. We show that if $\mathcal{F}$ is privately list-decodable then we can learn mixtures of distributions in $\mathcal{F}$. Finally, we show axis-aligned Gaussian distributions are privately list-decodable, thereby proving mixtures of such distributions are privately learnable.

",https://api.openreview.net/pdf/0e1165cd6094ddebc5df9538699296db89f94c1b.pdf,optimization;multimodal,https://scholar.google.com/scholar?q=Privately+Learning+Mixtures+of+Axis-Aligned+Gaussians
TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive?,2021,NIPS,"['Yuejiang Liu', 'Parth Kothari', 'Bastien Germain van Delft', 'Baptiste Bellot-Gurlet', 'Taylor Mordan', 'Alexandre Alahi']",poster,"['Test-time Adaptation', 'Source-free Domain Adaptation', 'Self-supervised Learning', 'Feature Alignment', 'Robustness', 'Generalization']","Test-time training (TTT) through self-supervised learning (SSL) is an emerging paradigm to tackle distributional shifts. Despite encouraging results, it remains unclear when this approach thrives or fails. In this work, we first provide an in-depth look at its limitations and show that TTT can possibly deteriorate, instead of improving, the test-time performance in the presence of severe distribution shifts. To address this issue, we introduce a test-time feature alignment strategy utilizing offline feature summarization and online moment matching, which regularizes adaptation without revisiting training data. We further scale this strategy in the online setting through batch-queue decoupling to enable robust moment estimates even with limited batch size. Given aligned feature distributions, we then shed light on the strong potential of TTT by theoretically analyzing its performance post adaptation. This analysis motivates our use of more informative self-supervision in the form of contrastive learning for visual recognition problems. We empirically demonstrate that our modified version of test-time training, termed TTT++, outperforms state-of-the-art methods by significant margins on several benchmarks. Our result indicates that storing and exploiting extra information, in addition to model parameters, can be a promising direction towards robust test-time adaptation.",https://api.openreview.net/pdf/5ce265fb0555853b99a99cb757ffc765397286ab.pdf,offline reinforcement learning;graph;zero_few-shot;online learning;contrastive learning;multimodal;self-supervision,https://scholar.google.com/scholar?q=TTT++:+When+Does+Self-Supervised+Test-Time+Training+Fail+or+Thrive?
Label Disentanglement in Partition-based Extreme Multilabel Classification,2021,NIPS,"['Xuanqing Liu', 'Wei-Cheng Chang', 'Hsiang-Fu Yu', 'Cho-Jui Hsieh', 'Inderjit S Dhillon']",poster,['Extreme multilabel classification'],"Partition-based methods are increasingly-used in extreme multi-label classification (XMC) problems due to their scalability to large output spaces (e.g., millions or more). However, existing methods partition the large label space into mutually exclusive clusters, which is sub-optimal when labels have multi-modality and rich semantics. For instance, the label “Apple” can be the fruit or the brand name, which leads to the following research question: can we disentangle these multi-modal labels with non-exclusive clustering tailored for downstream XMC tasks? In this paper, we show that the label assignment problem in partition-based XMC can be formulated as an optimization problem, with the objective of maximizing precision rates. This leads to an efficient algorithm to form  flexible and overlapped label clusters, and a method that can alternatively optimizes the cluster assignments and the model parameters for partition-based XMC. Experimental results on synthetic and real datasets show that our method can successfully disentangle multi-modal labels, leading to state-of-the-art (SOTA) results on four XMC benchmarks.",https://api.openreview.net/pdf/31b8d7511617f934703f5f6df7457591214b03c2.pdf,optimization;zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Label+Disentanglement+in+Partition-based+Extreme+Multilabel+Classification
Voxel-based 3D Detection and Reconstruction of Multiple Objects from a Single Image,2021,NIPS,"['Feng Liu', 'Xiaoming Liu']",poster,"['3D voxel features', 'Monocular 3D detection and reconstruction', 'Multiple objects', 'Local PCA-SDF']","Inferring 3D locations and shapes of multiple objects from a single 2D image is a long-standing objective of computer vision. Most of the existing works either predict one of these 3D properties or focus on solving both for a single object. One fundamental challenge lies in how to learn an effective representation of the image that is well-suited for 3D detection and reconstruction. In this work, we propose to learn a regular grid of 3D voxel features from the input image which is aligned with 3D scene space via a 3D feature lifting operator. Based on the 3D voxel features, our novel CenterNet-3D detection head formulates the 3D detection as keypoint detection in the 3D space. Moreover, we devise an efficient coarse-to-fine reconstruction module, including coarse-level voxelization and a novel local PCA-SDF shape representation, which enables fine detail reconstruction and two orders of magnitude faster inference than prior methods. With complementary supervision from both 3D detection and reconstruction, one enables the 3D voxel features to be geometry and context preserving, benefiting both tasks. The effectiveness of our approach is demonstrated through 3D detection and reconstruction on single-object and multiple-object scenarios. ",https://api.openreview.net/pdf/8300dfb2f5fafbae7f1f9d1aa5b3d79594cb895b.pdf,representation;inference;multimodal;3d,https://scholar.google.com/scholar?q=Voxel-based+3D+Detection+and+Reconstruction+of+Multiple+Objects+from+a+Single+Image
Drop-DTW: Aligning Common Signal Between Sequences While Dropping Outliers,2021,NIPS,"['Nikita Dvornik', 'Isma Hadji', 'Konstantinos G. Derpanis', 'Animesh Garg', 'Allan Douglas Jepson']",poster,"['Sequence Matching', 'video representation learning', 'instructional videos', 'step localization']","In this work, we consider the problem of sequence-to-sequence alignment for signals containing outliers.  Assuming the absence of outliers, the standard Dynamic Time Warping (DTW) algorithm efficiently computes the optimal alignment between two (generally) variable-length sequences.  While DTW is robust to temporal shifts and dilations of the signal, it fails to align sequences in a meaningful way in the presence of outliers that can be arbitrarily interspersed in the sequences.  To address this problem, we introduce Drop-DTW, a novel algorithm that aligns the common signal between the sequences while automatically dropping the outlier elements from the matching.  The entire procedure is implemented as a single dynamic program that is efficient and fully differentiable.  In our experiments, we show that Drop-DTW is a robust similarity measure for sequence retrieval and demonstrate its effectiveness as a training loss on diverse applications. With Drop-DTW, we address temporal step localization on instructional videos, representation learning from noisy videos, and cross-modal representation learning for audio-visual retrieval and localization. In all applications, we take a weakly- or unsupervised approach and demonstrate state-of-the-art results under these settings.",https://api.openreview.net/pdf/bc5ec12bf3e5a945c6fa2308d86f9d4dd3c19cbd.pdf,zero_few-shot;representation;multimodal,https://scholar.google.com/scholar?q=Drop-DTW:+Aligning+Common+Signal+Between+Sequences+While+Dropping+Outliers
HSVA: Hierarchical Semantic-Visual Adaptation for Zero-Shot Learning,2021,NIPS,"['Shiming Chen', 'Guosen Xie', 'Yang Liu', 'QINMU PENG', 'Baigui Sun', 'Hao Li', 'Xinge You', 'Ling Shao']",poster,"['Zero-shot learning', 'Domain adaptation', 'Text-image matching', 'Heterogeneous feature representation']","Zero-shot learning (ZSL) tackles the unseen class recognition problem,  transferring semantic knowledge from seen classes to unseen ones. Typically, to guarantee desirable knowledge transfer, a common (latent) space is adopted for associating the visual and semantic domains in ZSL.  However, existing common space learning methods align the semantic and visual domains by merely mitigating distribution disagreement through one-step adaptation. This strategy is usually ineffective due to the heterogeneous nature of the feature representations in the two domains, which intrinsically contain both distribution and structure variations. To address this and advance ZSL, we propose a novel hierarchical semantic-visual adaptation (HSVA) framework. Specifically, HSVA aligns the semantic and visual domains by adopting a hierarchical two-step adaptation, i.e., structure adaptation and distribution adaptation. In the structure adaptation step, we take two task-specific encoders to encode the source data (visual domain) and the target data (semantic domain) into a structure-aligned common space. To this end, a  supervised adversarial discrepancy (SAD)  module is proposed to adversarially minimize the discrepancy between the predictions of two task-specific classifiers, thus making the visual and semantic feature manifolds more closely aligned. In the distribution adaptation step, we directly minimize the Wasserstein distance between the latent multivariate Gaussian distributions to align the visual and semantic distributions using a common encoder. Finally, the structure and distribution adaptation are derived in a unified framework under two partially-aligned variational autoencoders. Extensive experiments on four benchmark datasets demonstrate that HSVA achieves superior performance on both conventional and generalized ZSL. The code is available at \url{https://github.com/shiming-chen/HSVA}.",https://api.openreview.net/pdf/3a2fc0e638bd43da8280e116bf2865f56e52d427.pdf,zero_few-shot;representation;vae;transfer learning;multimodal,https://scholar.google.com/scholar?q=HSVA:+Hierarchical+Semantic-Visual+Adaptation+for+Zero-Shot+Learning
Canonical Capsules: Self-Supervised Capsules in Canonical Pose,2021,NIPS,"['Weiwei Sun', 'Andrea Tagliasacchi', 'Boyang Deng', 'Sara Sabour', 'Soroosh Yazdani', 'Geoffrey Hinton', 'Kwang Moo Yi']",poster,"['object-centric representation learning', 'capsules', 'primary capsules', 'unsupervised', 'self-supervised', '3D point clouds']","We propose a self-supervised capsule architecture for 3D point clouds. We compute capsule decompositions of objects through permutation-equivariant attention, and self-supervise the process by training with pairs of randomly rotated objects. Our key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning. To train our neural network we require neither classification labels nor manually-aligned training datasets. Yet, by learning an object-centric representation in a self-supervised manner, our method outperforms the state-of-the-art on 3D point cloud reconstruction, canonicalization, and unsupervised classification.",https://api.openreview.net/pdf/0c2d278d9fce5ccd969e8baa610661b04a165def.pdf,zero_few-shot;transformer;representation;multimodal;3d,https://scholar.google.com/scholar?q=Canonical+Capsules:+Self-Supervised+Capsules+in+Canonical+Pose
Non-local Latent Relation Distillation for Self-Adaptive 3D Human Pose Estimation,2021,NIPS,"['Jogendra Nath Kundu', 'Siddharth Seth', 'Anirudh Gururaj Jamkhandi', 'Pradyumna YM', 'Varun Jampani', 'Anirban Chakraborty', 'Venkatesh Babu Radhakrishnan']",poster,"['3D human pose estimation', 'knowledge distillation', 'domain adaptation']","Available 3D human pose estimation approaches leverage different forms of strong (2D/3D pose) or weak (multi-view or depth) paired supervision. Barring synthetic or in-studio domains, acquiring such supervision for each new target environment is highly inconvenient. To this end, we cast 3D pose learning as a self-supervised adaptation problem that aims to transfer the task knowledge from a labeled source domain to a completely unpaired target. We propose to infer image-to-pose via two explicit mappings viz. image-to-latent and latent-to-pose where the latter is a pre-learned decoder obtained from a prior-enforcing generative adversarial auto-encoder. Next, we introduce relation distillation as a means to align the unpaired cross-modal samples i.e., the unpaired target videos and unpaired 3D pose sequences. To this end, we propose a new set of non-local relations in order to characterize long-range latent pose interactions, unlike general contrastive relations where positive couplings are limited to a local neighborhood structure. Further, we provide an objective way to quantify non-localness in order to select the most effective relation set. We evaluate different self-adaptation settings and demonstrate state-of-the-art 3D human pose estimation performance on standard benchmarks.",https://api.openreview.net/pdf/a81a3cc57fcde8acd817ec26533f893e389cbd19.pdf,zero_few-shot;generative model;adaptive;contrastive learning;transfer learning;distillation;multimodal;multi-view;3d,https://scholar.google.com/scholar?q=Non-local+Latent+Relation+Distillation+for+Self-Adaptive+3D+Human+Pose+Estimation
Perceptual Score: What Data Modalities Does Your Model Perceive?,2021,NIPS,"['Itai Gat', 'Idan Schwartz', 'Alex Schwing']",poster,"['VQA', 'Visual Question Answering', 'VQA-CP', 'Perceptiveness', 'Multimodal', 'Evaluation', 'Visual dialog', 'SocialIQ']","Machine learning advances in the last decade have relied significantly on large-scale datasets that continue to grow in size. Increasingly, those datasets also contain different data modalities. However, large multi-modal datasets are hard to annotate, and annotations may contain biases that we are often unaware of. Deep-net-based classifiers, in turn, are prone to exploit those biases and to find shortcuts. To study and quantify this concern, we introduce the perceptual score, a metric that assesses the degree to which a model relies on the different subsets of the input features, i.e., modalities. Using the perceptual score, we find a surprisingly consistent trend across four popular datasets: recent, more accurate state-of-the-art multi-modal models for visual question-answering or visual dialog tend to perceive the visual data less than their predecessors. This is concerning as answers are hence increasingly inferred from textual cues only. Using the perceptual score also helps to analyze model biases by decomposing the score into data subset contributions. We hope to spur a discussion on the perceptiveness of multi-modal models and also hope to encourage the community working on multi-modal classifiers to start quantifying perceptiveness via the proposed perceptual score.  ",https://api.openreview.net/pdf/f0284a8eee80ed0c37cc61a6e1d50f6a3023104d.pdf,metric;multimodal,https://scholar.google.com/scholar?q=Perceptual+Score:+What+Data+Modalities+Does+Your+Model+Perceive?
Multi-modal Dependency Tree for Video Captioning,2021,NIPS,"['Wentian Zhao', 'Xinxiao Wu', 'Jiebo Luo']",poster,"['video captioning', 'tree-structured decoding']","Generating fluent and relevant language to describe visual content is critical for the video captioning task. Many existing methods generate captions using sequence models that predict words in a left-to-right order. In this paper, we investigate a graph-structured model for caption generation by explicitly modeling the hierarchical structure in the sentences to further improve the fluency and relevance of sentences. To this end, we propose a novel video captioning method that generates a sentence by first constructing a multi-modal dependency tree and then traversing the constructed tree, where the syntactic structure and semantic relationship in the sentence are represented by the tree topology. To take full advantage of the information from both vision and language, both the visual and textual representation features are encoded into each tree node. Different from existing dependency parsing methods that generate uni-modal dependency trees for language understanding, our method construct s multi-modal dependency trees for language generation of images and videos. We also propose a tree-structured reinforcement learning algorithm to effectively optimize the captioning model where a novel reward is designed by evaluating the semantic consistency between the generated sub-tree and the ground-truth tree. Extensive experiments on several video captioning datasets demonstrate the effectiveness of the proposed method. ",https://api.openreview.net/pdf/d34a5d8de1fed83f81a8b2feea3e3356ee8603a0.pdf,reinforcement learning;graph;representation;generative model;multimodal,https://scholar.google.com/scholar?q=Multi-modal+Dependency+Tree+for+Video+Captioning
ToAlign: Task-Oriented Alignment for Unsupervised Domain Adaptation,2021,NIPS,"['Guoqiang Wei', 'Cuiling Lan', 'Wenjun Zeng', 'Zhizheng Zhang', 'Zhibo Chen']",poster,"['Domain adaptation', 'Image classification', 'Adversarial learning']","Unsupervised domain adaptive classifcation intends to improve the classifcation performance on unlabeled target domain. To alleviate the adverse effect of domain shift, many approaches align the source and target domains in the feature space. However, a feature is usually taken as a whole for alignment without explicitly making domain alignment proactively serve the classifcation task, leading to sub-optimal solution. In this paper, we propose an effective Task-oriented Alignment (ToAlign) for unsupervised domain adaptation (UDA). We study what features should be aligned across domains and propose to make the domain alignment proactively serve classifcation by performing feature decomposition and alignment under the guidance of the prior knowledge induced from the classifcation task itself. Particularly, we explicitly decompose a feature in the source domain into a task-related/discriminative feature that should be aligned, and a task-irrelevant feature that should be avoided/ignored, based on the classifcation meta-knowledge. Extensive experimental results on various benchmarks (e.g., Offce-Home, Visda-2017, and DomainNet) under different domain adaptation settings demonstrate the effectiveness of ToAlign which helps achieve the state-of-the-art performance. The code is publicly available at https://github.com/microsoft/UDA.",https://api.openreview.net/pdf/8b9f2ca2bb6c7bcc5e29a5a90795c40109016398.pdf,adaptive;meta-learning;active learning;multimodal,https://scholar.google.com/scholar?q=ToAlign:+Task-Oriented+Alignment+for+Unsupervised+Domain+Adaptation
End-to-end Multi-modal Video Temporal Grounding,2021,NIPS,"['Yi-Wen Chen', 'Yi-Hsuan Tsai', 'Ming-Hsuan Yang']",poster,"['Computer Vision', 'Vision and Language', 'Video Temporal Grounding', 'Multi-modal Learning', 'Transformer', 'Contrastive Learning']","We address the problem of text-guided video temporal grounding, which aims to identify the time interval of a certain event based on a natural language description. Different from most existing methods that only consider RGB images as visual features, we propose a multi-modal framework to extract complementary information from videos. Specifically, we adopt RGB images for appearance, optical flow for motion, and depth maps for image structure. While RGB images provide abundant visual cues of certain events, the performance may be affected by background clutters. Therefore, we use optical flow to focus on large motion and depth maps to infer the scene configuration when the action is related to objects recognizable with their shapes. To integrate the three modalities more effectively and enable inter-modal learning, we design a dynamic fusion scheme with transformers to model the interactions between modalities. Furthermore, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, which also facilitates multi-modal learning. We conduct extensive experiments on the Charades-STA and ActivityNet Captions datasets, and show that the proposed method performs favorably against state-of-the-art approaches.",https://api.openreview.net/pdf/719103046bcca6b7ca0f8be128f591dcb4cccbd9.pdf,zero_few-shot;transformer;representation;flow;multimodal;llm,https://scholar.google.com/scholar?q=End-to-end+Multi-modal+Video+Temporal+Grounding
Leveraging SE(3) Equivariance for Self-supervised Category-Level Object  Pose Estimation from Point Clouds,2021,NIPS,"['Xiaolong Li', 'Yijia Weng', 'Li Yi', 'Leonidas Guibas', 'A. Lynn Abbott', 'Shuran Song', 'He Wang']",poster,"['6D Pose', 'self-supervision', 'SE(3) equivariance', '3D point cloud', 'category-level', 'real data', 'shape reconstruction']","Category-level object pose estimation aims to find 6D object poses of previously unseen object instances from known categories without access to object CAD models. To reduce the huge amount of pose annotations needed for category-level learning, we propose for the first time a self-supervised learning framework to estimate category-level 6D object pose from single 3D point clouds. During training, our method assumes no ground-truth pose annotations, no CAD models, and no multi-view supervision. The key to our method is to disentangle shape and pose through an invariant shape reconstruction module and an equivariant pose estimation module, empowered by SE(3) equivariant point cloud networks. The invariant shape reconstruction module learns to perform aligned reconstructions, yielding a category-level reference frame without using any annotations. In addition, the equivariant pose estimation module achieves category-level pose estimation accuracy that is comparable to some fully supervised methods. Extensive experiments demonstrate the effectiveness of our approach on both complete and partial depth point clouds from the ModelNet40 benchmark, and on real depth point clouds from the NOCS-REAL 275 dataset. The project page with code and visualizations can be found at: dragonlong.github.io/equi-pose.",https://api.openreview.net/pdf/3684c693eef5ca7ace048aafa9b6c13f08e98a0f.pdf,graph;zero_few-shot;multimodal;multi-view;3d,https://scholar.google.com/scholar?q=Leveraging+SE(3)+Equivariance+for+Self-supervised+Category-Level+Object++Pose+Estimation+from+Point+Clouds
Cross-modal Domain Adaptation for Cost-Efficient Visual Reinforcement Learning,2021,NIPS,"['Xiong-Hui Chen', 'Shengyi Jiang', 'Feng Xu', 'Zongzhang Zhang', 'Yang Yu']",poster,['Reinforcement Learning'],"In visual-input sim-to-real scenarios, to overcome the reality gap between images rendered in simulators and those from the real world, domain adaptation, i.e., learning an aligned representation space between simulators and the real world, then training and deploying policies in the aligned representation, is a promising direction. Previous methods focus on same-modal domain adaptation. However, those methods require building and running simulators that render high-quality images, which can be difficult and costly. In this paper, we consider a more cost-efficient setting of visual-input sim-to-real where only low-dimensional states are simulated. We first point out that the objective of learning mapping functions in previous methods that align the representation spaces is ill-posed, prone to yield an incorrect mapping. When the mapping crosses modalities, previous methods are easier to fail. Our algorithm, Cross-mOdal Domain Adaptation with Sequential structure (CODAS), mitigates the ill-posedness by utilizing the sequential nature of the data sampling process in RL tasks. Experiments on MuJoCo and Hand Manipulation Suite tasks show that the agents deployed with our method achieve similar performance as it has in the source domain, while those deployed with previous methods designed for same-modal domain adaptation suffer a larger performance gap.",https://api.openreview.net/pdf/b36a569134b11f7fb557974b17fa253b8e344f07.pdf,reinforcement learning;zero_few-shot;representation;multimodal,https://scholar.google.com/scholar?q=Cross-modal+Domain+Adaptation+for+Cost-Efficient+Visual+Reinforcement+Learning
Trustworthy Multimodal Regression with Mixture of Normal-inverse Gamma Distributions,2021,NIPS,"['Huan Ma', 'Zongbo Han', 'Changqing Zhang', 'Huazhu Fu', 'Joey Tianyi Zhou', 'Qinghua Hu']",poster,"['Multimodal Regression', 'Trustworthy', 'Uncertainty', 'Evidential']","Multimodal regression is a fundamental task, which integrates the information from different sources to improve the performance of follow-up applications. However, existing methods mainly focus on improving the performance and often ignore the confidence of prediction for diverse situations. In this study, we are devoted to trustworthy multimodal regression which is critical in cost-sensitive domains. To this end, we introduce a novel Mixture of Normal-Inverse Gamma distributions (MoNIG) algorithm, which efficiently estimates uncertainty in principle for adaptive integration of different modalities and produces a trustworthy regression result. Our model can be dynamically aware of uncertainty for each modality, and also robust for corrupted modalities. Furthermore, the proposed MoNIG ensures explicitly representation of (modality-specific/global) epistemic and aleatoric uncertainties, respectively. Experimental results on both synthetic and different real-world data demonstrate the effectiveness and trustworthiness of our method on various multimodal regression tasks (e.g., temperature prediction for superconductivity, relative location prediction for CT slices, and multimodal sentiment analysis).",https://api.openreview.net/pdf/0cdd71f714a740149b60154b6833f521e03ec39d.pdf,zero_few-shot;representation;adaptive;multimodal;llm,https://scholar.google.com/scholar?q=Trustworthy+Multimodal+Regression+with+Mixture+of+Normal-inverse+Gamma+Distributions
CCVS: Context-aware Controllable Video Synthesis,2021,NIPS,"['Guillaume Le Moing', 'Jean Ponce', 'Cordelia Schmid']",poster,"['Video Synthesis', 'Multimodal Generative Modeling', 'Vision Transformers', 'Vector-Quantized Generative Adversarial Networks']","This presentation introduces a self-supervised learning approach to the synthesis of new videos clips from old ones, with several new key elements for improved spatial resolution and realism: It conditions the synthesis process on contextual information for temporal continuity and ancillary information for fine control. The prediction model is doubly autoregressive, in the latent space of an autoencoder for forecasting, and in image space for updating contextual information, which is also used to enforce spatio-temporal consistency through a learnable optical flow module. Adversarial training of the autoencoder in the appearance and temporal domains is used to further improve the realism of its output. A quantizer inserted between the encoder and the transformer in charge of forecasting future frames in latent space (and its inverse inserted between the transformer and the decoder) adds even more flexibility by affording simple mechanisms for handling multimodal ancillary information for controlling the synthesis process (e.g., a few sample frames, an audio track, a trajectory in image space) and taking into account the intrinsically uncertain nature of the future by allowing multiple predictions. Experiments with an implementation of the proposed approach give very good qualitative and quantitative results on multiple tasks and standard benchmarks.",https://api.openreview.net/pdf/ca636157cd6fdf87f3b457d72cb4defb79772fd5.pdf,zero_few-shot;transformer;flow;multimodal,https://scholar.google.com/scholar?q=CCVS:+Context-aware+Controllable+Video+Synthesis
Overcoming the Convex Barrier for Simplex Inputs,2021,NIPS,"['Harkirat Singh', 'M. Pawan Kumar', 'Philip Torr', 'Krishnamurthy Dj Dvijotham']",poster,"['Neural Network Verification', 'Optimisation for Deep Learning']","Recent progress in neural network verification has challenged the notion of a convex barrier, that is, an inherent weakness in the convex relaxation of the output of a neural network. Specifically, there now exists a tight relaxation for verifying the robustness of a neural network to $\ell_\infty$ input perturbations, as well as efficient primal and dual solvers for the relaxation. Buoyed by this success, we consider the problem of developing similar techniques for verifying robustness to input perturbations within the probability simplex. We prove a somewhat surprising result that, in this case, not only can one design a tight relaxation that overcomes the convex barrier, but the size of the relaxation remains linear in the number of neurons, thereby leading to simpler and more efficient algorithms. We establish the scalability of our overall approach via the specification of $\ell_1$ robustness for CIFAR-10 and MNIST classification, where our approach improves the state of the art verified accuracy by up to $14.4\%$. Furthermore, we establish its accuracy on a novel and highly challenging task of verifying the robustness of a multi-modal (text and image) classifier to arbitrary changes in its textual input. ",https://api.openreview.net/pdf/242aa91273085a17f21240eaec274f3764f014a0.pdf,graph;zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Overcoming+the+Convex+Barrier+for+Simplex+Inputs
Joint Semantic Mining for Weakly Supervised RGB-D Salient Object Detection,2021,NIPS,"['Jingjing Li', 'Wei Ji', 'Qi Bi', 'Cheng Yan', 'Miao Zhang', 'Yongri Piao', 'Huchuan Lu', 'Li Cheng']",poster,"['Salient Object Detection', 'Saliency Detection', 'Weakly Supervised RGB-D Salient Object Detection']","Training saliency detection models with weak supervisions, e.g., image-level tags or captions, is appealing as it removes the costly demand of per-pixel annotations. Despite the rapid progress of RGB-D saliency detection in fully-supervised setting, it however remains an unexplored territory when only weak supervision signals are available. This paper is set to tackle the problem of weakly-supervised RGB-D salient object detection. The key insight in this effort is the idea of maintaining per-pixel pseudo-labels with iterative refinements by reconciling the multimodal input signals in our joint semantic mining (JSM). Considering the large variations in the raw depth map and the lack of explicit pixel-level supervisions, we propose spatial semantic modeling (SSM) to capture saliency-specific depth cues from the raw depth and produce depth-refined pseudo-labels. Moreover, tags and captions are incorporated via a fill-in-the-blank training in our textual semantic modeling (TSM) to estimate the confidences of competing pseudo-labels. At test time, our model involves only a light-weight sub-network of the training pipeline, i.e., it requires only an RGB image as input, thus allowing efficient inference. Extensive evaluations demonstrate the effectiveness of our approach under the weakly-supervised setting. Importantly, our method could also be adapted to work in both fully-supervised and unsupervised paradigms. In each of these scenarios, superior performance has been attained by our approach with comparing to the state-of-the-art dedicated methods. As a by-product, a CapS dataset is constructed by augmenting existing benchmark training set with additional image tags and captions. ",https://api.openreview.net/pdf/6985ef9d5696f199ec80305a0f60cc6d801abf01.pdf,zero_few-shot;inference;multimodal,https://scholar.google.com/scholar?q=Joint+Semantic+Mining+for+Weakly+Supervised+RGB-D+Salient+Object+Detection
MERLOT: Multimodal Neural Script Knowledge Models,2021,NIPS,"['Rowan Zellers', 'Ximing Lu', 'Jack Hessel', 'Youngjae Yu', 'Jae Sung Park', 'Jize Cao', 'Ali Farhadi', 'Yejin Choi']",oral,"['commonsense reasoning', 'representation learning', 'vision and language']","As humans, we understand events in the visual world contextually, performing multimodal reasoning across time to make inferences about the past, present, and future. We introduce MERLOT, a model that learns multimodal script knowledge by watching millions of YouTube videos with transcribed speech -- in an entirely label-free, self-supervised manner. By pretraining with a mix of both frame-level (spatial) and video-level (temporal) objectives, our model not only learns to match images to temporally corresponding words, but also to contextualize what is happening globally over time. As a result, MERLOT exhibits strong out-of-the-box representations of temporal commonsense, and achieves state-of-the-art performance on 12 different video QA datasets when finetuned. It also transfers well to the world of static images, allowing models to reason about the dynamic context behind visual scenes. On Visual Commonsense Reasoning, MERLOT~answers questions correctly with 80.6\% accuracy, outperforming state-of-the-art models of similar size by over 3\%, even those that make heavy use of auxiliary supervised data (like object bounding boxes).

Ablation analyses demonstrate the complementary importance of: 1) training on videos versus static images; 2) scaling the magnitude and diversity of the pretraining video corpus; and 3) using diverse objectives that encourage full-stack multimodal reasoning, from the recognition to cognition level.",https://api.openreview.net/pdf/302a68368f21f2fd4b037911ae44f77ed56f3344.pdf,zero_few-shot;transformer;representation;inference;transfer learning;multimodal,https://scholar.google.com/scholar?q=MERLOT:+Multimodal+Neural+Script+Knowledge+Models
Learning with Noisy Correspondence for Cross-modal Matching,2021,NIPS,"['Zhenyu Huang', 'Guocheng Niu', 'Xiao Liu', 'Wenbiao Ding', 'Xinyan Xiao', 'hua wu', 'Xi Peng']",oral,"['multimodal learning', 'noisy labels', 'cross-modal matching', 'mismatching']","Cross-modal matching, which aims to establish the correspondence between two different modalities, is fundamental to a variety of tasks such as cross-modal retrieval and vision-and-language understanding. Although a huge number of cross-modal matching methods have been proposed and achieved remarkable progress in recent years, almost all of these methods implicitly assume that the multimodal training data are correctly aligned. In practice, however, such an assumption is extremely expensive even impossible to satisfy. Based on this observation, we reveal and study a latent and challenging direction in cross-modal matching, named noisy correspondence, which could be regarded as a new paradigm of noisy labels. Different from the traditional noisy labels which mainly refer to the errors in category labels, our noisy correspondence refers to the mismatch paired samples. To solve this new problem, we propose a novel method for learning with noisy correspondence, named Noisy Correspondence Rectifier (NCR). In brief, NCR divides the data into clean and noisy partitions based on the memorization effect of neural networks and then rectifies the correspondence via an adaptive prediction model in a co-teaching manner. To verify the effectiveness of our method, we conduct experiments by using the image-text matching as a showcase. Extensive experiments on Flickr30K, MS-COCO, and Conceptual Captions verify the effectiveness of our method. The code could be accessed from www.pengxi.me .",https://api.openreview.net/pdf/bea8fb9aa88c88bc0f403110d9403bbacc61a783.pdf,graph;adaptive;multimodal,https://scholar.google.com/scholar?q=Learning+with+Noisy+Correspondence+for+Cross-modal+Matching
Learning Debiased Representation via Disentangled Feature Augmentation,2021,NIPS,"['Jungsoo Lee', 'Eungyeup Kim', 'Juyoung Lee', 'Jihyeon Lee', 'Jaegul Choo']",oral,"['Debiasing', 'Disentangled representation', 'Feature-level augmentation', 'Image classification']","Image classification models tend to make decisions based on peripheral attributes of data items that have strong correlation with a target variable (i.e., dataset bias). These biased models suffer from the poor generalization capability when evaluated on unbiased datasets. Existing approaches for debiasing often identify and emphasize those samples with no such correlation (i.e., bias-conflicting) without defining the bias type in advance. However, such bias-conflicting samples are significantly scarce in biased datasets, limiting the debiasing capability of these approaches. This paper first presents an empirical analysis revealing that training with ""diverse"" bias-conflicting samples beyond a given training set is crucial for debiasing as well as the generalization capability. Based on this observation, we propose a novel feature-level data augmentation technique in order to synthesize diverse bias-conflicting samples.  To this end, our method learns the disentangled representation of (1) the intrinsic attributes (i.e., those inherently defining a certain class) and (2) bias attributes (i.e., peripheral attributes causing the bias), from a large number of bias-aligned samples, the bias attributes of which have strong correlation with the target variable.  Using the disentangled representation, we synthesize bias-conflicting samples that contain the diverse intrinsic attributes of bias-aligned samples by swapping their latent features. By utilizing these diversified bias-conflicting features during the training, our approach achieves superior classification accuracy and debiasing results against the existing baselines on both synthetic and real-world datasets.",https://api.openreview.net/pdf/cf404f31c4c4796d921c1e0cb287f8ae0bdcbe73.pdf,representation;augmentation;multimodal,https://scholar.google.com/scholar?q=Learning+Debiased+Representation+via+Disentangled+Feature+Augmentation
End-to-End Generative Pretraining for Multimodal Video Captioning,2022,CVPR,Paul Hongsuck Seo;Arsha Nagrani;Anurag Arnab;Cordelia Schmid,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Seo_End-to-End_Generative_Pretraining_for_Multimodal_Video_Captioning_CVPR_2022_paper.pdf,generative model;multimodal,https://scholar.google.com/scholar?q=End-to-End+Generative+Pretraining+for+Multimodal+Video+Captioning
Cross-Modal Map Learning for Vision and Language Navigation,2022,CVPR,Georgios Georgakis;Karl Schmeckpeper;Karan Wanchoo;Soham Dan;Eleni Miltsakaki;Dan Roth;Kostas Daniilidis,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Georgakis_Cross-Modal_Map_Learning_for_Vision_and_Language_Navigation_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Map+Learning+for+Vision+and+Language+Navigation
Cross-Modal Clinical Graph Transformer for Ophthalmic Report Generation,2022,CVPR,Mingjie Li;Wenjia Cai;Karin Verspoor;Shirui Pan;Xiaodan Liang;Xiaojun Chang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Cross-Modal_Clinical_Graph_Transformer_for_Ophthalmic_Report_Generation_CVPR_2022_paper.pdf,graph;transformer;generative model;multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Clinical+Graph+Transformer+for+Ophthalmic+Report+Generation
Accelerating DETR Convergence via Semantic-Aligned Matching,2022,CVPR,Gongjie Zhang;Zhipeng Luo;Yingchen Yu;Kaiwen Cui;Shijian Lu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Accelerating_DETR_Convergence_via_Semantic-Aligned_Matching_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Accelerating+DETR+Convergence+via+Semantic-Aligned+Matching
PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation With Photometrically Challenging Objects,2022,CVPR,Pengyuan Wang;HyunJun Jung;Yitong Li;Siyuan Shen;Rahul Parthasarathy Srikanth;Lorenzo Garattoni;Sven Meier;Nassir Navab;Benjamin Busam,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_PhoCaL_A_Multi-Modal_Dataset_for_Category-Level_Object_Pose_Estimation_With_CVPR_2022_paper.pdf,graph;metric;multimodal,https://scholar.google.com/scholar?q=PhoCaL:+A+Multi-Modal+Dataset+for+Category-Level+Object+Pose+Estimation+With+Photometrically+Challenging+Objects
Everything at Once - Multi-Modal Fusion Transformer for Video Retrieval,2022,CVPR,Nina Shvetsova;Brian Chen;Andrew Rouditchenko;Samuel Thomas;Brian Kingsbury;Rogerio S. Feris;David Harwath;James Glass;Hilde Kuehne,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Shvetsova_Everything_at_Once_-_Multi-Modal_Fusion_Transformer_for_Video_Retrieval_CVPR_2022_paper.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Everything+at+Once+-+Multi-Modal+Fusion+Transformer+for+Video+Retrieval
CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding,2022,CVPR,Mohamed Afham;Isuru Dissanayake;Dinithi Dissanayake;Amaya Dharmasiri;Kanchana Thilakarathna;Ranga Rodrigo,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Afham_CrossPoint_Self-Supervised_Cross-Modal_Contrastive_Learning_for_3D_Point_Cloud_Understanding_CVPR_2022_paper.pdf,zero_few-shot;contrastive learning;multimodal;3d,https://scholar.google.com/scholar?q=CrossPoint:+Self-Supervised+Cross-Modal+Contrastive+Learning+for+3D+Point+Cloud+Understanding
Versatile Multi-Modal Pre-Training for Human-Centric Perception,2022,CVPR,Fangzhou Hong;Liang Pan;Zhongang Cai;Ziwei Liu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_Versatile_Multi-Modal_Pre-Training_for_Human-Centric_Perception_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Versatile+Multi-Modal+Pre-Training+for+Human-Centric+Perception
Lite-MDETR: A Lightweight Multi-Modal Detector,2022,CVPR,Qian Lou;Yen-Chang Hsu;Burak Uzkent;Ting Hua;Yilin Shen;Hongxia Jin,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lou_Lite-MDETR_A_Lightweight_Multi-Modal_Detector_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Lite-MDETR:+A+Lightweight+Multi-Modal+Detector
A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation,2022,CVPR,Yutong Chen;Fangyun Wei;Xiao Sun;Zhirong Wu;Stephen Lin,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_A_Simple_Multi-Modality_Transfer_Learning_Baseline_for_Sign_Language_Translation_CVPR_2022_paper.pdf,transfer learning;multimodal,https://scholar.google.com/scholar?q=A+Simple+Multi-Modality+Transfer+Learning+Baseline+for+Sign+Language+Translation
End-to-End Referring Video Object Segmentation With Multimodal Transformers,2022,CVPR,Adam Botach;Evgenii Zheltonozhskii;Chaim Baskin,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Botach_End-to-End_Referring_Video_Object_Segmentation_With_Multimodal_Transformers_CVPR_2022_paper.pdf,transformer;segmentation;multimodal,https://scholar.google.com/scholar?q=End-to-End+Referring+Video+Object+Segmentation+With+Multimodal+Transformers
XYLayoutLM: Towards Layout-Aware Multimodal Networks for Visually-Rich Document Understanding,2022,CVPR,Zhangxuan Gu;Changhua Meng;Ke Wang;Jun Lan;Weiqiang Wang;Ming Gu;Liqing Zhang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_XYLayoutLM_Towards_Layout-Aware_Multimodal_Networks_for_Visually-Rich_Document_Understanding_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=XYLayoutLM:+Towards+Layout-Aware+Multimodal+Networks+for+Visually-Rich+Document+Understanding
STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes,2022,CVPR,Peishan Cong;Xinge Zhu;Feng Qiao;Yiming Ren;Xidong Peng;Yuenan Hou;Lan Xu;Ruigang Yang;Dinesh Manocha;Yuexin Ma,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Cong_STCrowd_A_Multimodal_Dataset_for_Pedestrian_Perception_in_Crowded_Scenes_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=STCrowd:+A+Multimodal+Dataset+for+Pedestrian+Perception+in+Crowded+Scenes
Towards Multimodal Depth Estimation From Light Fields,2022,CVPR,Titus Leistner;Radek Mackowiak;Lynton Ardizzone;Ullrich Köthe;Carsten Rother,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Leistner_Towards_Multimodal_Depth_Estimation_From_Light_Fields_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Towards+Multimodal+Depth+Estimation+From+Light+Fields
Multimodal Material Segmentation,2022,CVPR,Yupeng Liang;Ryosuke Wakaki;Shohei Nobuhara;Ko Nishino,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Multimodal_Material_Segmentation_CVPR_2022_paper.pdf,segmentation;multimodal,https://scholar.google.com/scholar?q=Multimodal+Material+Segmentation
Modeling Motion With Multi-Modal Features for Text-Based Video Segmentation,2022,CVPR,Wangbo Zhao;Kai Wang;Xiangxiang Chu;Fuzhao Xue;Xinchao Wang;Yang You,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Modeling_Motion_With_Multi-Modal_Features_for_Text-Based_Video_Segmentation_CVPR_2022_paper.pdf,segmentation;multimodal,https://scholar.google.com/scholar?q=Modeling+Motion+With+Multi-Modal+Features+for+Text-Based+Video+Segmentation
Tencent-MVSE: A Large-Scale Benchmark Dataset for Multi-Modal Video Similarity Evaluation,2022,CVPR,Zhaoyang Zeng;Yongsheng Luo;Zhenhua Liu;Fengyun Rao;Dian Li;Weidong Guo;Zhen Wen,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_Tencent-MVSE_A_Large-Scale_Benchmark_Dataset_for_Multi-Modal_Video_Similarity_Evaluation_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Tencent-MVSE:+A+Large-Scale+Benchmark+Dataset+for+Multi-Modal+Video+Similarity+Evaluation
Ranking Distance Calibration for Cross-Domain Few-Shot Learning,2022,CVPR,Pan Li;Shaogang Gong;Chengjie Wang;Yanwei Fu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Ranking_Distance_Calibration_for_Cross-Domain_Few-Shot_Learning_CVPR_2022_paper.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Ranking+Distance+Calibration+for+Cross-Domain+Few-Shot+Learning
Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis,2022,CVPR,Tianhan Xu;Yasuhiro Fujita;Eiichi Matsumoto,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Surface-Aligned_Neural_Radiance_Fields_for_Controllable_3D_Human_Synthesis_CVPR_2022_paper.pdf,multimodal;3d,https://scholar.google.com/scholar?q=Surface-Aligned+Neural+Radiance+Fields+for+Controllable+3D+Human+Synthesis
Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling,2022,CVPR,Dat Huynh;Jason Kuen;Zhe Lin;Jiuxiang Gu;Ehsan Elhamifar,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Huynh_Open-Vocabulary_Instance_Segmentation_via_Robust_Cross-Modal_Pseudo-Labeling_CVPR_2022_paper.pdf,segmentation;multimodal,https://scholar.google.com/scholar?q=Open-Vocabulary+Instance+Segmentation+via+Robust+Cross-Modal+Pseudo-Labeling
RFNet: Unsupervised Network for Mutually Reinforcing Multi-Modal Image Registration and Fusion,2022,CVPR,Han Xu;Jiayi Ma;Jiteng Yuan;Zhuliang Le;Wei Liu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_RFNet_Unsupervised_Network_for_Mutually_Reinforcing_Multi-Modal_Image_Registration_and_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=RFNet:+Unsupervised+Network+for+Mutually+Reinforcing+Multi-Modal+Image+Registration+and+Fusion
EI-CLIP: Entity-Aware Interventional Contrastive Learning for E-Commerce Cross-Modal Retrieval,2022,CVPR,Haoyu Ma;Handong Zhao;Zhe Lin;Ajinkya Kale;Zhangyang Wang;Tong Yu;Jiuxiang Gu;Sunav Choudhary;Xiaohui Xie,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_EI-CLIP_Entity-Aware_Interventional_Contrastive_Learning_for_E-Commerce_Cross-Modal_Retrieval_CVPR_2022_paper.pdf,contrastive learning;multimodal,https://scholar.google.com/scholar?q=EI-CLIP:+Entity-Aware+Interventional+Contrastive+Learning+for+E-Commerce+Cross-Modal+Retrieval
AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis,2022,CVPR,Zhiqin Chen;Kangxue Yin;Sanja Fidler,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_AUV-Net_Learning_Aligned_UV_Maps_for_Texture_Transfer_and_Synthesis_CVPR_2022_paper.pdf,transfer learning;multimodal,https://scholar.google.com/scholar?q=AUV-Net:+Learning+Aligned+UV+Maps+for+Texture+Transfer+and+Synthesis
Generalizable Cross-Modality Medical Image Segmentation via Style Augmentation and Dual Normalization,2022,CVPR,Ziqi Zhou;Lei Qi;Xin Yang;Dong Ni;Yinghuan Shi,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Generalizable_Cross-Modality_Medical_Image_Segmentation_via_Style_Augmentation_and_Dual_CVPR_2022_paper.pdf,augmentation;segmentation;multimodal,https://scholar.google.com/scholar?q=Generalizable+Cross-Modality+Medical+Image+Segmentation+via+Style+Augmentation+and+Dual+Normalization
Multimodal Token Fusion for Vision Transformers,2022,CVPR,Yikai Wang;Xinghao Chen;Lele Cao;Wenbing Huang;Fuchun Sun;Yunhe Wang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Multimodal_Token_Fusion_for_Vision_Transformers_CVPR_2022_paper.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Multimodal+Token+Fusion+for+Vision+Transformers
Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation,2022,CVPR,Xian Liu;Qianyi Wu;Hang Zhou;Yinghao Xu;Rui Qian;Xinyi Lin;Xiaowei Zhou;Wayne Wu;Bo Dai;Bolei Zhou,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Learning_Hierarchical_Cross-Modal_Association_for_Co-Speech_Gesture_Generation_CVPR_2022_paper.pdf,generative model;multimodal,https://scholar.google.com/scholar?q=Learning+Hierarchical+Cross-Modal+Association+for+Co-Speech+Gesture+Generation
Target-Aware Dual Adversarial Learning and a Multi-Scenario Multi-Modality Benchmark To Fuse Infrared and Visible for Object Detection,2022,CVPR,Jinyuan Liu;Xin Fan;Zhanbo Huang;Guanyao Wu;Risheng Liu;Wei Zhong;Zhongxuan Luo,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Target-Aware_Dual_Adversarial_Learning_and_a_Multi-Scenario_Multi-Modality_Benchmark_To_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Target-Aware+Dual+Adversarial+Learning+and+a+Multi-Scenario+Multi-Modality+Benchmark+To+Fuse+Infrared+and+Visible+for+Object+Detection
Balanced Multimodal Learning via On-the-Fly Gradient Modulation,2022,CVPR,Xiaokang Peng;Yake Wei;Andong Deng;Dong Wang;Di Hu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Peng_Balanced_Multimodal_Learning_via_On-the-Fly_Gradient_Modulation_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Balanced+Multimodal+Learning+via+On-the-Fly+Gradient+Modulation
X-Trans2Cap: Cross-Modal Knowledge Transfer Using Transformer for 3D Dense Captioning,2022,CVPR,Zhihao Yuan;Xu Yan;Yinghong Liao;Yao Guo;Guanbin Li;Shuguang Cui;Zhen Li,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Yuan_X-Trans2Cap_Cross-Modal_Knowledge_Transfer_Using_Transformer_for_3D_Dense_Captioning_CVPR_2022_paper.pdf,transformer;transfer learning;multimodal;3d,https://scholar.google.com/scholar?q=X-Trans2Cap:+Cross-Modal+Knowledge+Transfer+Using+Transformer+for+3D+Dense+Captioning
MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation,2022,CVPR,Inkyu Shin;Yi-Hsuan Tsai;Bingbing Zhuang;Samuel Schulter;Buyu Liu;Sparsh Garg;In So Kweon;Kuk-Jin Yoon,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Shin_MM-TTA_Multi-Modal_Test-Time_Adaptation_for_3D_Semantic_Segmentation_CVPR_2022_paper.pdf,segmentation;multimodal;3d,https://scholar.google.com/scholar?q=MM-TTA:+Multi-Modal+Test-Time+Adaptation+for+3D+Semantic+Segmentation
Exploring Endogenous Shift for Cross-Domain Detection: A Large-Scale Benchmark and Perturbation Suppression Network,2022,CVPR,Renshuai Tao;Hainan Li;Tianbo Wang;Yanlu Wei;Yifu Ding;Bowei Jin;Hongping Zhi;Xianglong Liu;Aishan Liu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_Exploring_Endogenous_Shift_for_Cross-Domain_Detection_A_Large-Scale_Benchmark_and_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Exploring+Endogenous+Shift+for+Cross-Domain+Detection:+A+Large-Scale+Benchmark+and+Perturbation+Suppression+Network
Interact Before Align: Leveraging Cross-Modal Knowledge for Domain Adaptive Action Recognition,2022,CVPR,Lijin Yang;Yifei Huang;Yusuke Sugano;Yoichi Sato,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Interact_Before_Align_Leveraging_Cross-Modal_Knowledge_for_Domain_Adaptive_Action_CVPR_2022_paper.pdf,graph;adaptive;multimodal,https://scholar.google.com/scholar?q=Interact+Before+Align:+Leveraging+Cross-Modal+Knowledge+for+Domain+Adaptive+Action+Recognition
MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-Based Visual Question Answering,2022,CVPR,Yang Ding;Jing Yu;Bang Liu;Yue Hu;Mingxin Cui;Qi Wu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_MuKEA_Multimodal_Knowledge_Extraction_and_Accumulation_for_Knowledge-Based_Visual_Question_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=MuKEA:+Multimodal+Knowledge+Extraction+and+Accumulation+for+Knowledge-Based+Visual+Question+Answering
Cross-Modal Transferable Adversarial Attacks From Images to Videos,2022,CVPR,Zhipeng Wei;Jingjing Chen;Zuxuan Wu;Yu-Gang Jiang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Wei_Cross-Modal_Transferable_Adversarial_Attacks_From_Images_to_Videos_CVPR_2022_paper.pdf,transfer learning;multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Transferable+Adversarial+Attacks+From+Images+to+Videos
WebQA: Multihop and Multimodal QA,2022,CVPR,Yingshan Chang;Mridu Narang;Hisami Suzuki;Guihong Cao;Jianfeng Gao;Yonatan Bisk,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Chang_WebQA_Multihop_and_Multimodal_QA_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=WebQA:+Multihop+and+Multimodal+QA
"Open-Domain, Content-Based, Multi-Modal Fact-Checking of Out-of-Context Images via Online Resources",2022,CVPR,Sahar Abdelnabi;Rakibul Hasan;Mario Fritz,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Abdelnabi_Open-Domain_Content-Based_Multi-Modal_Fact-Checking_of_Out-of-Context_Images_via_Online_Resources_CVPR_2022_paper.pdf,online learning;multimodal,"https://scholar.google.com/scholar?q=Open-Domain,+Content-Based,+Multi-Modal+Fact-Checking+of+Out-of-Context+Images+via+Online+Resources"
Node-Aligned Graph Convolutional Network for Whole-Slide Image Representation and Classification,2022,CVPR,Yonghang Guan;Jun Zhang;Kuan Tian;Sen Yang;Pei Dong;Jinxi Xiang;Wei Yang;Junzhou Huang;Yuyao Zhang;Xiao Han,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Guan_Node-Aligned_Graph_Convolutional_Network_for_Whole-Slide_Image_Representation_and_Classification_CVPR_2022_paper.pdf,graph;representation;multimodal,https://scholar.google.com/scholar?q=Node-Aligned+Graph+Convolutional+Network+for+Whole-Slide+Image+Representation+and+Classification
RADU: Ray-Aligned Depth Update Convolutions for ToF Data Denoising,2022,CVPR,Michael Schelling;Pedro Hermosilla;Timo Ropinski,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Schelling_RADU_Ray-Aligned_Depth_Update_Convolutions_for_ToF_Data_Denoising_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=RADU:+Ray-Aligned+Depth+Update+Convolutions+for+ToF+Data+Denoising
Learning Based Multi-Modality Image and Video Compression,2022,CVPR,Guo Lu;Tianxiong Zhong;Jing Geng;Qiang Hu;Dong Xu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Learning_Based_Multi-Modality_Image_and_Video_Compression_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Learning+Based+Multi-Modality+Image+and+Video+Compression
Cross-Domain Correlation Distillation for Unsupervised Domain Adaptation in Nighttime Semantic Segmentation,2022,CVPR,Huan Gao;Jichang Guo;Guoli Wang;Qian Zhang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Gao_Cross-Domain_Correlation_Distillation_for_Unsupervised_Domain_Adaptation_in_Nighttime_Semantic_CVPR_2022_paper.pdf,distillation;segmentation;multimodal,https://scholar.google.com/scholar?q=Cross-Domain+Correlation+Distillation+for+Unsupervised+Domain+Adaptation+in+Nighttime+Semantic+Segmentation
Cross-Modal Perceptionist: Can Face Geometry Be Gleaned From Voices?,2022,CVPR,Cho-Ying Wu;Chin-Cheng Hsu;Ulrich Neumann,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Cross-Modal_Perceptionist_Can_Face_Geometry_Be_Gleaned_From_Voices_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Perceptionist:+Can+Face+Geometry+Be+Gleaned+From+Voices?
On Generalizing Beyond Domains in Cross-Domain Continual Learning,2022,CVPR,Christian Simon;Masoud Faraki;Yi-Hsuan Tsai;Xiang Yu;Samuel Schulter;Yumin Suh;Mehrtash Harandi;Manmohan Chandraker,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Simon_On_Generalizing_Beyond_Domains_in_Cross-Domain_Continual_Learning_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=On+Generalizing+Beyond+Domains+in+Cross-Domain+Continual+Learning
Learning Memory-Augmented Unidirectional Metrics for Cross-Modality Person Re-Identification,2022,CVPR,Jialun Liu;Yifan Sun;Feng Zhu;Hongbin Pei;Yi Yang;Wenhui Li,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Learning_Memory-Augmented_Unidirectional_Metrics_for_Cross-Modality_Person_Re-Identification_CVPR_2022_paper.pdf,metric;augmentation;multimodal,https://scholar.google.com/scholar?q=Learning+Memory-Augmented+Unidirectional+Metrics+for+Cross-Modality+Person+Re-Identification
Cross-Modal Representation Learning for Zero-Shot Action Recognition,2022,CVPR,Chung-Ching Lin;Kevin Lin;Lijuan Wang;Zicheng Liu;Linjie Li,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_Cross-Modal_Representation_Learning_for_Zero-Shot_Action_Recognition_CVPR_2022_paper.pdf,zero_few-shot;representation;multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Representation+Learning+for+Zero-Shot+Action+Recognition
Undoing the Damage of Label Shift for Cross-Domain Semantic Segmentation,2022,CVPR,Yahao Liu;Jinhong Deng;Jiale Tao;Tong Chu;Lixin Duan;Wen Li,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Undoing_the_Damage_of_Label_Shift_for_Cross-Domain_Semantic_Segmentation_CVPR_2022_paper.pdf,segmentation;multimodal,https://scholar.google.com/scholar?q=Undoing+the+Damage+of+Label+Shift+for+Cross-Domain+Semantic+Segmentation
Dual-Key Multimodal Backdoors for Visual Question Answering,2022,CVPR,Matthew Walmer;Karan Sikka;Indranil Sur;Abhinav Shrivastava;Susmit Jha,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Walmer_Dual-Key_Multimodal_Backdoors_for_Visual_Question_Answering_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Dual-Key+Multimodal+Backdoors+for+Visual+Question+Answering
Multimodal Colored Point Cloud to Image Alignment,2022,CVPR,Noam Rotstein;Amit Bracha;Ron Kimmel,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Rotstein_Multimodal_Colored_Point_Cloud_to_Image_Alignment_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Multimodal+Colored+Point+Cloud+to+Image+Alignment
CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data,2022,CVPR,Qi Yan;Jianhao Zheng;Simon Reding;Shanci Li;Iordan Doytchinov,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_CrossLoc_Scalable_Aerial_Localization_Assisted_by_Multimodal_Synthetic_Data_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=CrossLoc:+Scalable+Aerial+Localization+Assisted+by+Multimodal+Synthetic+Data
H2FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-Domain Weakly Supervised Object Detection,2022,CVPR,Yunqiu Xu;Yifan Sun;Zongxin Yang;Jiaxu Miao;Yi Yang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_H2FA_R-CNN_Holistic_and_Hierarchical_Feature_Alignment_for_Cross-Domain_Weakly_CVPR_2022_paper.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=H2FA+R-CNN:+Holistic+and+Hierarchical+Feature+Alignment+for+Cross-Domain+Weakly+Supervised+Object+Detection
Audio-Visual Generalised Zero-Shot Learning With Cross-Modal Attention and Language,2022,CVPR,Otniel-Bogdan Mercea;Lukas Riesch;A. Sophia Koepke;Zeynep Akata,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Mercea_Audio-Visual_Generalised_Zero-Shot_Learning_With_Cross-Modal_Attention_and_Language_CVPR_2022_paper.pdf,zero_few-shot;transformer;multimodal,https://scholar.google.com/scholar?q=Audio-Visual+Generalised+Zero-Shot+Learning+With+Cross-Modal+Attention+and+Language
Robust Cross-Modal Representation Learning With Progressive Self-Distillation,2022,CVPR,Alex Andonian;Shixing Chen;Raffay Hamid,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Andonian_Robust_Cross-Modal_Representation_Learning_With_Progressive_Self-Distillation_CVPR_2022_paper.pdf,representation;distillation;multimodal,https://scholar.google.com/scholar?q=Robust+Cross-Modal+Representation+Learning+With+Progressive+Self-Distillation
Multi-Modal Alignment Using Representation Codebook,2022,CVPR,Jiali Duan;Liqun Chen;Son Tran;Jinyu Yang;Yi Xu;Belinda Zeng;Trishul Chilimbi,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Duan_Multi-Modal_Alignment_Using_Representation_Codebook_CVPR_2022_paper.pdf,representation;multimodal,https://scholar.google.com/scholar?q=Multi-Modal+Alignment+Using+Representation+Codebook
Wnet: Audio-Guided Video Object Segmentation via Wavelet-Based Cross-Modal Denoising Networks,2022,CVPR,Wenwen Pan;Haonan Shi;Zhou Zhao;Jieming Zhu;Xiuqiang He;Zhigeng Pan;Lianli Gao;Jun Yu;Fei Wu;Qi Tian,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Pan_Wnet_Audio-Guided_Video_Object_Segmentation_via_Wavelet-Based_Cross-Modal_Denoising_Networks_CVPR_2022_paper.pdf,segmentation;multimodal,https://scholar.google.com/scholar?q=Wnet:+Audio-Guided+Video+Object+Segmentation+via+Wavelet-Based+Cross-Modal+Denoising+Networks
ADAPT: Vision-Language Navigation With Modality-Aligned Action Prompts,2022,CVPR,Bingqian Lin;Yi Zhu;Zicong Chen;Xiwen Liang;Jianzhuang Liu;Xiaodan Liang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_ADAPT_Vision-Language_Navigation_With_Modality-Aligned_Action_Prompts_CVPR_2022_paper.pdf,multimodal;llm,https://scholar.google.com/scholar?q=ADAPT:+Vision-Language+Navigation+With+Modality-Aligned+Action+Prompts
Text2Pos: Text-to-Point-Cloud Cross-Modal Localization,2022,CVPR,Manuel Kolmet;Qunjie Zhou;Aljoša Ošep;Laura Leal-Taixé,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Kolmet_Text2Pos_Text-to-Point-Cloud_Cross-Modal_Localization_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Text2Pos:+Text-to-Point-Cloud+Cross-Modal+Localization
Cross-Domain Few-Shot Learning With Task-Specific Adapters,2022,CVPR,Wei-Hong Li;Xialei Liu;Hakan Bilen,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Cross-Domain_Few-Shot_Learning_With_Task-Specific_Adapters_CVPR_2022_paper.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Cross-Domain+Few-Shot+Learning+With+Task-Specific+Adapters
Multimodal Dynamics: Dynamical Fusion for Trustworthy Multimodal Classification,2022,CVPR,Zongbo Han;Fan Yang;Junzhou Huang;Changqing Zhang;Jianhua Yao,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Multimodal_Dynamics_Dynamical_Fusion_for_Trustworthy_Multimodal_Classification_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Multimodal+Dynamics:+Dynamical+Fusion+for+Trustworthy+Multimodal+Classification
LAKe-Net: Topology-Aware Point Cloud Completion by Localizing Aligned Keypoints,2022,CVPR,Junshu Tang;Zhijun Gong;Ran Yi;Yuan Xie;Lizhuang Ma,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_LAKe-Net_Topology-Aware_Point_Cloud_Completion_by_Localizing_Aligned_Keypoints_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=LAKe-Net:+Topology-Aware+Point+Cloud+Completion+by+Localizing+Aligned+Keypoints
AlignMixup: Improving Representations by Interpolating Aligned Features,2022,CVPR,Shashanka Venkataramanan;Ewa Kijak;Laurent Amsaleg;Yannis Avrithis,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Venkataramanan_AlignMixup_Improving_Representations_by_Interpolating_Aligned_Features_CVPR_2022_paper.pdf,representation;multimodal,https://scholar.google.com/scholar?q=AlignMixup:+Improving+Representations+by+Interpolating+Aligned+Features
Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos,2022,CVPR,Saghir Alfasly;Jian Lu;Chen Xu;Yuru Zou,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Alfasly_Learnable_Irrelevant_Modality_Dropout_for_Multimodal_Action_Recognition_on_Modality-Specific_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Learnable+Irrelevant+Modality+Dropout+for+Multimodal+Action+Recognition+on+Modality-Specific+Annotated+Videos
Reading To Listen at the Cocktail Party: Multi-Modal Speech Separation,2022,CVPR,Akam Rahimi;Triantafyllos Afouras;Andrew Zisserman,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Rahimi_Reading_To_Listen_at_the_Cocktail_Party_Multi-Modal_Speech_Separation_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Reading+To+Listen+at+the+Cocktail+Party:+Multi-Modal+Speech+Separation
Cross-Modal Background Suppression for Audio-Visual Event Localization,2022,CVPR,Yan Xia;Zhou Zhao,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Xia_Cross-Modal_Background_Suppression_for_Audio-Visual_Event_Localization_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Background+Suppression+for+Audio-Visual+Event+Localization
Mutual Quantization for Cross-Modal Search With Noisy Labels,2022,CVPR,Erkun Yang;Dongren Yao;Tongliang Liu;Cheng Deng,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Mutual_Quantization_for_Cross-Modal_Search_With_Noisy_Labels_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Mutual+Quantization+for+Cross-Modal+Search+With+Noisy+Labels
"3MASSIV: Multilingual, Multimodal and Multi-Aspect Dataset of Social Media Short Videos",2022,CVPR,Vikram Gupta;Trisha Mittal;Puneet Mathur;Vaibhav Mishra;Mayank Maheshwari;Aniket Bera;Debdoot Mukherjee;Dinesh Manocha,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_3MASSIV_Multilingual_Multimodal_and_Multi-Aspect_Dataset_of_Social_Media_Short_CVPR_2022_paper.pdf,multimodal,"https://scholar.google.com/scholar?q=3MASSIV:+Multilingual,+Multimodal+and+Multi-Aspect+Dataset+of+Social+Media+Short+Videos"
Remember the Difference: Cross-Domain Few-Shot Semantic Segmentation via Meta-Memory Transfer,2022,CVPR,Wenjian Wang;Lijuan Duan;Yuxi Wang;Qing En;Junsong Fan;Zhaoxiang Zhang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Remember_the_Difference_Cross-Domain_Few-Shot_Semantic_Segmentation_via_Meta-Memory_Transfer_CVPR_2022_paper.pdf,meta-learning;transfer learning;segmentation;multimodal,https://scholar.google.com/scholar?q=Remember+the+Difference:+Cross-Domain+Few-Shot+Semantic+Segmentation+via+Meta-Memory+Transfer
Multi-Modal Extreme Classification,2022,CVPR,Anshul Mittal;Kunal Dahiya;Shreya Malani;Janani Ramaswamy;Seba Kuruvilla;Jitendra Ajmera;Keng-hao Chang;Sumeet Agarwal;Purushottam Kar;Manik Varma,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Mittal_Multi-Modal_Extreme_Classification_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Multi-Modal+Extreme+Classification
Motron: Multimodal Probabilistic Human Motion Forecasting,2022,CVPR,Tim Salzmann;Marco Pavone;Markus Ryll,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Salzmann_Motron_Multimodal_Probabilistic_Human_Motion_Forecasting_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Motron:+Multimodal+Probabilistic+Human+Motion+Forecasting
COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval,2022,CVPR,Haoyu Lu;Nanyi Fei;Yuqi Huo;Yizhao Gao;Zhiwu Lu;Ji-Rong Wen,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_COTS_Collaborative_Two-Stream_Vision-Language_Pre-Training_Model_for_Cross-Modal_Retrieval_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=COTS:+Collaborative+Two-Stream+Vision-Language+Pre-Training+Model+for+Cross-Modal+Retrieval
ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval,2022,CVPR,Mengjun Cheng;Yipeng Sun;Longchao Wang;Xiongwei Zhu;Kun Yao;Jie Chen;Guoli Song;Junyu Han;Jingtuo Liu;Errui Ding;Jingdong Wang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_ViSTA_Vision_and_Scene_Text_Aggregation_for_Cross-Modal_Retrieval_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=ViSTA:+Vision+and+Scene+Text+Aggregation+for+Cross-Modal+Retrieval
Semantic-Aligned Fusion Transformer for One-Shot Object Detection,2022,CVPR,Yizhou Zhao;Xun Guo;Yan Lu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Semantic-Aligned_Fusion_Transformer_for_One-Shot_Object_Detection_CVPR_2022_paper.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Semantic-Aligned+Fusion+Transformer+for+One-Shot+Object+Detection
CroMo: Cross-Modal Learning for Monocular Depth Estimation,2022,CVPR,Yannick Verdié;Jifei Song;Barnabé Mas;Benjamin Busam;Ales̆ Leonardis;Steven McDonagh,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Verdie_CroMo_Cross-Modal_Learning_for_Monocular_Depth_Estimation_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=CroMo:+Cross-Modal+Learning+for+Monocular+Depth+Estimation
Polymorphic-GAN: Generating Aligned Samples Across Multiple Domains With Learned Morph Maps,2022,CVPR,Seung Wook Kim;Karsten Kreis;Daiqing Li;Antonio Torralba;Sanja Fidler,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_Polymorphic-GAN_Generating_Aligned_Samples_Across_Multiple_Domains_With_Learned_Morph_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Polymorphic-GAN:+Generating+Aligned+Samples+Across+Multiple+Domains+With+Learned+Morph+Maps
VisualHow: Multimodal Problem Solving,2022,CVPR,Jinhui Yang;Xianyu Chen;Ming Jiang;Shi Chen;Louis Wang;Qi Zhao,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_VisualHow_Multimodal_Problem_Solving_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=VisualHow:+Multimodal+Problem+Solving
CAT-Det: Contrastively Augmented Transformer for Multi-Modal 3D Object Detection,2022,CVPR,Yanan Zhang;Jiaxin Chen;Di Huang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_CAT-Det_Contrastively_Augmented_Transformer_for_Multi-Modal_3D_Object_Detection_CVPR_2022_paper.pdf,transformer;contrastive learning;augmentation;multimodal;3d,https://scholar.google.com/scholar?q=CAT-Det:+Contrastively+Augmented+Transformer+for+Multi-Modal+3D+Object+Detection
M5Product: Self-Harmonized Contrastive Learning for E-Commercial Multi-Modal Pretraining,2022,CVPR,Xiao Dong;Xunlin Zhan;Yangxin Wu;Yunchao Wei;Michael C. Kampffmeyer;Xiaoyong Wei;Minlong Lu;Yaowei Wang;Xiaodan Liang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_M5Product_Self-Harmonized_Contrastive_Learning_for_E-Commercial_Multi-Modal_Pretraining_CVPR_2022_paper.pdf,contrastive learning;multimodal,https://scholar.google.com/scholar?q=M5Product:+Self-Harmonized+Contrastive+Learning+for+E-Commercial+Multi-Modal+Pretraining
Bi-Level Alignment for Cross-Domain Crowd Counting,2022,CVPR,Shenjian Gong;Shanshan Zhang;Jian Yang;Dengxin Dai;Bernt Schiele,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Gong_Bi-Level_Alignment_for_Cross-Domain_Crowd_Counting_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Bi-Level+Alignment+for+Cross-Domain+Crowd+Counting
DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection,2022,CVPR,Yingwei Li;Adams Wei Yu;Tianjian Meng;Ben Caine;Jiquan Ngiam;Daiyi Peng;Junyang Shen;Yifeng Lu;Denny Zhou;Quoc V. Le;Alan Yuille;Mingxing Tan,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Li_DeepFusion_Lidar-Camera_Deep_Fusion_for_Multi-Modal_3D_Object_Detection_CVPR_2022_paper.pdf,multimodal;3d,https://scholar.google.com/scholar?q=DeepFusion:+Lidar-Camera+Deep+Fusion+for+Multi-Modal+3D+Object+Detection
Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning,2022,CVPR,Ligong Han;Jian Ren;Hsin-Ying Lee;Francesco Barbieri;Kyle Olszewski;Shervin Minaee;Dimitris Metaxas;Sergey Tulyakov,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Show_Me_What_and_Tell_Me_How_Video_Synthesis_via_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Show+Me+What+and+Tell+Me+How:+Video+Synthesis+via+Multimodal+Conditioning
M3L: Language-Based Video Editing via Multi-Modal Multi-Level Transformers,2022,CVPR,Tsu-Jui Fu;Xin Eric Wang;Scott T. Grafton;Miguel P. Eckstein;William Yang Wang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Fu_M3L_Language-Based_Video_Editing_via_Multi-Modal_Multi-Level_Transformers_CVPR_2022_paper.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=M3L:+Language-Based+Video+Editing+via+Multi-Modal+Multi-Level+Transformers
Multi-Modal Dynamic Graph Transformer for Visual Grounding,2022,CVPR,Sijia Chen;Baochun Li,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Multi-Modal_Dynamic_Graph_Transformer_for_Visual_Grounding_CVPR_2022_paper.pdf,graph;transformer;multimodal,https://scholar.google.com/scholar?q=Multi-Modal+Dynamic+Graph+Transformer+for+Visual+Grounding
UMT: Unified Multi-Modal Transformers for Joint Video Moment Retrieval and Highlight Detection,2022,CVPR,Ye Liu;Siyuan Li;Yang Wu;Chang-Wen Chen;Ying Shan;Xiaohu Qie,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_UMT_Unified_Multi-Modal_Transformers_for_Joint_Video_Moment_Retrieval_and_CVPR_2022_paper.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=UMT:+Unified+Multi-Modal+Transformers+for+Joint+Video+Moment+Retrieval+and+Highlight+Detection
Are Multimodal Transformers Robust to Missing Modality?,2022,CVPR,Mengmeng Ma;Jian Ren;Long Zhao;Davide Testuggine;Xi Peng,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Are_Multimodal_Transformers_Robust_to_Missing_Modality_CVPR_2022_paper.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Are+Multimodal+Transformers+Robust+to+Missing+Modality?
ContIG: Self-Supervised Multimodal Contrastive Learning for Medical Imaging With Genetics,2022,CVPR,Aiham Taleb;Matthias Kirchler;Remo Monti;Christoph Lippert,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Taleb_ContIG_Self-Supervised_Multimodal_Contrastive_Learning_for_Medical_Imaging_With_Genetics_CVPR_2022_paper.pdf,graph;zero_few-shot;contrastive learning;multimodal,https://scholar.google.com/scholar?q=ContIG:+Self-Supervised+Multimodal+Contrastive+Learning+for+Medical+Imaging+With+Genetics
XMP-Font: Self-Supervised Cross-Modality Pre-Training for Few-Shot Font Generation,2022,CVPR,Wei Liu;Fangyue Liu;Fei Ding;Qian He;Zili Yi,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_XMP-Font_Self-Supervised_Cross-Modality_Pre-Training_for_Few-Shot_Font_Generation_CVPR_2022_paper.pdf,zero_few-shot;generative model;multimodal,https://scholar.google.com/scholar?q=XMP-Font:+Self-Supervised+Cross-Modality+Pre-Training+for+Few-Shot+Font+Generation
X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval,2022,CVPR,Satya Krishna Gorti;Noël Vouitsis;Junwei Ma;Keyvan Golestan;Maksims Volkovs;Animesh Garg;Guangwei Yu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Gorti_X-Pool_Cross-Modal_Language-Video_Attention_for_Text-Video_Retrieval_CVPR_2022_paper.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=X-Pool:+Cross-Modal+Language-Video+Attention+for+Text-Video+Retrieval
Expanding Large Pre-Trained Unimodal Models With Multimodal Information Injection for Image-Text Multimodal Classification,2022,CVPR,Tao Liang;Guosheng Lin;Mingyang Wan;Tianrui Li;Guojun Ma;Fengmao Lv,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expanding_Large_Pre-Trained_Unimodal_Models_With_Multimodal_Information_Injection_for_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Expanding+Large+Pre-Trained+Unimodal+Models+With+Multimodal+Information+Injection+for+Image-Text+Multimodal+Classification
Cross-Domain Adaptive Teacher for Object Detection,2022,CVPR,Yu-Jhe Li;Xiaoliang Dai;Chih-Yao Ma;Yen-Cheng Liu;Kan Chen;Bichen Wu;Zijian He;Kris Kitani;Peter Vajda,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Cross-Domain_Adaptive_Teacher_for_Object_Detection_CVPR_2022_paper.pdf,adaptive;multimodal,https://scholar.google.com/scholar?q=Cross-Domain+Adaptive+Teacher+for+Object+Detection
Boosting 3D Object Detection by Simulating Multimodality on Point Clouds,2022,CVPR,Wu Zheng;Mingxuan Hong;Li Jiang;Chi-Wing Fu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Boosting_3D_Object_Detection_by_Simulating_Multimodality_on_Point_Clouds_CVPR_2022_paper.pdf,multimodal;3d,https://scholar.google.com/scholar?q=Boosting+3D+Object+Detection+by+Simulating+Multimodality+on+Point+Clouds
Leveraging Self-Supervision for Cross-Domain Crowd Counting,2022,CVPR,Weizhe Liu;Nikita Durasov;Pascal Fua,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Leveraging_Self-Supervision_for_Cross-Domain_Crowd_Counting_CVPR_2022_paper.pdf,graph;multimodal;self-supervision,https://scholar.google.com/scholar?q=Leveraging+Self-Supervision+for+Cross-Domain+Crowd+Counting
MNSRNet: Multimodal Transformer Network for 3D Surface Super-Resolution,2022,CVPR,Wuyuan Xie;Tengcong Huang;Miaohui Wang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_MNSRNet_Multimodal_Transformer_Network_for_3D_Surface_Super-Resolution_CVPR_2022_paper.pdf,transformer;multimodal;3d,https://scholar.google.com/scholar?q=MNSRNet:+Multimodal+Transformer+Network+for+3D+Surface+Super-Resolution
JIFF: Jointly-Aligned Implicit Face Function for High Quality Single View Clothed Human Reconstruction,2022,CVPR,Yukang Cao;Guanying Chen;Kai Han;Wenqi Yang;Kwan-Yee K. Wong,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_JIFF_Jointly-Aligned_Implicit_Face_Function_for_High_Quality_Single_View_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=JIFF:+Jointly-Aligned+Implicit+Face+Function+for+High+Quality+Single+View+Clothed+Human+Reconstruction
Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning,2022,CVPR,Chia-Wen Kuo;Zsolt Kira,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Kuo_Beyond_a_Pre-Trained_Object_Detector_Cross-Modal_Textual_and_Visual_Context_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Beyond+a+Pre-Trained+Object+Detector:+Cross-Modal+Textual+and+Visual+Context+for+Image+Captioning
Egocentric Scene Understanding via Multimodal Spatial Rectifier,2022,CVPR,Tien Do;Khiem Vuong;Hyun Soo Park,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Do_Egocentric_Scene_Understanding_via_Multimodal_Spatial_Rectifier_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Egocentric+Scene+Understanding+via+Multimodal+Spatial+Rectifier
SelecMix: Debiased Learning by Mixing up Contradicting Pairs,2022,ICML,"['Inwoo Hwang', 'Sangjun Lee', 'Yunhyeok Kwak', 'Seong Joon Oh', 'Damien Teney', 'Jin-Hwa Kim', 'Byoung-Tak Zhang']",poster,"['debias', 'spurious correlation', 'mixup']","Neural networks trained with ERM (empirical risk minimization) sometimes learn unintended decision rules, in particular when their training data is biased, i.e., when training labels are correlated with undesirable features. Techniques have been proposed to prevent a network from learning such features, using the heuristic that spurious correlations are ``simple'' and learned preferentially during training by SGD. Recent methods resample or augment training data such that examples displaying spurious correlations (a.k.a. bias-aligned examples) become a minority, whereas the other, bias-conflicting examples become prevalent. These approaches are difficult to train and scale to real-world data, e.g., because they rely on disentangled representations. We propose an alternative based on mixup that augments the bias-conflicting training data with convex combinations of existing examples and their labels. Our method, named SelecMix, applies mixup to selected pairs of examples, which show either (i)~the same label but dissimilar biased features, or (ii)~a different label but similar biased features. To compare examples with respect to the biased features, we use an auxiliary model relying on the heuristic that biased features are learned preferentially during training by SGD.
On semi-synthetic benchmarks where this heuristic is valid, we obtain results superior to existing methods, in particular in the presence of label noise that makes the identification of bias-conflicting examples challenging.",https://api.openreview.net/pdf/de00d08540430654378b06fd1a43dcf279b1e336.pdf,graph;representation;multimodal,https://scholar.google.com/scholar?q=SelecMix:+Debiased+Learning+by+Mixing+up+Contradicting+Pairs
Doubly Right Object Recognition,2022,ICML,"['Revant Teotia', 'Chengzhi Mao', 'Carl Vondrick']",oral,"['Interpretability', 'Spurious Correlations', 'Robustness']","Existing deep neural networks are optimized to predict the right thing, yet they may rely on the wrong evidence. Using the wrong evidence for prediction undermines out-of-distribution generalization, underscoring the gap between machine perception and human perception. In this paper, we introduce an overlooked but important problem: ``doubly right object recognition,'' which requires the model not only to predict the right outcome, but also to use the right reasons that are aligned with human perception. The existing benchmarks fail to learn or evaluate the doubly right object recognition task, because both the right reason and spurious correlations are predictive of the final outcome. Without additional supervision and annotation for what is the right reason for recognition, doubly right object recognition is impossible. To address this, we collect a dataset, which contains annotated right reasons that are aligned with human perception and train a fully interpretable model that only uses the attributes from our collected dataset for object prediction. Through empirical experiments, we demonstrate that our method can train models that are more likely to predict the right thing with the right reason, providing additional generalization ability on ObjectNet, and demonstrating zero-shot learning ability.",https://api.openreview.net/pdf/321cefc0b6961f224a872588ff78ade2d99f75aa.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Doubly+Right+Object+Recognition
Knowledge-Consistent Dialogue Generation with Knowledge Graphs,2022,ICML,"['Minki Kang', 'Jin Myung Kwak', 'Jinheon Baek', 'Sung Ju Hwang']",poster,[],"We propose a framework for generating knowledge consistent and context-relevant dialogues with a knowledge graph (KG), named SUbgraph Retrieval-augmented GEneration (SURGE).
First, our method retrieves the context-relevant subgraph from the KG, and then enforces consistency across the facts by perturbing their word embeddings conditioned on the retrieved subgraph. 
Then, it learns the latent representation space using graph-text multi-modal contrastive learning which ensures that the generated texts have high similarity to the retrieved subgraphs. We validate the performance of our SURGE framework on the OpendialKG dataset and show that our method generates high-quality dialogues that faithfully reflect the knowledge from the KG. ",https://api.openreview.net/pdf/3cbcbc47f030dbfa9e2df4a54399f272b4b2a086.pdf,graph;representation;generative model;contrastive learning;augmentation;multimodal,https://scholar.google.com/scholar?q=Knowledge-Consistent+Dialogue+Generation+with+Knowledge+Graphs
Bridging the Training-Inference Gap for Dense Phrase Retrieval,2022,ICML,"['Gyuwan Kim', 'Jinhyuk Lee', 'Barlas Oguz', 'Wenhan Xiong', 'Yizhe Zhang', 'Yashar Mehdad', 'William Yang Wang']",poster,"['Machine Learning', 'Information Retrieval', 'Question Answering', 'Dense Retrieval', 'Efficiency', 'Validation']","Building dense retrievers requires a series of standard procedures, including training and validating neural models and creating indexes for efficient search. However, these procedures are often misaligned in that training objectives do not exactly reflect the retrieval scenario at inference time. In this paper, we explore how the gap between training and inference in dense retrieval can be reduced, focusing on dense phrase retrieval (Lee et al., 2021) where billions of representations are indexed at inference. Since validating every dense retriever with a large-scale index is practically infeasible, we propose an efficient way of validating dense retrievers using a small subset of the entire corpus. This allows us to validate various training strategies including unifying contrastive loss terms and using hard negatives for phrase retrieval, which largely reduces the training-inference discrepancy. As a result, we improve phrase retrieval by 2-3% in top-1 accuracy and passage retrieval by 2-4% in top-20 accuracy for open-domain question answering. Our work urges modeling dense retrievers with careful consideration of training and inference via efficient validation while advancing phrase retrieval as a general solution for dense retrieval.",https://api.openreview.net/pdf/5c8cdbbb0ccd089bc0699bcc247ff02cf80aad32.pdf,graph;zero_few-shot;representation;contrastive learning;inference;multimodal,https://scholar.google.com/scholar?q=Bridging+the+Training-Inference+Gap+for+Dense+Phrase+Retrieval
Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning,2022,ICML,"['Weixin Liang', 'Yuhui Zhang', 'Yongchan Kwon', 'Serena Yeung', 'James Zou']",poster,"['Multi-modal Representation Learning', 'Contrastive Representation Learning', 'Cone Effect', 'Modality Gap', 'Geometry of Deep Multi-Model Learning']","We present modality gap, an intriguing geometric phenomenon of the representation space of multi-modal models. Specifically, we show that different data modalities (e.g. images and text) are embedded at arm's length in their shared representation in multi-modal models such as CLIP. Our systematic analysis demonstrates that this gap is caused by a combination of model initialization and contrastive learning optimization. In model initialization, we show empirically and theoretically that the representation of a common deep neural network is restricted to a narrow cone. As a consequence, in a multi-modal model with two encoders, the representations of the two modalities are clearly apart when the model is initialized. During optimization,  contrastive learning keeps the different modalities separate by a certain distance, which is influenced by the temperature parameter in the loss function. Our experiments further demonstrate that varying the modality gap distance has a significant impact in improving the model's downstream zero-shot classification performance and fairness. Our code and data are available at https://modalitygap.readthedocs.io/",https://api.openreview.net/pdf/9ba24e4ed894472ff991fcbdc0fc5413cc788e55.pdf,optimization;zero_few-shot;representation;contrastive learning;metric;multimodal,https://scholar.google.com/scholar?q=Mind+the+Gap:+Understanding+the+Modality+Gap+in+Multi-modal+Contrastive+Representation+Learning
ECLIP: Efficient Contrastive Language-Image Pretraining via Ensemble Confidence Learning and Masked Language Modeling,2022,ICML,"['Jue Wang', 'Haofan Wang', 'Weijia Wu', 'Jincan Deng', 'Yu Lu', 'Xiaofeng Guo', 'Debing Zhang']",poster,['Language-Image Pretraining'],"While large scale pre-training has achieved great achievements in bridging the gap between vision and language, it still faces three challenges. First, the cost for pre-training is expensive. Second, there is no efficient way to handle the data noise which degrades model performance. Third, previous methods only leverage limited image-text paired data, while ignoring richer single-modal data, which may result in poor generalization to single-modal downstream tasks. In this work, we propose \textbf{E}fficient \textbf{C}ontrastive \textbf{L}anguage-\textbf{I}mage \textbf{P}retraining (ECLIP) via Ensemble Confidence Learning and Masked Language Modeling. Specifically, We adaptively filter out noisy samples in the training process by means of Ensemble Confidence Learning strategy, and add a Masked Language Modeling objective to utilize extra non-paired text data. ECLIP achieves the state-of-the-art performance on Chinese cross-modal retrieval tasks with only 1/10 training resources compared with CLIP and WenLan, while showing excellent generalization to single-modal tasks including text retrieval and text classification.",https://api.openreview.net/pdf/7b6a3b98b01d423ebeb3b306cdd1aff085e0a536.pdf,graph;adaptive;contrastive learning;multimodal,https://scholar.google.com/scholar?q=ECLIP:+Efficient+Contrastive+Language-Image+Pretraining+via+Ensemble+Confidence+Learning+and+Masked+Language+Modeling
Generative Self-training Improves Pre-training for Visual Dialog,2022,ICML,"['Gi-Cheon Kang', 'Sungdong Kim', 'Jin-Hwa Kim', 'Donghyun Kwak', 'Byoung-Tak Zhang']",poster,"['Visual Dialog', 'Self-Training', 'Semi-Supervised Learning', 'Dialogue Generation', 'Vision-and-Language']","Visual dialog (VisDial) is a task of answering a series of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog models solely on VisDial data via supervised learning or leveraged pre-training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for VisDial, called Generative Self-Training (GST), to enhance the pre-training. Specifically, GST generates synthetic dialog data for unlabeled images via multimodal conditional text generation and trains the dialog model on the synthetic and the original VisDial data. Moreover, we also propose perplexity-based data selection and multimodal consistency regularization for robust training of the synthetic data. Evaluation on VisDial v1.0 dataset shows that GST improves the pre-training and achieves new state-of-the-art results.  ",https://api.openreview.net/pdf/cc21f9be527e95c8e256377ac20734f2d7c20935.pdf,graph;generative model;multimodal,https://scholar.google.com/scholar?q=Generative+Self-training+Improves+Pre-training+for+Visual+Dialog
Multimodal Masked Autoencoders Learn Transferable Representations,2022,ICML,"['Xinyang Geng', 'Hao Liu', 'Lisa Lee', 'Dale Schuurmans', 'Sergey Levine', 'Pieter Abbeel']",oral,"['Self-supervised pre-training', 'multi-modal', 'representation learning']","Building scalable models to learn from diverse, multimodal data remains an open challenge. For vision-language data, the dominant approaches are based on contrastive learning objectives that train a separate encoder for each modality. While effective, contrastive learning approaches introduce sampling bias depending on the data augmentations used, which can degrade performance on downstream tasks. Moreover, these methods are limited to paired image-text data, and cannot leverage widely-available unpaired data. In this paper, we investigate whether a large multimodal model trained purely via masked token prediction, without using modality-specific encoders or contrastive learning, can learn transferable representations for downstream tasks. We propose a simple and scalable network architecture, the Multimodal Masked Autoencoder (M3AE), which learns a unified encoder for both vision and language data via masked token prediction. We provide an empirical study of M3AE trained on a large-scale image-text dataset, and find that M3AE is able to learn generalizable representations that transfer well to downstream tasks. We demonstrate the scalability of M3AE with larger model size and training time, and its flexibility to train on both paired image-text data as well as unpaired data.",https://api.openreview.net/pdf/12751398b54b104064d6ea866ea0d22b3873e9ca.pdf,representation;contrastive learning;transfer learning;augmentation;multimodal,https://scholar.google.com/scholar?q=Multimodal+Masked+Autoencoders+Learn+Transferable+Representations
Self-Aware Personalized Federated Learning,2022,NIPS,"['Huili Chen', 'Jie Ding', 'Eric William Tramel', 'Shuang Wu', 'Anit Kumar Sahu', 'Salman Avestimehr', 'Tao Zhang']",poster,"['Federared Learning', 'Personalization']","In the context of personalized federated learning (FL), the critical challenge is to balance local model improvement and global model tuning when the personal and global objectives may not be exactly aligned. Inspired by Bayesian hierarchical models, we develop a self-aware personalized FL method where each client can automatically balance the training of its local personal model and the global model that implicitly contributes to other clients' training. Such a balance is derived from the inter-client and intra-client uncertainty quantification. A larger inter-client variation implies more personalization is needed. Correspondingly, our method uses uncertainty-driven local training steps an aggregation rule instead of conventional local fine-tuning and sample size-based aggregation. With experimental studies on synthetic data, Amazon Alexa audio data, and public datasets such as MNIST, FEMNIST, CIFAR10, and Sent140, we show that our proposed method can achieve significantly improved personalization performance compared with the existing counterparts. ",https://api.openreview.net/pdf/8deb0ab599ccbb5042e72dd48d2e3796a53d9016.pdf,bayesian;federated learning;multimodal,https://scholar.google.com/scholar?q=Self-Aware+Personalized+Federated+Learning
On Infinite Separations Between Simple and Optimal Mechanisms,2022,NIPS,"['Alexandros Psomas', 'Ariel Schvartzman', 'S. Matthew Weinberg']",poster,"['mechanism design', 'revenue maximization', 'correlated distributions']","We consider a revenue-maximizing seller with $k$ heterogeneous items for sale to a single additive buyer, whose values are drawn from a known, possibly correlated prior $\mathcal{D}$. It is known that there exist priors $\mathcal{D}$ such that simple mechanisms --- those with bounded menu complexity --- extract an arbitrarily small fraction of the optimal revenue~(Briest et al. 2015, Hart and Nisan 2019). This paper considers the opposite direction: given a correlated distribution $\mathcal{D}$ witnessing an infinite separation between simple and optimal mechanisms, what can be said about $\mathcal{D}$?

\citet{hart2019selling} provides a framework for constructing such $\mathcal{D}$: it takes as input a sequence of $k$-dimensional vectors satisfying some geometric property, and produces a $\mathcal{D}$ witnessing an infinite gap. Our first main result establishes that this framework is without loss: every $\mathcal{D}$ witnessing an infinite separation could have resulted from this framework. An earlier version of their work provided a more streamlined framework (Hart and Nisan 2013). Our second main result establishes that this restrictive framework is not tight. That is, we provide an instance $\mathcal{D}$ witnessing an infinite gap, but which provably could not have resulted from the restrictive framework. 

As a corollary, we discover a new kind of mechanism which can witness these infinite separations on instances where the previous ``aligned'' mechanisms do not.",https://api.openreview.net/pdf/9eb3ef956fb60d243467e5cf5786e6d36f9f88d6.pdf,metric;multimodal,https://scholar.google.com/scholar?q=On+Infinite+Separations+Between+Simple+and+Optimal+Mechanisms
Training language models to follow instructions with human feedback,2022,NIPS,"['Long Ouyang', 'Jeffrey Wu', 'Xu Jiang', 'Diogo Almeida', 'Carroll Wainwright', 'Pamela Mishkin', 'Chong Zhang', 'Sandhini Agarwal', 'Katarina Slama', 'Alex Gray', 'John Schulman', 'Jacob Hilton', 'Fraser Kelton', 'Luke Miller', 'Maddie Simens', 'Amanda Askell', 'Peter Welinder', 'Paul Christiano', 'Jan Leike', 'Ryan Lowe']",poster,[],"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",https://api.openreview.net/pdf/12f896590d59191cc66b798c10ffaba467bf8af6.pdf,reinforcement learning;zero_few-shot;generative model;multimodal;llm,https://scholar.google.com/scholar?q=Training+language+models+to+follow+instructions+with+human+feedback
TVLT: Textless Vision-Language Transformer,2022,NIPS,"['Zineng Tang', 'Jaemin Cho', 'Yixin Nie', 'Mohit Bansal']",poster,"['textless vision-and-language modeling', 'audiovisual', 'TVLT']","In this work, we present the Textless Vision-Language Transformer (TVLT), where homogeneous transformer blocks take raw visual and audio inputs for vision-and-language representation learning with minimal modality-specific design, and do not use text-specific modules such as tokenization or automatic speech recognition (ASR). TVLT is trained by reconstructing masked patches of continuous video frames and audio spectrograms (masked autoencoding) and contrastive modeling to align video and audio. TVLT attains performance comparable to its text-based counterpart on various multimodal tasks, such as visual question answering, image retrieval, video retrieval, and multimodal sentiment analysis, with 28x faster inference speed and only 1/3 of the parameters. Our findings suggest the possibility of learning compact and efficient visual-linguistic representations from low-level visual and audio signals without assuming the prior existence of text. Our code and checkpoints are available at: https://github.com/zinengtang/TVLT",https://api.openreview.net/pdf/7243d12f6c6a4865863762fb9701f49e9d1b2175.pdf,zero_few-shot;transformer;representation;contrastive learning;inference;multimodal,https://scholar.google.com/scholar?q=TVLT:+Textless+Vision-Language+Transformer
CARD: Classification and Regression Diffusion Models,2022,NIPS,"['Xizewen Han', 'Huangjie Zheng', 'Mingyuan Zhou']",poster,[],"Learning the distribution of a continuous or categorical response variable y given its covariates x is a fundamental problem in statistics and machine learning. Deep neural network-based supervised learning algorithms have made great progress in predicting the mean of y given x, but they are often criticized for their ability to accurately capture the uncertainty of their predictions. In this paper, we introduce classification and regression diffusion (CARD) models, which combine a denoising diffusion-based conditional generative model and a pre-trained conditional mean estimator, to accurately predict the distribution of y given x.  We demonstrate the outstanding ability of CARD in conditional distribution prediction with both toy examples and real-world datasets, the experimental results on which show that CARD, in general, outperforms state-of-the-art methods, including Bayesian neural network-based one, designed for uncertainty estimation, especially when the conditional distribution of y given x is multi-modal. In addition, we utilize the stochastic nature of the generative model outputs to obtain a finer granularity in model confidence assessment at the instance level for classification tasks.",https://api.openreview.net/pdf/aad8cc52d1ab38485c05dbb5577ca801955ea620.pdf,generative model;bayesian;multimodal;diffusion models,https://scholar.google.com/scholar?q=CARD:+Classification+and+Regression+Diffusion+Models
QUARK: Controllable Text Generation with Reinforced Unlearning,2022,NIPS,"['Ximing Lu', 'Sean Welleck', 'Jack Hessel', 'Liwei Jiang', 'Lianhui Qin', 'Peter West', 'Prithviraj Ammanabrolu', 'Yejin Choi']",poster,"['Language Generation', 'Language Models']","Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model’s input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO, while relying only on standard language modeling primitives.",https://api.openreview.net/pdf/35a7082e93ad9d69e6fbb68ca8a323b8f5067a2a.pdf,reinforcement learning;graph;generative model;multimodal,https://scholar.google.com/scholar?q=QUARK:+Controllable+Text+Generation+with+Reinforced+Unlearning
Cross-Linked Unified Embedding for cross-modality representation learning,2022,NIPS,"['Xinming Tu', 'Zhi-Jie Cao', 'Chen-Rui Xia', 'Sara Mostafavi', 'Ge Gao']",poster,"['Multimodal Learning', 'Representation Learning', 'Semi-supervised Learning', 'Deep Autoencoders', 'Computational Biology and Bioinformatics', 'Single-cell Genomics']","Multi-modal learning is essential for understanding information in the real world. Jointly learning from multi-modal data enables global integration of both shared and modality-specific information, but current strategies often fail when observa- tions from certain modalities are incomplete or missing for part of the subjects. To learn comprehensive representations based on such modality-incomplete data, we present a semi-supervised neural network model called CLUE (Cross-Linked Unified Embedding). Extending from multi-modal VAEs, CLUE introduces the use of cross-encoders to construct latent representations from modality-incomplete observations. Representation learning for modality-incomplete observations is common in genomics. For example, human cells are tightly regulated across multi- ple related but distinct modalities such as DNA, RNA, and protein, jointly defining a cell’s function. We benchmark CLUE on multi-modal data from single cell measurements, illustrating CLUE’s superior performance in all assessed categories of the NeurIPS 2021 Multimodal Single-cell Data Integration Competition. While we focus on analysis of single cell genomic datasets, we note that the proposed cross-linked embedding strategy could be readily applied to other cross-modality representation learning problems.",https://api.openreview.net/pdf/329215ed043733bba40c1c4f0c3dbd8d2ab5740d.pdf,representation;vae;multimodal,https://scholar.google.com/scholar?q=Cross-Linked+Unified+Embedding+for+cross-modality+representation+learning
Modular Flows: Differential Molecular Generation,2022,NIPS,"['Yogesh Verma', 'Samuel Kaski', 'Markus Heinonen', 'Vikas K Garg']",poster,"['normalizing flow', 'molecule generation', 'graph neural networks', 'neural ode']","Generating new molecules is fundamental to advancing critical applications such as drug discovery and material synthesis. Flows can generate molecules effectively by inverting the encoding process, however, existing flow models either require artifactual dequantization or specific node/edge orderings, lack desiderata such as permutation invariance, or induce discrepancy between encoding and decoding steps that necessitates post hoc validity correction. Inspired by graph PDEs, we circumvent these issues with novel continuous normalizing E(3)-equivariant flows, based on a system of coupled node ODEs, that repeatedly reconcile locally toward globally aligned densities. Our models can be cast as message passing temporal networks, and result in superlative density estimation and  molecular generation. In particular, our generated samples achieve state of the art on both the standard QM9 and ZINC250K benchmarks.",https://api.openreview.net/pdf/99be39a318c7c796a69a001f4b80de3eab960f14.pdf,graph;zero_few-shot;generative model;flow;multimodal,https://scholar.google.com/scholar?q=Modular+Flows:+Differential+Molecular+Generation
Kernel Multimodal Continuous Attention,2022,NIPS,"['Alexander Moreno', 'Zhenke Wu', 'Supriya Nagesh', 'Walter H. Dempsey', 'James Matthew Rehg']",poster,"['attention', 'continuous attention', 'kernel methods']","Attention mechanisms take an expectation of a data representation with respect to probability weights. Recently, (Martins et al. 2020, 2021) proposed continuous attention mechanisms, focusing on unimodal attention densities from the exponential and deformed exponential families: the latter has sparse support. (Farinhas et al 2021) extended this to to multimodality via Gaussian mixture attention densities. In this paper, we extend this to kernel exponential families (Canu and Smola 2006) and our new sparse counterpart, kernel deformed exponential families. Theoretically, we show new existence results for both kernel exponential and deformed exponential families, and that the deformed case has similar approximation capabilities to kernel exponential families. Lacking closed form expressions for the context vector, we use numerical integration: we show exponential convergence for both kernel exponential and deformed exponential families. Experiments show that kernel continuous attention often outperforms unimodal continuous attention, and the sparse variant tends to highlight peaks of time series.",https://api.openreview.net/pdf/82070133eda02f720fde0dd7d5dea2e51b048a45.pdf,transformer;representation;sparse;multimodal,https://scholar.google.com/scholar?q=Kernel+Multimodal+Continuous+Attention
Graph Neural Networks are Dynamic Programmers,2022,NIPS,"['Andrew Joseph Dudzik', 'Petar Veličković']",poster,"['algorithmic reasoning', 'graph neural networks', 'category theory', 'polynomial functors', 'bellman-ford', 'integral transform', 'pullback', 'pushforward', 'monads', 'message passing', 'dynamic programming']","Recent advances in neural algorithmic reasoning with graph neural networks (GNNs) are propped up by the notion of algorithmic alignment. Broadly, a neural network will be better at learning to execute a reasoning task (in terms of sample complexity) if its individual components align well with the target algorithm. Specifically, GNNs are claimed to align with dynamic programming (DP), a general problem-solving strategy which expresses many polynomial-time algorithms. However, has this alignment truly been demonstrated and theoretically quantified? Here we show, using methods from category theory and abstract algebra, that there exists an intricate connection between GNNs and DP, going well beyond the initial observations over individual algorithms such as Bellman-Ford. Exposing this connection, we easily verify several prior findings in the literature, produce better-grounded GNN architectures for edge-centric tasks, and demonstrate empirical results on the CLRS algorithmic reasoning benchmark. We hope our exposition will serve as a foundation for building stronger algorithmically aligned GNNs.",https://api.openreview.net/pdf/fce0daf8b741bc969a6e5a8922be3dba9fb9b90b.pdf,graph;multimodal,https://scholar.google.com/scholar?q=Graph+Neural+Networks+are+Dynamic+Programmers
Learning to Reconstruct Missing Data from Spatiotemporal Graphs with Sparse Observations,2022,NIPS,"['Ivan Marisca', 'Andrea Cini', 'Cesare Alippi']",poster,"['missing data', 'time series imputation', 'spatiotemporal graph neural networks']","Modeling multivariate time series as temporal signals over a (possibly dynamic) graph is an effective representational framework that allows for developing models for time series analysis. In fact, discrete sequences of graphs can be processed by autoregressive graph neural networks to recursively learn representations at each discrete point in time and space. Spatiotemporal graphs are often highly sparse, with time series characterized by multiple, concurrent, and long sequences of missing data, e.g., due to the unreliable underlying sensor network. In this context, autoregressive models can be brittle and exhibit unstable learning dynamics. The objective of this paper is, then, to tackle the problem of learning effective models to reconstruct, i.e., impute, missing data points by conditioning the reconstruction only on the available observations. In particular, we propose a novel class of attention-based architectures that, given a set of highly sparse discrete observations, learn a representation for points in time and space by exploiting a spatiotemporal propagation architecture aligned with the imputation task. Representations are trained end-to-end to reconstruct observations w.r.t. the corresponding sensor and its neighboring nodes. Compared to the state of the art, our model handles sparse data without propagating prediction errors or requiring a bidirectional model to encode forward and backward time dependencies. Empirical results on representative benchmarks show the effectiveness of the proposed method.",https://api.openreview.net/pdf/f3ecd4e958231fdff66e7e9e46b6807fdab47968.pdf,graph;zero_few-shot;transformer;representation;sparse;multimodal,https://scholar.google.com/scholar?q=Learning+to+Reconstruct+Missing+Data+from+Spatiotemporal+Graphs+with+Sparse+Observations
MACK: Multimodal Aligned Conceptual Knowledge for Unpaired Image-text Matching,2022,NIPS,"['Yan Huang', 'Yuming Wang', 'Yunan Zeng', 'Liang Wang']",poster,"['image-text matching', 'multimodal knowledge']","Recently, the accuracy of image-text matching has been greatly improved by multimodal pretrained models, all of which are trained on millions or billions of paired images and texts. Different from them, this paper studies a new scenario as unpaired image-text matching, in which paired images and texts are assumed to be unavailable during model training. To deal with this, we propose a simple yet effective method namely Multimodal Aligned Conceptual Knowledge (MACK), which is inspired by the knowledge use in human brain. It can be directly used as general knowledge to correlate images and texts even without model training, or further fine-tuned based on unpaired images and texts to better generalize to certain datasets. In addition, we extend it as a re-ranking method, which can be easily combined with existing image-text matching models to substantially improve their performance.",https://api.openreview.net/pdf/f33661ec0a37ed32a1dd3f69638f81fb417e9aeb.pdf,multimodal,https://scholar.google.com/scholar?q=MACK:+Multimodal+Aligned+Conceptual+Knowledge+for+Unpaired+Image-text+Matching
Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods,2022,NIPS,"['Randall Balestriero', 'Yann LeCun']",poster,"['self-supervised learning', 'interpretability', 'understanding', 'local spectral methods', 'global spectral methods']","Self-Supervised Learning (SSL) surmises that inputs and pairwise positive relationships are enough to learn meaningful representations. Although SSL has recently reached a milestone: outperforming supervised methods in many modalities\dots the theoretical foundations are limited, method-specific, and fail to provide principled design guidelines to practitioners. In this paper, we propose a unifying framework under the helm of spectral manifold learning. Through the course of this study, we will demonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous spectral methods such as Laplacian Eigenmaps, ISOMAP et al.
From this unified viewpoint, we obtain (i) the close-form optimal representation, (ii) the close-form optimal network parameters in the linear regime, (iii) the impact of the pairwise relations used during training on each of those quantities and on downstream task performances, and most importantly, (iv) the first theoretical bridge between contrastive and non-contrastive methods to global and local spectral methods respectively hinting at the benefits and limitations of each. For example, if the pairwise relation is aligned with the downstream task, all SSL methods produce optimal representations for that downstream task.",https://api.openreview.net/pdf/ba332d6e6ab8b6bd422c59ab9989f8b501a51410.pdf,zero_few-shot;representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Contrastive+and+Non-Contrastive+Self-Supervised+Learning+Recover+Global+and+Local+Spectral+Embedding+Methods
Multi-Lingual Acquisition on Multimodal Pre-training for Cross-modal Retrieval,2022,NIPS,"['Liang Zhang', 'Anwen Hu', 'Qin Jin']",poster,"['cross-lingual cross-modal retrieval', 'multilingual representation', 'multimodal representation']","Vision and diverse languages are important information sources in our living world. A model that understands multi-modalities and multi-languages can be applied to a wider range of real-life scenarios. To build such a multimodal and multilingual model, existing works try to ensemble vision-language data from multiple languages in pre-training. However, due to the large number of languages, these works often require huge computing resources and cannot be flexibly extended to new languages. In this work, we propose a MultiLingual Acquisition (MLA) framework that can easily empower a monolingual Vision-Language Pre-training (VLP) model with multilingual capability. Specifically, we design a lightweight language acquisition encoder based on state-of-the-art monolingual VLP models. We further propose a two-stage training strategy to optimize the language acquisition encoder, namely the Native Language Transfer stage and the Language Exposure stage. With much less multilingual training data and computing resources, our model achieves state-of-the-art performance on multilingual image-text and video-text retrieval benchmarks.",https://api.openreview.net/pdf/5e36c0e6cf18312332b9cec299fba1f1df23445d.pdf,transfer learning;multimodal,https://scholar.google.com/scholar?q=Multi-Lingual+Acquisition+on+Multimodal+Pre-training+for+Cross-modal+Retrieval
On the Representation Collapse of Sparse Mixture of Experts,2022,NIPS,"['Zewen Chi', 'Li Dong', 'Shaohan Huang', 'Damai Dai', 'Shuming Ma', 'Barun Patra', 'Saksham Singhal', 'Payal Bajaj', 'Xia Song', 'Xian-Ling Mao', 'Heyan Huang', 'Furu Wei']",poster,[],"Sparse mixture of experts provides larger model capacity while requiring a constant computational overhead. It employs the routing mechanism to distribute input tokens to the best-matched experts according to their hidden representations. However, learning such a routing mechanism encourages token clustering around expert centroids, implying a trend toward representation collapse. In this work, we propose to estimate the routing scores between tokens and experts on a low-dimensional hypersphere. We conduct extensive experiments on cross-lingual language model pre-training and fine-tuning on downstream tasks. Experimental results across seven multilingual benchmarks show that our method achieves consistent gains. We also present a comprehensive analysis on the representation and routing behaviors of our models. Our method alleviates the representation collapse issue and achieves more consistent routing than the baseline mixture-of-experts methods.",https://api.openreview.net/pdf/275bc15391f60ccabb21779a6c6c2817e5189fbe.pdf,zero_few-shot;representation;sparse;multimodal,https://scholar.google.com/scholar?q=On+the+Representation+Collapse+of+Sparse+Mixture+of+Experts
CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP,2022,NIPS,"['Andreas Fürst', 'Elisabeth Rumetshofer', 'Johannes Lehner', 'Viet Thuong Tran', 'Fei Tang', 'Hubert Ramsauer', 'D P Kreil', 'Michael K Kopp', 'Günter Klambauer', 'Angela Bitto-Nemling', 'Sepp Hochreiter']",poster,"['Deep learning', 'Associative memory', 'Hopfield networks', 'Contrastive learning', 'Multimodal learning']","CLIP yielded impressive results on zero-shot transfer learning tasks and is considered as a foundation model like BERT or GPT3. CLIP vision models that have a rich representation are pre-trained using the InfoNCE objective and natural language supervision before they are fine-tuned on particular tasks. Though CLIP excels at zero-shot transfer learning, it suffers from an explaining away problem, that is, it focuses on one or few features, while neglecting other relevant features. This problem is caused by insufficiently extracting the covariance structure in the original multi-modal data. We suggest to use modern Hopfield networks to tackle the problem of explaining away. Their retrieved embeddings have an enriched covariance structure derived from co-occurrences of features in the stored embeddings. However, modern Hopfield networks increase the saturation effect of the InfoNCE objective which hampers learning. We propose to use the InfoLOOB objective to mitigate this saturation effect. We introduce the novel ""Contrastive Leave One Out Boost"" (CLOOB), which uses modern Hopfield networks for covariance enrichment together with the InfoLOOB objective. In experiments we compare CLOOB to CLIP after pre-training on the Conceptual Captions and the YFCC dataset with respect to their zero-shot transfer learning performance on other datasets. CLOOB consistently outperforms CLIP at zero-shot transfer learning across all considered architectures and datasets.",https://api.openreview.net/pdf/79dbd240d49f0740e197faf3d82d3df84f6d0e46.pdf,graph;zero_few-shot;transformer;representation;contrastive learning;transfer learning;multimodal,https://scholar.google.com/scholar?q=CLOOB:+Modern+Hopfield+Networks+with+InfoLOOB+Outperform+CLIP
Embracing Consistency: A One-Stage Approach for Spatio-Temporal Video Grounding,2022,NIPS,"['Yang Jin', 'yongzhi li', 'Zehuan Yuan', 'Yadong MU']",poster,"['Computer Vision', 'Video Understanding', 'Visual Grounding']","Spatio-Temporal video grounding (STVG) focuses on retrieving the spatio-temporal tube of a specific object depicted by a free-form textual expression. Existing approaches mainly treat this complicated task as a parallel frame-grounding problem and thus suffer from two types of inconsistency drawbacks: feature alignment inconsistency and prediction inconsistency. In this paper, we present an end-to-end one-stage framework, termed Spatio-Temporal Consistency-Aware Transformer (STCAT), to alleviate these issues. Specially, we introduce a novel multi-modal template as the global objective to address this task, which explicitly constricts the grounding region and associates the predictions among all video frames. Moreover, to generate the above template under sufficient video-textual perception, an encoder-decoder architecture is proposed for effective global context modeling. Thanks to these critical designs, STCAT enjoys more consistent cross-modal feature alignment and tube prediction without reliance on any pre-trained object detectors. Extensive experiments show that our method outperforms previous state-of-the-arts with clear margins on two challenging video benchmarks (VidSTG and HC-STVG), illustrating the superiority of the proposed framework to better understanding the association between vision and natural language. Code is publicly available at https://github.com/jy0205/STCAT.",https://api.openreview.net/pdf/3322c09fe5deb79560740b2ca078d4d43a983fcb.pdf,graph;transformer;multimodal,https://scholar.google.com/scholar?q=Embracing+Consistency:+A+One-Stage+Approach+for+Spatio-Temporal+Video+Grounding
Distilled Gradient Aggregation: Purify Features for Input Attribution in the Deep Neural Network,2022,NIPS,"['Giyoung Jeon', 'Haedong Jeong', 'Jaesik Choi']",poster,"['Explainable AI', 'Input Attribution']","Measuring the attribution of input features toward the model output is one of the popular post-hoc explanations on the Deep Neural Networks (DNNs). Among various approaches to compute the attribution, the gradient-based methods are widely used to generate attributions, because of its ease of implementation and the model-agnostic characteristic. However, existing gradient integration methods such as Integrated Gradients (IG) suffer from (1) the noisy attributions which cause the unreliability of the explanation, and (2) the selection for the integration path which determines the quality of explanations. FullGrad (FG) is an another approach to construct the reliable attributions by focusing the locality of piece-wise linear network with the bias gradient. Although FG has shown reasonable performance for the given input, as the shortage of the global property, FG is vulnerable to the small perturbation, while IG which includes the exploration over the input space is robust. In this work, we design a new input attribution method which adopt the strengths of both local and global attributions.
In particular, we propose a novel approach to distill input features using weak and extremely positive contributor masks. We aggregate the intermediate local attributions obtained from the distillation sequence to provide reliable attribution. We perform the quantitative evaluation compared to various attribution methods and show that our method outperforms others. We also provide the qualitative result that our method obtains object-aligned and sharp attribution heatmap.",https://api.openreview.net/pdf/2b4b97603b288d16525dad6c8ad1393cab8664b9.pdf,zero_few-shot;distillation;multimodal,https://scholar.google.com/scholar?q=Distilled+Gradient+Aggregation:+Purify+Features+for+Input+Attribution+in+the+Deep+Neural+Network
Okapi: Generalising Better by Making Statistical Matches Match,2022,NIPS,"['Myles Bartlett', 'Sara Romiti', 'Viktoriia Sharmanska', 'Novi Quadrianto']",poster,"['Domain Generalisation', 'Semi-Supervised Learning', 'Statistical Matching']","We propose Okapi, a simple, efficient, and general method for robust semi-supervised learning based on online statistical matching. Our method uses a nearest-neighbours-based matching procedure to generate cross-domain views for a consistency loss, while eliminating statistical outliers. In order to perform the online matching in a runtime- and memory-efficient way, we draw upon the self-supervised literature and combine a memory bank with a slow-moving momentum encoder. The consistency loss is applied within the feature space, rather than on the predictive distribution, making the method agnostic to both the modality and the task in question. We experiment on the WILDS 2.0 datasets Sagawa et al., which significantly expands the range of modalities, applications, and shifts available for studying and benchmarking real-world unsupervised adaptation. Contrary to Sagawa et al., we show that it is in fact possible to leverage additional unlabelled data to improve upon empirical risk minimisation (ERM) results with the right method. Our method outperforms the baseline methods in terms of out-of-distribution (OOD) generalisation on the iWildCam (a multi-class classification task) and PovertyMap (a regression task) image datasets as well as the CivilComments (a binary classification task) text dataset. Furthermore, from a qualitative perspective, we show the matches obtained from the learned encoder are strongly semantically related. Code for our paper is publicly available at https://github.com/wearepal/okapi/.",https://api.openreview.net/pdf/90be02f57402f2cf671e4ce0a29b0097d2f15d82.pdf,zero_few-shot;online learning;multimodal,https://scholar.google.com/scholar?q=Okapi:+Generalising+Better+by+Making+Statistical+Matches+Match
SelecMix: Debiased Learning by Contradicting-pair Sampling,2022,NIPS,"['Inwoo Hwang', 'Sangjun Lee', 'Yunhyeok Kwak', 'Seong Joon Oh', 'Damien Teney', 'Jin-Hwa Kim', 'Byoung-Tak Zhang']",poster,"['debias', 'spurious correlation', 'mixup']","Neural networks trained with ERM (empirical risk minimization) sometimes learn unintended decision rules, in particular when their training data is biased, i.e., when training labels are strongly correlated with undesirable features. To prevent a network from learning such features, recent methods augment training data such that examples displaying spurious correlations (i.e., bias-aligned examples) become a minority, whereas the other, bias-conflicting examples become prevalent. However, these approaches are sometimes difficult to train and scale to real-world data because they rely on generative models or disentangled representations. We propose an alternative based on mixup, a popular augmentation that creates convex combinations of training examples. Our method, coined SelecMix, applies mixup to contradicting pairs of examples, defined as showing either (i) the same label but dissimilar biased features, or (ii) different labels but similar biased features. Identifying such pairs requires comparing examples with respect to unknown biased features. For this, we utilize an auxiliary contrastive model with the popular heuristic that biased features are learned preferentially during training. Experiments on standard benchmarks demonstrate the effectiveness of the method, in particular when label noise complicates the identification of bias-conflicting examples.",https://api.openreview.net/pdf/7afb79cc0769814dfe7b98ba537107e3e3d3407a.pdf,representation;generative model;contrastive learning;augmentation;multimodal,https://scholar.google.com/scholar?q=SelecMix:+Debiased+Learning+by+Contradicting-pair+Sampling
Paraphrasing Is All You Need for Novel Object Captioning,2022,NIPS,"['Cheng-Fu Yang', 'Yao-Hung Hubert Tsai', 'Wan-Cyuan Fan', 'Ruslan Salakhutdinov', 'Louis-Philippe Morency', 'Yu-Chiang Frank Wang']",poster,[],"Novel object captioning (NOC) aims to describe images containing objects without observing their ground truth captions during training. Due to the absence of caption annotation, captioning models cannot be directly optimized via sequence-to-sequence training or CIDEr optimization. As a result, we present Paraphrasing-to-Captioning (P2C), a two-stage learning framework for NOC, which would heuristically optimize the output captions via paraphrasing. With P2C, the captioning model first learns paraphrasing from a language model pre-trained on text-only corpus, allowing expansion of the word bank for improving linguistic fluency. To further enforce the output caption sufficiently describing the visual content of the input image, we perform self-paraphrasing for the captioning model with fidelity and adequacy objectives introduced. Since no ground truth captions are available for novel object images during training, our P2C leverages cross-modality (image-text) association modules to ensure the above caption characteristics can be properly preserved. In the experiments, we not only show that our P2C achieves state-of-the-art performances on nocaps and COCO Caption datasets, we also verify the effectiveness and flexibility of our learning framework by replacing language and cross-modality association models for NOC. Implementation details and code are available in the supplementary materials.",https://api.openreview.net/pdf/a1dd12abe0aa6a19053332a37c9e27858fb28a4b.pdf,optimization;zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Paraphrasing+Is+All+You+Need+for+Novel+Object+Captioning
FeLMi : Few shot Learning with hard Mixup,2022,NIPS,"['Aniket Roy', 'Anshul Shah', 'Ketul Shah', 'Prithviraj Dhar', 'Anoop Cherian', 'Rama Chellappa']",poster,"['few-shot learning', 'mixup']","Learning from a few examples is a challenging computer vision task. Traditionally,
meta-learning-based methods have shown promise towards solving this problem.
Recent approaches show benefits by learning a feature extractor on the abundant
base examples and transferring these to the fewer novel examples. However, the
finetuning stage is often prone to overfitting due to the small size of the novel
dataset. To this end, we propose Few shot Learning with hard Mixup (FeLMi)
using manifold mixup to synthetically generate samples that helps in mitigating
the data scarcity issue. Different from a naïve mixup, our approach selects the hard
mixup samples using an uncertainty-based criteria. To the best of our knowledge,
we are the first to use hard-mixup for the few-shot learning problem. Our approach
allows better use of the pseudo-labeled base examples through base-novel mixup
and entropy-based filtering. We evaluate our approach on several common few-shot
benchmarks - FC-100, CIFAR-FS, miniImageNet and tieredImageNet and obtain
improvements in both 1-shot and 5-shot settings. Additionally, we experimented on
the cross-domain few-shot setting (miniImageNet → CUB) and obtain significant
improvements.",https://api.openreview.net/pdf/4833a75d7cf9c663a603ad404787872f3ade11bb.pdf,graph;zero_few-shot;meta-learning;transfer learning;multimodal,https://scholar.google.com/scholar?q=FeLMi+:+Few+shot+Learning+with+hard+Mixup
 Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation,2022,NIPS,"['Chenze Shao', 'Yang Feng']",poster,"['Non-Monotonic', 'Latent Alignments', 'CTC', 'Non-Autoregressive', 'machine translation']","Non-autoregressive translation (NAT) models are typically trained with the cross-entropy loss, which forces the model outputs to be aligned verbatim with the target sentence and will highly penalize small shifts in word positions. Latent alignment models relax the explicit alignment by marginalizing out all monotonic latent alignments with the CTC loss. However, they cannot handle non-monotonic alignments, which is non-negligible as there is typically global word reordering in machine translation. In this work, we explore non-monotonic latent alignments for NAT. We extend the alignment space to non-monotonic alignments to allow for the global word reordering and further consider all alignments that overlap with the target sentence. We non-monotonically match the alignments to the target sentence and train the latent alignment model to maximize the F1 score of non-monotonic matching. Extensive experiments on major WMT benchmarks show that our method substantially improves the translation performance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14 En-De with only one-iteration decoding, closing the gap between non-autoregressive and autoregressive models.
",https://api.openreview.net/pdf/a7c15905190bf863fd2a202df891de3caac5f881.pdf,graph;zero_few-shot;multimodal,https://scholar.google.com/scholar?q=+Non-Monotonic+Latent+Alignments+for+CTC-Based+Non-Autoregressive+Machine+Translation
Text-Adaptive Multiple Visual Prototype Matching for Video-Text Retrieval,2022,NIPS,"['Chengzhi Lin', 'Ancong Wu', 'Junwei Liang', 'Jun Zhang', 'Wenhang Ge', 'Wei-Shi Zheng', 'Chunhua Shen']",poster,['Video-Text Retrieval'],"Cross-modal retrieval between videos and texts has gained increasing interest because of the rapid emergence of videos on the web. 
Generally, a video contains rich instance and event information and the query text  only describes a part of the information. Thus, a video can have multiple different text descriptions and queries. We call it the Video-Text Correspondence Ambiguity problem. Current techniques mostly concentrate on mining local or multi-level alignment between contents of video and text (e.g., object to entity and action to verb). It is difficult for these methods to alleviate video-text correspondence ambiguity by describing a video using only one feature, which is required to be matched with multiple different text features at the same time. To address this problem, we propose a Text-Adaptive Multiple Visual Prototype Matching Model. It automatically captures multiple prototypes to describe a video by adaptive aggregation on video token features. Given a query text, the similarity is determined by the most similar prototype to find correspondence in the video, which is called text-adaptive matching.  To learn diverse prototypes for representing the rich information in videos, we propose a variance loss to encourage different prototypes to attend to different contents of the video.  Our method outperforms the state-of-the-art methods on four public video retrieval datasets.",https://api.openreview.net/pdf/d27282fb71b05702c8814546c175e2b6e5eb9e75.pdf,adaptive;multimodal,https://scholar.google.com/scholar?q=Text-Adaptive+Multiple+Visual+Prototype+Matching+for+Video-Text+Retrieval
Contrastive Language-Image Pre-Training with Knowledge Graphs,2022,NIPS,"['Xuran Pan', 'Tianzhu Ye', 'Dongchen Han', 'Shiji Song', 'Gao Huang']",poster,"['vision-language pre-training', 'knowledge graph']","Recent years have witnessed the fast development of large-scale pre-training frameworks that can extract multi-modal representations in a unified form and achieve promising performances when transferred to downstream tasks. Nevertheless, existing approaches mainly focus on pre-training with simple image-text pairs, while neglecting the semantic connections between concepts from different modalities. In this paper, we propose a knowledge-based pre-training framework, dubbed Knowledge-CLIP, which injects semantic information into the widely used CLIP model. Through introducing knowledge-based objectives in the pre-training process and utilizing different types of knowledge graphs as training data, our model can semantically align the representations in vision and language with higher quality, and enhance the reasoning ability across scenarios and modalities. Extensive experiments on various vision-language downstream tasks demonstrate the effectiveness of Knowledge-CLIP compared with the original CLIP and competitive baselines.",https://api.openreview.net/pdf/1e7a8ab7dacefbb739ad80761e428076b1c6731b.pdf,graph;representation;contrastive learning;transfer learning;multimodal,https://scholar.google.com/scholar?q=Contrastive+Language-Image+Pre-Training+with+Knowledge+Graphs
Grounded Reinforcement Learning: Learning to Win the Game under Human Commands,2022,NIPS,"['Shusheng Xu', 'Huaijie Wang', 'Yi Wu']",poster,"['Reinforcement Learning', 'Language Grounding', 'Human-AI Interaction']","We consider the problem of building a reinforcement learning (RL) agent that can both accomplish non-trivial tasks, like winning a real-time strategy game, and strictly follow high-level language commands from humans, like “attack”, even if a command is sub-optimal. We call this novel yet important problem, Grounded Reinforcement Learning (GRL). Compared with other language grounding tasks, GRL is particularly non-trivial and cannot be simply solved by pure RL or behavior cloning (BC). From the RL perspective, it is extremely challenging to derive a precise reward function for human preferences since the commands are abstract and the valid behaviors are highly complicated and multi-modal. From the BC perspective, it is impossible to obtain perfect demonstrations since human strategies in complex games are typically sub-optimal. We tackle GRL via a simple, tractable, and practical constrained RL objective and develop an iterative RL algorithm, REinforced demonstration Distillation (RED), to obtain a strong GRL policy. We evaluate the policies derived by RED, BC and pure RL methods on a simplified real-time strategy game, MiniRTS. Experiment results and human studies show that the RED policy is able to consistently follow human commands and achieve a higher win rate than the baselines. We release our code and present more examples at https://sites.google.com/view/grounded-rl.",https://api.openreview.net/pdf/5a75058d33398ffcdb36439ca3e4a5b477027103.pdf,reinforcement learning;graph;optimization;zero_few-shot;distillation;multimodal,https://scholar.google.com/scholar?q=Grounded+Reinforcement+Learning:+Learning+to+Win+the+Game+under+Human+Commands
Mind Reader: Reconstructing complex images from brain activities,2022,NIPS,"['Sikun Lin', 'Thomas Christopher Sprague', 'Ambuj Singh']",poster,[],"Understanding how the brain encodes external stimuli and how these stimuli can be decoded from the measured brain activities are long-standing and challenging questions in neuroscience. In this paper, we focus on reconstructing the complex image stimuli from fMRI (functional magnetic resonance imaging) signals. Unlike previous works that reconstruct images with single objects or simple shapes, our work aims to reconstruct image stimuli that are rich in semantics, closer to everyday scenes, and can reveal more perspectives. However, data scarcity of fMRI datasets is the main obstacle to applying state-of-the-art deep learning models to this problem. We find that incorporating an additional text modality is beneficial for the reconstruction problem compared to directly translating brain signals to images. Therefore, the modalities involved in our method are: (i) voxel-level fMRI signals, (ii) observed images that trigger the brain signals, and (iii) textual description of the images. To further address data scarcity, we leverage an aligned vision-language latent space pre-trained on massive datasets. Instead of training models from scratch to find a latent space shared by the three modalities, we encode fMRI signals into this pre-aligned latent space. Then, conditioned on embeddings in this space, we reconstruct images with a generative model. The reconstructed images from our pipeline balance both naturalness and fidelity: they are photo-realistic and capture the ground truth image contents well.",https://api.openreview.net/pdf/edf0233d2639014c2f39880a88db1b34db8d5a1a.pdf,graph;generative model;multimodal;llm,https://scholar.google.com/scholar?q=Mind+Reader:+Reconstructing+complex+images+from+brain+activities
Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning,2022,NIPS,"['Yuchong Sun', 'Hongwei Xue', 'Ruihua Song', 'Bei Liu', 'Huan Yang', 'Jianlong Fu']",poster,['video-language pre-training'],"Large-scale video-language pre-training has shown significant improvement in video-language understanding tasks. Previous studies of video-language pretraining mainly focus on short-form videos (i.e., within 30 seconds) and sentences, leaving long-form video-language pre-training rarely explored. Directly learning representation from long-form videos and language may benefit many long-form
video-language understanding tasks. However, it is challenging due to the difficulty of modeling long-range relationships and the heavy computational burden caused by more frames. In this paper, we introduce a Long-Form VIdeo-LAnguage pre-training model (LF-VILA) and train it on a large-scale long-form video and paragraph dataset constructed from an existing public dataset. To effectively capture
the rich temporal dynamics and to better align video and language in an efficient end-to-end manner, we introduce two novel designs in our LF-VILA model. We first propose a Multimodal Temporal Contrastive (MTC) loss to learn the temporal relation across different modalities by encouraging fine-grained alignment between long-form videos and paragraphs. Second, we propose a Hierarchical Temporal Window Attention (HTWA) mechanism to effectively capture long-range dependency while reducing computational cost in Transformer. We fine-tune the pre-trained LF-VILA model on seven downstream long-form video-language understanding tasks of paragraph-to-video retrieval and long-form video question-answering, and achieve new state-of-the-art performances. Specifically, our model achieves 16.1% relative improvement on ActivityNet paragraph-to-video retrieval task and 2.4% on How2QA task, respectively. We release our code, dataset, and pre-trained models at https://github.com/microsoft/XPretrain.
",https://api.openreview.net/pdf/023253eaf12b20cd205bc854b4f8c504a41ca47a.pdf,graph;transformer;representation;contrastive learning;multimodal;llm,https://scholar.google.com/scholar?q=Long-Form+Video-Language+Pre-Training+with+Multimodal+Temporal+Contrastive+Learning
Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching,2022,NIPS,"['Byoungjip Kim', 'Sungik Choi', 'Dasol Hwang', 'Moontae Lee', 'Honglak Lee']",poster,"['Multimodal Representation Learning', 'Transfer Learning', 'CLIP']","Despite surprising performance on zero-shot transfer, pre-training a large-scale multimodal model is often prohibitive as it requires a huge amount of data and computing resources. In this paper, we propose a method (BeamCLIP) that can effectively transfer the representations of a large pre-trained multimodal model (CLIP-ViT) into a small target model (e.g., ResNet-18). For unsupervised transfer, we introduce cross-modal similarity matching (CSM) that enables a student model to learn the representations of a teacher model by matching the relative similarity distribution across text prompt embeddings. To better encode the text prompts, we design context-based prompt augmentation (CPA) that can alleviate the lexical ambiguity of input text prompts. Our experiments show that unsupervised representation transfer of a pre-trained vision-language model enables a small ResNet-18 to achieve a better ImageNet-1K top-1 linear probe accuracy (66.2%) than vision-only self-supervised learning (SSL) methods (e.g., SimCLR: 51.8%, SwAV: 63.7%), while closing the gap with supervised learning (69.8%).",https://api.openreview.net/pdf/c18eed48d618926e942b2df6833e215809d19706.pdf,zero_few-shot;representation;transfer learning;augmentation;multimodal;llm,https://scholar.google.com/scholar?q=Transferring+Pre-trained+Multimodal+Representations+with+Cross-modal+Similarity+Matching
Understanding Cross-Domain Few-Shot Learning Based on Domain Similarity and Few-Shot Difficulty,2022,NIPS,"['Jaehoon Oh', 'Sungnyun Kim', 'Namgyu Ho', 'Jin-Hwa Kim', 'Hwanjun Song', 'Se-Young Yun']",poster,"['Cross-domain Few-shot Learning', 'Pre-training', 'Domain Similarity', 'Few-Shot Difficulty']","Cross-domain few-shot learning (CD-FSL) has drawn increasing attention for handling large differences between the source and target domains--an important concern in real-world scenarios. To overcome these large differences, recent works have considered exploiting small-scale unlabeled data from the target domain during the pre-training stage. This data enables self-supervised pre-training on the target domain, in addition to supervised pre-training on the source domain. In this paper, we empirically investigate which pre-training is preferred based on domain similarity and few-shot difficulty of the target domain. We discover that the performance gain of self-supervised pre-training over supervised pre-training becomes large when the target domain is dissimilar to the source domain, or the target domain itself has low few-shot difficulty. We further design two pre-training schemes, mixed-supervised and two-stage learning, that improve performance. In this light, we present six findings for CD-FSL, which are supported by extensive experiments and analyses on three source and eight target benchmark datasets with varying levels of domain similarity and few-shot difficulty. Our code is available at https://github.com/sungnyun/understanding-cdfsl.",https://api.openreview.net/pdf/886eb202a893deb903a760976e524c86a33b626c.pdf,zero_few-shot;transformer;multimodal,https://scholar.google.com/scholar?q=Understanding+Cross-Domain+Few-Shot+Learning+Based+on+Domain+Similarity+and+Few-Shot+Difficulty
Cooperative Distribution Alignment via JSD Upper Bound,2022,NIPS,"['Wonwoong Cho', 'Ziyu Gong', 'David I. Inouye']",poster,"['Unsupervised dataset alignment', 'Invertible flows']","Unsupervised distribution alignment estimates a transformation that maps two or more source distributions to a shared aligned distribution given only samples from each distribution. This task has many applications including generative modeling, unsupervised domain adaptation, and socially aware learning. Most prior works use adversarial learning (i.e., min-max optimization), which can be challenging to optimize and evaluate. A few recent works explore non-adversarial flow-based (i.e., invertible) approaches, but they lack a unified perspective and are limited in efficiently aligning multiple distributions. Therefore, we propose to unify and generalize previous flow-based approaches under a single non-adversarial framework, which we prove is equivalent to minimizing an upper bound on the Jensen-Shannon Divergence (JSD). Importantly, our problem reduces to a min-min, i.e., cooperative, problem and can provide a natural evaluation metric for unsupervised distribution alignment. We show empirical results on both simulated and real-world datasets to demonstrate the benefits of our approach. Code is available at https://github.com/inouye-lab/alignment-upper-bound.",https://api.openreview.net/pdf/7c6bb62230ddb92c1460660439e61ce4b4cda901.pdf,graph;optimization;zero_few-shot;generative model;metric;flow;multimodal,https://scholar.google.com/scholar?q=Cooperative+Distribution+Alignment+via+JSD+Upper+Bound
Non-Linguistic Supervision for Contrastive Learning of Sentence Embeddings,2022,NIPS,"['Yiren Jian', 'Chongyang Gao', 'Soroush Vosoughi']",poster,[],"Semantic representation learning for sentences is an important and well-studied problem in NLP. The current trend for this task involves training a Transformer-based sentence encoder through a contrastive objective with text, i.e., clustering sentences with semantically similar meanings and scattering others. In this work, we find the performance of Transformer models as sentence encoders can be improved by training with multi-modal multi-task losses, using unpaired examples from another modality (e.g., sentences and unrelated image/audio data). In particular, besides learning by the contrastive loss on text, our model clusters examples from a non-linguistic domain (e.g., visual/audio) with a similar contrastive loss at the same time.  The reliance of our framework on unpaired non-linguistic data makes it language-agnostic, enabling it to be widely applicable beyond English NLP. Experiments on 7 semantic textual similarity benchmarks reveal that models trained with the additional non-linguistic (images/audio) contrastive objective lead to higher quality sentence embeddings. This indicates that Transformer models are able to generalize better by doing a similar task (i.e., clustering) with \textit{unpaired} examples from different modalities in a multi-task fashion. The code is available at https://github.com/yiren-jian/NonLing-CSE.",https://api.openreview.net/pdf/55a49f429c1821974530944386fae8818d5aec85.pdf,transformer;representation;contrastive learning;multi-task;multimodal,https://scholar.google.com/scholar?q=Non-Linguistic+Supervision+for+Contrastive+Learning+of+Sentence+Embeddings
Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits,2022,NIPS,"['Ruibo Liu', 'Chenyan Jia', 'Ge Zhang', 'Ziyu Zhuang', 'Tony X Liu', 'Soroush Vosoughi']",poster,"['human values', 'ai safety', 'alignment', 'social impact', 'human-AI interaction']","We present Second Thoughts, a new learning paradigm that enables language models (LMs) to re-align with human values. By modeling the chain-of-edits between value-unaligned and value-aligned text, with LM fine-tuning and additional refinement through reinforcement learning, Second Thoughts not only achieves superior performance in three value alignment benchmark datasets but also shows strong human-value transfer learning ability in few-shot scenarios. The generated editing steps also offer better interpretability and ease for interactive error correction. Extensive human evaluations further confirm its effectiveness.",https://api.openreview.net/pdf/96e3562e0c0cb77fcdbdd839c51444298c55ef7c.pdf,reinforcement learning;transfer learning;active learning;multimodal,https://scholar.google.com/scholar?q=Second+Thoughts+are+Best:+Learning+to+Re-Align+With+Human+Values+from+Text+Edits
ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings,2022,NIPS,"['Arjun Majumdar', 'Gunjan Aggarwal', 'Bhavika Suresh Devnani', 'Judy Hoffman', 'Dhruv Batra']",poster,"['Object-Goal Navigation', 'Visual Navigation', 'Embodied AI']","We present a scalable approach for learning open-world object-goal navigation (ObjectNav) – the task of asking a virtual robot (agent) to find any instance of an object in an unexplored environment (e.g., “find a sink”). Our approach is entirely zero-shot – i.e., it does not require ObjectNav rewards or demonstrations of any kind. Instead, we train on the image-goal navigation (ImageNav) task, in which agents find the location where a picture (i.e., goal image) was captured. Specifically, we encode goal images into a multimodal, semantic embedding space to enable training semantic-goal navigation (SemanticNav) agents at scale in unannotated 3D environments (e.g., HM3D). After training, SemanticNav agents can be instructed to find objects described in free-form natural language (e.g., “sink,” “bathroom sink,” etc.) by projecting language goals into the same multimodal, semantic embedding space. As a result, our approach enables open-world ObjectNav. We extensively evaluate our agents on three ObjectNav datasets (Gibson, HM3D, and MP3D) and observe absolute improvements in success of 4.2% - 20.0% over existing zero-shot methods. For reference, these gains are similar or better than the 5% improvement in success between the Habitat 2020 and 2021 ObjectNav challenge winners. In an open-world setting, we discover that our agents can generalize to compound instructions with a room explicitly mentioned (e.g., “Find a kitchen sink”) and when the target room can be inferred (e.g., “Find a sink and a stove”).",https://api.openreview.net/pdf/ec48cc9dada127f18689a7d49dd78af4e3d02e0c.pdf,reinforcement learning;zero_few-shot;multimodal;3d,https://scholar.google.com/scholar?q=ZSON:+Zero-Shot+Object-Goal+Navigation+using+Multimodal+Goal+Embeddings
Continuously Tempered PDMP samplers,2022,NIPS,"['Matthew Sutton', 'Robert Salomone', 'Augustin Chevallier', 'Paul Fearnhead']",poster,"['PDMP', 'Zig-Zag', 'tempering', 'MCMC', 'Monte Carlo', 'Markov Chain', 'Probabilistic Inference']","New sampling algorithms based on simulating continuous-time stochastic processes called piece-wise deterministic Markov processes (PDMPs) have shown considerable promise. However, these methods can struggle to sample from multi-modal or heavy-tailed distributions. We show how tempering ideas can improve the mixing of PDMPs in such cases. We introduce an extended distribution defined over the state of the posterior distribution and an inverse temperature, which interpolates between a tractable distribution when the inverse temperature is 0 and the posterior when the inverse temperature is 1. The marginal distribution of the inverse temperature is a mixture of a continuous distribution on $[0,1)$ and a point mass at 1: which means that we obtain samples when the inverse temperature is 1, and these are draws from the posterior, but sampling algorithms will also explore distributions at lower temperatures which will improve mixing. We show how PDMPs, and particularly the Zig-Zag sampler, can be implemented to sample from such an extended distribution. The resulting algorithm is easy to implement and we show empirically that it can outperform existing PDMP-based samplers on challenging multimodal posteriors.",https://api.openreview.net/pdf/9cd93795b08a28a6c186bd157b0608e4fbfa57a6.pdf,graph;zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Continuously+Tempered+PDMP+samplers
Scaling Multimodal Pre-Training via Cross-Modality Gradient Harmonization,2022,NIPS,"['Junru Wu', 'Yi Liang', 'feng han', 'Hassan Akbari', 'Zhangyang Wang', 'Cong Yu']",poster,"['Cross-Modality Alignment', 'Multimodal Pre-Training', 'Modality-agnostic']","Self-supervised pre-training recently demonstrates success on large-scale multimodal data, and state-of-the-art contrastive learning methods often enforce the feature consistency from cross-modality inputs, such as video/audio or video/text pairs. Despite its convenience to formulate and leverage in practice, such cross-modality alignment (CMA) is only a weak and noisy supervision, since two modalities can be semantically misaligned even they are temporally aligned. For example, even in the (often adopted) instructional videos, a speaker can sometimes refer to something that is not visually present in the current frame; and the semantic misalignment would only be more unpredictable for the raw videos collected from unconstrained internet sources. We conjecture that might cause conflicts and biases among modalities, and may hence prohibit CMA from scaling up to training with larger and more heterogeneous data. This paper first verifies our conjecture by observing that, even in the latest VATT pre-training using only narrated videos, there exist strong gradient conflicts between different CMA losses within the same sample triplet (video, audio, text), indicating them as the noisy source of supervision. We then propose to harmonize such gradients during pre-training, via two techniques: (i) cross-modality gradient realignment: modifying different CMA loss gradients for one sample triplet, so that their gradient directions are in more agreement; and (ii) gradient-based curriculum learning: leveraging the gradient conflict information on an indicator of sample noisiness, to develop a curriculum learning strategy to prioritize training with less noisy sample triplets. Applying those gradient harmonization techniques to pre-training VATT on the HowTo100M dataset, we consistently improve its performance on different downstream tasks. Moreover, we are able to scale VATT pre-training to more complicated non-narrative Youtube8M dataset to further improve the state-of-the-arts.",https://api.openreview.net/pdf/7e22c63d6dd3510e2ba88b42453a00ff156b2840.pdf,graph;optimization;zero_few-shot;transformer;contrastive learning;multimodal;curriculum learning,https://scholar.google.com/scholar?q=Scaling+Multimodal+Pre-Training+via+Cross-Modality+Gradient+Harmonization
Triangulation candidates for Bayesian optimization,2022,NIPS,"['Robert B. Gramacy', 'Annie Sauer', 'Nathan Wycoff']",poster,"['surrogate modeling', 'Gaussian process', 'active learning', 'sequential design', 'space-filling design', 'Delaunay triangulation', 'convex hull']","Bayesian optimization involves ""inner optimization"" over a new-data acquisition criterion which is non-convex/highly multi-modal, may be non-differentiable, or may otherwise thwart local numerical optimizers.  In such cases it is common to replace continuous search with a discrete one over random candidates.  Here we propose using candidates based on a Delaunay triangulation of the existing input design.  We detail the construction of these ""tricands"" and demonstrate empirically how they outperform both numerically optimized acquisitions and random candidate-based alternatives, and are well-suited for hybrid schemes, on benchmark synthetic and real simulation experiments.",https://api.openreview.net/pdf/edf2505001e331620f017278e67e8b00103872df.pdf,optimization;bayesian;multimodal,https://scholar.google.com/scholar?q=Triangulation+candidates+for+Bayesian+optimization
AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments,2022,NIPS,"['Sudipta Paul', 'Amit Roy-Chowdhury', 'Anoop Cherian']",poster,"['audio-visual navigation', 'hierarchical policy learning', 'vision and language navigation']","Recent years have seen embodied visual navigation advance in two distinct directions: (i) in equipping the AI agent to follow natural language instructions, and (ii) in making the navigable world multimodal, e.g., audio-visual navigation. However, the real world is not only multimodal, but also often complex, and thus in spite of these advances, agents still need to understand the uncertainty in their actions and seek instructions to navigate. To this end, we present AVLEN -- an interactive agent for Audio-Visual-Language Embodied Navigation. Similar to audio-visual navigation tasks, the goal of our embodied agent is to localize an audio event via navigating the 3D visual world; however, the agent may also seek help from a human (oracle), where the assistance is provided in free-form natural language. To realize these abilities, AVLEN uses a multimodal hierarchical reinforcement learning backbone that learns: (a) high-level policies to choose either audio-cues for navigation or to query the oracle, and (b) lower-level policies to select navigation actions based on its audio-visual and language inputs. The policies are trained via rewarding for the success on the navigation task while minimizing the number of queries to the oracle. To empirically evaluate AVLEN, we present experiments on the SoundSpaces framework for semantic audio-visual navigation tasks. Our results show that equipping the agent to ask for help leads to a clear improvement in performances, especially in challenging cases, e.g., when the sound is unheard during training or in the presence of distractor sounds.",https://api.openreview.net/pdf/788ecf9280379588419f5ac48526f2ab66dc888b.pdf,reinforcement learning;graph;zero_few-shot;active learning;multimodal;3d,https://scholar.google.com/scholar?q=AVLEN:+Audio-Visual-Language+Embodied+Navigation+in+3D+Environments
CyCLIP: Cyclic Contrastive Language-Image Pretraining,2022,NIPS,"['Shashank Goel', 'Hritik Bansal', 'Sumit Bhatia', 'Ryan A. Rossi', 'Vishwa Vinay', 'Aditya Grover']",poster,"['CLIP', 'Contrastive', 'Language-Image Pretraining', 'Multimodal Learning', 'Representation Learning', 'Cyclic Consistency', 'Zero-shot transfer', 'Robustness']","Recent advances in contrastive representation learning over paired image-text data have led to models such as CLIP that achieve state-of-the-art performance for zero-shot classification and distributional robustness. Such models typically require joint reasoning in the image and text representation spaces for downstream inference tasks. Contrary to prior beliefs, we demonstrate that the image and text representations learned via a standard contrastive objective are not interchangeable and can lead to inconsistent downstream predictions. To mitigate this issue, we formalize consistency and propose CyCLIP, a framework for contrastive representation learning that explicitly optimizes for the learned representations to be geometrically consistent in the image and text space. In particular, we show that consistent representations can be learned by explicitly symmetrizing (a) the similarity between the two mismatched image-text pairs (cross-modal consistency); and (b) the similarity between the image-image pair and the text-text pair (in-modal consistency). Empirically, we show that the improved consistency in CyCLIP translates to significant gains over CLIP, with gains ranging from 10%-24% for zero-shot classification on standard benchmarks (CIFAR-10, CIFAR-100, ImageNet1K) and 10%-27% for robustness to various natural distribution shifts.",https://api.openreview.net/pdf/1ef5d3a419aeebc43d8f4a8a890ffc06463c1c8d.pdf,graph;zero_few-shot;representation;contrastive learning;inference;metric;multimodal,https://scholar.google.com/scholar?q=CyCLIP:+Cyclic+Contrastive+Language-Image+Pretraining
Adaptive Distribution Calibration for Few-Shot Learning with Hierarchical Optimal Transport,2022,NIPS,"['Dan dan Guo', 'Long Tian', 'He Zhao', 'Mingyuan Zhou', 'Hongyuan Zha']",poster,[],"Few-shot classification aims to learn a classifier to recognize unseen classes during training, where the learned model can easily become over-fitted based on the biased distribution formed by only a few training examples. A recent solution to this problem is calibrating the distribution of these few sample classes by transferring statistics from the base classes with sufficient examples, where how to decide the transfer weights from base classes to novel classes is the key. However, principled approaches for learning the transfer weights have not been carefully studied. To this end, we propose a novel distribution calibration method by learning the adaptive weight matrix between novel samples and base classes, which is built upon a hierarchical Optimal Transport (H-OT) framework. By minimizing the high-level OT distance between novel samples and base classes, we can view the learned transport plan as the adaptive weight information for transferring the statistics of base classes. The learning of the cost function between a base class and novel class in the high-level OT leads to the introduction of the low-level OT, which considers the weights of all the data samples in the base class. Experimental results on standard benchmarks demonstrate that our proposed plug-and-play model outperforms competing approaches and owns desired cross-domain generalization ability, indicating the effectiveness of the learned adaptive weights.",https://api.openreview.net/pdf/2bf06e4515fb5fe867fef96f651b8050bcd4f564.pdf,zero_few-shot;adaptive;transfer learning;multimodal,https://scholar.google.com/scholar?q=Adaptive+Distribution+Calibration+for+Few-Shot+Learning+with+Hierarchical+Optimal+Transport
Flamingo: a Visual Language Model for Few-Shot Learning,2022,NIPS,"['Jean-Baptiste Alayrac', 'Jeff Donahue', 'Pauline Luc', 'Antoine Miech', 'Iain Barr', 'Yana Hasson', 'Karel Lenc', 'Arthur Mensch', 'Katherine Millican', 'Malcolm Reynolds', 'Roman Ring', 'Eliza Rutherford', 'Serkan Cabi', 'Tengda Han', 'Zhitao Gong', 'Sina Samangooei', 'Marianne Monteiro', 'Jacob Menick', 'Sebastian Borgeaud', 'Andrew Brock', 'Aida Nematzadeh', 'Sahand Sharifzadeh', 'Mikolaj Binkowski', 'Ricardo Barreira', 'Oriol Vinyals', 'Andrew Zisserman', 'Karen Simonyan']",poster,[],"Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer, captioning tasks, which evaluate the ability to describe a scene or an event, and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",https://api.openreview.net/pdf/4e2c7c5fc1974f94f48ead8ee5c9b6f51323e912.pdf,zero_few-shot;multimodal;llm,https://scholar.google.com/scholar?q=Flamingo:+a+Visual+Language+Model+for+Few-Shot+Learning
Bridge the Gap Between Architecture Spaces via A Cross-Domain Predictor,2022,NIPS,"['Yuqiao Liu', 'Yehui Tang', 'Zeqiong Lv', 'Yunhe Wang', 'Yanan Sun']",poster,"['neural architecture search', 'neural predictor', 'domain adaptation']","Neural Architecture Search (NAS) can automatically design promising neural architectures without artificial experience. Though it achieves great success, prohibitively high search cost is required to find a high-performance architecture, which blocks its practical implementation. Neural predictor can directly evaluate the performance of neural networks based on their architectures and thereby save much budget. However, existing neural predictors require substantial annotated architectures trained from scratch, which still consume many computational resources. To solve this issue, we propose a Cross-Domain Predictor (CDP), which is trained based on the existing NAS benchmark datasets (e.g., NAS-Bench-101), but can be used to find high-performance architectures in large-scale search spaces. Particularly, we propose a progressive subspace adaptation strategy to address the domain discrepancy between the source architecture space and the target space. Considering the large difference between two architecture spaces, an assistant space is developed to smooth the transfer process. Compared with existing NAS methods, the proposed CDP is much more efficient. For example, CDP only requires the search cost of 0.1 GPU Days to find architectures with 76.9% top-1 accuracy on ImageNet and 97.51% on CIFAR-10. ",https://api.openreview.net/pdf/2bfd48beb96f454f9d9d48a73bb63f74cb9ea869.pdf,transfer learning;multimodal,https://scholar.google.com/scholar?q=Bridge+the+Gap+Between+Architecture+Spaces+via+A+Cross-Domain+Predictor
A Differentiable Semantic Metric Approximation in Probabilistic Embedding for Cross-Modal Retrieval,2022,NIPS,"['Hao Li', 'Jingkuan Song', 'Lianli Gao', 'Pengpeng Zeng', 'Haonan Zhang', 'Gongfu Li']",poster,"['cross-modal retrieval', 'probabilistic embedding', 'image-text matching', 'multiplicity', 'metric learning', 'robust']","Cross-modal retrieval aims to build correspondence between multiple modalities by learning a common representation space. Typically, an image can match multiple texts semantically and vice versa, which significantly increases the difficulty of this task. To address this problem, probabilistic embedding is proposed to quantify these many-to-many relationships. However, existing datasets (e.g., MS-COCO) and metrics (e.g., Recall@K) cannot fully represent these diversity correspondences due to non-exhaustive annotations. Based on this observation, we utilize semantic correlation computed by CIDEr to find the potential correspondences. Then we present an effective metric, named Average Semantic Precision (ASP), which can measure the ranking precision of semantic correlation for retrieval sets. Additionally, we introduce a novel and concise objective, coined Differentiable ASP Approximation (DAA). Concretely, DAA can optimize ASP directly by making the ranking function of ASP differentiable through a sigmoid function. To verify the effectiveness of our approach, extensive experiments are conducted on MS-COCO, CUB Captions, and Flickr30K, which are commonly used in cross-modal retrieval. The results show that our approach obtains superior performance over the state-of-the-art approaches on all metrics. The code and trained models are released at https://github.com/leolee99/2022-NeurIPS-DAA.",https://api.openreview.net/pdf/86047d1e5ba8583b20d662b125e871ecd883a0c7.pdf,representation;metric;multimodal,https://scholar.google.com/scholar?q=A+Differentiable+Semantic+Metric+Approximation+in+Probabilistic+Embedding+for+Cross-Modal+Retrieval
FIRE: Semantic Field of Words Represented as Non-Linear Functions,2022,NIPS,"['Xin Du', 'Kumiko Tanaka-Ishii']",poster,"['natural language', 'nonlinear word representation', 'field representation', 'word polysemy', 'semantic compositionality']","State-of-the-art word embeddings presume a linear vector space, but this approach does not easily incorporate the nonlinearity that is necessary to represent polysemy. We thus propose a novel semantic FIeld REepresentation, called FIRE, which is a $D$-dimensional field in which every word is represented as a set of its locations and a nonlinear function covering the field. The strength of a word's relation to another word at a certain location is measured as the function value at that location. With FIRE, compositionality is represented via functional additivity, whereas polysemy is represented via the set of points and the function's multimodality. By implementing FIRE for English and comparing it with previous representation methods via word and sentence similarity tasks, we show that FIRE produces comparable or even better results. In an evaluation of polysemy to predict the number of word senses, FIRE greatly outperformed BERT and Word2vec, providing evidence of how FIRE represents polysemy. The code is available at https://github.com/kduxin/firelang.",https://api.openreview.net/pdf/8c6670b75022b9d867b437a95087445951211a86.pdf,transformer;representation;online learning;multimodal;llm,https://scholar.google.com/scholar?q=FIRE:+Semantic+Field+of+Words+Represented+as+Non-Linear+Functions
Why do We Need Large Batchsizes in Contrastive Learning? A Gradient-Bias Perspective,2022,NIPS,"['Changyou Chen', 'Jianyi Zhang', 'Yi Xu', 'Liqun Chen', 'Jiali Duan', 'Yiran Chen', 'Son Dinh Tran', 'Belinda Zeng', 'Trishul Chilimbi']",poster,"['Bayesian data augmentation', 'contrastive learning', 'representation learning']","Contrastive learning (CL) has been the de facto technique for self-supervised representation learning (SSL), with impressive empirical success such as multi-modal representation learning. However, traditional CL loss only considers negative samples from a minibatch, which could cause biased gradients due to the non-decomposibility of the loss. For the first time, we consider optimizing a more generalized contrastive loss, where each data sample is associated with an infinite number of negative samples. We show that directly using minibatch stochastic optimization could lead to gradient bias. To remedy this, we propose an efficient Bayesian data augmentation technique to augment the contrastive loss into a decomposable one, where standard stochastic optimization can be directly applied without gradient bias. Specifically, our augmented loss defines a joint distribution over the model parameters and the augmented parameters, which can be conveniently optimized by a proposed stochastic expectation-maximization algorithm. Our framework is more general and is related to several popular SSL algorithms. We verify our framework on both small scale models and several large foundation models, including SSL of ImageNet and SSL for vision-language representation learning. Experiment results indicate the existence of gradient bias in all cases, and demonstrate the effectiveness of the proposed method on improving previous state of the arts. Remarkably, our method can outperform the strong MoCo-v3 under the same hyper-parameter setting with only around half of the minibatch size; and also obtains strong results in the recent public benchmark ELEVATER for few-shot image classification. ",https://api.openreview.net/pdf/aac37cd8b32d008759feeba077aa197d2513ceea.pdf,optimization;zero_few-shot;representation;bayesian;contrastive learning;augmentation;multimodal,https://scholar.google.com/scholar?q=Why+do+We+Need+Large+Batchsizes+in+Contrastive+Learning?+A+Gradient-Bias+Perspective
When to Trust Your Simulator: Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning,2022,NIPS,"['Haoyi Niu', 'Shubham Sharma', 'Yiwen Qiu', 'Ming Li', 'Guyue Zhou', 'Jianming HU', 'Xianyuan Zhan']",poster,[],"Learning effective reinforcement learning (RL) policies to solve real-world complex tasks can be quite challenging without a high-fidelity simulation environment. In most cases, we are only given imperfect simulators with simplified dynamics, which inevitably lead to severe sim-to-real gaps in RL policy learning. The recently emerged field of offline RL provides another possibility to learn policies directly from pre-collected historical data. However, to achieve reasonable performance, existing offline RL algorithms need impractically large offline data with sufficient state-action space coverage for training. This brings up a new question: is it possible to combine learning from limited real data in offline RL and unrestricted exploration through imperfect simulators in online RL to address the drawbacks of both approaches? In this study, we propose the Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning (H2O) framework to provide an affirmative answer to this question. H2O introduces a dynamics-aware policy evaluation scheme, which adaptively penalizes the Q function learning on simulated state-action pairs with large dynamics gaps, while also simultaneously allowing learning from a fixed real-world dataset. Through extensive simulation and real-world tasks, as well as theoretical analysis, we demonstrate the superior performance of H2O against other cross-domain online and offline RL algorithms. H2O provides a brand new hybrid offline-and-online RL paradigm, which can potentially shed light on future RL algorithm design for solving practical real-world tasks.",https://api.openreview.net/pdf/58135669d26ec94b6efa9f2cbe6a7e4e8c9f3e34.pdf,reinforcement learning;offline reinforcement learning;graph;zero_few-shot;online learning;adaptive;multimodal;llm,https://scholar.google.com/scholar?q=When+to+Trust+Your+Simulator:+Dynamics-Aware+Hybrid+Offline-and-Online+Reinforcement+Learning
A Closer Look at the Adversarial Robustness of Deep Equilibrium Models,2022,NIPS,"['Zonghan Yang', 'Tianyu Pang', 'Yang Liu']",poster,[],"Deep equilibrium models (DEQs) refrain from the traditional layer-stacking paradigm and turn to find the fixed point of a single layer. DEQs have achieved promising performance on different applications with featured memory efficiency. At the same time, the adversarial vulnerability of DEQs raises concerns. Several works propose to certify robustness for monotone DEQs. However, limited efforts are devoted to studying empirical robustness for general DEQs. To this end, we observe that an adversarially trained DEQ requires more forward steps to arrive at the equilibrium state, or even violates its fixed-point structure. Besides, the forward and backward tracks of DEQs are misaligned due to the black-box solvers. These facts cause gradient obfuscation when applying the ready-made attacks to evaluate or adversarially train DEQs. Given this, we develop approaches to estimate the intermediate gradients of DEQs and integrate them into the attacking pipelines. Our approaches facilitate fully white-box evaluations and lead to effective adversarial defense for DEQs. Extensive experiments on CIFAR-10 validate the adversarial robustness of DEQs competitive with deep networks of similar sizes.",https://api.openreview.net/pdf/40f9c93ce28c6d9f311c28845daa95a34bab59a6.pdf,multimodal,https://scholar.google.com/scholar?q=A+Closer+Look+at+the+Adversarial+Robustness+of+Deep+Equilibrium+Models
Neural Collapse with Normalized Features: A Geometric Analysis over the Riemannian Manifold,2022,NIPS,"['Can Yaras', 'Peng Wang', 'Zhihui Zhu', 'Laura Balzano', 'Qing Qu']",poster,"['neural collapse', 'Riemannian manifold', 'feature normalization', 'nonconvex optimization']","When training overparameterized deep networks for classification tasks, it has been widely observed that the learned features exhibit a so-called ""neural collapse'"" phenomenon. More specifically, for the output features of the penultimate layer, for each class the within-class features converge to their means, and the means of different classes exhibit a certain tight frame structure, which is also aligned with the last layer's classifier. As feature normalization in the last layer becomes a common practice in modern representation learning, in this work we theoretically justify the neural collapse phenomenon under normalized features. Based on an unconstrained feature model, we simplify the empirical loss function in a multi-class classification task into a nonconvex optimization problem over the Riemannian manifold by constraining all features and classifiers over the sphere. In this context, we analyze the nonconvex landscape of the Riemannian optimization problem over the product of spheres, showing a benign global landscape in the sense that the only global minimizers are the neural collapse solutions while all other critical points are strict saddle points with negative curvature. Experimental results on practical deep networks corroborate our theory and demonstrate that better representations can be learned faster via feature normalization. Code for our experiments can be found at https://github.com/cjyaras/normalized-neural-collapse.",https://api.openreview.net/pdf/e49e5c2cf5823eda9d04975602147aa453691f0b.pdf,optimization;representation;metric;multimodal,https://scholar.google.com/scholar?q=Neural+Collapse+with+Normalized+Features:+A+Geometric+Analysis+over+the+Riemannian+Manifold
Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,2022,NIPS,"['Pan Lu', 'Swaroop Mishra', 'Tony Xia', 'Liang Qiu', 'Kai-Wei Chang', 'Song-Chun Zhu', 'Oyvind Tafjord', 'Peter Clark', 'Ashwin Kalyan']",poster,"['science question answering', 'multimodal reasoning', 'chain of thought']","When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https://scienceqa.github.io.",https://api.openreview.net/pdf/4ce5e8e5119e4445ef249b95def0241cc497e169.pdf,multimodal,https://scholar.google.com/scholar?q=Learn+to+Explain:+Multimodal+Reasoning+via+Thought+Chains+for+Science+Question+Answering
S-PIFu: Integrating Parametric Human Models with PIFu for Single-view Clothed Human Reconstruction,2022,NIPS,"['Kennard Chan', 'Guosheng Lin', 'Haiyu Zhao', 'Weisi Lin']",poster,"['Single-view clothed human reconstruction', 'parametric human body models', 'pixel-aligned implicit models']","We present three novel strategies to incorporate a parametric body model into a pixel-aligned implicit model for single-view clothed human reconstruction. Firstly, we introduce ray-based sampling, a novel technique that transforms a parametric model into a set of highly informative, pixel-aligned 2D feature maps. Next, we propose a new type of feature based on blendweights. Blendweight-based labels serve as soft human parsing labels and help to improve the structural fidelity of reconstructed meshes. Finally, we show how we can extract and capitalize on body part orientation information from a parametric model to further improve reconstruction quality. Together, these three techniques form our S-PIFu framework, which significantly outperforms state-of-the-arts methods in all metrics. Our code is available at https://github.com/kcyt/SPIFu.",https://api.openreview.net/pdf/b1aea9270db529344582cb0026573d2b91a0fb0c.pdf,metric;multimodal,https://scholar.google.com/scholar?q=S-PIFu:+Integrating+Parametric+Human+Models+with+PIFu+for+Single-view+Clothed+Human+Reconstruction
Target alignment in truncated kernel ridge regression,2022,NIPS,"['Arash A Amini', 'richard baumgartner', 'Dai Feng']",poster,"['kernel methods', 'target alignment', 'minimax rates', 'double descent']","Kernel ridge regression (KRR) has recently attracted renewed interest due to its potential for explaining the transient effects, such as double descent, that emerge during neural network training. In this work, we study how the alignment between the target function and the kernel affects the performance of the KRR. We focus on the truncated KRR (TKRR) which utilizes an additional parameter that controls the spectral truncation of the kernel matrix. We show that for polynomial alignment, there is an over-aligned regime, in which TKRR can achieve a faster rate than what is achievable by full KRR. The rate of TKRR can improve all the way to the parametric rate, while that of full KRR is capped at a sub-optimal value. This shows that target alignemnt can be better leveraged by utilizing spectral truncation in kernel methods. We also consider the bandlimited alignment setting and show that the regularization surface of TKRR can exhibit transient effects including multiple descent and non-monotonic behavior. Our results show that there is a strong and quantifable relation between the shape of the alignment spectrum and the generalization performance of kernel methods, both in terms of rates and in finite samples.
",https://api.openreview.net/pdf/d803b372e6deac58224777b631eacc625df7f747.pdf,metric;multimodal,https://scholar.google.com/scholar?q=Target+alignment+in+truncated+kernel+ridge+regression
UniCLIP: Unified Framework for Contrastive Language-Image Pre-training,2022,NIPS,"['Janghyeon Lee', 'Jongsuk Kim', 'Hyounguk Shon', 'Bumsoo Kim', 'Seung Hwan Kim', 'Honglak Lee', 'Junmo Kim']",poster,"['Contrastive Learning', 'Vision-Language Pre-training', 'Self-Supervised Learning']","Pre-training vision-language models with contrastive objectives has shown promising results that are both scalable to large uncurated datasets and transferable to many downstream applications. Some following works have targeted to improve data efficiency by adding self-supervision terms, but inter-domain (image-text) contrastive loss and intra-domain (image-image) contrastive loss are defined on individual spaces in those works, so many feasible combinations of supervision are overlooked. To overcome this issue, we propose UniCLIP, a Unified framework for Contrastive Language-Image Pre-training. UniCLIP integrates the contrastive loss of both inter-domain pairs and intra-domain pairs into a single universal space. The discrepancies that occur when integrating contrastive loss between different domains are resolved by the three key components of UniCLIP: (1) augmentation-aware feature embedding, (2) MP-NCE loss, and (3) domain dependent similarity measure. UniCLIP outperforms previous vision-language pre-training methods on various single- and multi-modality downstream tasks. In our experiments, we show that each component that comprises UniCLIP contributes well to the final performance.",https://api.openreview.net/pdf/a54610ad801bf0124740a966c6fca3067912febd.pdf,zero_few-shot;contrastive learning;transfer learning;augmentation;multimodal;self-supervision,https://scholar.google.com/scholar?q=UniCLIP:+Unified+Framework+for+Contrastive+Language-Image+Pre-training
u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality,2022,NIPS,"['Wei-Ning Hsu', 'Bowen Shi']",poster,"['multimodal speech', 'audio-visual speech', 'multimodal self-supervised learning', 'zero-shot', 'speech recognition', 'speech translation']","While audio-visual speech models can yield superior performance and robustness compared to audio-only models, their development and adoption are hindered by the lack of labeled and unlabeled audio-visual data and the cost to deploy one model per modality. In this paper, we present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, we demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, our model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks. In particular, our single model yields 1.2%/1.4%/27.2% speech recognition word error rate on LRS3 with audio-visual/audio/visual input.",https://api.openreview.net/pdf/2da196ac6b51052642bc97c50245aa674c3f9575.pdf,zero_few-shot;transformer;transfer learning;multimodal,https://scholar.google.com/scholar?q=u-HuBERT:+Unified+Mixed-Modal+Speech+Pretraining+And+Zero-Shot+Transfer+to+Unlabeled+Modality
ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning,2022,NIPS,"['Junting Pan', 'Ziyi Lin', 'Xiatian Zhu', 'Jing Shao', 'Hongsheng Li']",poster,"['parameter-efficient transfer learning', 'video recognition', 'adapters']","Capitalizing on large pre-trained models for various downstream tasks of interest have recently emerged with promising performance. Due to the ever-growing model size, the standard full fine-tuning based task adaptation strategy becomes prohibitively costly in terms of model training and storage. This has led to a new research direction in parameter-efficient transfer learning. However, existing attempts typically focus on downstream tasks from the same modality (e.g., image understanding) of the pre-trained model. This creates a limit because in some specific modalities, (e.g., video understanding) such a strong pre-trained model with sufficient knowledge is less or not available. In this work, we investigate such a novel cross-modality transfer learning setting, namely parameter-efficient image-to-video transfer learning. To solve this problem, we propose a new Spatio-Temporal Adapter (ST-Adapter) for parameter-efficient fine-tuning per video task. With a built-in spatio-temporal reasoning capability in a compact design, ST-Adapter enables a pre-trained image model without temporal knowledge to reason about dynamic video content at a small ~8% per-task parameter cost, requiring approximately 20 times fewer updated parameters compared to previous work. Extensive experiments on video action recognition tasks show that our ST-Adapter can match or even outperform the strong full fine-tuning strategy and state-of-the-art video models, whilst enjoying the advantage of parameter efficiency.",https://api.openreview.net/pdf/863e470f9a2235acb785d445d014f849d6fd7e22.pdf,transfer learning;multimodal,https://scholar.google.com/scholar?q=ST-Adapter:+Parameter-Efficient+Image-to-Video+Transfer+Learning
Towards Diverse and Faithful One-shot Adaption of Generative Adversarial Networks,2022,NIPS,"['Yabo Zhang', 'mingshuai Yao', 'Yuxiang Wei', 'Zhilong Ji', 'Jinfeng Bai', 'Wangmeng Zuo']",poster,"['StyleGAN', 'Domain Adaption', 'One-shot', 'CLIP']","One-shot generative domain adaption aims to transfer a pre-trained generator on one domain to a new domain using one reference image only. However, it remains very challenging for the adapted generator (i) to generate diverse images inherited from the pre-trained generator while (ii) faithfully acquiring the domain-specific attributes and styles of the reference image. In this paper, we present a novel one-shot generative domain adaption method, i.e., DiFa, for diverse generation and faithful adaptation. For global-level adaptation, we leverage the difference between the CLIP embedding of the reference image and the mean embedding of source images to constrain the target generator. For local-level adaptation, we introduce an attentive style loss which aligns each intermediate token of an adapted image with its corresponding token of the reference image. To facilitate diverse generation, selective cross-domain consistency is introduced to select and retain domain-sharing attributes in the editing latent $\mathcal{W}+$ space to inherit the diversity of the pre-trained generator. Extensive experiments show that our method outperforms the state-of-the-arts both quantitatively and qualitatively, especially for the cases of large domain gap. Moreover, our DiFa can easily be extended to zero-shot generative domain adaption with appealing results.",https://api.openreview.net/pdf/b423b85221790aee351b6fab0dd526b75b6a9778.pdf,graph;optimization;zero_few-shot;generative model;transfer learning;multimodal,https://scholar.google.com/scholar?q=Towards+Diverse+and+Faithful+One-shot+Adaption+of+Generative+Adversarial+Networks
Can Push-forward Generative Models Fit Multimodal Distributions?,2022,NIPS,"['Antoine Salmona', 'Valentin De Bortoli', 'Julie Delon', 'Agnès Desolneux']",poster,"['Generative Models', 'GAN', 'VAE', 'Diffusion Models', 'Score-based Models', 'Expressivity', 'Lipschitz Mappings']","Many generative models synthesize data by transforming a standard Gaussian random variable using a deterministic neural network. Among these models are the Variational Autoencoders and the Generative Adversarial Networks. In this work, we call them ""push-forward"" models and study their expressivity. We formally demonstrate that the Lipschitz constant of these generative networks has to be large in order to fit multimodal distributions. More precisely, we show that the total variation distance and the Kullback-Leibler divergence between the generated 
and the data distribution are bounded from below by a constant depending on the mode separation and the Lipschitz constant. Since constraining the Lipschitz constants of neural networks is a common way to stabilize generative models, there is a provable trade-off between the ability of push-forward models to approximate multimodal distributions and the stability of their training. We validate our findings on one-dimensional and image datasets and empirically show that the recently introduced diffusion models do not suffer of such limitation.",https://api.openreview.net/pdf/a1a6f024413fa9d8afd17e996b306215f206e7cb.pdf,optimization;zero_few-shot;vae;generative model;multimodal;diffusion models;llm,https://scholar.google.com/scholar?q=Can+Push-forward+Generative+Models+Fit+Multimodal+Distributions?
Towards Effective Multi-Modal Interchanges in Zero-Resource Sounding Object Localization,2022,NIPS,"['Yang Zhao', 'Chen Zhang', 'Haifeng Huang', 'Haoyuan Li', 'Zhou Zhao']",poster,"['sounding object localization', 'knowledge transfer', 'multi-modal', 'zero-resource learning']","Aiming to locate the object that emits a specified sound in complex scenes, the task of sounding object localization bridges two perception-oriented modalities of vision and acoustics, and brings enormous research value to the comprehensive perceptual understanding of machine intelligence. Although there are massive training data collected in this field, few of them contain accurate bounding box annotations, hindering the learning process and further application of proposed models. In order to address this problem, we try to explore an effective multi-modal knowledge transfer strategy to obtain precise knowledge from other similar tasks and transfer it through well-aligned multi-modal data to deal with this task in a zero-resource manner. Concretely, we design and propose a novel \textit{Two-stream Universal Referring localization Network} (TURN), which is composed of a localization stream and an alignment stream to carry out different functions. The former is utilized to extract the knowledge related to referring object localization from the image grounding task, while the latter is devised to learn a universal semantic space shared between texts and audios. Moreover, we further develop an adaptive sampling strategy to automatically identify the overlap between different data domains, thus boosting the performance and stability of our model. The extensive experiments on various publicly-available benchmarks demonstrate that TURN can achieve competitive performance compared with the state-of-the-art approaches without using any data in this field, which verifies the feasibility of our proposed mechanisms and strategies.  ",https://api.openreview.net/pdf/6157c82c109995534b68429b3a09a7647a9551ff.pdf,adaptive;transfer learning;multimodal,https://scholar.google.com/scholar?q=Towards+Effective+Multi-Modal+Interchanges+in+Zero-Resource+Sounding+Object+Localization
Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts,2022,NIPS,"['Basil Mustafa', 'Carlos Riquelme Ruiz', 'Joan Puigcerver', 'Rodolphe Jenatton', 'Neil Houlsby']",poster,"['machine learning', 'computer vision', 'natural language processing', 'multimodal', 'mixture of experts', 'conditional computation', 'sparse computation', 'contrastive learning', 'zeroshot classification']","Large sparsely-activated models have obtained excellent performance in multiple domains.
However, such models are typically trained on a single modality at a time.
We present the Language-Image MoE, LIMoE, a sparse mixture of experts model capable of multimodal learning.
LIMoE accepts both images and text simultaneously, while being trained using a contrastive loss.
MoEs are a natural fit for a multimodal backbone, since expert layers can learn an appropriate partitioning of modalities.
However, new challenges arise; in particular, training stability and balanced expert utilization, for which we propose an entropy-based regularization scheme.
Across multiple scales, we demonstrate performance improvement over dense models of equivalent computational cost.
LIMoE-L/16 trained comparably to CLIP-L/14 achieves 77.9% zero-shot ImageNet accuracy (vs. 76.2%), and when further scaled to H/14 (with additional data) it achieves 83.8%, approaching state-of-the-art methods which use custom per-modality backbones and pre-training schemes.
We analyse the quantitative and qualitative behavior of LIMoE, and demonstrate phenomena such as differing treatment of the modalities and the emergence of modality-specific experts.",https://api.openreview.net/pdf/636f5af51298445b1780d835db3fbf25a3f8c027.pdf,zero_few-shot;contrastive learning;sparse;multimodal,https://scholar.google.com/scholar?q=Multimodal+Contrastive+Learning+with+LIMoE:+the+Language-Image+Mixture+of+Experts
Unifying Voxel-based Representation with Transformer for 3D Object Detection,2022,NIPS,"['Yanwei Li', 'Yilun Chen', 'XIAOJUAN QI', 'Zeming Li', 'Jian Sun', 'Jiaya Jia']",poster,"['Unified representation', 'Multi-modality input', '3D object detection']","In this work, we present a unified framework for multi-modality 3D object detection, named UVTR. The proposed method aims to unify multi-modality representations in the voxel space for accurate and robust single- or cross-modality 3D detection. To this end, the modality-specific space is first designed to represent different inputs in the voxel feature space. Different from previous work, our approach preserves the voxel space without height compression to alleviate semantic ambiguity and enable spatial connections. To make full use of the inputs from different sensors, the cross-modality interaction is then proposed, including knowledge transfer and modality fusion. In this way, geometry-aware expressions in point clouds and context-rich features in images are well utilized for better performance and robustness. The transformer decoder is applied to efficiently sample features from the unified space with learnable positions, which facilitates object-level interactions. In general, UVTR presents an early attempt to represent different modalities in a unified framework. It surpasses previous work in single- or multi-modality entries. The proposed method achieves leading performance in the nuScenes test set for both object detection and the following object tracking task. Code is made publicly available at https://github.com/dvlab-research/UVTR.",https://api.openreview.net/pdf/03b6041d80d134e1e1452d8a2268fbdc3c70c9e6.pdf,zero_few-shot;transformer;representation;transfer learning;multimodal;3d,https://scholar.google.com/scholar?q=Unifying+Voxel-based+Representation+with+Transformer+for+3D+Object+Detection
CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis,2022,NIPS,"['Shichong Peng', 'Seyed Alireza Moazenipourasil', 'Ke Li']",poster,"['Mode-covering Generative Model', 'Diverse Conditional Image Synthesis', 'Implicit Maximum Likelihood Estimation (IMLE)']","A persistent challenge in conditional image synthesis has been to generate diverse output images from the same input image despite only one output image being observed per input image. GAN-based methods are prone to mode collapse, which leads to low diversity. To get around this, we leverage Implicit Maximum Likelihood Estimation (IMLE) which can overcome mode collapse fundamentally. IMLE uses the same generator as GANs but trains it with a different, non-adversarial objective which ensures each observed image has a generated sample nearby. Unfortunately, to generate high-fidelity images, prior IMLE-based methods require a large number of samples, which is expensive. In this paper, we propose a new method to get around this limitation, which we dub Conditional Hierarchical IMLE (CHIMLE), which can generate high-fidelity images without requiring many samples. We show CHIMLE significantly outperforms the prior best IMLE, GAN and diffusion-based methods in terms of image fidelity and mode coverage across four tasks, namely night-to-day, 16x single image super-resolution, image colourization and image decompression. Quantitatively, our method improves Fréchet Inception Distance (FID) by 36.9% on average compared to the prior best IMLE-based method, and by 27.5% on average compared to the best non-IMLE-based general-purpose methods. More results and code are available on the project website at https://niopeng.github.io/CHIMLE/.",https://api.openreview.net/pdf/ebcc41a5bcd01fcd219134877f42d51516459aae.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=CHIMLE:+Conditional+Hierarchical+IMLE+for+Multimodal+Conditional+Image+Synthesis
LION: Latent Point Diffusion Models for 3D Shape Generation,2022,NIPS,"['Xiaohui Zeng', 'Arash Vahdat', 'Francis Williams', 'Zan Gojcic', 'Or Litany', 'Sanja Fidler', 'Karsten Kreis']",poster,"['3D Shape Synthesis', 'Generative Learning', 'Diffusion Models', 'Point Cloud Generation', 'Denoising Diffusion', 'Variational Autoencoder']","Denoising diffusion models (DDMs) have shown promising results in 3D point cloud synthesis. To advance 3D DDMs and make them useful for digital artists, we require (i) high generation quality, (ii) flexibility for manipulation and applications such as conditional synthesis and shape interpolation, and (iii) the ability to output smooth surfaces or meshes. To this end, we introduce the hierarchical Latent Point Diffusion Model (LION) for 3D shape generation. LION is set up as a variational autoencoder (VAE) with a hierarchical latent space that combines a global shape latent representation with a point-structured latent space. For generation, we train two hierarchical DDMs in these latent spaces. The hierarchical VAE approach boosts performance compared to DDMs that operate on point clouds directly, while the point-structured latents are still ideally suited for DDM-based modeling. Experimentally, LION achieves state-of-the-art generation performance on multiple ShapeNet benchmarks. Furthermore, our VAE framework allows us to easily use LION for different relevant tasks: LION excels at multimodal shape denoising and voxel-conditioned synthesis, and it can be adapted for text- and image-driven 3D generation. We also demonstrate shape autoencoding and latent shape interpolation, and we augment LION with modern surface reconstruction techniques to generate smooth 3D meshes. We hope that LION provides a powerful tool for artists working with 3D shapes due to its high-quality generation, flexibility, and surface reconstruction. Project page and code: https://nv-tlabs.github.io/LION.",https://api.openreview.net/pdf/ee4ea47be942e2d2dd1be8180aa6d9674d8beee4.pdf,zero_few-shot;representation;vae;generative model;multimodal;diffusion models;3d,https://scholar.google.com/scholar?q=LION:+Latent+Point+Diffusion+Models+for+3D+Shape+Generation
BILCO: An Efficient Algorithm for Joint Alignment of Time Series,2022,NIPS,"['Xuelong Mi', 'Mengfan Wang', 'Alex Bo-Yuan Chen', 'Jing-Xuan Lim', 'Yizhi Wang', 'Misha Ahrens', 'Guoqiang Yu']",poster,"['Joint alignment', 'graphical time warping', 'bidirectional-pushing strategy', 'linear component operation', 'BILCO']","Multiple time series data occur in many real applications and the alignment among them is usually a fundamental step of data analysis. Frequently, these multiple time series are inter-dependent, which provides extra information for the alignment task and this information cannot be well utilized in the conventional pairwise alignment methods. Recently, the joint alignment was modeled as a max-flow problem, in which both the profile similarity between the aligned time series and the distance between adjacent warping functions are jointly optimized. However, despite the new model having elegant mathematical formulation and superior alignment accuracy, the long computation time and large memory usage, due to the use of the existing general-purpose max-flow algorithms, limit significantly its well-deserved wide use. In this report, we present BIdirectional pushing with Linear Component Operations (BILCO), a novel algorithm that solves the joint alignment max-flow problems efficiently and exactly. We develop the strategy of linear component operations that integrates dynamic programming technique and the push-relabel approach. This strategy is motivated by the fact that the joint alignment max-flow problem is a generalization of dynamic time warping (DTW) and numerous individual DTW problems are embedded. Further, a bidirectional-pushing strategy is proposed to introduce prior knowledge and reduce unnecessary computation, by leveraging another fact that good initialization can be easily computed for the joint alignment max-flow problem. We demonstrate the efficiency of BILCO using both synthetic and real experiments. Tested on thousands of datasets under various simulated scenarios and in three distinct application categories, BILCO consistently achieves at least 10 and averagely 20-folds increase in speed, and uses at most 1/8 and averagely 1/10 memory compared with the best existing max-flow method. Our source code can be found at https://github.com/yu-lab-vt/BILCO.",https://api.openreview.net/pdf/3281bb6ff49e0d0d0f015ad69f14e7dbd92502c8.pdf,graph;zero_few-shot;flow;multimodal,https://scholar.google.com/scholar?q=BILCO:+An+Efficient+Algorithm+for+Joint+Alignment+of+Time+Series
Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning,2022,NIPS,"['Yujia Xie', 'Luowei Zhou', 'Xiyang Dai', 'Lu Yuan', 'Nguyen Bach', 'Ce Liu', 'Michael Zeng']",poster,"['Image paragraph captioning', 'vision-language models', 'zero-shot learning']","People say, ""A picture is worth a thousand words"". Then how can we get the rich information out of the image? We argue that by using visual clues to bridge large pretrained vision foundation models and language models, we can do so without any extra cross-modal training. Thanks to the strong zero-shot capability of foundation models, we start by constructing a rich semantic representation of the image (e.g., image tags, object attributes / locations, captions) as a structured textual prompt, called visual clues, using a vision foundation model. Based on visual clues, we use large language model to produce a series of comprehensive descriptions for the visual content, which is then verified by the vision model again to select the candidate that aligns best with the image. We evaluate the quality of generated descriptions by quantitative and qualitative measurement. The results demonstrate the effectiveness of such a structured semantic representation. ",https://api.openreview.net/pdf/72869e40a968a7e085c1ac408bf472b0253740fd.pdf,graph;zero_few-shot;representation;multimodal;llm,https://scholar.google.com/scholar?q=Visual+Clues:+Bridging+Vision+and+Language+Foundations+for+Image+Paragraph+Captioning
High-dimensional limit theorems for SGD: Effective dynamics and critical scaling,2022,NIPS,"['Gerard Ben Arous', 'Reza Gheissari', 'Aukosh Jagannath']",poster,"['stochastic gradient descent', 'high-dimensional statistics', 'online algorithms', 'PCA', 'XOR', 'Gaussian mixture model', 'two-layer network']","We study the scaling limits of stochastic gradient descent (SGD) with constant step-size in the high-dimensional regime. We prove limit theorems for the trajectories of summary statistics (i.e., finite-dimensional functions) of SGD as the dimension goes to infinity. Our approach allows one to choose the summary statistics that are tracked, the initialization, and the step-size. It yields both ballistic (ODE) and diffusive (SDE) limits, with the limit depending dramatically on the former choices. We find a critical scaling regime for the step-size below which this ``effective dynamics"" matches gradient flow for the population loss, but at which, a new correction term appears which changes the phase diagram. About the fixed points of this effective dynamics, the corresponding diffusive limits can be quite complex and even degenerate. 
We demonstrate our approach on popular examples including estimation for spiked matrix and tensor models and classification via two-layer networks for binary and XOR-type Gaussian mixture models. These examples exhibit surprising phenomena including multimodal timescales to convergence as well as convergence to sub-optimal solutions with probability bounded away from zero from random (e.g., Gaussian) initializations. 
",https://api.openreview.net/pdf/ef870888e0c162708367b8bad71ca2d3126f0609.pdf,zero_few-shot;transformer;flow;multimodal,https://scholar.google.com/scholar?q=High-dimensional+limit+theorems+for+SGD:+Effective+dynamics+and+critical+scaling
Delving into Out-of-Distribution Detection with Vision-Language Representations,2022,NIPS,"['Yifei Ming', 'Ziyang Cai', 'Jiuxiang Gu', 'Yiyou Sun', 'Wei Li', 'Yixuan Li']",poster,[],"Recognizing out-of-distribution (OOD) samples is critical for machine learning systems deployed in the open world. The vast majority of OOD detection methods are driven by a single modality (e.g., either vision or language), leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of OOD detection from a single-modal to a multi-modal regime. Particularly, we propose Maximum Concept Matching (MCM), a simple yet effective zero-shot OOD detection method based on aligning visual features with textual concepts.  We contribute in-depth analysis and theoretical insights to understand the effectiveness of MCM. Extensive experiments demonstrate that MCM achieves superior performance on a wide variety of real-world tasks. MCM with vision-language features outperforms a common baseline with pure visual features on a hard OOD task with semantically similar classes by 13.1% (AUROC) Code is available at https://github.com/deeplearning-wisc/MCM. ",https://api.openreview.net/pdf/17712dcdb6ee191aaa4325a097bedc0addcc1c1d.pdf,zero_few-shot;representation;multimodal,https://scholar.google.com/scholar?q=Delving+into+Out-of-Distribution+Detection+with+Vision-Language+Representations
Forecasting Human Trajectory from Scene History,2022,NIPS,"['Mancheng Meng', 'Ziyan Wu', 'Terrence Chen', 'Xiran Cai', 'Xiang Sean Zhou', 'Fan Yang', 'Dinggang Shen']",poster,"['Human trajectory prediction', 'Scene history', 'Group trajectory', 'Cross-modal interaction']","Predicting the future trajectory of a person remains a challenging problem, due to randomness and subjectivity. However, the moving patterns of human in constrained scenario typically conform to a limited number of regularities to a certain extent, because of the scenario restrictions (\eg, floor plan, roads and obstacles) and person-person or person-object interactivity. Thus, an individual person in this scenario should follow one of the regularities as well. In other words, a person's subsequent trajectory has likely been traveled by others. Based on this hypothesis, we propose to forecast a person's future trajectory by learning from the implicit scene regularities. We call the regularities, inherently derived from the past dynamics of the people and the environment in the scene,  \emph{scene history}. We categorize scene history information into two types: historical group trajectories and individual-surroundings interaction. To exploit these information for trajectory prediction, we propose a novel framework Scene History Excavating Network (SHENet), where the scene history is leveraged in a simple yet effective approach. In particular, we design two components, the group trajectory bank module to extract representative group trajectories as the candidate for future path, and the cross-modal interaction module to model the interaction between individual past trajectory and its surroundings for trajectory refinement, respectively.  In addition, to mitigate the uncertainty in the evaluation, caused by the aforementioned randomness and subjectivity, we propose to include smoothness into evaluation metrics. We conduct extensive evaluations to validate the efficacy of proposed framework on ETH, UCY, as well as a new, challenging benchmark dataset PAV, demonstrating superior performance compared to state-of-the-art methods.",https://api.openreview.net/pdf/cd01204f4380910e9f932559c7e3b4f2a9c20eca.pdf,graph;optimization;zero_few-shot;metric;multimodal;llm,https://scholar.google.com/scholar?q=Forecasting+Human+Trajectory+from+Scene+History
CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers,2022,NIPS,"['Ming Ding', 'Wendi Zheng', 'Wenyi Hong', 'Jie Tang']",poster,"['text-to-image generation', 'pretraining', 'transformer']","Development of transformer-based text-to-image models is impeded by its slow generation and complexity, for high-resolution images. In this work, we put forward a solution based on hierarchical transformers and local parallel autoregressive generation.  
We pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, a cross-modal general language model (CogLM), and fine-tune it for fast super-resolution. 
The new text-to-image system, CogView2, shows very competitive generation compared to concurrent state-of-the-art DALL-E-2, and naturally supports interactive text-guided editing on images.",https://api.openreview.net/pdf/aa75f6b22500a6ea605ad2f52cc2e35272992c0d.pdf,zero_few-shot;transformer;generative model;active learning;multimodal,https://scholar.google.com/scholar?q=CogView2:+Faster+and+Better+Text-to-Image+Generation+via+Hierarchical+Transformers
Cross-modal Learning for Image-Guided Point Cloud Shape Completion,2022,NIPS,"['Emanuele Aiello', 'Diego Valsesia', 'Enrico Magli']",poster,"['Point Cloud Completion', 'View-guided completion', 'Self-supervised completion', 'Multimodal Learning']","In this paper we explore the recent topic of point cloud completion, guided by an auxiliary image. We show how it is possible to effectively combine the information from the two modalities in a localized latent space, thus avoiding the need for complex point cloud reconstruction methods from single views used by the state-of-the-art. We also investigate a novel self-supervised setting where the auxiliary image provides a supervisory signal to the training process by using a differentiable renderer on the completed point cloud to measure fidelity in the image space. Experiments show significant improvements over state-of-the-art supervised methods for both unimodal and multimodal completion. We also show the effectiveness of the self-supervised approach which outperforms a number of supervised methods and is competitive with the latest supervised models only exploiting point cloud information.",https://api.openreview.net/pdf/f673aae78e9f3a4eddae048377318410a42cd57f.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Cross-modal+Learning+for+Image-Guided+Point+Cloud+Shape+Completion
Less-forgetting Multi-lingual Fine-tuning,2022,NIPS,"['Yuren Mao', 'Yaobo Liang', 'Nan Duan', 'Haobo Wang', 'Kai Wang', 'Lu Chen', 'Yunjun Gao']",poster,"['Multi-lingual Language  Models', 'Multi-lingual Fine-tuning', 'Less-forgetting']","Multi-lingual fine-tuning (MLF), which fine-tunes a multi-lingual language model (MLLM) with multiple source languages, aims to gain good zero-shot performance on target languages. In MLF, the fine-tuned model tends to fit the source languages while forgetting its cross-lingual knowledge obtained from the pre-training stage. This forgetting phenomenon degenerates the zero-shot performance of MLF, which remains under-explored. To fill this gap, this paper proposes a multi-lingual fine-tuning method, dubbed Less-forgetting Multi-lingual Fine-tuning (LF-MLF). In LF-MLF, we cast multi-lingual fine-tuning as a constrained optimization problem, where the optimization objective is to minimize forgetting, and constraints are reducing the fine-tuning loss. The proposed method has superior zero-shot performance; furthermore, it can achieve the Pareto stationarity. Extensive experiments on Named Entity Recognition, Question Answering and Natural Language Inference back up our theoretical analysis and validate the superiority of our proposals.",https://api.openreview.net/pdf/2a83e695e2cb127e5a29f0de1903cafe3808ebb5.pdf,optimization;zero_few-shot;inference;multimodal,https://scholar.google.com/scholar?q=Less-forgetting+Multi-lingual+Fine-tuning
Language Conditioned Spatial Relation Reasoning for 3D Object Grounding,2022,NIPS,"['Shizhe Chen', 'Pierre-Louis Guhur', 'Makarand Tapaswi', 'Cordelia Schmid', 'Ivan Laptev']",poster,[],"Localizing objects in 3D scenes based on natural language requires understanding and reasoning about spatial relations. In particular, it is often crucial to distinguish similar objects referred by the text, such as ""the left most chair"" and ""a chair next to the window"". In this work we propose a language-conditioned transformer model for grounding 3D objects and their spatial relations. To this end, we design a spatial self-attention layer that accounts for relative distances and orientations between objects in input 3D point clouds. Training such a layer with visual and language inputs enables to disambiguate spatial relations and to localize objects referred by the text. To facilitate the cross-modal learning of relations, we further propose a teacher-student approach where the teacher model is first trained using ground-truth object labels, and then helps to train a student model using point cloud inputs. We perform ablation studies showing advantages of our approach. We also demonstrate our model to significantly outperform the state of the art on the challenging Nr3D, Sr3D and ScanRefer 3D object grounding datasets.",https://api.openreview.net/pdf/a7ca7db6d4af88e0d2d12a4501ae57642811ae40.pdf,graph;transformer;multimodal;3d,https://scholar.google.com/scholar?q=Language+Conditioned+Spatial+Relation+Reasoning+for+3D+Object+Grounding
M$^4$I: Multi-modal Models Membership Inference,2022,NIPS,"['Pingyi Hu', 'Zihan Wang', 'Ruoxi Sun', 'Hu Wang', 'Minhui Xue']",poster,"['Membership inference attack', 'Data privacy leakage', 'Multimodality']","With the development of machine learning techniques, the attention of research has been moved from single-modal learning to multi-modal learning, as real-world data exist in the form of different modalities. However, multi-modal models often carry more information than single-modal models and they are usually applied in sensitive scenarios, such as medical report generation or disease identification. Compared with the existing membership inference against machine learning classifiers, we focus on the problem that the input and output of the multi-modal models are in different modalities, such as image captioning. This work studies the privacy leakage of multi-modal models through the lens of membership inference attack, a process of determining whether a data record involves in the model training process or not. To achieve this, we propose Multi-modal Models Membership Inference (M$^4$I) with two attack methods to infer the membership status, named metric-based (MB) M$^4$I and feature-based (FB) M$^4$I, respectively. More specifically, MB M$^4$I adopts similarity metrics while attacking to infer target data membership. FB M$^4$I uses a pre-trained shadow multi-modal feature extractor to achieve the purpose of data inference attack by comparing the similarities from extracted input and output features. Extensive experimental results show that both attack methods can achieve strong performances. Respectively, 72.5% and 94.83% of attack success rates on average can be obtained under unrestricted scenarios. Moreover, we evaluate multiple defense mechanisms against our attacks. The source code of M$^4$I attacks is publicly available at https://github.com/MultimodalMI/Multimodal-membership-inference.git.",https://api.openreview.net/pdf/5c618b1492a4a832e71f351e146a416c8cb6d530.pdf,transformer;generative model;inference;metric;multimodal,https://scholar.google.com/scholar?q=M$^4$I:+Multi-modal+Models+Membership+Inference
Deep Multi-Modal Structural Equations For Causal Effect Estimation With Unstructured Proxies,2022,NIPS,"['Shachi Deshpande', 'Kaiwen Wang', 'Dhruv Sreenivas', 'Zheng Li', 'Volodymyr Kuleshov']",poster,[],"Estimating the effect of intervention from observational data while accounting for confounding variables is a key task in causal inference. Oftentimes, the confounders are unobserved, but we have access to large amounts of additional unstructured data (images, text) that contain valuable proxy signal about the missing confounders. This paper argues that leveraging this unstructured data can greatly improve the accuracy of causal effect estimation. Specifically, we introduce deep multi-modal structural equations, a generative model for causal effect estimation in which confounders are latent variables and unstructured data are proxy variables. This model supports multiple multimodal proxies (images, text) as well as missing data. We empirically demonstrate that our approach outperforms existing methods based on propensity scores and corrects for confounding using unstructured inputs on tasks in genomics and healthcare. Our methods can potentially support the use of large amounts of data that were previously not used in causal inference",https://api.openreview.net/pdf/7b760d317a1719fb7347e4bd92e823a1cf92501b.pdf,graph;generative model;inference;multimodal,https://scholar.google.com/scholar?q=Deep+Multi-Modal+Structural+Equations+For+Causal+Effect+Estimation+With+Unstructured+Proxies
Learning State-Aware Visual Representations from Audible Interactions,2022,NIPS,"['Himangi Mittal', 'Pedro Morgado', 'Unnat Jain', 'Abhinav Gupta']",poster,"['Video Representation learning', 'self-supervised learning', 'contrastive learning', 'audio-visual learning', 'egocentric videos', 'Ego4D', 'EPIC-Kitchens']","We propose a self-supervised algorithm to learn representations from egocentric video data. Recently, significant efforts have been made to capture humans interacting with their own environments as they go about their daily activities. In result, several large egocentric datasets of interaction-rich multi-modal data have emerged. However, learning representations from videos can be challenging. First, given the uncurated nature of long-form continuous videos, learning effective representations require focusing on moments in time when interactions take place. Second, visual representations of daily activities should be sensitive to changes in the state of the environment. However, current successful multi-modal learning frameworks encourage representation invariance over time. To address these challenges, we leverage audio signals to identify moments of likely interactions which are conducive to better learning. We also propose a novel self-supervised objective that learns from audible state changes caused by interactions. We validate these contributions extensively on two large-scale egocentric datasets, EPIC-Kitchens-100 and the recently released Ego4D, and show improvements on several downstream tasks, including action recognition, long-term action anticipation, and object state change classification.",https://api.openreview.net/pdf/3c00a294611b2926715d5743b6eb92bea655be53.pdf,graph;zero_few-shot;representation;multimodal;llm,https://scholar.google.com/scholar?q=Learning+State-Aware+Visual+Representations+from+Audible+Interactions
Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone ,2022,NIPS,"['Zi-Yi Dou', 'Aishwarya Kamath', 'Zhe Gan', 'Pengchuan Zhang', 'Jianfeng Wang', 'Linjie Li', 'Zicheng Liu', 'Ce Liu', 'Yann LeCun', 'Nanyun Peng', 'Jianfeng Gao', 'Lijuan Wang']",poster,"['vision-language pre-training', 'VQA', 'image captioning', 'object detection']","Vision-language (VL) pre-training has recently received considerable attention. However, most existing end-to-end pre-training approaches either only aim to tackle VL tasks such as image-text retrieval, visual question answering (VQA) and image captioning that test high-level understanding of images, or only target region-level understanding for tasks such as phrase grounding and object detection. We present FIBER (Fusion-In-the-Backbone-based transformER), a new VL model architecture that can seamlessly handle both these types of tasks. Instead of having dedicated transformer layers for fusion after the uni-modal backbones, FIBER pushes multimodal fusion deep into the model by inserting cross-attention into the image and text backbones to better capture multimodal interactions. In addition, unlike previous work that is either only pre-trained on image-text data or on fine-grained data with box-level annotations, we present a two-stage pre-training strategy that uses both these kinds of data efficiently: (i) coarse-grained pre-training based on image-text data; followed by (ii) fine-grained pre-training based on image-text-box data. We conduct comprehensive experiments on a wide range of VL tasks, ranging from VQA, image captioning, and retrieval, to phrase grounding, referring expression comprehension, and object detection. Using deep multimodal fusion coupled with the two-stage pre-training, FIBER provides consistent performance improvements over strong baselines across all tasks, often outperforming methods using magnitudes more data. Code is released at https://github.com/microsoft/FIBER.",https://api.openreview.net/pdf/50af76dd54980e4509405ad7a9b5de133d71ef8b.pdf,graph;zero_few-shot;transformer;multimodal,https://scholar.google.com/scholar?q=Coarse-to-Fine+Vision-Language+Pre-training+with+Fusion+in+the+Backbone+
CoupAlign: Coupling Word-Pixel with Sentence-Mask Alignments for Referring Image Segmentation,2022,NIPS,"['Zicheng Zhang', 'Yi Zhu', 'Jianzhuang Liu', 'Xiaodan Liang', 'Wei Ke']",poster,"['Referring Image Segmentation', 'Vision Language Modeling', 'Cross-model Alignment']","Referring image segmentation aims at localizing all pixels of the visual objects described by a natural language sentence. Previous works learn to straightforwardly align the sentence embedding and pixel-level embedding for highlighting the referred objects, but ignore the semantic consistency of pixels within the same object, leading to incomplete masks and localization errors in predictions. To tackle this problem, we propose CoupAlign, a simple yet effective multi-level visual-semantic alignment method, to couple sentence-mask alignment with word-pixel alignment to enforce object mask constraint for achieving more accurate localization and segmentation. Specifically, the Word-Pixel Alignment (WPA) module performs early fusion of linguistic and pixel-level features in intermediate layers of the vision and language encoders. Based on the word-pixel aligned embedding, a set of mask proposals are generated to hypothesize possible objects. Then in the Sentence-Mask Alignment (SMA) module, the masks are weighted by the sentence embedding to localize the referred object, and finally projected back to aggregate the pixels for the target. To further enhance the learning of the two alignment modules, an auxiliary loss is designed to contrast the foreground and background pixels. By hierarchically aligning pixels and masks with linguistic features, our CoupAlign captures the pixel coherence at both visual and semantic levels, thus generating more accurate predictions. Extensive experiments on popular datasets (e.g., RefCOCO and G-Ref) show that our method achieves consistent improvements over state-of-the-art methods, e.g., about 2% oIoU increase on the validation and testing set of RefCOCO. Especially, CoupAlign has remarkable ability in distinguishing the target from multiple objects of the same class. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/CoupAlign.",https://api.openreview.net/pdf/aeca2504a4964317446f49d945f9af2d7900b822.pdf,optimization;segmentation;multimodal,https://scholar.google.com/scholar?q=CoupAlign:+Coupling+Word-Pixel+with+Sentence-Mask+Alignments+for+Referring+Image+Segmentation
Generalized One-shot Domain Adaptation of Generative Adversarial Networks,2022,NIPS,"['Zicheng Zhang', 'Yinglu Liu', 'Congying Han', 'Tiande Guo', 'Ting Yao', 'Tao Mei']",poster,"['Generative Adversarial Network', 'Computer vision', 'domain adaptation']","The adaptation of a Generative Adversarial Network (GAN) aims to transfer a pre-trained GAN to a target domain with limited training data. In this paper, we focus on the one-shot case, which is more challenging and rarely explored in previous works. We consider that the adaptation from a source domain to a target domain can be decoupled into two parts: the transfer of global style like texture and color, and the emergence of new entities that do not belong to the source domain. While previous works mainly focus on style transfer, we propose a novel and concise framework to address the \textit{generalized one-shot adaptation} task for both style and entity transfer, in which a reference image and its binary entity mask are provided. Our core idea is to constrain the gap between the internal distributions of the reference and syntheses by sliced Wasserstein distance. To better achieve it, style fixation is used at first to roughly obtain the exemplary style, and an auxiliary network is introduced to the generator to disentangle entity and style transfer. Besides, to realize cross-domain correspondence, we propose the variational Laplacian regularization to constrain the smoothness of the adapted generator. Both quantitative and qualitative experiments demonstrate the effectiveness of our method in various scenarios. Code is available at \url{https://github.com/zhangzc21/Generalized-One-shot-GAN-adaptation}.",https://api.openreview.net/pdf/0399ea2ec310fbc01e09ba7d06a71f9e8acf1c18.pdf,graph;optimization;generative model;transfer learning;multimodal,https://scholar.google.com/scholar?q=Generalized+One-shot+Domain+Adaptation+of+Generative+Adversarial+Networks
Mutual Information Divergence: A Unified Metric for Multimodal Generative Models,2022,NIPS,"['Jin-Hwa Kim', 'Yunji Kim', 'Jiyoung Lee', 'Kang Min Yoo', 'Sang-Woo Lee']",poster,"['text-to-image generation', 'image captioning', 'evaluation metric', 'mutual information', 'vision and language']","Text-to-image generation and image captioning are recently emerged as a new experimental paradigm to assess machine intelligence. They predict continuous quantity accompanied by their sampling techniques in the generation, making evaluation complicated and intractable to get marginal distributions. Based on a recent trend that multimodal generative evaluations exploit a vison-and-language pre-trained model, we propose the negative Gaussian cross-mutual information using the CLIP features as a unified metric, coined by Mutual Information Divergence (MID). To validate, we extensively compare it with competing metrics using carefully-generated or human-annotated judgments in text-to-image generation and image captioning tasks. The proposed MID significantly outperforms the competitive methods by having consistency across benchmarks, sample parsimony, and robustness toward the exploited CLIP model. We look forward to seeing the underrepresented implications of the Gaussian cross-mutual information in multimodal representation learning and future works based on this novel proposition. ",https://api.openreview.net/pdf/366a9ea4cc25ad029656ebef6aa8988a965ee78e.pdf,graph;representation;generative model;metric;multimodal,https://scholar.google.com/scholar?q=Mutual+Information+Divergence:+A+Unified+Metric+for+Multimodal+Generative+Models
Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning,2022,NIPS,"['Fuying Wang', 'Yuyin Zhou', 'Shujun Wang', 'Varut Vardhanabhuti', 'Lequan Yu']",poster,"['medical image', 'medical report', 'cross-modal', 'representation learning']","Learning medical visual representations directly from paired radiology reports has become an emerging topic in representation learning. However, existing medical image-text joint learning methods are limited by instance or local supervision analysis, ignoring disease-level semantic correspondences. In this paper, we present a novel Multi-Granularity Cross-modal Alignment (MGCA) framework for generalized medical visual representation learning by harnessing the naturally exhibited semantic correspondences between medical image and radiology reports at three different levels, i.e., pathological region-level, instance-level, and disease-level. Specifically, we first incorporate the instance-wise alignment module by maximizing the agreement between image-report pairs. Further, for token-wise alignment, we introduce a bidirectional cross-attention strategy to explicitly learn the matching between fine-grained visual tokens and text tokens, followed by contrastive learning to align them. More important, to leverage the high-level inter-subject relationship semantic (e.g., disease) correspondences, we design a novel cross-modal disease-level alignment paradigm to enforce the cross-modal cluster assignment consistency. Extensive experimental results on seven downstream medical image datasets covering image classification, object detection, and semantic segmentation tasks demonstrate the stable and superior performance of our framework.",https://api.openreview.net/pdf/bd00288b42b7ffcd93a62299543607e969de102a.pdf,graph;zero_few-shot;transformer;representation;contrastive learning;segmentation;multimodal,https://scholar.google.com/scholar?q=Multi-Granularity+Cross-modal+Alignment+for+Generalized+Medical+Visual+Representation+Learning
Implicit Warping for Animation with Image Sets,2022,NIPS,"['Arun Mallya', 'Ting-chun Wang', 'Ming-Yu Liu']",poster,"['image animation', 'attention', 'motion transfer', 'video synthesis']","We present a new implicit warping framework for image animation using sets of source images through the transfer of motion of a driving video. A single cross-modal attention layer is used to find correspondences between the source images and the driving image, choose the most appropriate features from different source images, and warp the selected features. This is in contrast to the existing methods that use explicit flow-based warping, which is designed for animation using a single source and does not extend well to multiple sources. The pick-and-choose capability of our framework helps it achieve state-of-the-art results on multiple datasets for image animation using both single and multiple source images.",https://api.openreview.net/pdf/b48851c387029b8c8e9fe57b06438b93428426ec.pdf,zero_few-shot;transformer;transfer learning;flow;multimodal,https://scholar.google.com/scholar?q=Implicit+Warping+for+Animation+with+Image+Sets
I2DFormer: Learning Image to Document Attention for Zero-Shot Image Classification,2022,NIPS,"['Muhammad Ferjad Naeem', 'Yongqin Xian', 'Luc Van Gool', 'Federico Tombari']",poster,"['Zero-shot Learning', 'Multimodal learning', 'Transformer', 'Attention']","Despite the tremendous progress in zero-shot learning (ZSL), the majority of existing methods still rely on human-annotated attributes, which are difficult to annotate and scale. An unsupervised alternative is to represent each class using the word embedding associated with its semantic class name. However, word embeddings extracted from pre-trained language models do not necessarily capture visual similarities, resulting in poor zero-shot performance.  In this work, we argue that online textual documents e.g., Wikipedia, contain rich visual descriptions about object classes, therefore can be used as powerful unsupervised side information for ZSL. To this end, we propose I2DFormer, a novel transformer-based ZSL framework that jointly learns to encode images and documents by aligning both modalities in a shared embedding space. In order to distill discriminative visual words from noisy documents, we introduce a new cross-modal attention module that learns fine-grained interactions between image patches and document words. Consequently, our I2DFormer not only learns highly discriminative document embeddings that capture visual similarities but also gains the ability to localize visually relevant words in image regions. Quantitatively, we demonstrate that our I2DFormer significantly outperforms previous unsupervised semantic embeddings under both zero-shot and generalized zero-shot learning settings on three public datasets. Qualitatively, we show that our method leads to highly interpretable results where document words can be grounded in the image regions. ",https://api.openreview.net/pdf/8499b50156f453b96f3c41d3543078333af56126.pdf,zero_few-shot;transformer;online learning;distillation;multimodal,https://scholar.google.com/scholar?q=I2DFormer:+Learning+Image+to+Document+Attention+for+Zero-Shot+Image+Classification
Large-batch Optimization for Dense Visual Predictions: Training Faster R-CNN in 4.2 Minutes,2022,NIPS,"['Zeyue Xue', 'Jianming Liang', 'Guanglu Song', 'Zhuofan Zong', 'Liang Chen', 'Yu Liu', 'Ping Luo']",poster,"['Large-batch Training', 'Dense Visual Predictions', 'Object Detection and Segmentation']","Training a large-scale deep neural network in a large-scale dataset is challenging and time-consuming. The recent breakthrough of large-batch optimization is a promising way to tackle this challenge. However, although the current advanced algorithms such as LARS and LAMB succeed in classification models, the complicated pipelines of dense visual predictions such as object detection and segmentation still suffer from the heavy performance drop in the large-batch training regime. To address this challenge, we propose a simple yet effective algorithm, named Adaptive Gradient Variance Modulator (AGVM), which can train dense visual predictors with very large batch size, enabling several benefits more appealing than prior arts. Firstly, AGVM can align the gradient variances between different modules in the dense visual predictors, such as backbone, feature pyramid network (FPN), detection, and segmentation heads. We show that training with a large batch size can fail with the gradient variances misaligned among them, which is a phenomenon primarily overlooked in previous work. Secondly, AGVM is a plug-and-play module that generalizes well to many different architectures (e.g., CNNs and Transformers) and different tasks (e.g., object detection, instance segmentation, semantic segmentation, and panoptic segmentation). It is also compatible with different optimizers (e.g., SGD and AdamW). Thirdly, a theoretical analysis of AGVM is provided. Extensive experiments on the COCO and ADE20K datasets demonstrate the superiority of AGVM. For example, AGVM demonstrates more stable generalization performance than prior arts under extremely large batch size (i.e., 10k). AGVM can train Faster R-CNN+ResNet50 in 4.2 minutes without losing performance. It enables training an object detector with one billion parameters in just 3.5 hours, reducing the training time by 20.9×, whilst achieving 62.2 mAP on COCO. The deliverables will be released at https://github.com/Sense-X/AGVM.",https://api.openreview.net/pdf/b53144ecb706d8b9bc7a875f205daf6301feee56.pdf,graph;optimization;transformer;adaptive;segmentation;multimodal,https://scholar.google.com/scholar?q=Large-batch+Optimization+for+Dense+Visual+Predictions:+Training+Faster+R-CNN+in+4.2+Minutes
Physically-Based Face Rendering for NIR-VIS Face Recognition,2022,NIPS,"['Yunqi Miao', 'Alexandros Lattas', 'Jiankang Deng', 'Jungong Han', 'Stefanos Zafeiriou']",poster,[],"Near infrared (NIR) to Visible (VIS) face matching is challenging due to the significant domain gaps as well as a lack of sufficient data for cross-modality model training. To overcome this problem, we propose a novel method for paired NIR-VIS facial image generation. Specifically, we reconstruct 3D face shape and reflectance from a large 2D facial dataset and introduce a novel method of transforming the VIS reflectance to NIR reflectance. We then use a physically-based renderer to generate a vast, high-resolution and photorealistic dataset consisting of various poses and identities in the NIR and VIS spectra. Moreover, to facilitate the identity feature learning, we propose an IDentity-based Maximum Mean Discrepancy (ID-MMD) loss, which not only reduces the modality gap between NIR and VIS images at the domain level but encourages the network to focus on the identity features instead of facial details, such as poses and accessories. Extensive experiments conducted on four challenging NIR-VIS face recognition benchmarks demonstrate that the proposed method can achieve comparable performance with the state-of-the-art (SOTA) methods without requiring any existing NIR-VIS face recognition datasets. With slightly fine-tuning on the target NIR-VIS face recognition datasets, our method can significantly surpass the SOTA performance. Code and pretrained models are released under the insightface GitHub.",https://api.openreview.net/pdf/d16a9eaf50abb5eeb1e2194f9105e8debfea21c9.pdf,graph;generative model;multimodal;3d,https://scholar.google.com/scholar?q=Physically-Based+Face+Rendering+for+NIR-VIS+Face+Recognition
TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition,2022,NIPS,"['Yongwei Chen', 'Rui Chen', 'Jiabao Lei', 'Yabin Zhang', 'Kui Jia']",poster,"['3D content creation', 'CLIP', 'Text-to-3D']","Creation of 3D content by stylization is a promising yet challenging problem in computer vision and graphics research. In this work, we focus on stylizing photorealistic appearance renderings of a given surface mesh of arbitrary topology. Motivated by the recent surge of cross-modal supervision of the Contrastive Language-Image Pre-training (CLIP) model, we propose TANGO, which transfers the appearance style of a given 3D shape according to a text prompt in a photorealistic manner. Technically, we propose to disentangle the appearance style as the spatially varying bidirectional reflectance distribution function, the local geometric variation, and the lighting condition, which are jointly optimized, via supervision of the CLIP loss, by a spherical Gaussians based differentiable renderer. As such, TANGO enables photorealistic 3D style transfer by automatically predicting reflectance effects even for bare, low-quality meshes, without training on a task-specific dataset. Extensive experiments show that TANGO outperforms existing methods of text-driven 3D style transfer in terms of photorealistic quality, consistency of 3D geometry, and robustness when stylizing low-quality meshes. Our codes and results are available at our project webpage https://cyw-3d.github.io/tango/.",https://api.openreview.net/pdf/1b4bba1a76796e0b775ed188ee7b300cb980666f.pdf,graph;zero_few-shot;contrastive learning;metric;transfer learning;multimodal;3d;llm,https://scholar.google.com/scholar?q=TANGO:+Text-driven+Photorealistic+and+Robust+3D+Stylization+via+Lighting+Decomposition
CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks,2022,NIPS,"['Xuanli He', 'Qiongkai Xu', 'Yi Zeng', 'Lingjuan Lyu', 'Fangzhao Wu', 'Jiwei Li', 'Ruoxi Jia']",poster,"['natural language generation', 'conditional lexical watermarks', 'IP protection']","Previous works have validated that text generation APIs can be stolen through imitation attacks, causing IP violations. In order to protect the IP of text generation APIs, recent work has introduced a watermarking algorithm and utilized the null-hypothesis test as a post-hoc ownership verification on the imitation models. However, we find that it is possible to detect those watermarks via sufficient statistics of the frequencies of candidate watermarking words. To address this drawback, in this paper, we propose a novel Conditional wATERmarking framework (CATER) for protecting the IP of text generation APIs. An optimization method is proposed to decide the watermarking rules that can minimize the distortion of overall word distributions while maximizing the change of conditional word selections. Theoretically, we prove that it is infeasible for even the savviest attacker (they know how CATER works) to reveal the used watermarks from a large pool of potential word pairs based on statistical inspection. Empirically, we observe that high-order conditions lead to an exponential growth of suspicious (unused) watermarks, making our crafted watermarks more stealthy. In addition, CATER can effectively identify IP infringement under architectural mismatch and cross-domain imitation attacks, with negligible impairments on the generation quality of victim APIs. We envision our work as a milestone for stealthily protecting the IP of text generation APIs.",https://api.openreview.net/pdf/fe4cd1a64f04e9edd3aa70996e3a860cc77c8096.pdf,optimization;generative model;multimodal,https://scholar.google.com/scholar?q=CATER:+Intellectual+Property+Protection+on+Text+Generation+APIs+via+Conditional+Watermarks
BMU-MoCo: Bidirectional Momentum Update for Continual Video-Language Modeling,2022,NIPS,"['Yizhao Gao', 'Nanyi Fei', 'Haoyu Lu', 'Zhiwu Lu', 'Hao Jiang', 'Yijie Li', 'Zhao Cao']",poster,"['Video-language modeling', 'continual learning', 'catastrophic forgetting', 'representation learning']","Video-language models suffer from forgetting old/learned knowledge when trained with streaming data. In this work, we thus propose a continual video-language modeling (CVLM) setting, where models are supposed to be sequentially trained on five widely-used video-text datasets with different data distributions. Although most of existing continual learning methods have achieved great success by exploiting extra information (e.g., memory data of past tasks) or dynamically extended networks, they cause enormous resource consumption when transferred to our CVLM setting. To overcome the challenges (i.e., catastrophic forgetting and heavy resource consumption) in CVLM, we propose a novel cross-modal MoCo-based model with bidirectional momentum update (BMU), termed BMU-MoCo. Concretely, our BMU-MoCo has two core designs: (1) Different from the conventional MoCo, we apply the momentum update to not only momentum encoders but also encoders (i.e., bidirectional) at each training step, which enables the model to review the learned knowledge retained in the momentum encoders. (2) To further enhance our BMU-MoCo by utilizing earlier knowledge, we additionally maintain a pair of global momentum encoders (only initialized at the very beginning) with the same BMU strategy. Extensive results show that our BMU-MoCo remarkably outperforms recent competitors w.r.t. video-text retrieval performance and forgetting rate, even without using any extra data or dynamic networks.",https://api.openreview.net/pdf/c2052514b60822901da319ba21f9072ecf6e3a35.pdf,graph;transfer learning;multimodal,https://scholar.google.com/scholar?q=BMU-MoCo:+Bidirectional+Momentum+Update+for+Continual+Video-Language+Modeling
Rethinking Alignment in Video Super-Resolution Transformers,2022,NIPS,"['Shuwei Shi', 'Jinjin Gu', 'Liangbin Xie', 'Xintao Wang', 'Yujiu Yang', 'Chao Dong']",poster,"['Video Super-Resolution', 'Transformer', 'Self-attention', 'Alignment']","The alignment of adjacent frames is considered an essential operation in video super-resolution (VSR). Advanced VSR models, including the latest VSR Transformers, are generally equipped with well-designed alignment modules. However, the progress of the self-attention mechanism may violate this common sense. In this paper, we rethink the role of alignment in VSR Transformers and make several counter-intuitive observations. Our experiments show that: (i) VSR Transformers can directly utilize multi-frame information from unaligned videos, and (ii) existing alignment methods are sometimes harmful to VSR Transformers. These observations indicate that we can further improve the performance of VSR Transformers simply by removing the alignment module and adopting a larger attention window. Nevertheless, such designs will dramatically increase the computational burden, and cannot deal with large motions. Therefore, we propose a new and efficient alignment method called patch alignment, which aligns image patches instead of pixels. VSR Transformers equipped with patch alignment could demonstrate state-of-the-art performance on multiple benchmarks. Our work provides valuable insights on how multi-frame information is used in VSR and how to select alignment methods for different networks/datasets. Codes and models will be released at https://github.com/XPixelGroup/RethinkVSRAlignment.",https://api.openreview.net/pdf/1ac85b53fb97273d0fd73058ef3c49517afe303a.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Rethinking+Alignment+in+Video+Super-Resolution+Transformers
OTKGE: Multi-modal Knowledge Graph Embeddings via Optimal Transport,2022,NIPS,"['Zongsheng Cao', 'Qianqian Xu', 'Zhiyong Yang', 'Yuan He', 'Xiaochun Cao', 'Qingming Huang']",poster,"['Multi-modal knowledge graph', 'Representation learning', 'Optimal transport']","Multi-modal knowledge graph embeddings (KGE) have caught more and more attention in learning representations of entities and relations for link prediction tasks. Different from previous uni-modal KGE approaches, multi-modal KGE can leverage expressive knowledge from a wealth of modalities (image, text, etc.), leading to more comprehensive representations of real-world entities. However, the critical challenge along this course lies in that the multi-modal embedding spaces are usually heterogeneous. In this sense, direct fusion will destroy the inherent spatial structure of different modal embeddings. To overcome this challenge, we revisit multi-modal KGE from a distributional alignment perspective and propose optimal transport knowledge graph embeddings (OTKGE). Specifically, we model the multi-modal fusion procedure as a transport plan moving different modal embeddings to a unified space by minimizing the Wasserstein distance between multi-modal distributions. Theoretically, we show that by minimizing the Wasserstein distance between the individual modalities and the unified embedding space, the final results are guaranteed to maintain consistency and comprehensiveness. Moreover, experimental results on well-established multi-modal knowledge graph completion benchmarks show that our OTKGE achieves state-of-the-art performance.",https://api.openreview.net/pdf/ddd9bf5b1c669d8ccc81132bc5fc978e6450d4c0.pdf,graph;transformer;representation;multimodal,https://scholar.google.com/scholar?q=OTKGE:+Multi-modal+Knowledge+Graph+Embeddings+via+Optimal+Transport
Learning Generalizable Part-based Feature Representation for 3D Point Clouds,2022,NIPS,"['Xin Wei', 'Xiang Gu', 'Jian Sun']",poster,"['Point cloud classification', 'domain generalization', 'generalizable part-based representation']","Deep networks on 3D point clouds have achieved remarkable success in 3D classification, while they are vulnerable to geometry variations caused by inconsistent data acquisition procedures. This results in a challenging 3D domain generalization (3DDG) problem, that is to generalize a model trained on source domain to an unseen target domain. Based on the observation that local geometric structures are more generalizable than the whole shape, we propose to reduce the geometry shift by a generalizable part-based feature representation and design a novel part-based domain generalization network (PDG) for 3D point cloud classification. Specifically, we build a part-template feature space shared by source and target domains. Shapes from distinct domains are first organized to part-level features and then represented by part-template features. The transformed part-level features, dubbed aligned part-based representations, are then aggregated by a part-based feature aggregation module. To improve the robustness of the part-based representations, we further propose a contrastive learning framework upon part-based shape representation. Experiments and ablation studies on 3DDA and 3DDG benchmarks justify the efficacy of the proposed approach for domain generalization, compared with the previous state-of-the-art methods. Our code will be available on http://github.com/weixmath/PDG.",https://api.openreview.net/pdf/d64e2474fcd00214570fc4df4f0d0d96e9a15a53.pdf,graph;representation;contrastive learning;metric;multimodal;3d,https://scholar.google.com/scholar?q=Learning+Generalizable+Part-based+Feature+Representation+for+3D+Point+Clouds
HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes,2022,NIPS,"['Zan Wang', 'Yixin Chen', 'Tengyu Liu', 'Yixin Zhu', 'Wei Liang', 'Siyuan Huang']",poster,"['language and 3D scene', 'motion generation', 'language-conditioned generation', 'human-scene interaction']","Learning to generate diverse scene-aware and goal-oriented human motions in 3D scenes remains challenging due to the mediocre characters of the existing datasets on Human-Scene Interaction (HSI); they only have limited scale/quality and lack semantics. To fill in the gap, we propose a large-scale and semantic-rich synthetic HSI dataset, denoted as HUMANISE, by aligning the captured human motion sequences with various 3D indoor scenes. We automatically annotate the aligned motions with language descriptions that depict the action and the individual interacting objects; e.g., sit on the armchair near the desk. HUMANIZE thus enables a new generation task, language-conditioned human motion generation in 3D scenes. The proposed task is challenging as it requires joint modeling of the 3D scene, human motion, and natural language. To tackle this task, we present a novel scene-and-language conditioned generative model that can produce 3D human motions of the desirable action interacting with the specified objects. Our experiments demonstrate that our model generates diverse and semantically consistent human motions in 3D scenes. 
",https://api.openreview.net/pdf/32e7e62ebdcb591558908433c1e4e8c75f4d3bbc.pdf,graph;generative model;multimodal;3d,https://scholar.google.com/scholar?q=HUMANISE:+Language-conditioned+Human+Motion+Generation+in+3D+Scenes
Multi-modal Grouping Network for Weakly-Supervised Audio-Visual Video Parsing,2022,NIPS,"['Shentong Mo', 'Yapeng Tian']",poster,"['Weakly-Supervised Audio-Visual Video Parsing', 'Multi-modal Grouping']","The audio-visual video parsing task aims to parse a video into modality- and category-aware temporal segments. Previous work mainly focuses on weakly-supervised approaches, which learn from video-level event labels. During training, they do not know which modality perceives and meanwhile which temporal segment contains the video event. Since there is no explicit grouping in the existing frameworks, the modality and temporal uncertainties make these methods suffer from false predictions. For instance, segments in the same category could be predicted in different event classes. Learning compact and discriminative multi-modal subspaces is essential for mitigating the issue. To this end, in this paper, we propose a novel Multi-modal Grouping Network, namely MGN, for explicitly semantic-aware grouping. Specifically, MGN aggregates event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens. Furthermore, it leverages the cross-modal grouping for modality-aware prediction to match the video-level target. Our simple framework achieves improving results against previous baselines on weakly-supervised audio-visual video parsing. In addition, our MGN is much more lightweight, using only 47.2% of the parameters of baselines (17 MB vs. 36 MB). Code is available at https://github.com/stoneMo/MGN.",https://api.openreview.net/pdf/dc0041f6a0b5a067380ad2bdb596cb1ca2c5bc02.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Multi-modal+Grouping+Network+for+Weakly-Supervised+Audio-Visual+Video+Parsing
Harmonizing the object recognition strategies of deep neural networks with humans,2022,NIPS,"['Thomas FEL', 'Ivan F Rodriguez Rodriguez', 'Drew Linsley', 'Thomas Serre']",poster,"['Cognitive science', 'human vision', 'explainable AI', 'models of biological vision', 'AI alignment', 'scaling laws']","The many successes of deep neural networks (DNNs) over the past decade have largely been driven by computational scale rather than insights from biological intelligence. Here, we explore if these trends have also carried concomitant improvements in explaining the visual strategies humans rely on for object recognition. We do this by comparing two related but distinct properties of visual strategies in humans and DNNs: where they believe important visual features are in images and how they use those features to categorize objects. Across 84 different DNNs trained on ImageNet and three independent datasets measuring the where and the how of human visual strategies for object recognition on those images, we find a systematic trade-off between DNN categorization accuracy and alignment with human visual strategies for object recognition. \textit{State-of-the-art DNNs are progressively becoming less aligned with humans as their accuracy improves}. We rectify this growing issue with our neural harmonizer: a general-purpose training routine that both aligns DNN and human visual strategies and improves categorization accuracy. Our work represents the first demonstration that the scaling laws that are guiding the design of DNNs today have also produced worse models of human vision. We release our code and data at https://serre-lab.github.io/Harmonization to help the field build more human-like DNNs.
",https://api.openreview.net/pdf/e9d4db7e055c16582021f2913d1fe8cbca64eebd.pdf,transformer;multimodal,https://scholar.google.com/scholar?q=Harmonizing+the+object+recognition+strategies+of+deep+neural+networks+with+humans
One Model to Edit Them All: Free-Form Text-Driven Image Manipulation with Semantic Modulations,2022,NIPS,"['Yiming Zhu', 'Hongyu Liu', 'Yibing Song', 'Ziyang Yuan', 'Xintong Han', 'Chun Yuan', 'Qifeng Chen', 'Jue Wang']",poster,[],"Free-form text prompts allow users to describe their intentions during image manipulation conveniently. Based on the visual latent space of StyleGAN[21] and text embedding space of CLIP[34], studies focus on how to map these two latent spaces for text-driven attribute manipulations. Currently, the latent mapping between these two spaces is empirically designed and confines that each manipulation model can only handle one fixed text prompt. In this paper, we propose a method named Free-Form CLIP (FFCLIP), aiming to  establish an automatic latent mapping so that one manipulation model handles free-form text prompts. Our FFCLIP has a cross-modality semantic modulation module containing semantic alignment and injection. The semantic alignment performs the automatic latent mapping via linear transformations with a cross attention mechanism. After alignment, we inject semantics from text prompt embeddings to the StyleGAN latent space. For one type of image (e.g., `human portrait'), one FFCLIP model can be learned to handle free-form text prompts. Meanwhile, we observe that although each training text prompt only contains a single semantic meaning, FFCLIP can leverage text prompts with multiple semantic meanings for image manipulation. In the experiments, we evaluate FFCLIP on three types of images (i.e., `human portraits', `cars', and `churches'). Both visual and numerical results show that FFCLIP effectively produces semantically accurate and visually realistic images. Project page:  https://github.com/KumapowerLIU/FFCLIP.",https://api.openreview.net/pdf/894979cb9ce6af3c7cb669007b63963895841b0c.pdf,zero_few-shot;transformer;multimodal;llm,https://scholar.google.com/scholar?q=One+Model+to+Edit+Them+All:+Free-Form+Text-Driven+Image+Manipulation+with+Semantic+Modulations
Divert More Attention to Vision-Language Tracking,2022,NIPS,"['Mingzhe Guo', 'Zhipeng Zhang', 'Heng Fan', 'Liping Jing']",poster,"['Visual Object Tracking', 'Multimodal Learning', 'Vision-Language Representation', 'Asymmetrical Searching Strategy']","Relying on Transformer for complex visual feature learning, object tracking has witnessed the new standard for state-of-the-arts (SOTAs). However, this advancement accompanies by larger training data and longer training period, making tracking increasingly expensive. In this paper, we demonstrate that the Transformer-reliance is not necessary and the pure ConvNets are still competitive and even better yet more economical and friendly in achieving SOTA tracking. Our solution is to unleash the power of multimodal vision-language (VL) tracking, simply using ConvNets. The essence lies in learning novel unified-adaptive VL representations with our modality mixer (ModaMixer) and asymmetrical ConvNet search. We show that our unified-adaptive VL representation, learned purely with the ConvNets, is a simple yet strong alternative to Transformer visual features, by unbelievably improving a CNN-based Siamese tracker by 14.5% in SUC on challenging LaSOT (50.7%$\rightarrow$65.2%), even outperforming several Transformer-based SOTA trackers. Besides empirical results, we theoretically analyze our approach to evidence its effectiveness. By revealing the potential of VL representation, we expect the community to divert more attention to VL tracking and hope to open more possibilities for future tracking beyond Transformer. Code and models are released at https://github.com/JudasDie/SOTS.",https://api.openreview.net/pdf/238c77e8206b8221a48562b2e1299ac620e72e8f.pdf,graph;transformer;representation;adaptive;metric;multimodal,https://scholar.google.com/scholar?q=Divert+More+Attention+to+Vision-Language+Tracking
Motion Transformer with Global Intention Localization and Local Movement Refinement,2022,NIPS,"['Shaoshuai Shi', 'Li Jiang', 'Dengxin Dai', 'Bernt Schiele']",poster,"['Motion Prediction', 'Autonomous Driving', 'Transformer']","Predicting multimodal future behavior of traffic participants is essential for robotic vehicles to make safe decisions. Existing works explore to directly predict future trajectories based on latent features or utilize dense goal candidates to identify agent's destinations, where the former strategy converges slowly since all motion modes are derived from the same feature while the latter strategy has efficiency issue since its performance highly relies on the density of goal candidates. In this paper, we propose the Motion TRansformer (MTR) framework that models motion prediction as the joint optimization of global intention localization and local movement refinement. Instead of using goal candidates, MTR incorporates spatial intention priors by adopting a small set of learnable motion query pairs. Each motion query pair takes charge of trajectory prediction and refinement for a specific motion mode, which stabilizes the training process and facilitates better multimodal predictions. Experiments show that MTR achieves state-of-the-art performance on both the marginal and joint motion prediction challenges, ranking 1st on the leaderbaords of Waymo Open Motion Dataset. Code will be available at https://github.com/sshaoshuai/MTR.",https://api.openreview.net/pdf/bd8b622711a5534d7691d1f1c6c653ea427c5612.pdf,reinforcement learning;graph;optimization;zero_few-shot;transformer;multimodal,https://scholar.google.com/scholar?q=Motion+Transformer+with+Global+Intention+Localization+and+Local+Movement+Refinement
Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection,2022,NIPS,"['Hanoona Abdul Rasheed', 'Muhammad Maaz', 'Muhammd Uzair Khattak', 'Salman Khan', 'Fahad Khan']",poster,"['object detection', 'open vocabulary', 'vision-language pretraining']","Existing open-vocabulary object detectors typically enlarge their vocabulary sizes by leveraging different forms of weak supervision. This helps generalize to novel objects at inference. Two popular forms of weak-supervision used in open-vocabulary detection (OVD) include pretrained CLIP model and image-level supervision. We note that both these modes of supervision are not optimally aligned for the detection task: CLIP is trained with image-text pairs and lacks precise localization of objects while the image-level supervision has been used with heuristics that do not accurately specify local object regions. In this work, we propose to address this problem by performing object-centric alignment  of the language embeddings from the CLIP model. Furthermore, we visually ground the objects with only image-level supervision using a pseudo-labeling process that provides high-quality object proposals and helps expand the vocabulary during training. We establish a bridge between the above two object-alignment strategies via a novel weight transfer function that aggregates their complimentary strengths. In essence, the proposed model seeks to minimize the gap between object and image-centric representations in the OVD setting. On the COCO benchmark, our proposed approach achieves 36.6 AP50 on novel classes, an absolute 8.2 gain over the previous best performance. For LVIS, we surpass the state-of-the-art ViLD model by 5.0 mask AP for rare categories and 3.4 overall. Code: https://github.com/hanoonaR/object-centric-ovd.",https://api.openreview.net/pdf/47936ea4dc7738bd4a0a5724d104213bd02bd0bf.pdf,graph;zero_few-shot;representation;inference;transfer learning;multimodal,https://scholar.google.com/scholar?q=Bridging+the+Gap+between+Object+and+Image-level+Representations+for+Open-Vocabulary+Detection
Behavior Transformers: Cloning $k$ modes with one stone,2022,NIPS,"['Nur Muhammad Mahi Shafiullah', 'Zichen Jeff Cui', 'Ariuntuya Altanzaya', 'Lerrel Pinto']",poster,"['Behavioral cloning', 'learning from demonstrations']","While behavior learning has made impressive progress in recent times, it lags behind computer vision and natural language processing due to its inability to leverage large, human-generated datasets. Human behavior has a wide variance, multiple modes, and human demonstrations naturally do not come with reward labels. These properties limit the applicability of current methods in Offline RL and Behavioral Cloning to learn from large, pre-collected datasets. In this work, we present Behavior Transformer (BeT), a new technique to model unlabeled demonstration data with multiple modes. BeT retrofits standard transformer architectures with action discretization coupled with a multi-task action correction inspired by offset prediction in object detection. This allows us to leverage the multi-modal modeling ability of modern transformers to predict multi-modal continuous actions. We experimentally evaluate BeT on a variety of robotic manipulation and self-driving behavior datasets. We show that BeT significantly improves over prior state-of-the-art work on solving demonstrated tasks while capturing the major modes present in the pre-collected datasets. Finally, through an extensive ablation study, we further analyze the importance of every crucial component in BeT. Videos of behavior generated by BeT are available here: https://mahis.life/bet",https://api.openreview.net/pdf/29ea450898bb415f691a46ae04ed4eea62f029a0.pdf,offline reinforcement learning;zero_few-shot;transformer;multi-task;multimodal,https://scholar.google.com/scholar?q=Behavior+Transformers:+Cloning+$k$+modes+with+one+stone
Deliberated Domain Bridging for Domain Adaptive Semantic Segmentation,2022,NIPS,"['Lin Chen', 'Zhixiang Wei', 'Xin Jin', 'Huaian Chen', 'Miao Zheng', 'Kai Chen', 'Yi Jin']",poster,"['domain adaptive semantic segmentation', 'domain bridging', 'data mixing']","In unsupervised domain adaptation (UDA), directly adapting from the source to the target domain usually suffers significant discrepancies and leads to insufficient alignment. Thus, many UDA works attempt to vanish the domain gap gradually and softly via various intermediate spaces, dubbed domain bridging (DB). However, for dense prediction tasks such as domain adaptive semantic segmentation (DASS), existing solutions have mostly relied on rough style transfer and how to elegantly bridge domains is still under-explored. In this work, we resort to data mixing to establish a deliberated domain bridging (DDB) for DASS, through which the joint distributions of source and target domains are aligned and interacted with each in the intermediate space. At the heart of DDB lies a dual-path domain bridging step for generating two intermediate domains using the coarse-wise and the fine-wise data mixing techniques, alongside a cross-path knowledge distillation step for taking two complementary models trained on generated intermediate samples as ‘teachers’ to develop a superior ‘student’ in a multi-teacher distillation manner. These two optimization steps work in an alternating way and reinforce each other to give rise to DDB with strong adaptation power. Extensive experiments on adaptive segmentation tasks with different settings demonstrate that our DDB significantly outperforms state-of-the-art methods.",https://api.openreview.net/pdf/31bffac3587ff471008c570251865d69693e5b03.pdf,graph;optimization;adaptive;transfer learning;distillation;segmentation;multimodal,https://scholar.google.com/scholar?q=Deliberated+Domain+Bridging+for+Domain+Adaptive+Semantic+Segmentation
4D Unsupervised Object Discovery,2022,NIPS,"['Yuqi Wang', 'Yuntao Chen', 'Zhaoxiang Zhang']",poster,['Unsupervised Object Detection'],"Object discovery is a core task in computer vision. While fast progresses have been made in supervised object detection, its unsupervised counterpart remains largely unexplored. With the growth of data volume, the expensive cost of annotations is the major limitation hindering further study.  Therefore, discovering objects without annotations has great significance. However, this task seems impractical on still-image or point cloud alone due to the lack of discriminative information. Previous studies underlook the crucial temporal information and constraints naturally behind multi-modal inputs. In this paper, we propose 4D unsupervised object discovery, jointly discovering objects from 4D data -- 3D point clouds and 2D RGB images with temporal information. We present the first practical approach for this task by proposing a ClusterNet on 3D point clouds, which is jointly iteratively optimized with a 2D localization network. Extensive experiments on the large-scale Waymo Open Dataset suggest that the localization network and ClusterNet achieve competitive performance on both class-agnostic 2D object detection and 3D instance segmentation, bridging the gap between unsupervised methods and full supervised ones. Codes and models will be made available at https://github.com/Robertwyq/LSMOL.",https://api.openreview.net/pdf/e4c5f36340507ecdb9ccc068ee0733463a60eb68.pdf,graph;optimization;transformer;segmentation;multimodal;3d,https://scholar.google.com/scholar?q=4D+Unsupervised+Object+Discovery
Learn what matters: cross-domain imitation learning with task-relevant embeddings,2022,NIPS,"['Tim Franzmeyer', 'Philip Torr', 'Joao F. Henriques']",poster,"['Reinforcement Learning', 'Learning from Observations', 'Inverse Reinforcement Learning', 'Imitation Learning', 'Domain Transfer']","We study how an autonomous agent learns to perform a task from demonstrations in a different domain, such as a different environment or different agent. Such cross-domain imitation learning is required to, for example, train an artificial agent from demonstrations of a human expert. We propose a scalable framework that enables cross-domain imitation learning without access to additional demonstrations or further domain knowledge. We jointly train the learner agent's policy and learn a mapping between the learner and expert domains with adversarial training. We effect this by using a mutual information criterion to find an embedding of the expert's state space that contains task-relevant information and is invariant to domain specifics. This step significantly simplifies estimating the mapping between the learner and expert domains and hence facilitates end-to-end learning. We demonstrate successful transfer of policies between considerably different domains, without extra supervision such as additional demonstrations, and in situations where other methods fail.",https://api.openreview.net/pdf/039caa11df2310d4b7a8f753da86c287559f7feb.pdf,reinforcement learning;transfer learning;multimodal;imitation learning,https://scholar.google.com/scholar?q=Learn+what+matters:+cross-domain+imitation+learning+with+task-relevant+embeddings
DeepInteraction: 3D Object Detection via Modality Interaction,2022,NIPS,"['Zeyu Yang', 'Jiaqi Chen', 'Zhenwei Miao', 'Wei Li', 'Xiatian Zhu', 'Li Zhang']",poster,[],"Existing top-performance 3D object detectors typically rely on the multi-modal fusion strategy. This design is however fundamentally restricted due to overlooking the modality-specific useful information and finally hampering the model performance. To address this limitation, in this work we introduce a novel modality interaction strategy where individual per-modality representations are learned and maintained throughout for enabling their unique characteristics to be exploited during object detection. To realize this proposed strategy, we design a DeepInteraction architecture characterized by a multi-modal representational interaction encoder and a multi-modal predictive interaction decoder. Experiments on the large-scale nuScenes dataset show that our proposed method surpasses all prior arts often by a large margin. Crucially, our method is ranked at the first position at the highly competitive nuScenes object detection leaderboard.",https://api.openreview.net/pdf/7b0565b7a443be615111e9624d564f4da5f25cae.pdf,graph;representation;multimodal;3d,https://scholar.google.com/scholar?q=DeepInteraction:+3D+Object+Detection+via+Modality+Interaction
Let Images Give You More: Point Cloud Cross-Modal Training for Shape Analysis,2022,NIPS,"['Xu Yan', 'Heshen Zhan', 'Chaoda Zheng', 'Jiantao Gao', 'Ruimao Zhang', 'Shuguang Cui', 'Zhen Li']",poster,[],"Although recent point cloud analysis achieves impressive progress, the paradigm of representation learning from single modality gradually meets its bottleneck. In this work, we take a step towards more discriminative 3D point cloud representation using 2D images, which inherently contain richer appearance information, e.g., texture, color, and shade. Specifically, this paper introduces a simple but effective point cloud cross-modality training (PointCMT) strategy, which utilizes view-images, i.e., rendered or projected 2D images of the 3D object, to boost point cloud classification. In practice, to effectively acquire auxiliary knowledge from view-images, we develop a teacher-student framework and formulate the cross-modal learning as a knowledge distillation problem. Through novel feature and classifier enhancement criteria, PointCMT eliminates the distribution discrepancy between different modalities and avoid potential negative transfer effectively. Note that PointCMT efficiently improves the point-only representation without any architecture modification. Sufficient experiments verify significant gains on various datasets based on several backbones, i.e., equipped with PointCMT, PointNet++ and PointMLP achieve state-of-the-art performance on two benchmarks, i.e., 94.4% and 86.7% accuracy on ModelNet40 and ScanObjectNN, respectively.",https://api.openreview.net/pdf/36d2915aefb0dcef5c657875afe8fd1dd735c094.pdf,representation;transfer learning;distillation;multimodal;3d,https://scholar.google.com/scholar?q=Let+Images+Give+You+More:+Point+Cloud+Cross-Modal+Training+for+Shape+Analysis
LGDN: Language-Guided Denoising Network for Video-Language Modeling,2022,NIPS,"['Haoyu Lu', 'Mingyu Ding', 'Nanyi Fei', 'Yuqi Huo', 'Zhiwu Lu']",poster,"['video-language modeling', 'video-text retrieval', 'language supervision', 'cross-modal alignment']","Video-language modeling has attracted much attention with the rapid growth of web videos. Most existing methods assume that the video frames and text description are semantically correlated, and focus on video-language modeling at video level. However, this hypothesis often fails for two reasons: (1) With the rich semantics of video contents, it is difficult to cover all frames with a single video-level description; (2) A raw video typically has noisy/meaningless information (e.g., scenery shot, transition or teaser). Although a number of recent works deploy attention mechanism to alleviate this problem, the irrelevant/noisy information still makes it very difficult to address. To overcome such challenge, we thus propose an efficient and effective model, termed Language-Guided Denoising Network (LGDN), for video-language modeling. Different from most existing methods that utilize all extracted video frames, LGDN dynamically filters out the misaligned or redundant frames under the language supervision and obtains only 2--4 salient frames per video for cross-modal token-level alignment. Extensive experiments on five public datasets show that our LGDN outperforms the state-of-the-arts by large margins. We also provide detailed ablation study to reveal the critical importance of solving the noise issue, in hope of inspiring future video-language work.",https://api.openreview.net/pdf/9ab0c0f58eacbbc1301359a2c6b19189a5ba4b36.pdf,graph;transformer;multimodal,https://scholar.google.com/scholar?q=LGDN:+Language-Guided+Denoising+Network+for+Video-Language+Modeling
OmniVL: One Foundation Model for Image-Language and Video-Language Tasks,2022,NIPS,"['Junke Wang', 'Dongdong Chen', 'Zuxuan Wu', 'Chong Luo', 'Luowei Zhou', 'Yucheng Zhao', 'Yujia Xie', 'Ce Liu', 'Yu-Gang Jiang', 'Lu Yuan']",poster,"['Vision-Language Pretraining', 'Unified Foundation Model']","This paper presents OmniVL, a new foundation model to support both image-language and video-language tasks using one universal architecture. It adopts a unified transformer-based visual encoder for both image and video inputs, and thus can perform joint image-language and video-language pretraining. We demonstrate, for the first time, such a paradigm benefits both image and video tasks, as opposed to the conventional one-directional transfer (e.g., use image-language to help video-language). To this end, we propose a \emph{decoupled} joint pretraining of image-language and video-language to effectively decompose the vision-language modeling into spatial and temporal dimensions and obtain performance boost on both image and video tasks. Moreover, we introduce a novel unified vision-language contrastive (UniVLC) loss to leverage image-text, video-text, image-label (e.g., image classification), video-label (e.g., video action recognition) data together, so that both supervised and noisily supervised pretraining data are utilized as much as possible. Without incurring extra task-specific adaptors, OmniVL can simultaneously support visual only tasks (e.g., image classification, video action recognition), cross-modal alignment tasks (e.g., image/video-text retrieval), and multi-modal understanding and generation tasks (e.g., image/video question answering, captioning). We evaluate OmniVL on a wide range of downstream tasks and achieve state-of-the-art or competitive results with similar model size and data scale.",https://api.openreview.net/pdf/0fe438e64bb5cdf642a6f7f624c76f13c181bd4a.pdf,transformer;generative model;contrastive learning;transfer learning;multimodal,https://scholar.google.com/scholar?q=OmniVL:+One+Foundation+Model+for+Image-Language+and+Video-Language+Tasks
Zero-Shot Video Question Answering via Frozen Bidirectional Language Models,2022,NIPS,"['Antoine Yang', 'Antoine Miech', 'Josef Sivic', 'Ivan Laptev', 'Cordelia Schmid']",poster,"['Video Question Answering', 'Zero-Shot', 'Vision and Language', 'Computer Vision']","Video question answering (VideoQA) is a complex task that requires diverse multi-modal data for training. Manual annotation of question and answers for videos, however, is tedious and prohibits scalability. To tackle this problem, recent methods consider zero-shot settings with no manual annotation of visual question-answer. In particular, a promising approach adapts frozen autoregressive language models pretrained on Web-scale text-only data to multi-modal inputs. In contrast, we here build on frozen bidirectional language models (BiLM) and show that such an approach provides a stronger and cheaper alternative for zero-shot VideoQA. In particular, (i) we combine visual inputs with the frozen BiLM using light trainable modules, (ii) we train such modules using Web-scraped multi-modal data, and finally (iii) we perform zero-shot VideoQA inference through masked language modeling, where the masked text is the answer to a given question. Our proposed approach, FrozenBiLM, outperforms the state of the art in zero-shot VideoQA by a significant margin on a variety of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA, TGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in the few-shot and fully-supervised setting. Our code and models are publicly available at https://github.com/antoyang/FrozenBiLM.",https://api.openreview.net/pdf/77eb1542ab76214cf61612cee1b78cc12ac39763.pdf,graph;zero_few-shot;inference;multimodal;llm,https://scholar.google.com/scholar?q=Zero-Shot+Video+Question+Answering+via+Frozen+Bidirectional+Language+Models
Align then Fusion: Generalized Large-scale Multi-view Clustering with Anchor Matching Correspondences,2022,NIPS,"['Siwei Wang', 'Xinwang Liu', 'Suyuan Liu', 'Jiaqi Jin', 'Wenxuan Tu', 'Xinzhong Zhu', 'En Zhu']",poster,"['multi-view graph clustering', 'anchor graph clustering']","Multi-view anchor graph clustering selects representative anchors to avoid full pair-wise similarities and therefore reduce the complexity of graph methods. Although widely applied in large-scale applications, existing approaches do not pay sufficient attention to establishing correct correspondences between the anchor sets across views. To be specific, anchor graphs obtained from different views are not aligned column-wisely. Such an Anchor-Unaligned Problem (AUP) would cause inaccurate graph fusion and degrade the clustering performance. Under multi-view scenarios, generating correct correspondences could be extremely difficult since anchors are not consistent in feature dimensions. To solve this challenging issue, we propose the first study of the generalized and flexible anchor graph fusion framework termed Fast Multi-View Anchor-Correspondence Clustering (FMVACC). Specifically, we show how to find anchor correspondence with both feature and structure information, after which anchor graph fusion is performed column-wisely. Moreover, we theoretically show the connection between FMVACC and existing multi-view late fusion and partial view-aligned clustering, which further demonstrates our generality. Extensive experiments on seven benchmark datasets demonstrate the effectiveness and efficiency of our proposed method. Moreover, the proposed alignment module also shows significant performance improvement applying to existing multi-view anchor graph competitors indicating the importance of anchor alignment. Our code is available at \url{https://github.com/wangsiwei2010/NeurIPS22-FMVACC}.",https://api.openreview.net/pdf/13e0c6bc1ebe75ecd5fe66b95f4e75d3f0d84c32.pdf,graph;transformer;multimodal;multi-view,https://scholar.google.com/scholar?q=Align+then+Fusion:+Generalized+Large-scale+Multi-view+Clustering+with+Anchor+Matching+Correspondences
Mix and Reason: Reasoning over Semantic Topology with Data Mixing for Domain Generalization,2022,NIPS,"['Chaoqi Chen', 'Luyao Tang', 'Feng Liu', 'Gangming Zhao', 'Yue Huang', 'Yizhou Yu']",poster,"['Domain generalization', 'distribution shift', 'semantic topology', 'structural invariance', 'data mixing']","Domain generalization (DG) enables generalizing a learning machine from multiple seen source domains to an unseen target one. The general objective of DG methods is to learn semantic representations that are independent of domain labels, which is theoretically sound but empirically challenged due to the complex mixture of common and domain-specific factors. Although disentangling the representations into two disjoint parts has been gaining momentum in DG, the strong presumption over the data limits its efficacy in many real-world scenarios. In this paper, we propose Mix and Reason (MiRe), a new DG framework that learns semantic representations via enforcing the structural invariance of semantic topology. MiRe consists of two key components, namely,  Category-aware Data Mixing (CDM) and Adaptive Semantic Topology Refinement (ASTR). CDM mixes two images from different domains in virtue of activation maps generated by two complementary classification losses, making the classifier focus on the representations of semantic objects. ASTR introduces relation graphs to represent semantic topology, which is progressively refined via the interactions between local feature aggregation and global cross-domain relational reasoning. Experiments on multiple DG benchmarks validate the effectiveness and robustness of the proposed MiRe. ",https://api.openreview.net/pdf/7954cfae8ed1b6251a78e1d0c5315a7a2b79e26b.pdf,graph;representation;adaptive;multimodal,https://scholar.google.com/scholar?q=Mix+and+Reason:+Reasoning+over+Semantic+Topology+with+Data+Mixing+for+Domain+Generalization
Fine-Grained Semantically Aligned Vision-Language Pre-Training,2022,NIPS,"['Juncheng Li', 'XIN HE', 'Longhui Wei', 'Long Qian', 'Linchao Zhu', 'Lingxi Xie', 'Yueting Zhuang', 'Qi Tian', 'Siliang Tang']",poster,"['Vision-Language Pre-Training', 'Multimodal Pre-Training', 'Vision and Language', 'Cross-Modal Reasoning', 'Image-Text Retrieval']","Large-scale vision-language pre-training has shown impressive advances in a wide range of downstream tasks. Existing methods mainly model the cross-modal alignment by the similarity of the global representations of images and text, or advanced cross-modal attention upon image and text features. However, they fail to explicitly learn the fine-grained semantic alignment between visual regions and textual phrases, as only global image-text alignment information is available. In this paper, we introduce LOUPE, a fine-grained semantically aLigned visiOn-langUage PrE-training framework, which learns fine-grained semantic alignment from the novel perspective of game-theoretic interactions. To efficiently estimate the game-theoretic interactions, we further propose an uncertainty-aware neural Shapley interaction learning module. Experiments show that LOUPE achieves state-of-the-art performance on a variety of  vision-language tasks. Without any object-level human annotations and fine-tuning, LOUPE achieves competitive performance on object detection and visual grounding. More importantly, LOUPE opens a new promising direction of learning fine-grained semantics from large-scale raw image-text pairs.",https://api.openreview.net/pdf/76a1bb2b0e5ea025f5f32ef2a39bd5c9c7cd9ffc.pdf,transformer;representation;multimodal,https://scholar.google.com/scholar?q=Fine-Grained+Semantically+Aligned+Vision-Language+Pre-Training
Domino: Discovering Systematic Errors with Cross-Modal Embeddings,2022,ICLR,"['Sabri Eyuboglu', 'Maya Varma', 'Khaled Kamal Saab', 'Jean-Benoit Delbrouck', 'Christopher Lee-Messer', 'Jared Dunnmon', 'James Zou', 'Christopher Re']",oral,"['robustness', 'subgroup analysis', 'error analysis', 'multimodal', 'slice discovery']","Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices on which a model performs poorly. To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent (i.e. united by a human-understandable concept). However, no quantitative evaluation framework currently exists for rigorously assessing SDMs with respect to these criteria. Additionally, prior qualitative evaluations have shown that SDMs often identify slices that are incoherent. In this work, we address these challenges by first designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data).
Then, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices. We find that Domino accurately identifies 36% of the 1,235 slices in our framework -- a 12 percentage point improvement over prior methods. Further, Domino is the first SDM that can provide natural language descriptions of identified slices, correctly generating the exact name of the slice in 35% of settings. ",https://api.openreview.net/pdf/a5ca838a35d810400cfa090453cd85abe02ab6b0.pdf,graph;representation;multimodal,https://scholar.google.com/scholar?q=Domino:+Discovering+Systematic+Errors+with+Cross-Modal+Embeddings
Poisoning and Backdooring Contrastive Learning,2022,ICLR,"['Nicholas Carlini', 'Andreas Terzis']",oral,"['Contrastive Learning', 'Poisoning attack', 'Backdoor attack', 'CLIP']","Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input  with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.",https://api.openreview.net/pdf/abd77f0543a72cd26da355efc5680de233f120af.pdf,contrastive learning;multimodal,https://scholar.google.com/scholar?q=Poisoning+and+Backdooring+Contrastive+Learning
Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling,2022,ICLR,"['Bo Wan', 'Wenjuan Han', 'Zilong Zheng', 'Tinne Tuytelaars']",oral,"['Grammar Induction', 'Vision-Language Matching', 'Unsupervised Learning']","We introduce a new task, unsupervised vision-language (VL) grammar induction. Given an image-caption pair, the goal is to extract a shared hierarchical structure for both image and language simultaneously.  We argue that such structured output, grounded in both modalities, is a clear step towards the high-level understanding of multimodal information. Besides challenges existing in conventional visually grounded grammar induction tasks, VL grammar induction requires a model to capture contextual semantics and perform a fine-grained alignment. To address these challenges, we propose a novel method, CLIORA, which constructs a shared vision-language constituency tree structure with context-dependent semantics for all possible phrases in different levels of the tree. It computes a matching score between each constituent and image region, trained via contrastive learning.  It integrates two levels of fusion, namely at feature-level and at score-level, so as to allow fine-grained alignment. We introduce a new evaluation metric for VL grammar induction, CCRA, and show a 3.3% improvement over a strong baseline on Flickr30k Entities. We also evaluate our model via two derived tasks, i.e., language grammar induction and phrase grounding, and improve over the state-of-the-art for both.",https://api.openreview.net/pdf/5c104842d13e8d6efd55b6d7c04f4373a39eae18.pdf,zero_few-shot;contrastive learning;metric;multimodal,https://scholar.google.com/scholar?q=Unsupervised+Vision-Language+Grammar+Induction+with+Shared+Structure+Modeling
PiCO: Contrastive Label Disambiguation for Partial Label Learning,2022,ICLR,"['Haobo Wang', 'Ruixuan Xiao', 'Yixuan Li', 'Lei Feng', 'Gang Niu', 'Gang Chen', 'Junbo Zhao']",oral,"['Partial Label Learning', 'Contrastive Learning', 'Prototype-based Disambiguation']","Partial label learning (PLL) is an important problem that allows each training example to be labeled with a coarse candidate set, which well suits many real-world data annotation scenarios with label ambiguity.  Despite the promise, the performance of PLL often lags behind the supervised counterpart. In this work, we bridge the gap by addressing two key research challenges in PLL---representation learning and label disambiguation---in one coherent framework. Specifically, our proposed framework PiCO consists of a contrastive learning module along with a novel class prototype-based label disambiguation algorithm. PiCO produces closely aligned representations for examples from the same classes and facilitates label disambiguation. Theoretically, we show that these two components are mutually beneficial, and can be rigorously justified from an expectation-maximization (EM) algorithm perspective. Extensive experiments demonstrate that PiCO significantly outperforms the current state-of-the-art approaches in PLL and even achieves comparable results to fully supervised learning. Code and data available: https://github.com/hbzju/PiCO.",https://api.openreview.net/pdf/f9275b96d741f229db4e61a15ce5f2a499c9ee67.pdf,zero_few-shot;representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=PiCO:+Contrastive+Label+Disambiguation+for+Partial+Label+Learning
StyleAlign: Analysis and Applications of Aligned StyleGAN Models,2022,ICLR,"['Zongze Wu', 'Yotam Nitzan', 'Eli Shechtman', 'Dani Lischinski']",oral,"['StyleGAN', 'transfer learning', 'fine tuning', 'model alignment', 'image-to-image translation', 'image morphing']","In this paper, we perform an in-depth study of the properties and applications of aligned generative models.
We refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the first detailed exploration of model alignment, also focusing on StyleGAN. First, we empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, we find that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple fine-tuning and inversion. ",https://api.openreview.net/pdf/a75f48f49713ac38baaaee51cb3273177975f96b.pdf,zero_few-shot;generative model;transfer learning;multimodal,https://scholar.google.com/scholar?q=StyleAlign:+Analysis+and+Applications+of+Aligned+StyleGAN+Models
Large Language Models Can Be Strong Differentially Private Learners,2022,ICLR,"['Xuechen Li', 'Florian Tramer', 'Percy Liang', 'Tatsunori Hashimoto']",oral,"['language model', 'differential privacy', 'language generation', 'fine-tuning', 'NLP']","Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead.
We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure.
With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines---by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. 
To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. 
The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. 
Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models tends to not suffer from dimension-dependent performance degradation.
Code to reproduce results can be found at https://github.com/lxuechen/private-transformers.
",https://api.openreview.net/pdf/d88e1e721c4085b8a6403837f45b8c483ad0225b.pdf,optimization;zero_few-shot;transformer;multimodal;llm,https://scholar.google.com/scholar?q=Large+Language+Models+Can+Be+Strong+Differentially+Private+Learners
RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation,2022,ICLR,"['Pingchuan Ma', 'Tao Du', 'Joshua B. Tenenbaum', 'Wojciech Matusik', 'Chuang Gan']",oral,"['differentiable rendering', 'differentiable simulation', 'system identification']","This work considers identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible. Existing solutions require massive training data or lack generalizability to unknown rendering configurations. We propose a novel approach that marries domain randomization and differentiable rendering gradients to address this problem. Our core idea is to train a rendering-invariant state-prediction (RISP) network that transforms image differences into state differences independent of rendering configurations, e.g., lighting, shadows, or material reflectance. To train this predictor, we formulate a new loss on rendering variances using gradients from differentiable rendering. Moreover, we present an efficient, second-order method to compute the gradients of this loss, allowing it to be integrated seamlessly into modern deep learning frameworks. We evaluate our method in rigid-body and deformable-body simulation environments using four tasks: state estimation, system identification, imitation learning, and visuomotor control. We further demonstrate the efficacy of our approach on a real-world example: inferring the state and action sequences of a quadrotor from a video of its motion sequences. Compared with existing methods, our approach achieves significantly lower reconstruction errors and has better generalizability among unknown rendering configurations.",https://api.openreview.net/pdf/999353870633727a2d50bc5b4ee873b50401eba7.pdf,zero_few-shot;multimodal;imitation learning,https://scholar.google.com/scholar?q=RISP:+Rendering-Invariant+State+Predictor+with+Differentiable+Simulation+and+Rendering+for+Cross-Domain+Parameter+Estimation
L0-Sparse Canonical Correlation Analysis,2022,ICLR,"['Ofir Lindenbaum', 'Moshe Salhov', 'Amir Averbuch', 'Yuval Kluger']",poster,[],"Canonical Correlation Analysis (CCA) models are powerful for studying the associations between two sets of variables. The canonically correlated representations, termed \textit{canonical variates} are widely used in unsupervised learning to analyze unlabeled multi-modal registered datasets. Despite their success, CCA models may break (or overfit) if the number of variables in either of the modalities exceeds the number of samples. Moreover, often a significant fraction of the variables measures modality-specific information, and thus removing them is beneficial for identifying the \textit{canonically correlated variates}. Here, we propose $\ell_0$-CCA, a method for learning correlated representations based on sparse subsets of variables from two observed modalities.
Sparsity is obtained by multiplying the input variables by stochastic gates, whose parameters are learned together with the CCA weights via an $\ell_0$-regularized correlation loss. 
We further propose $\ell_0$-Deep CCA for solving the problem of non-linear sparse CCA by modeling the correlated representations using deep nets. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, by gating nuisance input variables, our approach improves the extracted representations compared to other linear, non-linear and sparse CCA-based models.",https://api.openreview.net/pdf/69ae8c04ac43812f7523f009313daec68f09ea3d.pdf,representation;sparse;multimodal,https://scholar.google.com/scholar?q=L0-Sparse+Canonical+Correlation+Analysis
DIVA: Dataset Derivative of a Learning Task,2022,ICLR,"['Yonatan Dukler', 'Alessandro Achille', 'Giovanni Paolini', 'Avinash Ravichandran', 'Marzia Polito', 'Stefano Soatto']",poster,"['Leave one out cross validation', 'AutoML', 'dataset optimization']","We present a method to compute the derivative of a learning task with respect to a dataset. A learning task is a function from a training set to the validation error, which can be represented by a trained deep neural network (DNN). The ``dataset derivative'' is a linear operator, computed around the trained model, that informs how perturbations of the weight of each training sample affect the validation error, usually computed on a separate validation dataset.  Our method, DIVA (Differentiable Validation) hinges on a closed-form differentiable expression of the leave-one-out cross-validation error around a pre-trained DNN. Such expression constitutes the dataset derivative. DIVA could be used for dataset auto-curation, for example removing samples with faulty annotations, augmenting a dataset with additional relevant samples, or rebalancing. More generally, DIVA can be used to optimize the dataset, along with the parameters of the model, as part of the training process without the need for a separate validation dataset, unlike bi-level optimization methods customary in AutoML. To illustrate the flexibility of DIVA, we report experiments on sample auto-curation tasks such as outlier rejection, dataset extension, and automatic aggregation of multi-modal data.",https://api.openreview.net/pdf/c20ae574c689fe5fbecb96f791b3e678973e0053.pdf,optimization;multimodal,https://scholar.google.com/scholar?q=DIVA:+Dataset+Derivative+of+a+Learning+Task
MCMC Should Mix: Learning Energy-Based Model with Neural Transport Latent Space MCMC,2022,ICLR,"['Erik Nijkamp', 'Ruiqi Gao', 'Pavel Sountsov', 'Srinivas Vasudevan', 'Bo Pang', 'Song-Chun Zhu', 'Ying Nian Wu']",poster,"['Generative models', 'energy-based models', 'MCMC']","Learning energy-based model (EBM) requires MCMC sampling of the learned model as an inner loop of the learning algorithm. However, MCMC sampling of EBMs in high-dimensional data space is generally not mixing, because the energy function, which is usually parametrized by deep network, is highly multi-modal in the data space. This is a serious handicap for both theory and practice of EBMs. In this paper, we propose to learn EBM with a flow-based model (or in general latent variable model) serving as a backbone, so that the EBM is a correction or an exponential tilting of the flow-based model. We show that the model has a particularly simple form in the space of the latent variables of the generative model, and MCMC sampling of the EBM in the latent space mixes well and traverses modes in the data space. This enables proper sampling and learning of EBMs.",https://api.openreview.net/pdf/e59fd3452037dfc60c95270a1328f0a3077b10f9.pdf,zero_few-shot;generative model;flow;multimodal,https://scholar.google.com/scholar?q=MCMC+Should+Mix:+Learning+Energy-Based+Model+with+Neural+Transport+Latent+Space+MCMC
The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,2022,ICLR,"['Alexander Pan', 'Kush Bhatia', 'Jacob Steinhardt']",poster,"['reward misspecification', 'reinforcement learning', 'reward hacking', 'alignment', 'ml safety']","Reward hacking---where RL agents exploit gaps in misspecified proxy rewards---has been widely observed, but not yet systematically studied. To understand reward hacking, we construct four RL environments with different misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, and observation space noise. Typically, more capable agents are able to better exploit reward misspecifications, causing them to attain higher proxy reward and lower true reward. Moreover, we find instances of \emph{phase transitions}: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To encourage further research on reward misspecification, address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.",https://api.openreview.net/pdf/772b3ddc867fd13a9c98fb15c99f94d5b68ae558.pdf,reinforcement learning;zero_few-shot;multimodal,https://scholar.google.com/scholar?q=The+Effects+of+Reward+Misspecification:+Mapping+and+Mitigating+Misaligned+Models
Cross-Domain Imitation Learning via Optimal Transport,2022,ICLR,"['Arnaud Fickinger', 'Samuel Cohen', 'Stuart Russell', 'Brandon Amos']",poster,"['optimal transportation', 'imitation learning', 'cross-domain imitation learning', 'gromov-Wasserstein']","Cross-domain imitation learning studies how to leverage expert demonstrations of one agent to train an imitation agent with a different embodiment or morphology. Comparing trajectories and stationary distributions between the expert and imitation agents is challenging because they live on different systems that may not even have the same dimensionality. We propose Gromov-Wasserstein Imitation Learning (GWIL), a method for cross-domain imitation that uses the Gromov-Wasserstein distance to align and compare states between the different spaces of the agents. Our theory formally characterizes the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. We demonstrate the effectiveness of GWIL in non-trivial continuous control domains ranging from simple rigid transformation of the expert domain to arbitrary transformation of the state-action space.",https://api.openreview.net/pdf/0e1aa9f9ddcbcd903dbecbe6f034779d158d2260.pdf,reinforcement learning;graph;multimodal;imitation learning,https://scholar.google.com/scholar?q=Cross-Domain+Imitation+Learning+via+Optimal+Transport
Normalization of Language Embeddings for Cross-Lingual Alignment,2022,ICLR,"['Prince Osei Aboagye', 'Yan Zheng', 'Chin-Chia Michael Yeh', 'Junpeng Wang', 'Wei Zhang', 'Liang Wang', 'Hao Yang', 'Jeff Phillips']",poster,"['cross-lingual word embeddings', 'natural language processing']","Learning a good transfer function to map the word vectors from two languages into a shared cross-lingual word vector space plays a crucial role in cross-lingual NLP. It is useful in translation tasks and important in allowing complex models built on a high-resource language like English to be directly applied on an aligned low resource language.  While Procrustes and other techniques can align language models with some success, it has recently been identified that structural differences (for instance, due to differing word frequency) create different profiles for various monolingual embedding. When these profiles differ across languages, it correlates with how well languages can align and their performance on cross-lingual downstream tasks.  In this work, we develop a very general language embedding normalization procedure, building and subsuming various previous approaches, which removes these structural profiles across languages without destroying their intrinsic meaning.  We demonstrate that meaning is retained and alignment is improved on similarity, translation, and cross-language classification tasks.  Our proposed normalization clearly outperforms all prior approaches like centering and vector normalization on each task and with each alignment approach. ",https://api.openreview.net/pdf/1cdf6da2d049db967f45c6454ba03f40aa1e3849.pdf,zero_few-shot;transfer learning;multimodal,https://scholar.google.com/scholar?q=Normalization+of+Language+Embeddings+for+Cross-Lingual+Alignment
Attention-based Interpretability with Concept Transformers,2022,ICLR,"['Mattia Rigotti', 'Christoph Miksovic', 'Ioana Giurgiu', 'Thomas Gschwind', 'Paolo Scotton']",poster,"['attention', 'transformer', 'concepts', 'interpretability']","Attention is a mechanism that has been instrumental in driving remarkable performance gains of deep neural network models in a host of visual, NLP and multimodal tasks.
One additional notable aspect of attention is that it conveniently exposes the ``reasoning'' behind each particular output generated by the model.
Specifically, attention scores over input regions or intermediate features have been interpreted as a measure of the contribution of the attended element to the model inference.
While the debate in regard to the interpretability of attention is still not settled, researchers have pointed out the existence of architectures and scenarios that afford a meaningful interpretation of the attention mechanism.

Here we propose the generalization of attention from low-level input features to high-level concepts as a mechanism to ensure the interpretability of attention scores within a given application domain.
In particular, we design the ConceptTransformer, a deep learning module that exposes explanations of the output of a model in which it is embedded in terms of attention over user-defined high-level concepts.
Such explanations are \emph{plausible} (i.e.\ convincing to the human user) and \emph{faithful} (i.e.\ truly reflective of the reasoning process of the model).
Plausibility of such explanations is obtained by construction by training the attention heads to conform with known relations between inputs, concepts and outputs dictated by domain knowledge.
Faithfulness is achieved by design by enforcing a linear relation between the transformer value vectors that represent the concepts and their contribution to the classification log-probabilities.

We validate our ConceptTransformer module on established explainability benchmarks and show how it can be used to infuse domain knowledge into classifiers to improve accuracy, and conversely to extract concept-based explanations of classification outputs. Code to reproduce our results is available at: \url{https://github.com/ibm/concept_transformer}.",https://api.openreview.net/pdf/d910731148e5b8279d1974d45e83aada94c35e55.pdf,zero_few-shot;transformer;inference;multimodal,https://scholar.google.com/scholar?q=Attention-based+Interpretability+with+Concept+Transformers
THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling,2022,ICLR,"['Thomas Gilles', 'Stefano Sabatini', 'Dzmitry Tsishkou', 'Bogdan Stanciulescu', 'Fabien Moutarde']",poster,"['Trajectory prediction', 'Multi-agent', 'Motion forecasting', 'Motion estimation', 'Autonomous driving']","In this paper, we propose THOMAS, a joint multi-agent trajectory prediction framework allowing for an efficient and consistent prediction of multi-agent multi-modal trajectories. We present a unified model architecture for simultaneous agent future heatmap estimation, in which we leverage hierarchical and sparse image generation for fast and memory-efficient inference. We propose a learnable trajectory recombination model that takes as input a set of predicted trajectories for each agent and outputs its consistent reordered recombination. This recombination module is able to realign the initially independent modalities so that they do no collide and are coherent with each other.  We report our results on the Interaction multi-agent prediction challenge and rank $1^{st}$ on the online test leaderboard.",https://api.openreview.net/pdf/a8ce9facf1e0dfc642c02f9849f5b7910589efad.pdf,reinforcement learning;zero_few-shot;generative model;online learning;inference;sparse;multi-agent;multimodal,https://scholar.google.com/scholar?q=THOMAS:+Trajectory+Heatmap+Output+with+learned+Multi-Agent+Sampling
Language model compression with weighted low-rank factorization,2022,ICLR,"['Yen-Chang Hsu', 'Ting Hua', 'Sungen Chang', 'Qian Lou', 'Yilin Shen', 'Hongxia Jin']",poster,"['model compression', 'low-rank approximation', 'transformer', 'language model']","Factorizing a large matrix into small matrices is a popular strategy for model compression. Singular value decomposition (SVD) plays a vital role in this compression strategy, approximating a learned matrix with fewer parameters. However, SVD minimizes the squared error toward reconstructing the original matrix without gauging the importance of the parameters, potentially giving a larger reconstruction error for those who affect the task accuracy more. In other words, the optimization objective of SVD is not aligned with the trained model's task accuracy. We analyze this previously unexplored problem, make observations, and address it by introducing Fisher information to weigh the importance of parameters affecting the model prediction. This idea leads to our method: Fisher-Weighted SVD (FWSVD). Although the factorized matrices from our approach do not result in smaller reconstruction errors, we find that our resulting task accuracy is much closer to the original model's performance. We perform analysis with the transformer-based language models, showing our weighted SVD largely alleviates the mismatched optimization objectives and can maintain model performance with a higher compression rate. Our method can directly compress a task-specific model while achieving better performance than other compact model strategies requiring expensive model pre-training. Moreover, the evaluation of compressing an already compact model shows our method can further reduce 9% to 30% parameters with an insignificant impact on task accuracy.",https://api.openreview.net/pdf/a5edead703a518eda031d7e25734d372b8287883.pdf,graph;optimization;zero_few-shot;transformer;multimodal;low-rank;llm,https://scholar.google.com/scholar?q=Language+model+compression+with+weighted+low-rank+factorization
Enhancing Cross-lingual Transfer by Manifold Mixup,2022,ICLR,"['Huiyun Yang', 'Huadong Chen', 'Hao Zhou', 'Lei Li']",poster,"['cross-lingual transfer', 'cross-lingual understanding', 'manifold mixup']","Based on large-scale pre-trained multilingual representations, recent cross-lingual transfer methods have achieved impressive transfer performances. However, the performance of target languages still lags far behind the source language. In this paper, our analyses indicate such a performance gap is strongly associated with the cross-lingual representation discrepancy. To achieve better cross-lingual transfer performance, we propose the cross-lingual manifold mixup (X-Mixup) method, which adaptively calibrates the representation discrepancy and gives a compromised representation for target languages. Experiments on the XTREME benchmark show X-Mixup achieves 1.8% performance gains on multiple text understanding tasks, compared with strong baselines, and significantly reduces the cross-lingual representation discrepancy.",https://api.openreview.net/pdf/dbe81cd4937fe7d696c1a2beb6c1a81c871a7a56.pdf,representation;adaptive;transfer learning;multimodal,https://scholar.google.com/scholar?q=Enhancing+Cross-lingual+Transfer+by+Manifold+Mixup
Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning,2022,ICLR,"['Yutong Wang', 'Ke Xue', 'Chao Qian']",poster,"['Reinforcement learning', 'Quality-Diversity', 'Evolutionary algorithms']","Reinforcement Learning (RL) has achieved significant successes, which aims to obtain a single policy maximizing the expected cumulative rewards for a given task. However, in many real-world scenarios, e.g., navigating in complex environments and controlling robots, one may need to find a set of policies having both high rewards and diverse behaviors, which can bring better exploration and robust few-shot adaptation. Recently, some methods have been developed by using evolutionary techniques, including iterative reproduction and selection of policies. However, due to the inefficient selection mechanisms, these methods cannot fully guarantee both high quality and diversity. In this paper, we propose EDO-CS, a new Evolutionary Diversity Optimization algorithm with Clustering-based Selection. In each iteration, the policies are divided into several clusters based on their behaviors, and a high-quality policy is selected from each cluster for reproduction. EDO-CS also adaptively balances the importance between quality and diversity in the reproduction process. Experiments on various (i.e., deceptive and multi-modal) continuous control tasks, show the superior performance of EDO-CS over previous methods, i.e., EDO-CS can achieve a set of policies with both high quality and diversity efficiently while previous methods cannot.",https://api.openreview.net/pdf/b887e71bcdd86242a6fbbc501be12552141c01ed.pdf,reinforcement learning;optimization;adaptive;multimodal,https://scholar.google.com/scholar?q=Evolutionary+Diversity+Optimization+with+Clustering-based+Selection+for+Reinforcement+Learning
Open-vocabulary Object Detection via Vision and Language Knowledge Distillation,2022,ICLR,"['Xiuye Gu', 'Tsung-Yi Lin', 'Weicheng Kuo', 'Yin Cui']",poster,"['Open-vocabulary recognition', 'Object detection', 'Knowledge distillation']","We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data.  It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask APr with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 APr. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP50 on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art (Zareian et al., 2021) by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild.",https://api.openreview.net/pdf/25cfe8bb2fa27d8a1c86a575dbf3b997754148be.pdf,zero_few-shot;transfer learning;flow;distillation;multimodal,https://scholar.google.com/scholar?q=Open-vocabulary+Object+Detection+via+Vision+and+Language+Knowledge+Distillation
Cross-Lingual Transfer with Class-Weighted Language-Invariant Representations,2022,ICLR,"['Ruicheng Xian', 'Heng Ji', 'Han Zhao']",poster,"['cross-lingual transfer', 'unsupervised cross-lingual learning', 'multilingual neural language model', 'domain adaptation']","Recent advances in neural modeling have produced deep multilingual language models capable of extracting cross-lingual knowledge from non-parallel texts and enabling zero-shot downstream transfer. While their success is often attributed to shared representations, quantitative analyses are limited. Towards a better understanding, through empirical analyses, we show that the invariance of feature representations across languages—an effect of shared representations—strongly correlates with transfer performance. We also observe that distributional shifts in class priors between source and target language task data negatively affect performance, a largely overlooked issue that could cause negative transfer with existing unsupervised approaches. Based on these findings, we propose and evaluate a method for unsupervised transfer, called importance-weighted domain alignment (IWDA), that performs representation alignment with prior shift estimation and correction using unlabeled target language task data. Experiments demonstrate its superiority under large prior shifts, and show further performance gains when combined with existing semi-supervised learning techniques.",https://api.openreview.net/pdf/c75daaf7c5ca8ed7ab01e92c4cc16d55f5d6aff5.pdf,zero_few-shot;representation;transfer learning;multimodal,https://scholar.google.com/scholar?q=Cross-Lingual+Transfer+with+Class-Weighted+Language-Invariant+Representations
ConFeSS: A Framework for Single Source Cross-Domain Few-Shot Learning,2022,ICLR,"['Debasmit Das', 'Sungrack Yun', 'Fatih Porikli']",poster,[],"Most current few-shot learning methods train a model from abundantly labeled base category data and then transfer and adapt the model to sparsely labeled novel category data. These methods mostly generalize well on novel categories from the same domain as the base categories but perform poorly for distant domain categories. In this paper, we propose a framework for few-shot learning coined as ConFeSS (Contrastive Learning and Feature Selection System) that tackles large domain shift between base and novel categories. The first step of our framework trains a feature extracting backbone with the contrastive loss on the base category data. Since the contrastive loss does not use supervision, the features can generalize better to distant target domains. For the second step, we train a masking module to select relevant features that are more suited to target domain classification. Finally, a classifier is fine-tuned along with the backbone such that the backbone produces features similar to the relevant ones. To evaluate our framework, we tested it on a recently introduced cross-domain few-shot learning benchmark. Experimental results demonstrate that our framework outperforms all meta-learning approaches and produces competitive results against recent cross-domain methods. Additional analyses are also performed to better understand our framework.",https://api.openreview.net/pdf/9a6c5c7a1d3338348f0f985794c41857eabbb501.pdf,zero_few-shot;contrastive learning;meta-learning;sparse;transfer learning;multimodal,https://scholar.google.com/scholar?q=ConFeSS:+A+Framework+for+Single+Source+Cross-Domain+Few-Shot+Learning
Granger causal inference on DAGs identifies genomic loci regulating transcription,2022,ICLR,"['Alexander P Wu', 'Rohit Singh', 'Bonnie Berger']",poster,"['Granger causality', 'causal inference', 'graph neural networks', 'gene regulation', 'single-cell genomics', 'chromatin accessibility', 'directed acyclic graphs', 'single-cell multimodal']","When a dynamical system can be modeled as a sequence of observations, Granger causality is a powerful approach for detecting predictive interactions between its variables. However, traditional Granger causal inference has limited utility in domains where the dynamics need to be represented as directed acyclic graphs (DAGs) rather than as a linear sequence, such as with cell differentiation trajectories. Here, we present GrID-Net, a framework based on graph neural networks with lagged message passing for Granger causal inference on DAG-structured systems. Our motivating application is the analysis of single-cell multimodal data to identify genomic loci that mediate the regulation of specific genes. To our knowledge, GrID-Net is the first single-cell analysis tool that accounts for the temporal lag between a genomic locus becoming accessible and its downstream effect on a target gene's expression. We applied GrID-Net on multimodal single-cell assays that profile chromatin accessibility (ATAC-seq) and gene expression (RNA-seq) in the same cell and show that it dramatically outperforms existing methods for inferring regulatory locus-gene links, achieving up to 71% greater agreement with independent population genetics-based estimates. By extending Granger causality to DAG-structured dynamical systems, our work unlocks new domains for causal analyses and, more specifically, opens a path towards elucidating gene regulatory interactions relevant to cellular differentiation and complex human diseases at unprecedented scale and resolution.",https://api.openreview.net/pdf/85cdf969129ce01319414cf3e7f1ffc801bb8db0.pdf,graph;inference;multimodal,https://scholar.google.com/scholar?q=Granger+causal+inference+on+DAGs+identifies+genomic+loci+regulating+transcription
Graph-Guided Network for Irregularly Sampled Multivariate Time Series,2022,ICLR,"['Xiang Zhang', 'Marko Zeman', 'Theodoros Tsiligkaridis', 'Marinka Zitnik']",poster,"['time seres', 'irregular time series', 'graph neural networks', 'attention mechanism', 'time series classification', 'multivariate time series', 'representation learning', 'embeddings']","In many domains, including healthcare, biology, and climate science, time series are irregularly sampled with varying time intervals between successive readouts and different subsets of variables (sensors) observed at different time points. Here, we introduce RAINDROP, a graph neural network that embeds irregularly sampled and multivariate time series while also learning the dynamics of sensors purely from observational data. RAINDROP represents every sample as a separate sensor graph and models time-varying dependencies between sensors with a novel message passing operator. It estimates the latent sensor graph structure and leverages the structure together with nearby observations to predict misaligned readouts. This model can be interpreted as a graph neural network that sends messages over graphs that are optimized for capturing time-varying dependencies among sensors. We use RAINDROP to classify time series and interpret temporal dynamics on three healthcare and human activity datasets. RAINDROP outperforms state-of-the-art methods by up to 11.4% (absolute F1-score points), including techniques that deal with irregular sampling using fixed discretization and set functions. RAINDROP shows superiority in diverse setups, including challenging leave-sensor-out settings. ",https://api.openreview.net/pdf/6486d4fdf5e5db05e22c8ecd34aaab8401569f24.pdf,graph;multimodal;llm,https://scholar.google.com/scholar?q=Graph-Guided+Network+for+Irregularly+Sampled+Multivariate+Time+Series
FILM: Following Instructions in Language with Modular Methods,2022,ICLR,"['So Yeon Min', 'Devendra Singh Chaplot', 'Pradeep Kumar Ravikumar', 'Yonatan Bisk', 'Ruslan Salakhutdinov']",poster,"['Instruction Following', 'Visual Language Navigation', 'Embodied Instruction Following', 'VLN', 'ALFRED']","Recent methods for embodied instruction following are typically trained end-to-end using imitation learning. This often requires the use of expert trajectories and low-level language instructions. Such approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning. In contrast, we propose a modular method with structured representations that (1) builds a semantic map of the scene and (2) performs exploration with a semantic search policy, to achieve the natural language goal. Our modular method achieves SOTA performance (24.46 %) with a substantial (8.17 % absolute) gap from previous work while using less data by eschewing both expert trajectories and low-level instructions. Leveraging low-level language, however, can further increase our performance (26.49 %). Our findings suggest that an explicit spatial memory and a semantic search policy can provide a stronger and more general representation for state-tracking and guidance, even in the absence of expert trajectories or low-level instructions.",https://api.openreview.net/pdf/097027dcab1f74c4d68412bd97c6299ec48d6c67.pdf,graph;zero_few-shot;representation;multimodal;imitation learning,https://scholar.google.com/scholar?q=FILM:+Following+Instructions+in+Language+with+Modular+Methods
CrowdPlay: Crowdsourcing Human Demonstrations for Offline Learning,2022,ICLR,"['Matthias Gerstgrasser', 'Rakshit Trivedi', 'David C. Parkes']",poster,[],"Crowdsourcing has been instrumental for driving AI advances that rely on large-scale data. At the same time, reinforcement learning has seen rapid progress through  benchmark environments that strike a balance between tractability and real-world complexity, such as ALE and OpenAI Gym. In this paper, we aim to fill a gap at the intersection of these two: The use of crowdsourcing to generate large-scale human demonstration data in the support of advancing research into imitation learning and offline learning.
To this end, we present CrowdPlay, a complete crowdsourcing pipeline for any standard RL environment including OpenAI Gym (made available under an open-source license); a large-scale publicly available crowdsourced dataset of human gameplay demonstrations in Atari 2600 games, including multimodal behavior and human-human and human-AI multiagent data; offline learning benchmarks with extensive human data evaluation; and a detailed study of incentives, including real-time feedback to drive high quality data.
We hope that this will drive the improvement in design of algorithms that  account for the complexity of human, behavioral data and thereby enable a step forward in direction of effective learning for real-world settings. Our code and dataset are available at https://mgerstgrasser.github.io/crowdplay/.",https://api.openreview.net/pdf/b52b98ab7e0985b4d419899df029ac52613153b4.pdf,reinforcement learning;offline reinforcement learning;multimodal;imitation learning,https://scholar.google.com/scholar?q=CrowdPlay:+Crowdsourcing+Human+Demonstrations+for+Offline+Learning
Hierarchical Variational Memory for Few-shot Learning Across Domains,2022,ICLR,"['Yingjun Du', 'Xiantong Zhen', 'Ling Shao', 'Cees G. M. Snoek']",poster,"['Meta-learning', 'Variational hierarchical memory', 'Variational hierarchical prototype', 'Cross-domain few-shot learning']","Neural memory enables fast adaptation to new tasks with just a few training samples. Existing memory models store features only from the single last layer, which does not generalize well in presence of a domain shift between training and test distributions. Rather than relying on a flat memory, we propose a hierarchical alternative that stores features at different semantic levels. We introduce a hierarchical prototype model, where each level of the prototype fetches corresponding information from the hierarchical memory. The model is endowed with the ability to flexibly rely on features at different semantic levels if the domain shift circumstances so demand. We meta-learn the model by a newly derived hierarchical variational inference framework, where hierarchical memory and prototypes are jointly optimized. To explore and exploit the importance of different semantic levels, we further propose to learn the weights associated with the prototype at each level in a data-driven way, which enables the model to adaptively choose the most generalizable features. We conduct thorough ablation studies to demonstrate the effectiveness of each component in our model. The new state-of-the-art performance on cross-domain and competitive performance on traditional few-shot classification further substantiates the benefit of hierarchical variational memory.",https://api.openreview.net/pdf/97127f112b9804e1962c3670a2de7a6c351ac591.pdf,zero_few-shot;adaptive;inference;meta-learning;multimodal,https://scholar.google.com/scholar?q=Hierarchical+Variational+Memory+for+Few-shot+Learning+Across+Domains
Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,2022,ICLR,"['Bowen Shi', 'Wei-Ning Hsu', 'Kushal Lakhotia', 'Abdelrahman Mohamed']",poster,"['audio-visual speech recognition', 'lip reading', 'speech recognition', 'self-supervised learning']","Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker’s lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours) (Makino et al., 2019). The lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40% relative WER reduction over the state-of-the-art performance (1.3% vs 2.3%). Our code and models are available at https://github.com/facebookresearch/av_hubert.",https://api.openreview.net/pdf/db3061a63dfde7babf9a5fa76d250390f56b8771.pdf,zero_few-shot;transformer;representation;multimodal,https://scholar.google.com/scholar?q=Learning+Audio-Visual+Speech+Representation+by+Masked+Multimodal+Cluster+Prediction
FILIP: Fine-grained Interactive Language-Image Pre-Training,2022,ICLR,"['Lewei Yao', 'Runhui Huang', 'Lu Hou', 'Guansong Lu', 'Minzhe Niu', 'Hang Xu', 'Xiaodan Liang', 'Zhenguo Li', 'Xin Jiang', 'Chunjing Xu']",poster,"['Visual-language pretraining', 'Language-Image Pretraining', 'Multi-modality model']","Unsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the cross-modal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross/self-attention upon visual and textual tokens. However, cross/self-attention suffers from inferior efficiency in both training and inference. In this paper, we introduce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) to achieve finer-level alignment through a cross-modal late interaction mechanism, which uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. FILIP successfully leverages the finer-grained expressiveness between image patches and textual words by modifying only contrastive loss, while simultaneously gaining the ability to pre-compute image and text representations offline at inference, keeping both large-scale training and inference efficient. Furthermore, we construct a new large-scale image-text pair dataset called FILIP300M for pre-training. Experiments show that FILIP achieves state-of-the-art performance on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval. The visualization on word-patch alignment further shows that FILIP can learn meaningful fine-grained features with promising localization ability.",https://api.openreview.net/pdf/e8f6807c88ea1d0d0090f2c381f21739b217efb9.pdf,offline reinforcement learning;zero_few-shot;transformer;representation;contrastive learning;inference;active learning;multimodal,https://scholar.google.com/scholar?q=FILIP:+Fine-grained+Interactive+Language-Image+Pre-Training
On the Limitations of Multimodal VAEs,2022,ICLR,"['Imant Daunhawer', 'Thomas M. Sutter', 'Kieran Chin-Cheong', 'Emanuele Palumbo', 'Julia E Vogt']",poster,"['multimodal learning', 'variational autoencoder', 'variational information bottleneck', 'information theory']","Multimodal variational autoencoders (VAEs) have shown promise as efficient generative models for weakly-supervised data. Yet, despite their advantage of weak supervision, they exhibit a gap in generative quality compared to unimodal VAEs, which are completely unsupervised. In an attempt to explain this gap, we uncover a fundamental limitation that applies to a large family of mixture-based multimodal VAEs. We prove that the sub-sampling of modalities enforces an undesirable upper bound on the multimodal ELBO and thereby limits the generative quality of the respective models.  Empirically, we showcase the generative quality gap on both synthetic and real data and present the tradeoffs between different variants of multimodal VAEs. We find that none of the existing approaches fulfills all desired criteria of an effective multimodal generative model when applied on more complex datasets than those used in previous benchmarks. In summary, we identify, formalize, and validate fundamental limitations of VAE-based approaches for modeling weakly-supervised data and discuss implications for real-world applications.",https://api.openreview.net/pdf/e25daec9628954edce262e1cda172567415510fc.pdf,zero_few-shot;vae;generative model;multimodal,https://scholar.google.com/scholar?q=On+the+Limitations+of+Multimodal+VAEs
"FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations",2022,ICLR,"['Lingjie Mei', 'Jiayuan Mao', 'Ziqi Wang', 'Chuang Gan', 'Joshua B. Tenenbaum']",poster,"['Neuro-Symbolic Reasoning', 'Concept Learning', 'Meta-Learning']","We present a meta-learning framework for learning new visual concepts quickly, from just one or a few examples, guided by multiple naturally occurring data streams: simultaneously looking at images, reading sentences that describe the objects in the scene, and interpreting supplemental sentences that relate the novel concept with other concepts. The learned concepts support downstream applications, such as answering questions by reasoning about unseen images. Our model, namely FALCON, represents individual visual concepts, such as colors and shapes, as axis-aligned boxes in a high-dimensional space (the ``box embedding space''). Given an input image and its paired sentence, our model first resolves the referential expression in the sentence and associates the novel concept with particular objects in the scene. Next, our model interprets supplemental sentences to relate the novel concept with other known concepts, such as ``X has property Y'' or ``X is a kind of Y''. Finally, it infers an optimal box embedding for the novel concept that jointly 1) maximizes the likelihood of the observed instances in the image, and 2) satisfies the relationships between the novel concepts and the known ones. We demonstrate the effectiveness of our model on both synthetic and real-world datasets.",https://api.openreview.net/pdf/074074edfbe3b59bf1651653fcf8002522df2588.pdf,meta-learning;multimodal,"https://scholar.google.com/scholar?q=FALCON:+Fast+Visual+Concept+Learning+by+Integrating+Images,+Linguistic+descriptions,+and+Conceptual+Relations"
Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs,2022,ICLR,"['Jaewoong Choi', 'Junho Lee', 'Changyeon Yoon', 'Jung Ho Park', 'Geonho Hwang', 'Myungjoo Kang']",poster,"['generative adversarial network', 'disentanglement', 'semantic factorization', 'latent space control', 'image manipulation', 'grassmannian']","The discovery of the disentanglement properties of the latent space in GANs motivated a lot of research to find the semantically meaningful directions on it. In this paper, we suggest that the disentanglement property is closely related to the geometry of the latent space. In this regard, we propose an unsupervised method for finding the semantic-factorizing directions on the intermediate latent space of GANs based on the local geometry. Intuitively, our proposed method, called $\textit{Local Basis}$, finds the principal variation of the latent space in the neighborhood of the base latent variable. Experimental results show that the local principal variation corresponds to the semantic factorization and traversing along it provides strong robustness to image traversal. Moreover, we suggest an explanation for the limited success in finding the global traversal directions in the latent space, especially $\mathcal{W}$-space of StyleGAN2. We show that $\mathcal{W}$-space is warped globally by comparing the local geometry, discovered from Local Basis, through the metric on Grassmannian Manifold. The global warpage implies that the latent space is not well-aligned globally and therefore the global traversal directions are bound to show limited success on it.",https://api.openreview.net/pdf/35267777d7fe835879ddec7c83aecd9d170d070d.pdf,metric;multimodal,https://scholar.google.com/scholar?q=Do+Not+Escape+From+the+Manifold:+Discovering+the+Local+Coordinates+on+the+Latent+Space+of+GANs
Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling,2022,ICLR,"['Gianluigi Silvestri', 'Emily Fertig', 'Dave Moore', 'Luca Ambrogioni']",poster,"['Normalizing Flows', 'Probabilistic model', 'Probabilistic programming', 'Generative modeling', 'Variational Inference']","Normalizing flows have shown great success as general-purpose density estimators. However, many real world applications require the use of domain-specific knowledge, which normalizing flows cannot readily incorporate. We propose embedded-model flows (EMF), which alternate general-purpose transformations with structured layers that embed domain-specific inductive biases. These layers are automatically constructed by converting user-specified differentiable probabilistic models into equivalent bijective transformations. We also introduce gated structured layers, which allow bypassing the parts of the models that fail to capture the statistics of the data. We demonstrate that EMFs can be used to induce desirable properties such as multimodality and continuity. Furthermore, we show that EMFs enable a high performance form of variational inference where the structure of the prior model is embedded in the variational architecture. In our experiments, we show that this approach outperforms a large number of alternative methods in common structured inference problems.",https://api.openreview.net/pdf/ea5ccb4619c2399f3a652126f47196d475ee7c9c.pdf,zero_few-shot;inference;flow;multimodal,https://scholar.google.com/scholar?q=Embedded-model+flows:+Combining+the+inductive+biases+of+model-free+deep+learning+and+explicit+probabilistic+modeling
Multi-Task Processes,2022,ICLR,"['Donggyun Kim', 'Seongwoong Cho', 'Wonkwang Lee', 'Seunghoon Hong']",poster,"['stochastic processes', 'neural processes', 'multi-task learning', 'incomplete data']","Neural Processes (NPs) consider a task as a function realized from a stochastic process and flexibly adapt to unseen tasks through inference on functions. However, naive NPs can model data from only a single stochastic process and are designed to infer each task independently. Since many real-world data represent a set of correlated tasks from multiple sources (e.g., multiple attributes and multi-sensor data), it is beneficial to infer them jointly and exploit the underlying correlation to improve the predictive performance.
To this end, we propose Multi-Task Neural Processes (MTNPs), an extension of NPs designed to jointly infer tasks realized from multiple stochastic processes. We build MTNPs in a hierarchical way such that inter-task correlation is considered by conditioning all per-task latent variables on a single global latent variable. In addition, we further design our MTNPs so that they can address multi-task settings with incomplete data (i.e., not all tasks share the same set of input points), which has high practical demands in various applications.
Experiments demonstrate that MTNPs can successfully model multiple tasks jointly by discovering and exploiting their correlations in various real-world data such as time series of weather attributes and pixel-aligned visual modalities. We release our code at https://github.com/GitGyun/multi_task_neural_processes.",https://api.openreview.net/pdf/37cf7d07aad3da1d05b566f738c88764b8d55db2.pdf,inference;multi-task;multimodal,https://scholar.google.com/scholar?q=Multi-Task+Processes
Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning,2022,ICLR,"['Seanie Lee', 'Hae Beom Lee', 'Juho Lee', 'Sung Ju Hwang']",poster,"['multilingual language model', 'gradient alignment']","Multilingual models jointly pretrained on multiple languages have achieved remarkable performance on various multilingual downstream tasks. Moreover, models finetuned on a single monolingual downstream task have shown to generalize to unseen languages. In this paper, we first show that it is crucial for those tasks to align gradients between them in order to maximize knowledge transfer while minimizing negative transfer. Despite its importance, the existing methods for gradient alignment either have a completely different purpose, ignore inter-task alignment, or aim to solve continual learning problems in rather inefficient ways. As a result of the misaligned gradients between tasks, the model suffers from severe negative transfer in the form of catastrophic forgetting of the knowledge acquired from the pretraining. To overcome the limitations, we propose a simple yet effective method that can efficiently align gradients between tasks. Specifically, we perform each inner-optimization by sequentially sampling batches from all the tasks, followed by a Reptile outer update. Thanks to the gradients aligned between tasks by our method, the model becomes less vulnerable to negative transfer and catastrophic forgetting. We extensively validate our method on various multi-task learning and zero-shot cross-lingual transfer tasks, where our method largely outperforms all the relevant baselines we consider.",https://api.openreview.net/pdf/a902aca6de0e705d273cc4715fde1741a44e649e.pdf,optimization;zero_few-shot;transfer learning;multi-task;multimodal,https://scholar.google.com/scholar?q=Sequential+Reptile:+Inter-Task+Gradient+Alignment+for+Multilingual+Learning
Decoupled Adaptation for Cross-Domain Object Detection,2022,ICLR,"['Junguang Jiang', 'Baixu Chen', 'Jianmin Wang', 'Mingsheng Long']",poster,"['Object Detection', 'Domain Adaptation', 'Object Localization', 'Deep Learning', 'Transfer Learning']","Cross-domain object detection is more challenging than object classification since multiple objects exist in an image and the location of each object is unknown in the unlabeled target domain. As a result, when we adapt features of different objects to enhance the transferability of the detector, the features of the foreground and the background are easy to be confused, which may hurt the discriminability of the detector. Besides, previous methods focused on category adaptation but ignored another important part for object detection, i.e., the adaptation on bounding box regression. To this end, we propose D-adapt, namely Decoupled Adaptation, to decouple the adversarial adaptation and the training of the detector. Besides, we fill the blank of regression domain adaptation in object detection by introducing a bounding box adaptor. Experiments show that \textit{D-adapt} achieves state-of-the-art results on four cross-domain object detection tasks and yields 17\%  and 21\% relative improvement on benchmark datasets Clipart1k and Comic2k in particular.",https://api.openreview.net/pdf/88ba4a7c16c8aa0fe22280dd84bf63ff1eccdbee.pdf,graph;transfer learning;multimodal,https://scholar.google.com/scholar?q=Decoupled+Adaptation+for+Cross-Domain+Object+Detection
SimVLM: Simple Visual Language Model Pretraining with Weak Supervision,2022,ICLR,"['Zirui Wang', 'Jiahui Yu', 'Adams Wei Yu', 'Zihang Dai', 'Yulia Tsvetkov', 'Yuan Cao']",poster,"['Vision-Language Pretraining', 'Multimodal Language Model', 'Weak Supervision']","With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.",https://api.openreview.net/pdf/6212c3fec8ad1418306ef5cebb5e0b76abbc4c26.pdf,optimization;zero_few-shot;representation;generative model;transfer learning;multimodal,https://scholar.google.com/scholar?q=SimVLM:+Simple+Visual+Language+Model+Pretraining+with+Weak+Supervision
Towards Better Understanding and Better Generalization of Low-shot Classification in Histology Images with Contrastive Learning,2022,ICLR,"['Jiawei Yang', 'Hanbo Chen', 'Jiangpeng Yan', 'Xiaoyu Chen', 'Jianhua Yao']",poster,"['Few shot learning', 'Histology Image', 'Knowledge Transferring']","Few-shot learning is an established topic in natural images for years, but few work is attended to histology images, which is of high clinical value since well-labeled datasets and rare abnormal samples are expensive to collect. Here, we facilitate the study of few-shot learning in histology images by setting up three cross-domain tasks that simulate real clinics problems. To enable label-efficient learning and better generalizability, we propose to incorporate contrastive learning (CL) with latent augmentation (LA) to build a few-shot system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. These two components fully exploit unlabeled training data and can scale gracefully to other label-hungry problems. In experiments, we find i) models learned by CL generalize better than supervised learning for histology images in unseen classes, and ii) LA brings consistent gains over baselines. Prior studies of self-supervised learning mainly focus on ImageNet-like images, which only present a dominant object in their centers. Recent attention has been paid to images with multi-objects and multi-textures. Histology images are a natural choice for such a study. We show the superiority of CL over supervised learning in terms of generalization for such data and provide our empirical understanding for this observation. The findings in this work could contribute to understanding how the model generalizes in the context of both representation learning and histological image analysis. Code is available.",https://api.openreview.net/pdf/e31401d33fda8d1009bf242636e6aa638632482a.pdf,zero_few-shot;transformer;representation;contrastive learning;transfer learning;augmentation;multimodal,https://scholar.google.com/scholar?q=Towards+Better+Understanding+and+Better+Generalization+of+Low-shot+Classification+in+Histology+Images+with+Contrastive+Learning
Handling Distribution Shifts on Graphs: An Invariance Perspective,2022,ICLR,"['Qitian Wu', 'Hengrui Zhang', 'Junchi Yan', 'David Wipf']",poster,"['Representation Learning on Graphs', 'Out-of-Distribution Generalization', 'Domain Shift', 'Graph Structure Learning', 'Invariant Models']","There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given two-fold fundamental challenges: 1) the inter-connection among nodes in one graph, which induces non-IID generation of data points even under the same environment, and 2) the structural information in the input graph, which is also informative for prediction. In this paper, we formulate the OOD problem on graphs and develop a new invariant learning approach, Explore-to-Extrapolate Risk Minimization (EERM), that facilitates graph neural networks to leverage invariance principles for prediction. EERM resorts to multiple context explorers (specified as graph structure editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node-level prediction. We prove the validity of our method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution.",https://api.openreview.net/pdf/f55776307b1b806dc62cd1e642289d70df555fd2.pdf,graph;generative model;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=Handling+Distribution+Shifts+on+Graphs:+An+Invariance+Perspective
A Biologically Interpretable Graph Convolutional Network to Link Genetic Risk Pathways and Imaging Phenotypes of Disease ,2022,ICLR,"['Sayan Ghosal', 'Qiang Chen', 'Giulio Pergola', 'Aaron L Goldman', 'William Ulrich', 'Daniel R Weinberger', 'Archana Venkataraman']",poster,"['Imaging-genetics', 'Hierarchical Graph Convolution', 'Gene Ontology', 'Bayesian Feature Selection', 'Schizophrenia']","We propose a novel end-to-end framework for whole-brain and whole-genome imaging-genetics. Our genetics network uses hierarchical graph convolution and pooling operations to embed subject-level data onto a low-dimensional latent space. The hierarchical network implicitly tracks the convergence of genetic risk across well-established biological pathways, while an attention mechanism automatically identifies the salient edges of this network at the subject level. In parallel, our imaging network projects multimodal data onto a set of latent embeddings. For interpretability, we implement a Bayesian feature selection strategy to extract the discriminative imaging biomarkers; these feature weights are optimized alongside the other model parameters. We couple the imaging and genetic embeddings with a predictor network, to ensure that the learned representations are linked to phenotype. We evaluate our framework on a schizophrenia dataset that includes two functional MRI paradigms and gene scores derived from Single Nucleotide Polymorphism data. Using repeated 10-fold cross-validation, we show that our imaging-genetics fusion achieves the better classification performance than state-of-the-art baselines. In an exploratory analysis, we further show that the biomarkers identified by our model are reproducible and closely associated with deficits in schizophrenia. ",https://api.openreview.net/pdf/e88ef5a6ee79b4188e9a4d9cb222409009729992.pdf,graph;zero_few-shot;transformer;representation;bayesian;multimodal,https://scholar.google.com/scholar?q=A+Biologically+Interpretable+Graph+Convolutional+Network+to+Link+Genetic+Risk+Pathways+and+Imaging+Phenotypes+of+Disease+
InfinityGAN: Towards Infinite-Pixel Image Synthesis,2022,ICLR,"['Chieh Hubert Lin', 'Hsin-Ying Lee', 'Yen-Chi Cheng', 'Sergey Tulyakov', 'Ming-Hsuan Yang']",poster,"['generative modeling', 'image synthesis', 'generative adversarial networks', 'infinite-pixel synthesis', 'GANs']","We present InfinityGAN, a method to generate arbitrary-sized images. The problem is associated with several key challenges. First, scaling existing models to an arbitrarily large image size is resource-constrained, both in terms of computation and availability of large-field-of-view training data. InfinityGAN trains and infers patch-by-patch seamlessly with low computational resources. Second, large images should be locally and globally consistent, avoid repetitive patterns, and look realistic. To address these, InfinityGAN takes global appearance, local structure and texture into account. With this formulation, we can generate images with spatial size and level of detail not attainable before. Experimental evaluation supports that InfinityGAN generates images with superior global structure compared to baselines and features parallelizable inference. Finally, we show several applications unlocked by our approach, such as fusing styles spatially, multi-modal outpainting and image inbetweening at arbitrary input and output sizes.",https://api.openreview.net/pdf/0907e14b44a58aca8f7577b9e7f45870865cbfc4.pdf,optimization;zero_few-shot;transformer;inference;multimodal,https://scholar.google.com/scholar?q=InfinityGAN:+Towards+Infinite-Pixel+Image+Synthesis
CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation,2022,ICLR,"['Tongkun Xu', 'Weihua Chen', 'Pichao WANG', 'Fan Wang', 'Hao Li', 'Rong Jin']",poster,[],"Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. Most existing UDA methods focus on learning domain-invariant feature representation, either from the domain level or category level, using convolution neural networks (CNNs)-based frameworks. One fundamental problem for the category level based UDA is the production of pseudo labels for samples in target domain, which are usually too noisy for accurate domain alignment, inevitably compromising the UDA performance.  With the success of Transformer in various tasks, we find that the cross-attention in Transformer is robust to the noisy input pairs for better feature alignment, thus in this paper Transformer is adopted for the challenging UDA task. Specifically, to generate accurate input pairs, we design a two-way center-aware labeling algorithm to produce pseudo labels for target samples. Along with the pseudo labels, a weight-sharing triple-branch transformer framework is proposed to apply self-attention and cross-attention for source/target feature learning and source-target domain alignment, respectively. 
Such design explicitly enforces the framework to learn discriminative domain-specific and domain-invariant representations simultaneously. The proposed method is dubbed CDTrans (cross-domain transformer), and it provides one of the first attempts to solve UDA tasks with a pure transformer solution. Experiments show that our proposed method achieves the best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet. Code and models are available at https://github.com/CDTrans/CDTrans.",https://api.openreview.net/pdf/5af495124dcff5b772396db65ab98725f7b036b7.pdf,graph;transformer;representation;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=CDTrans:+Cross-domain+Transformer+for+Unsupervised+Domain+Adaptation
Switch to Generalize: Domain-Switch Learning for Cross-Domain Few-Shot Classification,2022,ICLR,"['Zhengdong Hu', 'Yifan Sun', 'Yi Yang']",poster,[],"This paper considers few-shot learning under the cross-domain scenario. The cross-domain setting imposes a critical challenge, i.e., using very few (support) samples to generalize the already-learned model to a novel domain. We hold a hypothesis, i.e., if a deep model is capable to fast generalize itself to different domains (using very few samples) during training, it will maintain such domain generalization capacity for testing. It motivates us to propose a novel Domain-Switch Learning (DSL) framework. DSL embeds the cross-domain scenario into the training stage in a ``fast switching'' manner. Specifically, DSL uses a single domain for a training iteration and switches into another domain for the following iteration. During the switching, DSL enforces two constraints: 1) the deep model should not over-fit the domain in the current iteration and 2) the deep model should not forget the already-learned knowledge of other domains. These two constraints jointly promote fast generalization across different domains. Experimental results confirm that the cross-domain generalization capacity can be inherited from the training stage to the testing stage, validating our key hypothesis. Consequentially, DSL significantly improves cross-domain few-shot classification and sets up new state of the art.",https://api.openreview.net/pdf/d34f743c2c676ac34213466790c222f7aa5a1b28.pdf,optimization;zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Switch+to+Generalize:+Domain-Switch+Learning+for+Cross-Domain+Few-Shot+Classification
Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games,2022,ICLR,"['Stefanos Leonardos', 'Will Overman', 'Ioannis Panageas', 'Georgios Piliouras']",poster,"['Multi-agent Reinforcement Learning', 'Markov Potential Games', 'Policy Gradient']","Potential games are arguably one of the most important and widely studied classes of normal form games. They define the archetypal setting of multi-agent coordination in which all agents utilities are perfectly aligned via a common potential function. Can this intuitive framework be transplanted in the setting of Markov games? What are the similarities and differences between multi-agent coordination with and without state dependence? To answer these questions, we study a natural class of Markov Potential Games (MPGs) that generalize prior attempts at capturing complex stateful multi-agent coordination. Counter-intuitively, insights from normal-form potential games do not carry over as MPGs involve settings where state-games can be zero-sum games. In the opposite direction, Markov games where every state-game is a potential game are not necessarily MPGs. Nevertheless, MPGs showcase standard desirable properties such as the existence of deterministic Nash policies. In our main technical result, we prove convergence of independent policy gradient and its stochastic counterpart to Nash policies (polynomially fast in the approximation error) by adapting recent gradient dominance property arguments developed for single-agent Markov decision processes to multi-agent learning settings. 
",https://api.openreview.net/pdf/91e8010784f5d9c14cf26f0a2e86f8b46808d3ae.pdf,reinforcement learning;multi-agent;multimodal,https://scholar.google.com/scholar?q=Global+Convergence+of+Multi-Agent+Policy+Gradient+in+Markov+Potential+Games
Learning Efficient Image Super-Resolution Networks via Structure-Regularized Pruning,2022,ICLR,"['Yulun Zhang', 'Huan Wang', 'Can Qin', 'Yun Fu']",poster,['image super-resolution'],"Several image super-resolution (SR) networks have been proposed of late for efficient SR, achieving promising results. However, they are still not lightweight enough and neglect to be extended to larger networks. At the same time, model compression techniques, like neural architecture search and knowledge distillation, typically consume considerable computation resources. In contrast, network pruning is a cheap and effective model compression technique. However, it is hard to be applied to SR networks directly because filter pruning for residual blocks is well-known tricky. To address the above issues, we propose structure-regularized pruning (SRP), which imposes regularization on the pruned structure to ensure the locations of pruned filters are aligned across different layers. Specifically, for the layers connected by the same residual, we select the filters of the same indices as unimportant filters. To transfer the expressive power in the unimportant filters to the rest of the network, we employ $L_2$ regularization to drive the weights towards zero so that eventually, their absence will cause minimal performance degradation. We apply SRP to train efficient image SR networks, resulting in a lightweight network SRPN-Lite and a very deep one SRPN. We conduct extensive comparisons with both lightweight and larger networks. SRPN-Lite and SRPN perform favorably against other recent efficient SR approaches quantitatively and visually.",https://api.openreview.net/pdf/538af917ae93b620773d2ce7ce65f827922d0e3f.pdf,transfer learning;distillation;multimodal,https://scholar.google.com/scholar?q=Learning+Efficient+Image+Super-Resolution+Networks+via+Structure-Regularized+Pruning
Perceiver IO: A General Architecture for Structured Inputs & Outputs,2022,ICLR,"['Andrew Jaegle', 'Sebastian Borgeaud', 'Jean-Baptiste Alayrac', 'Carl Doersch', 'Catalin Ionescu', 'David Ding', 'Skanda Koppula', 'Daniel Zoran', 'Andrew Brock', 'Evan Shelhamer', 'Olivier J Henaff', 'Matthew Botvinick', 'Andrew Zisserman', 'Oriol Vinyals', 'Joao Carreira']",spotlight,"['Perceiver', 'BERT', 'natural language processing', 'optical flow', 'computer vision', 'multimodal', 'GLUE', 'ImageNet', 'StarCraft']","A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.",https://api.openreview.net/pdf/be7bf6b12e6abb37fb7853467cc6ef71ea5a1659.pdf,graph;zero_few-shot;transformer;flow;multi-task;multimodal,https://scholar.google.com/scholar?q=Perceiver+IO:+A+General+Architecture+for+Structured+Inputs+&+Outputs
Learning Multimodal VAEs through Mutual Supervision,2022,ICLR,"['Tom Joy', 'Yuge Shi', 'Philip Torr', 'Tom Rainforth', 'Sebastian M Schmon', 'Siddharth N']",spotlight,"['Multimodal Variational Autoencoder', 'Variational Autoencoder']","Multimodal VAEs seek to model the joint distribution over heterogeneous data (e.g.\ vision, language), whilst also capturing a shared representation across such modalities. Prior work has typically combined information from the modalities by reconciling idiosyncratic representations directly in the recognition model through explicit products, mixtures, or other such factorisations. Here we introduce a novel alternative, the MEME, that avoids such explicit combinations by repurposing semi-supervised VAEs to combine information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing---something that most existing approaches either cannot handle, or do so to a limited extent. We demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image--image) and CUB (image--text) datasets. We also contrast the quality of the representations learnt by mutual supervision against standard approaches and observe interesting trends in its ability to capture relatedness between data.",https://api.openreview.net/pdf/6e3f8005627ed71e89af82b1e6d063771b707c3e.pdf,zero_few-shot;representation;vae;metric;multimodal,https://scholar.google.com/scholar?q=Learning+Multimodal+VAEs+through+Mutual+Supervision
Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers,2022,ICLR,"['Ruihan Yang', 'Minghao Zhang', 'Nicklas Hansen', 'Huazhe Xu', 'Xiaolong Wang']",spotlight,"['Reinforcement Learning', 'Robotics', 'Locomotion Control', 'Multi-Modal Transformer']","We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to challenging terrains. Our key insight is that proprioceptive states only offer contact measurements for immediate reaction, whereas an agent equipped with visual sensory observations can learn to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL method that leverages both proprioceptive states and visual observations for locomotion control. We evaluate our method in challenging simulated environments with different obstacles and uneven terrain. We transfer our learned policy from simulation to a real robot by running it indoor and in-the-wild with unseen obstacles and terrain. Our method not only significantly improves over baselines, but also achieves far better generalization performance, especially when transferred to the real robot. Our project page with videos is at https://rchalyang.github.io/LocoTransformer/.",https://api.openreview.net/pdf/44a74037c4089730c34de558a6d679925f866a56.pdf,reinforcement learning;graph;transformer;transfer learning;active learning;multimodal,https://scholar.google.com/scholar?q=Learning+Vision-Guided+Quadrupedal+Locomotion+End-to-End+with+Cross-Modal+Transformers
On the Importance of Firth Bias Reduction in Few-Shot Classification,2022,ICLR,"['Saba Ghaffari', 'Ehsan Saleh', 'David Forsyth', 'Yu-Xiong Wang']",spotlight,"['Few-shot Classification', 'Firth Regularization', 'MLE Bias']","Learning accurate classifiers for novel categories from very few examples, known as few-shot image classification, is a challenging task in statistical machine learning and computer vision. The performance in few-shot classification suffers from the bias in the estimation of classifier parameters; however, an effective underlying bias reduction technique that could alleviate this issue in training few-shot classifiers has been overlooked. In this work, we demonstrate the effectiveness of Firth bias reduction in few-shot classification. Theoretically, Firth bias reduction removes the $O(N^{-1})$ first order term from the small-sample bias of the Maximum Likelihood Estimator. Here we show that the general Firth bias reduction technique simplifies to encouraging uniform class assignment probabilities for multinomial logistic classification, and almost has the same effect in cosine classifiers. We derive an easy-to-implement optimization objective for Firth penalized multinomial logistic and cosine classifiers, which is equivalent to penalizing the cross-entropy loss with a KL-divergence between the predictions and the uniform label distribution. Then, we empirically evaluate that it is consistently effective across the board for few-shot image classification, regardless of (1) the feature representations from different backbones, (2) the number of samples per class, and (3) the number of classes. Furthermore, we demonstrate the effectiveness of Firth bias reduction on cross-domain and imbalanced data settings. Our implementation is available at https://github.com/ehsansaleh/firth_bias_reduction.",https://api.openreview.net/pdf/fafc00da00fec3e0c8db049a4e2e14a588fa4aa7.pdf,graph;optimization;representation;multimodal,https://scholar.google.com/scholar?q=On+the+Importance+of+Firth+Bias+Reduction+in+Few-Shot+Classification
"Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining",2022,ICLR,"['Lu Miao', 'Xiaolong Luo', 'Tianlong Chen', 'Wuyang Chen', 'Dong Liu', 'Zhangyang Wang']",spotlight,"['Pruning', 'Frank-Wolfe']","We present a novel framework to train a large deep neural network (DNN) for only $\textit{once}$, which can then be pruned to $\textit{any sparsity ratio}$ to preserve competitive accuracy $\textit{without any re-training}$. Conventional methods often require (iterative) pruning followed by re-training, which not only incurs large overhead beyond the original DNN training but also can be sensitive to retraining hyperparameters. Our core idea is to re-cast the DNN training as an explicit $\textit{pruning-aware}$ process: that is formulated with an auxiliary $K$-sparse polytope constraint, to encourage network weights to lie in a convex hull spanned by $K$-sparse vectors, potentially resulting in more sparse weight matrices. We then leverage a stochastic Frank-Wolfe (SFW) algorithm to solve this new constrained optimization, which naturally leads to sparse weight updates each time. We further note an overlooked fact that existing DNN initializations were derived to enhance SGD training (e.g., avoid gradient explosion or collapse), but was unaligned with the challenges of training with SFW. We hence also present the first learning-based initialization scheme specifically for boosting SFW-based DNN training. Experiments on CIFAR-10 and Tiny-ImageNet datasets demonstrate that our new framework named $\textbf{SFW-pruning}$ consistently achieves the state-of-the-art performance on various benchmark DNNs over a wide range of pruning ratios. Moreover, SFW-pruning only needs to train once on the same model and dataset, for obtaining arbitrary ratios, while requiring neither iterative pruning nor retraining. All codes will be released to the public. ",https://api.openreview.net/pdf/c893710fa491c04dc86547df19635fae45a567c7.pdf,graph;optimization;zero_few-shot;sparse;multimodal,"https://scholar.google.com/scholar?q=Learning+Pruning-Friendly+Networks+via+Frank-Wolfe:+One-Shot,+Any-Sparsity,+And+No+Retraining"
Generalized Decision Transformer for Offline Hindsight Information Matching,2022,ICLR,"['Hiroki Furuta', 'Yutaka Matsuo', 'Shixiang Shane Gu']",spotlight,"['Hindsight Information Matching', 'Decision Transformer', 'State-Marginal Matching', 'Hindsight Experience Replay', 'Reinforcement Learning']","How to extract as much learning signal from each trajectory data has been a key problem in reinforcement learning (RL), where sample inefficiency has posed serious challenges for practical applications. Recent works have shown that using expressive policy function approximators and conditioning on future trajectory information -- such as future states in hindsight experience replay (HER) or returns-to-go in Decision Transformer (DT) -- enables efficient learning of multi-task policies, where at times online RL is fully replaced by offline behavioral cloning (BC), e.g. sequence modeling. We demonstrate that all these approaches are doing hindsight information matching (HIM) -- training policies that can output the rest of trajectory that matches some statistics of future state information. We present Generalized Decision Transformer (GDT) for solving any HIM problem, and show how different choices for the feature function and the anti-causal aggregator not only recover DT as a special case, but also lead to novel Categorical DT (CDT) and Bi-directional DT (BDT) for matching different statistics of the future. For evaluating CDT and BDT, we define offline multi-task state-marginal matching (SMM) and imitation learning (IL) as two generic HIM problems, propose a Wasserstein distance loss as a metric for both, and empirically study them on MuJoCo continuous control benchmarks. Categorical DT, which simply replaces anti-causal summation with anti-causal binning in DT, enables arguably the first effective offline multi-task SMM algorithm that generalizes well to unseen (and even synthetic) multi-modal reward or state-feature distributions. Bi-directional DT, which uses an anti-causal second transformer as the aggregator, can learn to model any statistics of the future and outperforms DT variants in offline multi-task IL, i.e. one-shot IL. Our generalized formulations from HIM and GDT greatly expand the role of powerful sequence modeling architectures in modern RL.",https://api.openreview.net/pdf/86d7058e78842b10462a9f0e0311ca3040adfe97.pdf,reinforcement learning;offline reinforcement learning;graph;transformer;online learning;metric;multi-task;multimodal;imitation learning,https://scholar.google.com/scholar?q=Generalized+Decision+Transformer+for+Offline+Hindsight+Information+Matching
Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks,2022,ICLR,"['Anne Harrington', 'Arturo Deza']",spotlight,"['Peripheral Computation', 'Adversarial Robustness', 'Perceptual Invariance', 'Metamerism', 'Texture', 'Psychophysics']","Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). The representations learned by such adversarially robust networks have also been shown to be more human perceptually-aligned than non-robust networks via image manipulations (Santurkar et al., 2019, Engstrom et al., 2019). Despite appearing closer to human visual perception, it is unclear if the constraints in robust DNN representations match biological constraints found in human vision. Human vision seems to rely on texture-based/summary statistic representations in the periphery, which have been shown to explain phenomena such as crowding (Balas et al., 2009) and performance on visual search tasks (Rosenholtz et al., 2012). To understand how adversarially robust optimizations/representations compare to human vision, we performed a psychophysics experiment using a metamer task similar to Freeman \& Simoncelli, 2011, Wallis et al., 2016 and Deza et al., 2019 where we evaluated how well human observers could distinguish between images synthesized to match adversarially robust representations compared to non-robust representations and a texture synthesis model of peripheral vision (Texforms a la Long et al., 2018).  We found that the discriminability of robust representation and texture model images decreased to near chance performance as stimuli were presented farther in the periphery.  Moreover, performance on robust and texture-model images showed similar trends within participants, while performance on non-robust representations changed minimally across the visual field.  These results together suggest that (1) adversarially robust representations capture peripheral computation better than non-robust representations and (2) robust representations capture peripheral computation similar to current state-of-the-art texture peripheral vision models. More broadly, our findings support the idea that localized texture summary statistic representations may drive human invariance to adversarial perturbations and that the incorporation of such representations in DNNs could give rise to useful properties like adversarial robustness.",https://api.openreview.net/pdf/7f2e10fe0e775d6b9a7ac2d2d46206fcefd3f1ca.pdf,optimization;representation;meta-learning;multimodal,https://scholar.google.com/scholar?q=Finding+Biological+Plausibility+for+Adversarially+Robust+Features+via+Metameric+Tasks
Tackling the Generative Learning Trilemma with Denoising Diffusion GANs,2022,ICLR,"['Zhisheng Xiao', 'Karsten Kreis', 'Arash Vahdat']",spotlight,[],"A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000$\times$ faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively.",https://api.openreview.net/pdf/6570cfc5a990e77af23d8a4c6b934ac249ba4426.pdf,graph;zero_few-shot;generative model;multimodal;diffusion models,https://scholar.google.com/scholar?q=Tackling+the+Generative+Learning+Trilemma+with+Denoising+Diffusion+GANs
Probabilistic Implicit Scene Completion,2022,ICLR,"['Dongsu Zhang', 'Changwoon Choi', 'Inbum Park', 'Young Min Kim']",spotlight,"['3D shape completion', '3D generative model']","We propose a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a considerable amount of missing data cluttered with unsegmented objects. The problem of shape completion is inherently ill-posed, and high-quality result requires scalable solutions that consider multiple possible outcomes. We employ the Generative Cellular Automata that learns the multi-modal distribution and transform the formulation to process large-scale continuous geometry. The local continuous shape is incrementally generated as a sparse voxel embedding, which contains the latent code for each occupied cell. We formally derive that our training objective for the sparse voxel embedding maximizes the variational lower bound of the complete shape distribution and therefore our progressive generation constitutes a valid generative model. Experiments show that our model successfully generates diverse plausible scenes faithful to the input, especially when the input suffers from a significant amount of missing data. We also demonstrate that our approach outperforms deterministic models even in less ambiguous cases with a small amount of missing data, which infers that probabilistic formulation is crucial for high-quality geometry completion on input scans exhibiting any levels of completeness.",https://api.openreview.net/pdf/d9aba3c3efb0ad334f7b6a755f29a0b39bc05867.pdf,zero_few-shot;generative model;sparse;multimodal;3d,https://scholar.google.com/scholar?q=Probabilistic+Implicit+Scene+Completion
Iterative Circuit Repair Against Formal Specifications,2023,ICLR,"['Matthias Cosler', 'Frederik Schmitt', 'Christopher Hahn', 'Bernd Finkbeiner']",poster,"['sequential circuits', 'repair', 'synthesis', 'transformer']","We present a deep learning approach for repairing sequential circuits against formal specifications given in linear-time temporal logic (LTL). Given a defective circuit and its formal specification, we train Transformer models to output circuits that satisfy the corresponding specification. We propose a separated hierarchical Transformer for multimodal representation learning of the formal specification and the circuit. We introduce a data generation algorithm that enables generalization to more complex specifications and out-of-distribution datasets. In addition, our proposed repair mechanism significantly improves the automated synthesis of circuits from LTL specifications with Transformers. It improves the state-of-the-art by $6.8$ percentage points on held-out instances and $11.8$ percentage points on an out-of-distribution dataset from the annual reactive synthesis competition.",https://api.openreview.net/pdf/836416358c35826ddb12f100d55e28a66973ef30.pdf,transformer;representation;generative model;active learning;multimodal,https://scholar.google.com/scholar?q=Iterative+Circuit+Repair+Against+Formal+Specifications
HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention,2023,ICLR,"['Shijie Geng', 'Jianbo Yuan', 'Yu Tian', 'Yuxiao Chen', 'Yongfeng Zhang']",poster,[],"The success of large-scale contrastive vision-language pretraining (CLIP) has benefited both visual recognition and multimodal content understanding. The concise design brings CLIP the advantage in inference efficiency against other vision-language models with heavier cross-attention fusion layers, making it a popular choice for a wide spectrum of downstream tasks. However, CLIP does not explicitly capture the hierarchical nature of high-level and fine-grained semantics conveyed in images and texts, which is arguably critical to vision-language understanding and reasoning. To this end, we equip both the visual and language branches in CLIP with hierarchy-aware attentions, namely Hierarchy-aware CLIP (HiCLIP), to progressively discover semantic hierarchies layer-by-layer from both images and texts in an unsupervised manner. As a result, such hierarchical aggregation significantly improves the cross-modal alignment. To demonstrate the advantages of HiCLIP, we conduct qualitative analysis on its unsupervised hierarchy induction during inference, as well as extensive quantitative experiments on both visual recognition and vision-language downstream tasks.",https://api.openreview.net/pdf/2e082778e9c948cf856dc93b13cc4d0734583c61.pdf,transformer;contrastive learning;inference;multimodal,https://scholar.google.com/scholar?q=HiCLIP:+Contrastive+Language-Image+Pretraining+with+Hierarchy-aware+Attention
Broken Neural Scaling Laws,2023,ICLR,"['Ethan Caballero', 'Kshitij Gupta', 'Irina Rish', 'David Krueger']",poster,"['Scaling Laws', 'Scaling', 'Scale', 'Big Learning', 'Deep Learning', 'Artificial Neural Networks']","We present a smoothly broken power law functional form (referred to by us as a broken neural scaling law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning (single agent and multi-agent). When compared to other functional forms for neural scaling behavior, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models and extrapolates scaling behavior that other functional forms are incapable of expressing such as the non-monotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. See arXiv for longer version of this paper. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws",https://api.openreview.net/pdf/b9c49ee6a7c5cbe69e7836796acf1b820da86174.pdf,reinforcement learning;zero_few-shot;transformer;generative model;contrastive learning;metric;multi-agent;multimodal;llm,https://scholar.google.com/scholar?q=Broken+Neural+Scaling+Laws
$\mathrm{SE}(3)$-Equivariant Attention Networks for Shape Reconstruction in Function Space,2023,ICLR,"['Evangelos Chatzipantazis', 'Stefanos Pertigkiozoglou', 'Edgar Dobriban', 'Kostas Daniilidis']",poster,"['shape reconstruction', 'equivariance', 'neural fields', 'attention', '3D vision', 'point clouds']","We propose a method for 3D shape reconstruction from unoriented point clouds. Our method consists of a novel SE(3)-equivariant coordinate-based network (TF-ONet), that parametrizes the occupancy field of the shape and respects the inherent symmetries of the problem. In contrast to previous shape reconstruction methods that align the input to a regular grid, we operate directly on the irregular point cloud. Our architecture leverages equivariant attention layers that operate on local tokens. This mechanism enables local shape modelling, a crucial property for scalability to large scenes. Given an unoriented, sparse, noisy point cloud as input, we produce equivariant features for each point. These serve as keys and values for the subsequent equivariant cross-attention blocks that parametrize the occupancy field. By querying an arbitrary point in space, we predict its occupancy score. We show that our method outperforms previous SO(3)-equivariant methods, as well as non-equivariant methods trained on SO(3)-augmented datasets. More importantly, local modelling together with SE(3)-equivariance create an ideal setting for SE(3) scene reconstruction. We show that by training only on single, aligned objects and without any pre-segmentation, we can reconstruct novel scenes containing arbitrarily many objects in random poses without any performance loss. 
",https://api.openreview.net/pdf/9411d2b932a2b7a46202e67d58a5f8067fd33c77.pdf,transformer;sparse;augmentation;segmentation;multimodal;3d,https://scholar.google.com/scholar?q=$\mathrm{SE}(3)$-Equivariant+Attention+Networks+for+Shape+Reconstruction+in+Function+Space
Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus,2023,ICLR,"['Gang Li', 'Yang Li']",poster,"['vision-language', 'UI', 'few-shot', 'finetuning', 'multi-task']","Mobile UI understanding is important for enabling various interaction tasks such as UI automation and accessibility. Previous mobile UI modeling often depends on the view hierarchy information of a screen, which directly provides the structural data of the UI, with the hope to bypass challenging tasks of visual modeling from screen pixels. However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information. As a result, despite the use of view hierarchies could offer short-term gains, it may ultimately hinder the applicability and performance of the model. In this paper, we propose Spotlight, a vision-only approach for mobile UI understanding. Specifically, we enhance a vision-language model that only takes the screenshot of the UI and a region of interest on the screen---the focus---as the input. This general architecture of Spotlight is easily scalable and capable of performing a range of UI modeling tasks. Our experiments show that our model establishes SoTA results on several representative UI tasks and outperforms previous methods that use both screenshots and view hierarchies as inputs. Furthermore, we explore multi-task learning and few-shot prompting capacities of the proposed models, demonstrating promising results in the multi-task learning direction.",https://api.openreview.net/pdf/ad305a1a4c4b0a2571863546dc680f91c8b5b9f1.pdf,graph;multi-task;multimodal;llm,https://scholar.google.com/scholar?q=Spotlight:+Mobile+UI+Understanding+using+Vision-Language+Models+with+a+Focus
Progressive Mix-Up for Few-Shot Supervised Multi-Source Domain Transfer,2023,ICLR,"['Ronghang Zhu', 'Ronghang Zhu', 'Xiang Yu', 'Sheng Li']",poster,"['Representation Learning', 'Domain Adaptation']","This paper targets at a new and challenging setting of knowledge transfer from multiple source domains to a single target domain, where target data is few shot or even one shot with label. Traditional domain generalization or adaptation methods cannot directly work since there is no sufficient target domain distribution serving as the transfer object. The multi-source setting further prevents the transfer task as excessive domain gap introduced from all the source domains. To tackle this problem, we newly propose a progressive mix-up (P-Mixup) mechanism to introduce an intermediate mix-up domain, pushing both the source domains and the few-shot target domain aligned to this mix-up domain. Further by enforcing the mix-up domain to progressively move towards the source domains, we achieve the domain transfer from multi-source domains to the single one-shot target domain. Our P-Mixup is different from traditional mix-up that ours is with a progressive and adaptive mix-up ratio, following the curriculum learning spirit to better align the source and target domains. Moreover, our P-Mixup combines both pixel-level and feature-level mix-up to better enrich the data diversity. Experiments on two benchmarks show that our P-Mixup significantly outperforms the state-of-the-art methods, i.e., 6.0\% and 6.8\% improvements on Office-Home and DomainNet.",https://api.openreview.net/pdf/9e54132af1c66891c78ab803b81ba0fbbccdfc8b.pdf,graph;zero_few-shot;adaptive;transfer learning;multimodal;curriculum learning,https://scholar.google.com/scholar?q=Progressive+Mix-Up+for+Few-Shot+Supervised+Multi-Source+Domain+Transfer
Reward Design with Language Models,2023,ICLR,"['Minae Kwon', 'Sang Michael Xie', 'Kalesha Bullard', 'Dorsa Sadigh']",poster,"['reward design', 'foundation models', 'gpt3', 'reward specification', 'reinforcement learning', 'human-ai interaction']","Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by using a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperforms RL agents trained with reward functions learned via supervised learning. ",https://api.openreview.net/pdf/696171827b35dfe4e639dfe0644bf0f279f84c75.pdf,reinforcement learning;graph;zero_few-shot;multimodal;llm,https://scholar.google.com/scholar?q=Reward+Design+with+Language+Models
Make-A-Video: Text-to-Video Generation without Text-Video Data,2023,ICLR,"['Uriel Singer', 'Adam Polyak', 'Thomas Hayes', 'Xi Yin', 'Jie An', 'Songyang Zhang', 'Qiyuan Hu', 'Harry Yang', 'Oron Ashual', 'Oran Gafni', 'Devi Parikh', 'Sonal Gupta', 'Yaniv Taigman']",poster,[],"We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. 
We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.",https://api.openreview.net/pdf/89dbebc8608b7115596225380f7f411d9c944eaf.pdf,transformer;representation;generative model;multimodal,https://scholar.google.com/scholar?q=Make-A-Video:+Text-to-Video+Generation+without+Text-Video+Data
MMVAE+: Enhancing the Generative Quality of Multimodal VAEs without Compromises,2023,ICLR,"['Emanuele Palumbo', 'Imant Daunhawer', 'Julia E Vogt']",poster,"['Multimodal Variational Autoencoder', 'Variational Autoencoder', 'Multimodal Generative Learning']","Multimodal VAEs have recently gained attention as efficient models for weakly-supervised generative learning with multiple modalities. However, all existing variants of multimodal VAEs are affected by a non-trivial trade-off between generative quality and generative coherence. In particular mixture-based models achieve good coherence only at the expense of sample diversity and a resulting lack of generative quality. We present a novel variant of the mixture-of-experts multimodal variational autoencoder that improves its generative quality, while maintaining high semantic coherence. We model shared and modality-specific information in separate latent subspaces, proposing an objective that overcomes certain dependencies on hyperparameters that arise for existing approaches with the same latent space structure. Compared to these existing approaches, we show increased robustness with respect to changes in the design of the latent space, in terms of the capacity allocated to modality-specific subspaces. We show that our model achieves both good generative coherence and high generative quality in challenging experiments, including more complex multimodal datasets than those used in previous works.",https://api.openreview.net/pdf/629389d94e1def86f254ed563a21b9df2af23304.pdf,graph;zero_few-shot;transformer;vae;generative model;multimodal,https://scholar.google.com/scholar?q=MMVAE+:+Enhancing+the+Generative+Quality+of+Multimodal+VAEs+without+Compromises
Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning,2023,ICLR,"['Ivona Najdenkoska', 'Xiantong Zhen', 'Marcel Worring']",poster,"['multimodal', 'few-shot learning', 'meta-learning', 'transformers', 'vision and language models']","Multimodal few-shot learning is challenging due to the large domain gap between vision and language modalities. Existing methods are trying to communicate visual concepts as prompts to frozen language models, but rely on hand-engineered task induction to reduce the hypothesis space. To make the whole process learnable, we introduce a multimodal meta-learning approach. Specifically, our approach decomposes the training of the model into a set of related multimodal few-shot tasks. We define a meta-mapper network, acting as a meta-learner, to efficiently bridge frozen large-scale vision and language models and leverage their already learned capacity. By updating the learnable parameters only of the meta-mapper, it learns to accrue shared meta-knowledge among these tasks. Thus, it can rapidly adapt to newly presented samples with only a few gradient updates. Importantly, it induces the task in a completely data-driven manner, with no need for a hand-engineered task induction. We evaluate our approach on recently proposed multimodal few-shot benchmarks, measuring how rapidly the model can bind novel visual concepts to words and answer visual questions by observing only a limited set of labeled examples. The experimental results show that our meta-learning approach outperforms the baseline across multiple datasets and various training settings while being computationally more efficient.",https://api.openreview.net/pdf/0e9bd6133a3659d2a5883ce2063de3dfff12c275.pdf,graph;zero_few-shot;meta-learning;multimodal;llm,https://scholar.google.com/scholar?q=Meta+Learning+to+Bridge+Vision+and+Language+Models+for+Multimodal+Few-Shot+Learning
Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning,2023,ICLR,"['Zhendong Wang', 'Jonathan J Hunt', 'Mingyuan Zhou']",poster,"['offline RL', 'diffusion models', 'behavior cloning', 'policy regularization', 'Q-learning']","Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Q-learning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness of the diffusion model-based policy, and the coupling of the behavior cloning and policy improvement under the diffusion model both contribute to the outstanding performance of Diffusion-QL. We illustrate the superiority of our method compared to prior works in a simple 2D bandit example with a multimodal behavior policy. We then show that our method can achieve state-of-the-art performance on the majority of the D4RL benchmark tasks.",https://api.openreview.net/pdf/a55a26f03a11dcb7991b40527008d900c4a75044.pdf,reinforcement learning;offline reinforcement learning;optimization;generative model;multimodal;diffusion models,https://scholar.google.com/scholar?q=Diffusion+Policies+as+an+Expressive+Policy+Class+for+Offline+Reinforcement+Learning
HypeR: Multitask Hyper-Prompted Training Enables Large-Scale Retrieval Generalization,2023,ICLR,"['ZeFeng Cai', 'Chongyang Tao', 'Tao Shen', 'Can Xu', 'Xiubo Geng', 'Xin Alex Lin', 'Liang He', 'Daxin Jiang']",poster,"['Uniformed Large-Scale Retrieval', 'Multi-Task hyper-prompted training', 'Retrieval Generalization']","Recently, large-scale text retrieval has made impressive progress, facilitating both information retrieval and downstream knowledge-intensive tasks (e.g., open-domain QA and dialogue). With a moderate amount of data, a neural text retriever can outperform traditional methods such as BM25 by a large step. However, while being applied to out-of-domain data, the performance of a neural retriever degrades considerably. Therefore, how to enable a retriever to perform more robustly across different domains or tasks  and even show strong zero-shot transfer ability is critical for building scalable IR systems. To this end, we propose HypeR, a hyper-prompted training mechanism to enable uniform retrieval across tasks of different domains. Specifically, our approach jointly trains the query encoder with a shared prompt-based parameter pool and a prompt synthesizer that dynamically composes hyper-prompt for encoding each query from different tasks or domains. Besides, to avoid the mode collapse of prompt attention distribution for different queries, we design a contrastive prompt regularization that promotes the mode of prompt attention to be aligned and uniform. Through multi-task hyper-prompted training, our retriever can master the ability to dynamically represent different types of queries and transfer knowledge across different domains and tasks. Extensive experiments show our model attains better retrieval performance across different tasks and better zero-shot transfer ability compared with various previous methods.",https://api.openreview.net/pdf/ffa8e64f13b08c527104225518e4a9fd1371ace2.pdf,zero_few-shot;transformer;contrastive learning;transfer learning;multi-task;multimodal;llm,https://scholar.google.com/scholar?q=HypeR:+Multitask+Hyper-Prompted+Training+Enables+Large-Scale+Retrieval+Generalization
Identifiability Results for Multimodal Contrastive Learning,2023,ICLR,"['Imant Daunhawer', 'Alice Bizeul', 'Emanuele Palumbo', 'Alexander Marx', 'Julia E Vogt']",poster,"['multimodal learning', 'multi-view learning', 'contrastive learning', 'causal representation learning', 'nonlinear ica', 'identifiability']","Contrastive learning is a cornerstone underlying recent progress in multi-view and multimodal learning, e.g., in representation learning with image/caption pairs. While its effectiveness is not yet fully understood, a line of recent work reveals that contrastive learning can invert the data generating process and recover ground truth latent factors shared between views. In this work, we present new identifiability results for multimodal contrastive learning, showing that it is possible to recover shared factors in a more general setup than the multi-view setting studied previously. Specifically, we distinguish between the multi-view setting with one generative mechanism (e.g., multiple cameras of the same type) and the multimodal setting that is characterized by distinct mechanisms (e.g., cameras and microphones). Our work generalizes previous identifiability results by redefining the generative process in terms of distinct mechanisms with modality-specific latent variables. We prove that contrastive learning can block-identify latent factors shared between modalities, even when there are nontrivial dependencies between factors. We empirically verify our identifiability results with numerical simulations and corroborate our findings on a complex multimodal dataset of image/text pairs. Zooming out, our work provides a theoretical basis for multimodal representation learning and explains in which settings multimodal contrastive learning can be effective in practice.",https://api.openreview.net/pdf/5aa78fe66dfb42e06ee2ff4f83753a0161bedc84.pdf,representation;generative model;contrastive learning;multimodal;multi-view,https://scholar.google.com/scholar?q=Identifiability+Results+for+Multimodal+Contrastive+Learning
Imitating Human Behaviour with Diffusion Models,2023,ICLR,"['Tim Pearce', 'Tabish Rashid', 'Anssi Kanervisto', 'Dave Bignell', 'Mingfei Sun', 'Raluca Georgescu', 'Sergio Valcarcel Macua', 'Shan Zheng Tan', 'Ida Momennejad', 'Katja Hofmann', 'Sam Devlin']",poster,"['imitation learning', 'behavioral cloning', 'behavioral cloning', 'diffusion models', 'generative models']","Diffusion models have emerged as powerful generative models in the text-to-image domain. This paper studies their application as observation-to-action models for imitating human behaviour in sequential environments. Human behaviour is stochastic and multimodal, with structured correlations between action dimensions. Meanwhile, standard modelling choices in behaviour cloning are limited in their expressiveness and may introduce bias into the cloned policy. We begin by pointing out the limitations of these choices. We then propose that diffusion models are an excellent fit for imitating human behaviour, since they learn an expressive distribution over the joint action space. We introduce several innovations to make diffusion models suitable for sequential environments; designing suitable architectures, investigating the role of guidance, and developing reliable sampling strategies. Experimentally, diffusion models closely match human demonstrations in a simulated robotic control task and a modern 3D gaming environment.",https://api.openreview.net/pdf/b6515fc810687c08e9e8cbc72cd22ef8e7cdc7b6.pdf,graph;generative model;multimodal;diffusion models;3d,https://scholar.google.com/scholar?q=Imitating+Human+Behaviour+with+Diffusion+Models
DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training,2023,ICLR,"['Wei Li', 'Linchao Zhu', 'Longyin Wen', 'Yi Yang']",poster,"['Zero-shot captioning', 'Decoder training', 'Multi-modal learning']","Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong zero-shot transfer capability in many discriminative tasks, e.g., image classification. Their adaptation to zero-shot image-conditioned text generation tasks has drawn increasing interest. Prior arts approach to zero-shot captioning by either utilizing the existing large language models (e.g., GPT-2) or pre-training the encoder-decoder network in an end-to-end manner. However, the large language models may not generate sensible descriptions due to the task discrepancy between captioning and language modeling, while the end-to-end pre-training requires paired data and extensive computational resources. In this work, we propose a simple framework, named DeCap, for zero-shot captioning. We introduce a lightweight visual-aware language decoder. This decoder is both data-efficient and computation-efficient: 1) it only requires the \textit{text} data for training, easing the burden on the collection of paired data. 2) it does not require end-to-end training. When trained with text-only data, the decoder takes the text embedding extracted from the off-the-shelf CLIP encoder as a prefix embedding. The challenge is that the decoder is trained on the text corpus but at the inference stage, it needs to generate captions based on visual inputs. Though the CLIP text embedding and the visual embedding are correlated, the \textit{modality gap} issue is widely observed in multi-modal contrastive models that prevents us from directly taking the visual embedding as the prefix embedding. We propose a training-free mechanism to reduce the modality gap. We project the visual embedding into the CLIP text embedding space, while the projected embedding retains the information of the visual input. Taking the projected embedding as the prefix embedding, the decoder generates high-quality descriptions that match the visual input. The experiments show that DeCap outperforms other zero-shot captioning methods and unpaired captioning methods by a large margin on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps. We apply DeCap to video captioning and achieve state-of-the-art zero-shot performance on MSR-VTT and ActivityNet-Captions. The code is available at https://github.com/dhg-wei/DeCap.",https://api.openreview.net/pdf/20281fe81003b21131076887ded62556d1c2dc19.pdf,graph;zero_few-shot;generative model;contrastive learning;inference;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=DeCap:+Decoding+CLIP+Latents+for+Zero-Shot+Captioning+via+Text-Only+Training
Revisit Finetuning strategy for Few-Shot Learning to Transfer the Emdeddings,2023,ICLR,"['Heng Wang', 'Tan Yue', 'Xiang Ye', 'Zihang He', 'Bohan Li', 'Yong Li']",poster,"['Few-Shot Learning', 'Finetuning', 'Equivariance']","Few-Shot Learning (FSL) aims to learn a simple and effective bias on limited novel samples. Recently, many methods have been focused on re-training a randomly initialized linear classifier to adapt it to the novel features extracted by the pre-trained feature extractor (called Linear-Probing-based methods). These methods typically assumed the pre-trained feature extractor was robust enough, i.e., finetuning was not needed, and hence the pre-trained feature extractor does not change on the novel samples. However, the unchanged pre-trained feature extractor will distort the features of novel samples because the robustness assumption may not hold, especially on the out-of-distribution samples. To extract the undistorted features, we designed Linear-Probing-Finetuning with Firth-Bias (LP-FT-FB) to yield an accurate bias on the limited samples for better finetuning the pre-trained feature extractor, providing stronger transferring ability. In LP-FT-FB, we further proposed inverse Firth Bias Reduction (i-FBR) to regularize the over-parameterized feature extractor on which FBR does not work well.	The proposed i-FBR effectively alleviates the over-fitting problem of the feature extractor in the process of finetuning and helps extract undistorted novel features. To show the effectiveness of the designed LP-FT-FB, we conducted a lot of experiments on the commonly used FSL datasets under different backbones, including in-domain and cross-domain FSL tasks. The experimental results show that the proposed FT-LP-FB outperforms the SOTA FSL methods. The code is available at https://github.com/whzyf951620/LinearProbingFinetuningFirthBias.",https://api.openreview.net/pdf/b382c59b1c6211dd6e1fbea2766b109e56e8f766.pdf,zero_few-shot;transfer learning;multimodal,https://scholar.google.com/scholar?q=Revisit+Finetuning+strategy+for+Few-Shot+Learning+to+Transfer+the+Emdeddings
SLTUNET: A Simple Unified Model for Sign Language Translation,2023,ICLR,"['Biao Zhang', 'Mathias Müller', 'Rico Sennrich']",poster,"['Unified Modeling', 'Multi-task Learning', 'Sign Language Translation', 'Cross-modality Learning']","Despite recent successes with neural models for sign language translation (SLT), translation quality still lags behind spoken languages because of the data scarcity and modality gap between sign video and text. To address both problems, we investigate strategies for cross-modality representation sharing for SLT. We propose SLTUNET, a simple unified neural model designed to support multiple SLTrelated tasks jointly, such as sign-to-gloss, gloss-to-text and sign-to-text translation. Jointly modeling different tasks endows SLTUNET with the capability to explore the cross-task relatedness that could help narrow the modality gap. In addition, this allows us to leverage the knowledge from external resources, such as abundant parallel data used for spoken-language machine translation (MT). We show in experiments that SLTUNET achieves competitive and even state-of-the-art performance on PHOENIX-2014T and CSL-Daily when augmented with MT data and equipped with a set of optimization techniques. We further use the DGS Corpus for end-to-end SLT for the first time. It covers broader domains with a significantly larger vocabulary, which is more challenging and which we consider to allow for a more realistic assessment of the current state of SLT than the former two. Still, SLTUNET obtains improved results on the DGS Corpus. Code is available at https://github.com/bzhangGo/sltunet.",https://api.openreview.net/pdf/9507e2116df8f18cfd2b45279e7e83f544567dbe.pdf,graph;optimization;zero_few-shot;representation;augmentation;multimodal,https://scholar.google.com/scholar?q=SLTUNET:+A+Simple+Unified+Model+for+Sign+Language+Translation
Global Explainability of GNNs via Logic Combination of Learned Concepts,2023,ICLR,"['Steve Azzolin', 'Antonio Longa', 'Pietro Barbiero', 'Pietro Lio', 'Andrea Passerini']",poster,"['Explainability', 'Graph Neural Networks', 'Concept Learning']","While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned.
In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. 
Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted formulas are faithful to the model predictions, to the point of providing insights into some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.",https://api.openreview.net/pdf/4bc7378db838b1014f2e7b981b34a3e0aadaaf09.pdf,graph;multimodal,https://scholar.google.com/scholar?q=Global+Explainability+of+GNNs+via+Logic+Combination+of+Learned+Concepts
Visually-Augmented Language Modeling,2023,ICLR,"['Weizhi Wang', 'Li Dong', 'Hao Cheng', 'Haoyu Song', 'Xiaodong Liu', 'Xifeng Yan', 'Jianfeng Gao', 'Furu Wei']",poster,"['visually-grounded language modeling', 'visual commonsense reasoning', 'pre-trained visually-augmented language model']","Human language is grounded on multimodal knowledge including visual knowledge like colors, sizes, and shapes. However, current large-scale pre-trained language models rely on the text-only self-supervised training with massive text data, which precludes them from utilizing relevant visual information when necessary. To address this, we propose a novel pre-training framework, named VaLM, to Visually-augment text tokens with retrieved relevant images for Language Modeling. Specifically, VaLM builds on a novel latent text-image alignment method via an image retrieval module to fetch corresponding images given a textual context. With the visually-augmented context, VaLM uses a visual knowledge fusion layer to enable multimodal grounded language modeling by attending on both text context and visual knowledge in images. We evaluate VaLM on various visual knowledge intensive commonsense reasoning tasks, which require visual information to excel. The experimental results illustrate that VaLM outperforms all strong language-only and vision-language baselines with substantial gains on reasoning object commonsense including color, size, and shape.",https://api.openreview.net/pdf/c73c81bf4faecceb125dd37e5452d0ba0431a662.pdf,zero_few-shot;augmentation;multimodal,https://scholar.google.com/scholar?q=Visually-Augmented+Language+Modeling
DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing,2023,ICLR,"['Pengcheng He', 'Jianfeng Gao', 'Weizhu Chen']",poster,[],"This paper presents a new pre-trained language model, NewModel, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the “tug-of-war” dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained NewModel using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the NewModel Large model achieves a 91.37% average score, which is 1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mNew-Model and observed a larger improvement over strong baselines compared to English models. For example, the mNewModel Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6% improvement over XLM-R Base, creating a new SOTA on this benchmark. We will make our model and code publicly available.",https://api.openreview.net/pdf/553181e6a53d384858f9fdfabb4dc41b0c245d8e.pdf,graph;zero_few-shot;transformer;multimodal,https://scholar.google.com/scholar?q=DeBERTaV3:+Improving+DeBERTa+using+ELECTRA-Style+Pre-Training+with+Gradient-Disentangled+Embedding+Sharing
Winning Both the Accuracy of Floating Point Activation and the Simplicity of Integer Arithmetic,2023,ICLR,"['Yulhwa Kim', 'Jaeyong Jang', 'Jehun Lee', 'Jihoon Park', 'Jeonghoon Kim', 'Byeongwook Kim', 'Baeseong park', 'Se Jung Kwon', 'Dongsoo Lee', 'jae-joon kim']",poster,[],"Even though floating point (FP) numbers have been adopted as a de facto standard data format for deep learning computing, the complexity of FP arithmetic impedes a broader deployment of Deep Neural Networks (DNNs). Recent works such as quantization have attempted to replace the FP matrix multiplication (MatMul) of DNNs with simple integer MatMul by transforming the datatypes of both weights and activations into integers. Unfortunately, unlike weight values that are static, it is challenging to represent dynamic activations with integers. In this paper, to simultaneously achieve the accuracy of FP activation and the simplicity of integer arithmetic, we present a method for replacing FP arithmetic with integer one without changing FP activations in the storage format while weights are quantized. The proposed method pre-aligns the significands of FP activations just ahead of the MatMul on-the-fly so that the aligned significands (integers) can be used for the computation. Inspired by an observation that conventional FP arithmetic does not produce precise results due to rounding, we demonstrate that our proposed integer arithmetic-based scheme can produce the same level of errors as that of the FP arithmetic in case DNNs use FP activations and quantized weights. Experimental results show that the hardware based on the proposed scheme shows significant improvement over FP arithmetic-based designs in terms of energy efficiency and throughput-per-area while maintaining a similar level of accuracy.",https://api.openreview.net/pdf/23401b9976bc4354ab4085a198b754762b23330f.pdf,graph;multimodal,https://scholar.google.com/scholar?q=Winning+Both+the+Accuracy+of+Floating+Point+Activation+and+the+Simplicity+of+Integer+Arithmetic
Preference Transformer: Modeling Human Preferences using Transformers for RL,2023,ICLR,"['Changyeon Kim', 'Jongjin Park', 'Jinwoo Shin', 'Honglak Lee', 'Pieter Abbeel', 'Kimin Lee']",poster,"['preference-based reinforcement learning', 'human-in-the-loop reinforcement learning', 'deep reinforcement learning']","Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.",https://api.openreview.net/pdf/8a47190a33890c3b90463d493dc6f9bb78af91ee.pdf,reinforcement learning;graph;transformer;multimodal,https://scholar.google.com/scholar?q=Preference+Transformer:+Modeling+Human+Preferences+using+Transformers+for+RL
Equivariant Shape-Conditioned Generation of 3D Molecules for Ligand-Based Drug Design,2023,ICLR,"['Keir Adams', 'Connor W. Coley']",poster,"['molecules', 'equivariance', 'generation']","Shape-based virtual screening is widely used in ligand-based drug design to search chemical libraries for molecules with similar 3D shapes yet novel 2D graph structures compared to known ligands. 3D deep generative models can potentially automate this exploration of shape-conditioned 3D chemical space; however, no existing models can reliably generate geometrically realistic drug-like molecules in conformations with a specific shape. We introduce a new multimodal 3D generative model that enables shape-conditioned 3D molecular design by equivariantly encoding molecular shape and variationally encoding chemical identity. We ensure local geometric and chemical validity of generated molecules by using autoregressive fragment-based generation with heuristic bonding geometries, allowing the model to prioritize the scoring of rotatable bonds to best align the growing conformation to the target shape. We evaluate our 3D generative model in tasks relevant to drug design including shape-conditioned generation of chemically diverse molecular structures and shape-constrained molecular property optimization, demonstrating its utility over virtual screening of enumerated libraries.",https://api.openreview.net/pdf/42d19238140ebee340546b8dafb68f8b62d7cb2b.pdf,graph;optimization;zero_few-shot;generative model;metric;multimodal;3d,https://scholar.google.com/scholar?q=Equivariant+Shape-Conditioned+Generation+of+3D+Molecules+for+Ligand-Based+Drug+Design
Re-Imagen: Retrieval-Augmented Text-to-Image Generator,2023,ICLR,"['Wenhu Chen', 'Hexiang Hu', 'Chitwan Saharia', 'William W. Cohen']",poster,"['Diffusion Model', 'Information Retrieval', 'Knowledge Grounding', 'Image Generation']","Research on text-to-image generation has witnessed significant progress in generating diverse and photo-realistic images, driven by diffusion and auto-regressive models trained on large-scale image-text data. Though state-of-the-art models can generate high-quality images of common entities, they often have difficulty generating images of uncommon entities, such as `Chortai (dog)' or `Picarones (food)'. To tackle this issue, we present the Retrieval-Augmented Text-to-Image Generator (Re-Imagen), a generative model that uses retrieved information to produce high-fidelity and faithful images, even for rare or unseen entities. Given a text prompt, Re-Imagen accesses an external multi-modal knowledge base to retrieve relevant (image, text) pairs, and uses them as references to generate the image. With this retrieval step, Re-Imagen is augmented with the knowledge of high-level semantics and low-level visual details of the mentioned entities, and thus improves its accuracy in generating the entities' visual appearances. We train Re-Imagen on a constructed dataset containing (image,text,retrieval) triples to teach the model to ground on both text prompt and retrieval. Furthermore, we develop a new sampling strategy to interleave the classifier-free guidance for text and retrieval condition to balance the text and retrieval alignment. Re-Imagen achieves new SoTA FID results on two image generation benchmarks, such as COCO (\ie, FID = 5.25) and WikiImage (\ie, FID = 5.82) without fine-tuning. To further evaluate the capabilities of the model, we introduce EntityDrawBench, a new benchmark that evaluates image generation for diverse entities, from frequent to rare, across multiple visual domains. Human evaluation on EntityDrawBench shows that Re-Imagen performs on par with the best prior models in photo-realism, but with significantly better real-world faithfulness, especially on less frequent entities. ",https://api.openreview.net/pdf/cd7624d9396c506925128bb17ad6c851714db6e4.pdf,zero_few-shot;generative model;augmentation;multimodal;llm,https://scholar.google.com/scholar?q=Re-Imagen:+Retrieval-Augmented+Text-to-Image+Generator
Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic,2023,ICLR,"['Zijun Wu', 'Zi Xuan Zhang', 'Atharva Naik', 'Zhijian Mei', 'Mauajama Firdaus', 'Lili Mou']",poster,"['Neural Fuzzy Logic', 'Weakly Supervised Reasoning', 'Natural Language Inference', 'Explainability and Interpretability']","Natural language inference (NLI) aims to determine the logical relationship between two sentences, such as Entailment, Contradiction, and Neutral. In recent years, deep learning models have become a prevailing approach to NLI, but they lack interpretability and explainability. In this work, we address the explainability of NLI by weakly supervised logical reasoning, and propose an Explainable Phrasal Reasoning (EPR) approach. Our model first detects phrases as the semantic unit and aligns corresponding phrases in the two sentences. Then, the model predicts the NLI label for the aligned phrases, and induces the sentence label by fuzzy logic formulas. Our EPR is almost everywhere differentiable and thus the system can be trained end to end. In this way, we are able to provide explicit explanations of phrasal logical relationships in a weakly supervised manner. We further show that such reasoning results help textual explanation generation.",https://api.openreview.net/pdf/028e5f4cb530b4105893cd7dcabbd0eae72dc100.pdf,zero_few-shot;generative model;inference;multimodal,https://scholar.google.com/scholar?q=Weakly+Supervised+Explainable+Phrasal+Reasoning+with+Neural+Fuzzy+Logic
Partial Label Unsupervised Domain Adaptation with Class-Prototype Alignment,2023,ICLR,"['Yan Yan', 'Yuhong Guo']",poster,"['Partial label learning', 'label noise', 'domain adaptation']","Partial label learning (PLL) tackles the problem where each instance is associated with a set of candidate labels, only one of which is the ground-truth label. Most existing PLL approaches assume that both the training and test sets share an identical data distribution. However, this assumption does not hold in many real-world scenarios where the training and test data come from different distributions. In this paper, we formalize this learning scenario as a new problem called partial label unsupervised domain adaptation (PLUDA). To address this challenging PLUDA problem, we propose a novel Prototype Alignment based PLUDA method named PAPLUDA, which dynamically refines the pseudo-labels of instances from both the source and target domains by consulting the outputs of a teacher-student model in a moving-average manner, and bridges the cross-domain discrepancy through inter-domain class-prototype alignment. In addition, a teacher-student model based contrastive regularization is deployed to enhance prediction stability and hence improve the class-prototypes in both domains for PLUDA. Comprehensive experimental results demonstrate that PAPLUDA achieves state-of-the-art performance on the widely used benchmark datasets.",https://api.openreview.net/pdf/dd2d4c58a30acac15a00d195cf88a31841339611.pdf,graph;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Partial+Label+Unsupervised+Domain+Adaptation+with+Class-Prototype+Alignment
An Extensible Multi-modal Multi-task Object Dataset with Materials,2023,ICLR,"['Trevor Scott Standley', 'Ruohan Gao', 'Dawn Chen', 'Jiajun Wu', 'Silvio Savarese']",poster,"['Multi-task', 'multi-modal', 'dataset', 'materials', 'weak supervision']","We present EMMa, an Extensible, Multimodal dataset of Amazon product listings that contains rich Material annotations. It contains more than 2.8 million objects, each with image(s), listing text, mass, price, product ratings, and position in Amazon’s product-category taxonomy. We also design a comprehensive taxonomy of 182 physical materials (e.g., Plastic → Thermoplastic → Acrylic). Objects areannotated with one or more materials from this taxonomy. With the numerous attributes available for each object, we develop a Smart Labeling framework to quickly add new binary labels to all objects with very little manual labeling effort, making the dataset extensible. Each object attribute in our dataset can be included in either the model inputs or outputs, leading to combinatorial possibilities in task configurations. For example, we can train a model to predict the object category from the listing text, or the mass and price from the product listing image. EMMa offers a new benchmark for multi-task learning in computer vision and NLP, and allows practitioners to efficiently add new tasks and object attributes at scale.",https://api.openreview.net/pdf/e8afd23963050ab7468dcd74394d5b194f47d7e7.pdf,zero_few-shot;multi-task;multimodal,https://scholar.google.com/scholar?q=An+Extensible+Multi-modal+Multi-task+Object+Dataset+with+Materials
Learning Multimodal Data Augmentation in Feature Space,2023,ICLR,"['Zichang Liu', 'Zhiqiang Tang', 'Xingjian Shi', 'Aston Zhang', 'Mu Li', 'Anshumali Shrivastava', 'Andrew Gordon Wilson']",poster,[],"The ability to jointly learn from multiple modalities, such as text, audio, and visual data, is a defining feature of intelligent systems. While there have been promising advances in designing neural networks to harness multimodal data, the enormous success of data augmentation currently remains limited to single-modality tasks like image classification. Indeed, it is particularly difficult to augment each modality while preserving the overall semantic structure of the data; for example, a caption may no longer be a good description of an image after standard augmentations have been applied, such as translation. Moreover, it is challenging to specify reasonable transformations that are not tailored to a particular modality. In this paper, we introduce LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that automatically learns to jointly augment multimodal data in feature space, with no constraints on the identities of the modalities or the relationship between modalities. We show that LeMDA can (1) profoundly improve the performance of multimodal deep learning architectures, (2) apply to combinations of modalities that have not been previously considered, and (3) achieve state-of-the-art results on a wide range of applications comprised of image, text, and tabular data.",https://api.openreview.net/pdf/32d27280c4c7b2da8c17dcd9e4ef4b4f16ec9733.pdf,graph;optimization;augmentation;multimodal,https://scholar.google.com/scholar?q=Learning+Multimodal+Data+Augmentation+in+Feature+Space
Discovering Latent Knowledge in Language Models Without Supervision,2023,ICLR,"['Collin Burns', 'Haotian Ye', 'Dan Klein', 'Jacob Steinhardt']",poster,"['AI safety', 'AI alignment', 'truthfulness', 'large language models', 'honesty', 'interpretability']","Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.",https://api.openreview.net/pdf/1a2f757c6a56314972af9e4b40e580d5691e0b55.pdf,zero_few-shot;multimodal;imitation learning;llm,https://scholar.google.com/scholar?q=Discovering+Latent+Knowledge+in+Language+Models+Without+Supervision
Is Attention All That NeRF Needs?,2023,ICLR,"['Mukund Varma T', 'Peihao Wang', 'Xuxi Chen', 'Tianlong Chen', 'Subhashini Venugopalan', 'Zhangyang Wang']",poster,"['Neural Radiance Field', 'Transformer', 'Neural Rendering']","We present Generalizable NeRF Transformer (GNT), a transformer-based architecture that reconstructs Neural Radiance Fields (NeRFs) and learns to render novel views on the fly from source views. While prior works on NeRFs optimize a scene representation by inverting a handcrafted rendering equation, GNT achieves neural representation and rendering that generalizes across scenes using transformers at two stages. (1) The view transformer leverages multi-view geometry as an inductive bias for attention-based scene representation, and predicts coordinate-aligned features by aggregating information from epipolar lines on the neighboring views. (2) The ray transformer renders novel views using attention to decode the features from the view transformer along the sampled points during ray marching. Our experiments demonstrate that when optimized on a single scene, GNT can successfully reconstruct NeRF without an explicit rendering formula due to the learned ray renderer. When trained on multiple scenes, GNT consistently achieves state-of-the-art performance when transferring to unseen scenes and outperform all other methods by ~10% on average. Our analysis of the learned attention maps to infer depth and occlusion indicate that attention enables learning a physically-grounded rendering. Our results show the promise of transformers as a universal modeling tool for graphics. Please refer to our project page for video results: https://vita-group.github.io/GNT/",https://api.openreview.net/pdf/d875ba2409ec78faf50cee666b3866b2b99b54f8.pdf,graph;transformer;representation;transfer learning;multimodal;multi-view;llm,https://scholar.google.com/scholar?q=Is+Attention+All+That+NeRF+Needs?
TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation,2023,ICLR,"['Rongjie Huang', 'Jinglin Liu', 'Huadai Liu', 'Yi Ren', 'Lichao Zhang', 'Jinzheng He', 'Zhou Zhao']",poster,"['Speech-to-speech translation', 'Multimodal challenge', 'Non-autoregressive generation']","Direct speech-to-speech translation (S2ST) with discrete units leverages recent progress in speech representation learning. Specifically, a sequence of discrete representations derived in a self-supervised manner are predicted from the model and passed to a vocoder for speech reconstruction, while still facing the following challenges: 1) Acoustic multimodality: the discrete units derived from speech with same content could be indeterministic due to the acoustic property (e.g., rhythm, pitch, and energy), which causes deterioration of translation accuracy; 2) high latency: current S2ST systems utilize autoregressive models which predict each unit conditioned on the sequence previously generated, failing to take full advantage of parallelism. In this work, we propose TranSpeech, a speech-to-speech translation model with bilateral perturbation. To alleviate the acoustic multimodal problem, we propose bilateral perturbation (BiP), which consists of the style normalization and information enhancement stages, to learn only the linguistic information from speech samples and generate more deterministic representations. With reduced multimodality, we step forward and become the first to establish a non-autoregressive S2ST technique, which repeatedly masks and predicts unit choices and produces high-accuracy results in just a few cycles. Experimental results on three language pairs demonstrate that BiP yields an improvement of 2.9 BLEU on average compared with a baseline textless S2ST model. Moreover, our parallel decoding shows a significant reduction of inference latency, enabling speedup up to 21.4x than autoregressive technique. Audio samples are available at https://TranSpeech.github.io",https://api.openreview.net/pdf/616759f9860441b1cc2f980b7e2becb2afd49833.pdf,zero_few-shot;representation;inference;multimodal,https://scholar.google.com/scholar?q=TranSpeech:+Speech-to-Speech+Translation+With+Bilateral+Perturbation
Jointly Learning Visual and Auditory Speech Representations from Raw Data,2023,ICLR,"['Alexandros Haliassos', 'Pingchuan Ma', 'Rodrigo Mira', 'Stavros Petridis', 'Maja Pantic']",poster,"['self-supervised learning', 'lipreading', 'speech recognition']","We present RAVEn, a self-supervised multi-modal approach to jointly learn visual and auditory speech representations. Our pre-training objective involves encoding masked inputs, and then predicting contextualised targets generated by slowly-evolving momentum encoders. Driven by the inherent differences between video and audio, our design is asymmetric w.r.t. the two modalities' pretext tasks: Whereas the auditory stream predicts both the visual and auditory targets, the visual one predicts only the auditory targets. We observe strong results in low- and high-resource labelled data settings when fine-tuning the visual and auditory encoders resulting from a single pre-training stage, in which the encoders are jointly trained. Notably, RAVEn surpasses all self-supervised methods on visual speech recognition (VSR) on LRS3, and combining RAVEn with self-training using only 30 hours of labelled data even outperforms a recent semi-supervised method trained on 90,000 hours of non-public data. At the same time, we achieve state-of-the-art results in the LRS3 low-resource setting for auditory speech recognition (as well as for VSR). Our findings point to the viability of learning powerful speech representations entirely from raw video and audio, i.e., without relying on handcrafted features. Code and models are available at https://github.com/ahaliassos/raven.",https://api.openreview.net/pdf/309f109b8dacce5715eeb3408e76860321dc637a.pdf,zero_few-shot;representation;metric;multimodal,https://scholar.google.com/scholar?q=Jointly+Learning+Visual+and+Auditory+Speech+Representations+from+Raw+Data
CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Alignment,2023,ICLR,"['Hongwei Xue', 'Yuchong Sun', 'Bei Liu', 'Jianlong Fu', 'Ruihua Song', 'Houqiang Li', 'Jiebo Luo']",poster,[],"Pre-trained image-text models, like CLIP, have demonstrated the strong power of vision-language representation learned from a large scale of web-collected image-text data. In light of the well-learned visual features, there are works that transfer image representation to the video domain and achieve good results. However, adapting image-text pre-trained models to video-text pre-training (i.e., post-pretraining) has not demonstrated a significant advantage yet. In this paper, we tackle this challenge by raising and addressing two questions: 1) what are the factors hindering post-pretraining CLIP from improving performance on video-text tasks, and 2) how to mitigate the impact of these factors. Through a series of comparative experiments and analyses, we find that the data scale and domain gap between language sources have large impacts. By these observations, we propose an Omnisource Cross-modal Learning method equipped with a Video Proxy mechanism on the basis of CLIP, namely CLIP-ViP. Extensive results show that our approach improves the performance of CLIP on video-text retrieval by a large margin. Our model achieves state-of-the-art results on a variety of datasets, including MSR-VTT, DiDeMo, LSMDC, and ActivityNet. We release our code and pre-trained CLIP-ViP models at \url{https://github.com/microsoft/XPretrain/tree/main/CLIP-ViP}.",https://api.openreview.net/pdf/f8c079d34aee5b9409dbf8a160ba5d1d8b547b1f.pdf,graph;representation;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=CLIP-ViP:+Adapting+Pre-trained+Image-Text+Model+to+Video-Language+Alignment
ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency,2023,ICLR,"['Pengzhen Ren', 'Changlin Li', 'Hang Xu', 'Yi Zhu', 'Guangrun Wang', 'Jianzhuang Liu', 'Xiaojun Chang', 'Xiaodan Liang']",poster,"['Zero-shot semantic segmentation', 'Vision-Language Pretraining', 'Visual Self-Supervision', 'Consistent Semantics']","Recently, great success has been made in learning visual representations from text supervision, facilitating the emergence of text-supervised semantic segmentation. However, existing works focus on pixel grouping and cross-modal semantic alignment, while ignoring the correspondence among multiple augmented views of the same image. To overcome such limitation, we propose multi-View Consistent learning (ViewCo) for text-supervised semantic segmentation. Specifically, we first propose text-to-views consistency modeling to learn correspondence for multiple views of the same input image. Additionally, we propose cross-view segmentation consistency modeling to address the ambiguity issue of text supervision by contrasting the segment features of Siamese visual encoders.  The text-to-views consistency benefits dense assignment of the visual features by encouraging different crops to align with the same text, while the cross-view segmentation consistency modeling provides additional self-supervision, overcoming the limitation of ambiguous text supervision for segmentation masks. Trained with large-scale image-text data, our model can directly segment objects of arbitrary categories in a zero-shot manner. Extensive experiments show that ViewCo outperforms state-of-the-art methods on average by up to 2.9%, 1.6%, and 2.4% mIoU on PASCAL VOC2012, PASCAL Context, and COCO, respectively.",https://api.openreview.net/pdf/1c4d99da565a5a77b48c93ace235f1f1c7922953.pdf,graph;zero_few-shot;representation;augmentation;segmentation;multimodal;multi-view;self-supervision,https://scholar.google.com/scholar?q=ViewCo:+Discovering+Text-Supervised+Segmentation+Masks+via+Multi-View+Semantic+Consistency
Massively Scaling Heteroscedastic Classifiers,2023,ICLR,"['Mark Collier', 'Rodolphe Jenatton', 'Basil Mustafa', 'Neil Houlsby', 'Jesse Berent', 'Effrosyni Kokiopoulou']",poster,[],"Heteroscedastic classifiers, which learn a multivariate Gaussian distribution over prediction logits, have been shown to perform well on image classification problems with hundreds to thousands of classes. However, compared to standard classifiers, they introduce extra parameters that scale linearly with the number of classes. This makes them infeasible to apply to larger-scale problems. In addition heteroscedastic classifiers introduce a critical temperature hyperparameter which must be tuned. We propose HET-XL, a heteroscedastic classifier whose parameter count when compared to a standard classifier scales independently of the number of classes. In our large-scale settings, we show that we can remove the need to tune the temperature hyperparameter, by directly learning it on the training data. On large image classification datasets with up to 4B images and 30k classes our method requires 14X fewer additional parameters, does not require tuning the temperature on a held-out set and performs consistently better than the baseline heteroscedastic classifier. HET-XL improves ImageNet 0-shot classification in a multimodal contrastive learning setup which can be viewed as a 3.5 billion class classification problem.",https://api.openreview.net/pdf/7be14f2821d211766625240b0311bd47ece2f5d8.pdf,transformer;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Massively+Scaling+Heteroscedastic+Classifiers
MultiViz: Towards Visualizing and Understanding Multimodal Models,2023,ICLR,"['Paul Pu Liang', 'Yiwei Lyu', 'Gunjan Chhablani', 'Nihal Jain', 'Zihao Deng', 'Xingbo Wang', 'Louis-Philippe Morency', 'Ruslan Salakhutdinov']",poster,"['multimodal learning', 'representation learning', 'interpretation', 'visualization']","The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing MultiViz, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. MultiViz is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.",https://api.openreview.net/pdf/34c8d3975874963fb014c66baab9fc0b36443d92.pdf,graph;representation;metric;multimodal,https://scholar.google.com/scholar?q=MultiViz:+Towards+Visualizing+and+Understanding+Multimodal+Models
Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning ,2023,ICLR,"['Zaid Khan', 'Yun Fu']",poster,"['vision-language', 'CLIP', 'image-text retrieval', 'transformers']","Contrastive vision-language models (e.g. CLIP) are typically created by updating all the parameters of a vision model and language model through contrastive training. Can such models be created by a small number of parameter updates to an already-trained language model and vision model? The literature describes techniques that can create vision-language models by updating a small number of parameters in a language model, but these require already aligned visual representations and are non-contrastive, hence unusable for latency-sensitive applications such as neural search. We explore the feasibility and benefits of parameter-efficient contrastive vision-language alignment through transfer learning: creating a model such as CLIP by minimally updating an already-trained vision and language model. We find that a minimal set of parameter updates ($<$7\%) can achieve the same performance as full-model training, and updating specific components ($<$1\% of parameters) can match 75\% of full-model training. We describe a series of experiments: we show that existing knowledge is conserved more strongly in parameter-efficient training and that parameter-efficient scaling scales with model and dataset size. Where paired-image text data is scarce but strong multilingual language models exist (e.g. low resource languages), parameter-efficient training is even preferable to full-model training. Given a fixed compute budget, parameter-efficient training allows training larger models on the same hardware, achieving equivalent performance in less time. Parameter-efficient training hence constitutes an energy-efficient and effective training strategy for contrastive vision-language models that may be preferable to the full-model training paradigm for common use cases.
Code and weights at https://github.com/codezakh/LilT.",https://api.openreview.net/pdf/616f79e13971d683f9f48a16c7436a115b065d7f.pdf,zero_few-shot;transformer;representation;contrastive learning;transfer learning;multimodal,https://scholar.google.com/scholar?q=Contrastive+Alignment+of+Vision+to+Language+Through+Parameter-Efficient+Transfer+Learning+
Write and Paint: Generative Vision-Language Models are Unified Modal Learners,2023,ICLR,"['Shizhe Diao', 'Wangchunshu Zhou', 'Xinsong Zhang', 'Jiawei Wang']",poster,"['Foundation model', 'Multi-modal learning', 'Vision-language pre-training']","Recent advances in vision-language pre-training have pushed the state-of-the-art on various vision-language tasks, making machines more capable of multi-modal writing (image-to-text generation) and painting (text-to-image generation). However, few studies investigate if these two essential capabilities can be learned together and boost each other, making a versatile and powerful multi-modal foundation model. In this work, we disclose the potential of symmetric generative vision-language pre-training in learning to write and paint concurrently, and propose a new unified modal model, named DaVinci, trained with prefix language modeling and prefix image modeling, a simple generative self-supervised objective on image-text pairs. Thanks to the proposed prefix multi-modal modeling framework, DaVinci is simple to train, scalable to huge data, adaptable to both writing and painting tasks, and also strong on other vision, text, and multi-modal understanding tasks. DaVinci achieves competitive performance on a wide range of 27 generation/understanding tasks and demonstrates the superiority of combining vision/language generative pre-training. Furthermore, we carefully benchmark the performance of different vision-language pre-training objectives on different scales of pre-training datasets on a heterogeneous and broad distribution coverage. Our results demonstrate the potential of exploiting self-supervision in both language and vision inputs, and establish new, stronger baselines for future comparisons at different data scales. The code and pre-trained models are available at https://github.com/shizhediao/DaVinci.",https://api.openreview.net/pdf/782da1a13b9ef3872a63284ecb7f4a21c757982c.pdf,zero_few-shot;generative model;metric;multimodal;self-supervision,https://scholar.google.com/scholar?q=Write+and+Paint:+Generative+Vision-Language+Models+are+Unified+Modal+Learners
Masked Vision and Language Modeling for Multi-modal Representation Learning,2023,ICLR,"['Gukyeong Kwon', 'Zhaowei Cai', 'Avinash Ravichandran', 'Erhan Bas', 'Rahul Bhotika', 'Stefano Soatto']",poster,"['Vision and language', 'multi-modal learning']","In this paper, we study how to use masked signal modeling in vision and language (V+L) representation learning. Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, we propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. This is motivated by the nature of image-text paired data that both of the image and the text convey almost the same information but in different formats. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. Our experiments on various V+L tasks show that the proposed method, along with common V+L alignment losses, not only achieves state-of-the-art performance by using a large amount of data but also outperforms the other competitors by a significant margin in the regimes of limited training data. ",https://api.openreview.net/pdf/327b556a6ea9da1009be2554324f7d7f94a99cae.pdf,graph;representation;multimodal,https://scholar.google.com/scholar?q=Masked+Vision+and+Language+Modeling+for+Multi-modal+Representation+Learning
Learning Human-Compatible Representations for Case-Based Decision Support,2023,ICLR,"['Han Liu', 'Yizhou Tian', 'Chacha Chen', 'Shi Feng', 'Yuxin Chen', 'Chenhao Tan']",poster,"['human-compatible representation learning', 'human triplet judgments']","Algorithmic case-based decision support provides examples to help human make sense of predicted labels and aid human in decision-making tasks. Despite the promising performance of supervised learning, representations learned by supervised models may not align well with human intuitions: what models consider as similar examples can be perceived as distinct by humans. As a result, they have limited effectiveness in case-based decision support. In this work, we incorporate ideas from metric learning with supervised learning to examine the importance of alignment for effective decision support. In addition to instance-level labels, we use human-provided triplet judgments to learn human-compatible decision-focused representations. Using both synthetic data and human subject experiments in multiple classification tasks, we demonstrate that such representation is better aligned with human perception than representation solely optimized for classification. Human-compatible representations identify nearest neighbors that are perceived as more similar by humans and allow humans to make more accurate predictions, leading to substantial improvements in human decision accuracies (17.8% in butterfly vs. moth classification and 13.2% in pneumonia classification).",https://api.openreview.net/pdf/8a16439b8778f846cdadf78cfd6ce542a48cbfbe.pdf,zero_few-shot;representation;metric;multimodal,https://scholar.google.com/scholar?q=Learning+Human-Compatible+Representations+for+Case-Based+Decision+Support
AnyDA: Anytime Domain Adaptation,2023,ICLR,"['Omprakash Chakraborty', 'Aadarsh Sahoo', 'Rameswar Panda', 'Abir Das']",poster,"['Efficient Domain Adaptation', 'Anytime Prediction', 'Knowledge Distillation', 'Resource-constrained Learning']","Unsupervised domain adaptation is an open and challenging problem in computer vision. While existing research shows encouraging results in addressing cross-domain distribution shift on common benchmarks, they are often constrained to testing under a specific target setting, limiting their impact for many real-world applications. In this paper, we introduce a simple yet effective framework for anytime domain adaptation that is executable with dynamic resource constraints to achieve accuracy-efficiency trade-offs under domain-shifts. We achieve this by training a single shared network using both labeled source and unlabeled data, with switchable depth, width and input resolutions on the fly to enable testing under a wide range of computation budgets. Starting with a teacher network trained from a label-rich source domain, we utilize bootstrapped recursive knowledge distillation as a nexus between source and target domains to jointly train the student network with switchable subnetworks. Experiments on multiple datasets well demonstrate the superiority of our approach over state-of-the-art methods.",https://api.openreview.net/pdf/cd2f8530081a65f6a7fee97f47a107998f5a9a17.pdf,graph;optimization;distillation;multimodal,https://scholar.google.com/scholar?q=AnyDA:+Anytime+Domain+Adaptation
Unified Discrete Diffusion for Simultaneous Vision-Language Generation,2023,ICLR,"['Minghui Hu', 'Chuanxia Zheng', 'Zuopeng Yang', 'Tat-Jen Cham', 'Heliang Zheng', 'Chaoyue Wang', 'Dacheng Tao', 'Ponnuthurai N. Suganthan']",poster,"['Multi-modal', 'Image generation', 'Image Caption.']","The recently developed discrete diffusion model performs extraordinarily well in generation tasks, especially in the text-to-image task, showing great potential for modeling multimodal signals. In this paper, we leverage these properties and present a unified multimodal generation model, which can perform text-based, image-based, and even vision-language simultaneous generation using a single model. Specifically, we unify the discrete diffusion process for multimodal signals by proposing a unified Markov transition matrix and a unified objective. Moreover, we design a multimodal mutual attention module to highlight the inter-modal linkages, which is vital for multimodal generation. Extensive experiments indicate that our proposed method can perform comparably to the state-of-the-art solutions in various generation tasks.",https://api.openreview.net/pdf/d386e35cf9491b25a9017b063c051ef1d750c527.pdf,transformer;generative model;multimodal;diffusion models;llm,https://scholar.google.com/scholar?q=Unified+Discrete+Diffusion+for+Simultaneous+Vision-Language+Generation
Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive Machine Translation,2023,ICLR,"['Zhengrui Ma', 'Chenze Shao', 'Shangtong Gui', 'Min Zhang', 'Yang Feng']",poster,"['Machine translation', 'Non-autoregressive generation', 'Fuzzy alignment']","Non-autoregressive translation (NAT) reduces the decoding latency but suffers from performance degradation due to the multi-modality problem. Recently, the structure of directed acyclic graph has achieved great success in NAT, which tackles the multi-modality problem by introducing dependency between vertices. However, training it with negative log-likelihood loss implicitly requires a strict alignment between reference tokens and vertices, weakening its ability to handle multiple translation modalities. In this paper, we hold the view that all paths in the graph are fuzzily aligned with the reference sentence. We do not require the exact alignment but train the model to maximize a fuzzy alignment score between the graph and reference, which takes captured translations in all modalities into account. Extensive experiments on major WMT benchmarks show that our method substantially improves translation performance and increases prediction confidence, setting a new state of the art for NAT on the raw training data.",https://api.openreview.net/pdf/69c77c3cf5e152302524f0544984352da29c2d74.pdf,graph;zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Fuzzy+Alignments+in+Directed+Acyclic+Graph+for+Non-Autoregressive+Machine+Translation
3D Segmenter: 3D Transformer based Semantic Segmentation via 2D Panoramic Distillation,2023,ICLR,"['ZHENNAN WU', 'YANG LI', 'Yifei Huang', 'Lin Gu', 'Tatsuya Harada', 'Hiroyuki Sato']",poster,"['3D semantic segmentation', 'knowledge distillation']","Recently, 2D semantic segmentation has witnessed a significant advancement thanks to the huge amount of 2D image datasets available. Therefore, in this work, we propose the first 2D-to-3D knowledge distillation strategy to enhance 3D semantic segmentation model with knowledge embedded in the latent space of powerful 2D models. Specifically, unlike standard knowledge distillation, where teacher and student models take the same data as input, we use 2D panoramas properly aligned with corresponding 3D rooms to train the teacher network and use the learned knowledge from 2D teacher to guide 3D student. To facilitate our research, we create a large-scale, fine-annotated 3D semantic segmentation benchmark, containing voxel-wise semantic labels and aligned panoramas of 5175 scenes. Based on this benchmark, we propose a 3D volumetric semantic segmentation network, which adapts Video Swin Transformer as backbone and introduces a skip connected linear decoder.  Achieving a state-of-the-art performance, our 3D Segmenter is computationally efficient and only requires $3.8\%$ of the parameters compared to the prior art. Our code and data will be released upon acceptance.",https://api.openreview.net/pdf/bad84a91fee36327634c83f4ba69dc11b0e52751.pdf,transformer;metric;distillation;segmentation;multimodal;3d,https://scholar.google.com/scholar?q=3D+Segmenter:+3D+Transformer+based+Semantic+Segmentation+via+2D+Panoramic+Distillation
Modeling Sequential Sentence Relation to Improve Cross-lingual Dense Retrieval,2023,ICLR,"['Shunyu Zhang', 'Yaobo Liang', 'MING GONG', 'Daxin Jiang', 'Nan Duan']",poster,[],"Recently multi-lingual pre-trained language models (PLM) such as mBERT and XLM-R have achieved impressive strides in cross-lingual dense retrieval. Despite its successes, they are general-purpose PLM while the multilingual PLM tailored for cross-lingual retrieval is still unexplored. Motivated by an observation that the sentences in parallel documents are approximately in the same order, which is universal across languages, we propose to model this sequential sentence relation to facilitate cross-lingual representation learning. Specifically, we propose a multilingual PLM called masked sentence model (MSM), which consists of a sentence encoder to generate the sentence representations, and a document encoder applied to a sequence of sentence vectors from a document. The document encoder is shared for all languages to model the universal sequential sentence relation across languages. To train the model, we propose a masked sentence prediction task, which masks and predicts the sentence vector via a hierarchical contrastive loss with sampled negatives. Comprehensive experiments on four cross-lingual retrieval tasks show MSM significantly outperforms existing advanced pre-training models, demonstrating the effectiveness and stronger cross-lingual retrieval capabilities of our approach. ",https://api.openreview.net/pdf/8649b63e1d2dd8ed68f852a7da6ab541293d32a9.pdf,transformer;representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Modeling+Sequential+Sentence+Relation+to+Improve+Cross-lingual+Dense+Retrieval
Multimodal Federated Learning via Contrastive Representation Ensemble,2023,ICLR,"['Qiying Yu', 'Yang Liu', 'Yimu Wang', 'Ke Xu', 'Jingjing Liu']",poster,"['Federated Learning', 'Multi-modal Learning', 'Representation-level Ensemble Knowledge Transfer']","With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose \textit{Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL)}, a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-local cross-modal ensemble strategy to aggregate client representations. To mitigate local model drift caused by two unprecedented heterogeneous factors stemming from multimodal discrepancy (\textit{modality gap} and \textit{task gap}), we further propose two inter-modal and intra-modal contrasts to regularize local training, which complements information of the absent modality for uni-modal clients and regularizes local clients to head towards global consensus. Thorough evaluations and ablation studies on image-text retrieval and visual question answering tasks showcase the superiority of CreamFL over state-of-the-art FL methods and its practical value.",https://api.openreview.net/pdf/a51674bdcfdff01c5f5e3ad87446148a5ce6b4be.pdf,representation;contrastive learning;federated learning;multimodal,https://scholar.google.com/scholar?q=Multimodal+Federated+Learning+via+Contrastive+Representation+Ensemble
Discrete Contrastive Diffusion for Cross-Modal Music and Image Generation,2023,ICLR,"['Ye Zhu', 'Yu Wu', 'Kyle Olszewski', 'Jian Ren', 'Sergey Tulyakov', 'Yan Yan']",poster,"['Contrastive Diffusion', 'Conditioned Generations', 'Music Generation', 'Image Synthesis']","Diffusion probabilistic models (DPMs) have become a popular approach to conditional generation, due to their promising results and support for cross-modal synthesis. A key desideratum in conditional synthesis is to achieve high correspondence between the conditioning input and generated output. Most existing methods learn such relationships implicitly, by incorporating the prior into the variational lower bound. In this work, we take a different route---we explicitly enhance input-output connections by maximizing their mutual information. To this end, we introduce a Conditional Discrete Contrastive Diffusion (CDCD) loss and design two contrastive diffusion mechanisms to effectively incorporate it into the denoising process, combining the diffusion training and contrastive learning for the first time by connecting it with the conventional variational objectives. We demonstrate the efficacy of our approach in evaluations with diverse multimodal conditional synthesis tasks: dance-to-music generation, text-to-image synthesis, as well as class-conditioned image synthesis. On each, we enhance the input-output correspondence and achieve higher or competitive general synthesis quality. Furthermore, the proposed approach improves the convergence of diffusion models, reducing the number of required diffusion steps by more than 35% on two benchmarks, significantly increasing the inference speed.",https://api.openreview.net/pdf/d217aca9d2b4ac1be080c492bb9d4939061d6de7.pdf,zero_few-shot;generative model;contrastive learning;inference;multimodal;diffusion models,https://scholar.google.com/scholar?q=Discrete+Contrastive+Diffusion+for+Cross-Modal+Music+and+Image+Generation
Modeling Multimodal Aleatoric Uncertainty in Segmentation with Mixture of Stochastic Experts,2023,ICLR,"['Zhitong Gao', 'Yucong Chen', 'Chuyu Zhang', 'Xuming He']",poster,"['Semantic Segmentation', 'Aleatoric Uncertainty', 'Stochastic Segmentation', 'Multiple Annotations']","Equipping predicted segmentation with calibrated uncertainty is essential for safety-critical applications. In this work, we focus on capturing the data-inherent uncertainty (aka aleatoric uncertainty) in segmentation, typically when ambiguities exist in input images. Due to the high-dimensional output space and potential multiple modes in segmenting ambiguous images, it remains challenging to predict well-calibrated uncertainty for segmentation. To tackle this problem, we propose a novel mixture of stochastic experts (MoSE) model, where each expert network estimates a distinct mode of the aleatoric uncertainty and a gating network predicts the probabilities of an input image being segmented in those modes. This yields an efficient two-level uncertainty representation. To learn the model, we develop a Wasserstein-like loss that directly minimizes the distribution distance between the MoSE and ground truth annotations. The loss can easily integrate traditional segmentation quality measures and be efficiently optimized via constraint relaxation. We validate our method on the LIDC-IDRI dataset and a modified multimodal Cityscapes dataset. Results demonstrate that our method achieves the state-of-the-art or competitive performance on all metrics.",https://api.openreview.net/pdf/a6592dd089cd504236b41eb25f4594b99c263305.pdf,graph;optimization;representation;metric;segmentation;multimodal,https://scholar.google.com/scholar?q=Modeling+Multimodal+Aleatoric+Uncertainty+in+Segmentation+with+Mixture+of+Stochastic+Experts
A General Rank Preserving Framework for Asymmetric Image Retrieval,2023,ICLR,"['Hui Wu', 'Min Wang', 'Wengang Zhou', 'Houqiang Li']",poster,['Asymmetric image retreival'],"Asymmetric image retrieval aims to deploy compatible models on platforms of different resources to achieve a balance between computational efficiency and retrieval accuracy. The most critical issue is how to align the output features of different models. Despite the great progress, existing approaches apply strong constraints so that features or neighbor structures are strictly aligned across different models. However, such a one-to-one constraint is too strict to be well preserved for the query models with low capacity. Considering that the primary concern of the users is the rank of the returned images, we propose a generic rank preserving framework, which achieves feature compatibility and the order consistency between query and gallery models simultaneously. Specifically, we propose two alternatives to instantiate the framework. One realizes straightforward rank order preservation by directly preserving the consistency of the sorting results. To make sorting process differentiable, the Heaviside step function in sorting is approximated by the sigmoid function. The other aims to preserve a learnable monotonic mapping relationship between the returned similarity scores of query and gallery models. The mapped similarity scores of gallery model are considered as pseudo-supervision to guide the query model training. Extensive experiments on various large-scale datasets demonstrate the superiority of our two proposed methods.",https://api.openreview.net/pdf/7c38e05c89cf2c2dc6799815ab82d96a7d0e445d.pdf,optimization;zero_few-shot;metric;multimodal,https://scholar.google.com/scholar?q=A+General+Rank+Preserving+Framework+for+Asymmetric+Image+Retrieval
Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval,2023,ICLR,"['Zhenghao Liu', 'Chenyan Xiong', 'Yuanhuiyi Lv', 'Zhiyuan Liu', 'Ge Yu']",poster,"['Multi-Modal Retrieval', 'Dense Retrieval', 'Universal Embedding Space', 'Modality-Balanced Hard Negative Training', 'Image Verbalization']","This paper presents Universal Vision-Language Dense Retrieval (UniVL-DR), which builds a unified model for multi-modal retrieval. UniVL-DR encodes queries and multi-modality resources in an embedding space for searching candidates from different modalities. To learn a unified embedding space for multi-modal retrieval, UniVL-DR proposes two techniques: 1) Universal embedding optimization strategy, which contrastively optimizes the embedding space using the modality-balanced hard negatives; 2) Image verbalization method, which bridges the modality gap between images and texts in the raw data space. UniVL-DR achieves the state-of-the-art on the multi-modal open-domain question answering benchmark, WebQA, and outperforms all retrieval models on the two subtasks, text-text retrieval and text-image retrieval. It demonstrates that universal multi-modal search is feasible to replace the divide-and-conquer pipeline with a united model and also benefits single/cross modality tasks. All source codes of this work are available at https://github.com/OpenMatch/UniVL-DR.",https://api.openreview.net/pdf/b896aabb513ac882a484a404912e8485dbb11cc8.pdf,optimization;representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Universal+Vision-Language+Dense+Retrieval:+Learning+A+Unified+Representation+Space+for+Multi-Modal+Retrieval
Learning to Jointly Share and Prune Weights for Grounding Based Vision and Language Models,2023,ICLR,"['Shangqian Gao', 'Burak Uzkent', 'Yilin Shen', 'Heng Huang', 'Hongxia Jin']",poster,[],"Transformers have seen growing interest in processing different modalities,  including language and image data. As a result, we can process vision and language data using transformers that are architecturally similar. Leveraging this feature of transformers, we propose weight sharing across two transformer backbones and within the same transformer backbone and pruning across two backbones in a unified framework. More specifically, we investigate weight sharing and pruning for two components of the transformers: (1) Multi-Head Attention (MSA) and (2) Feed-Forward Network (FFN) layers. To jointly perform weight sharing and pruning, we propose to use a regularization term to align model weights and the desired structure during the multimodal pre-training step. The structure vectors of sharing and pruning are generated by using a hypernetwork, which can capture complex interactions between pruning and sharing across layers and modalities. We train the hypernetwork and model weights iteratively so that the learned structure evolves along with model weights. After minimizing the proposed objective in the pre-training step, we perform weight sharing and pruning and fine-tune the compressed model on downstream tasks. Finally, we perform experiments on vision and language tasks, including Referring Expression Comprehension (REC), Visual Question Answering (VQA), and Object Detection using the state-of-the-art grounding based models: MDETR and GLIP. Our experiments show that we can compress these models by $35-40\%$ by sharing and pruning MSA and FFN weights without almost any loss in accuracy.",https://api.openreview.net/pdf/090eb3d3debddf7f3522e5122d1f1f190e4f4082.pdf,graph;transformer;multimodal,https://scholar.google.com/scholar?q=Learning+to+Jointly+Share+and+Prune+Weights+for+Grounding+Based+Vision+and+Language+Models
Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness,2023,ICLR,"['Joel Dapello', 'Kohitij Kar', 'Martin Schrimpf', 'Robert Baldwin Geary', 'Michael Ferguson', 'David Daniel Cox', 'James J. DiCarlo']",oral,"['Computer Vision', 'Primate Vision', 'Adversarial Robustness', 'Behavioral Alignment', 'Inferior Temporal Cortex']","While some state-of-the-art artificial neural network systems in computer vision are strikingly accurate models of the corresponding primate visual processing, there are still many discrepancies between these models and the behavior of primates on object recognition tasks. Many current models suffer from extreme sensitivity to adversarial attacks and often do not align well with the image-by-image behavioral error patterns observed in humans. Previous research has provided strong evidence that primate object recognition behavior can be very accurately predicted by neural population activity in the inferior temporal (IT) cortex, a brain area in the late stages of the visual processing hierarchy. Therefore, here we directly test whether making the late stage representations of models more similar to that of macaque IT produces new models that exhibit more robust, primate-like behavior. We conducted chronic, large-scale multi-electrode recordings  across the IT cortex in six non-human primates (rhesus macaques). We then use these data to fine-tune (end-to-end) the model ""IT"" representations such that they are more aligned with the biological IT representations, while preserving accuracy on object recognition tasks. We generate a cohort of models with a range of IT similarity scores validated on held-out animals across two image sets with distinct statistics. Across a battery of optimization conditions, we observed a strong correlation between the models' IT-likeness and alignment with human behavior, as well as an increase in its adversarial robustness. We further assessed the limitations of this approach and find that the improvements in behavioral alignment and adversarial robustness generalize across different image statistics, but not to object categories outside of those covered in our IT training set. Taken together, our results demonstrate that building models that are more aligned with the primate brain leads to more robust and human-like behavior, and call for larger neural data-sets to further augment these gains.",https://api.openreview.net/pdf/9c4c1940dba43cb5ad6502b7a23339d19d3a9a49.pdf,optimization;representation;multimodal;llm,https://scholar.google.com/scholar?q=Aligning+Model+and+Macaque+Inferior+Temporal+Cortex+Representations+Improves+Model-to-Human+Behavioral+Alignment+and+Adversarial+Robustness
PaLI: A Jointly-Scaled Multilingual Language-Image Model,2023,ICLR,"['Xi Chen', 'Xiao Wang', 'Soravit Changpinyo', 'AJ Piergiovanni', 'Piotr Padlewski', 'Daniel Salz', 'Sebastian Goodman', 'Adam Grycner', 'Basil Mustafa', 'Lucas Beyer', 'Alexander Kolesnikov', 'Joan Puigcerver', 'Nan Ding', 'Keran Rong', 'Hassan Akbari', 'Gaurav Mishra', 'Linting Xue', 'Ashish V Thapliyal', 'James Bradbury', 'Weicheng Kuo', 'Mojtaba Seyedhosseini', 'Chao Jia', 'Burcu Karagol Ayan', 'Carlos Riquelme Ruiz', 'Andreas Peter Steiner', 'Anelia Angelova', 'Xiaohua Zhai', 'Neil Houlsby', 'Radu Soricut']",oral,[],"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI, a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",https://api.openreview.net/pdf/1870a0455d0e7a6ed7d8f02e8e156cf63f5d6b6a.pdf,zero_few-shot;transformer;multimodal;llm,https://scholar.google.com/scholar?q=PaLI:+A+Jointly-Scaled+Multilingual+Language-Image+Model
Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness,2023,ICLR,"['Shuaichen Chang', 'Jun Wang', 'Mingwen Dong', 'Lin Pan', 'Henghui Zhu', 'Alexander Hanbo Li', 'Wuwei Lan', 'Sheng Zhang', 'Jiarong Jiang', 'Joseph Lilien', 'Steve Ash', 'William Yang Wang', 'Zhiguo Wang', 'Vittorio Castelli', 'Patrick Ng', 'Bing Xiang']",oral,[],"Neural text-to-SQL models have achieved remarkable performance in translating natural language questions into SQL queries. However, recent studies reveal that text-to-SQL models are vulnerable to task-specific perturbations. Previous curated robustness test sets usually focus on individual phenomena. In this paper, we propose a comprehensive robustness benchmark based on Spider, a cross-domain text-to-SQL benchmark, to diagnose the model robustness. We design 17 perturbations on databases, natural language questions, and SQL queries to measure the robustness from different angles. In order to collect more diversified natural question perturbations, we utilize large pretrained language models (PLMs) to simulate human behaviors in creating natural questions. We conduct a diagnostic study of the state-of-the-art models on the robustness set. Experimental results reveal that even the most robust model suffers from a 14.0% performance drop overall and a 50.7% performance drop on the most challenging perturbation. We also present a breakdown analysis regarding text-to-SQL model designs and provide insights for improving model robustness.",https://api.openreview.net/pdf/28dd8eb27d485f652c4874af1d995452557ae2b3.pdf,graph;multimodal,https://scholar.google.com/scholar?q=Dr.Spider:+A+Diagnostic+Evaluation+Benchmark+towards+Text-to-SQL+Robustness
From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data,2023,ICLR,"['Zichen Jeff Cui', 'Yibin Wang', 'Nur Muhammad Mahi Shafiullah', 'Lerrel Pinto']",oral,"['behavior generation', 'robot manipulation', 'learning from play']","While large-scale sequence modelling from offline data has led to impressive performance gains in natural language generation and image generation, directly translating such ideas to robotics has been challenging. One critical reason for this is that uncurated robot demonstration data, i.e. play data, collected from non-expert human demonstrators are often noisy, diverse, and distributionally multi-modal. This makes extracting useful, task-centric behaviors from such data a difficult generative modelling problem. In this work, we present Conditional Behavior Transformers (C-BeT), a method that combines the multi-modal generation ability of Behavior Transformer with future-conditioned goal specification. On a suite of simulated benchmark tasks, we find that C-BeT improves upon prior state-of-the-art work in learning from play data by an average of 45.7%. Further, we demonstrate for the first time that useful task-centric behaviors can be learned on a real-world robot purely from play data without any task labels or reward information. Robot videos are best viewed on our project website: play-to-policy.github.io",https://api.openreview.net/pdf/2ac61e4b87940fa144ced394ae19abce9e89a184.pdf,offline reinforcement learning;graph;transformer;generative model;multimodal,https://scholar.google.com/scholar?q=From+Play+to+Policy:+Conditional+Behavior+Generation+from+Uncurated+Robot+Data
The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation,2023,ICLR,"['Zihui Xue', 'Zhengqi Gao', 'Sucheng Ren', 'Hang Zhao']",oral,"['multimodal learning', 'knowledge distillation']","Crossmodal knowledge distillation (KD) extends traditional knowledge distillation to the area of multimodal learning and demonstrates great success in various applications. To achieve knowledge transfer across modalities, a pretrained network from one modality is adopted as the teacher to provide supervision signals to a student network learning from the other modality. In contrast to the empirical success reported in prior works, the working mechanism of crossmodal KD remains a mystery. In this paper, we present a thorough understanding of crossmodal KD. We begin by providing two failure cases and demonstrate that KD is not a universal cure in crossmodal knowledge transfer. We then present the modality Venn diagram to understand modality relationships and the modality focusing hypothesis revealing the decisive factor in the efficacy of crossmodal KD. Experimental results on 6 multimodal datasets help justify our hypothesis, diagnose failure cases, and point directions to improve crossmodal knowledge transfer in the future.",https://api.openreview.net/pdf/741eead42fe714d67fac001285243a76fd4ad259.pdf,graph;transfer learning;distillation;multimodal,https://scholar.google.com/scholar?q=The+Modality+Focusing+Hypothesis:+Towards+Understanding+Crossmodal+Knowledge+Distillation
Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?,2023,ICLR,"['Runpei Dong', 'Zekun Qi', 'Linfeng Zhang', 'Junbo Zhang', 'Jianjian Sun', 'Zheng Ge', 'Li Yi', 'Kaisheng Ma']",poster,"['Representation Learning', 'Cross-Modal Learning', '3D Point Clouds']","The success of deep learning heavily relies on large-scale data with comprehensive labels, which is more expensive and time-consuming to fetch in 3D compared to 2D images or natural languages. This promotes the potential of utilizing models pretrained with data more than 3D as teachers for cross-modal knowledge transferring. In this paper, we revisit masked modeling in a unified fashion of knowledge distillation, and we show that foundational Transformers pretrained with 2D images or natural languages can help self-supervised 3D representation learning through training Autoencoders as Cross-Modal Teachers (ACT). The pretrained Transformers are transferred as cross-modal 3D teachers using discrete variational autoencoding self-supervision, during which the Transformers are frozen with prompt tuning for better knowledge inheritance. The latent features encoded by the 3D teachers are used as the target of masked point modeling, wherein the dark knowledge is distilled to the 3D Transformer students as foundational geometry understanding. Our ACT pretrained 3D learner achieves state-of-the-art generalization capacity across various downstream benchmarks, e.g., 88.21% overall accuracy on ScanObjectNN. Codes have been released at https://github.com/RunpeiDong/ACT.",https://api.openreview.net/pdf/79fadd685bdeb6df98bf287dbb0824d4b1885bb9.pdf,zero_few-shot;transformer;representation;transfer learning;distillation;multimodal;3d;self-supervision;llm,https://scholar.google.com/scholar?q=Autoencoders+as+Cross-Modal+Teachers:+Can+Pretrained+2D+Image+Transformers+Help+3D+Representation+Learning?
StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training,2023,ICLR,"['Yuechen Yu', 'Yulin Li', 'Chengquan Zhang', 'Xiaoqiang Zhang', 'Zengyuan Guo', 'Xiameng Qin', 'Kun Yao', 'Junyu Han', 'Errui Ding', 'Jingdong Wang']",poster,[],"In this paper, we present StrucTexTv2, an effective document image pre-training framework, by performing masked visual-textual prediction. It consists of two self-supervised pre-training tasks: masked image modeling and masked language modeling, based on text region-level image masking. The proposed method randomly masks some image regions according to the bounding box coordinates of text words. The objectives of our pre-training tasks are reconstructing the pixels of masked image regions and the corresponding masked tokens simultaneously. Hence the pre-trained encoder can capture more textual semantics in comparison to the masked image modeling that usually predicts the masked image patches. Compared to the masked multi-modal modeling methods for document image understanding that rely on both the image and text modalities, StrucTexTv2 models image-only input and potentially deals with more application scenarios free from OCR pre-processing. Extensive experiments on mainstream benchmarks of document image understanding demonstrate the effectiveness of StrucTexTv2. It achieves competitive or even new state-of-the-art performance in various downstream tasks such as image classification, layout analysis, table structure recognition, document OCR, and information extraction under the end-to-end scenario.",https://api.openreview.net/pdf/ee5c222d76dee2155c9e5e5058f2ded043e2e3f5.pdf,zero_few-shot;multimodal,https://scholar.google.com/scholar?q=StrucTexTv2:+Masked+Visual-Textual+Prediction+for+Document+Image+Pre-training
Discovering Informative and Robust Positives for Video Domain Adaptation,2023,ICLR,"['Chang Liu', 'Kunpeng Li', 'Michael Stopa', 'Jun Amano', 'Yun Fu']",poster,"['Domain Adaptation', 'Video Recognition']","Unsupervised domain adaptation for video recognition is challenging where the domain shift includes both spatial variations and temporal dynamics. Previous works have focused on exploring contrastive learning for cross-domain alignment. However, limited variations in intra-domain positives, false cross-domain positives, and false negatives hinder contrastive learning from fulfilling intra-domain discrimination and cross-domain closeness. This paper presents a non-contrastive learning framework without relying on negative samples for unsupervised video domain adaptation. To address the limited variations in intra-domain positives, we set unlabeled target videos as anchors and explored to mine ""informative intra-domain positives"" in the form of spatial/temporal augmentations and target nearest neighbors (NNs).
To tackle the false cross-domain positives led by noisy pseudo-labels, we reversely set source videos as anchors and sample the synthesized target videos as ""robust cross-domain positives"" from an estimated target distribution, which are naturally more robust to the pseudo-label noise. Our approach is demonstrated to be superior to state-of-the-art methods through extensive experiments on several cross-domain action recognition benchmarks.
",https://api.openreview.net/pdf/a84f553396a5cb7380355eabf385194e1ab0612a.pdf,graph;contrastive learning;augmentation;multimodal,https://scholar.google.com/scholar?q=Discovering+Informative+and+Robust+Positives+for+Video+Domain+Adaptation
Composing Ensembles of Pre-trained Models via Iterative Consensus,2023,ICLR,"['Shuang Li', 'Yilun Du', 'Joshua B. Tenenbaum', 'Antonio Torralba', 'Igor Mordatch']",poster,"['composing pre-trained models', 'zero-shot', 'multimodal', 'wisdom of the crowds']","Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information, while vision models such as DALL-E can generate photorealistic photos but fail to understand complex language descriptions. In this work, we propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as ""generators"" or ""scorers"" and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5%, without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation. 
",https://api.openreview.net/pdf/3a0f22bd7748e3a5e79b5973a182c5777b2aab02.pdf,graph;optimization;zero_few-shot;generative model;multimodal,https://scholar.google.com/scholar?q=Composing+Ensembles+of+Pre-trained+Models+via+Iterative+Consensus
Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning,2023,ICLR,"['Pan Lu', 'Liang Qiu', 'Kai-Wei Chang', 'Ying Nian Wu', 'Song-Chun Zhu', 'Tanmay Rajpurohit', 'Peter Clark', 'Ashwin Kalyan']",poster,"['Mathematical Reasoning', 'Tabular Math Word Problems', 'Prompt Learning', 'Policy Gradient']","Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples. The data and code are available at https://promptpg.github.io.",https://api.openreview.net/pdf/1f2f51f57875ec48e1bb27c936aa39ee2e65d06e.pdf,metric;multimodal;llm,https://scholar.google.com/scholar?q=Dynamic+Prompt+Learning+via+Policy+Gradient+for+Semi-structured+Mathematical+Reasoning
TempCLR: Temporal Alignment Representation with Contrastive Learning,2023,ICLR,"['Yuncong Yang', 'Jiawei Ma', 'Shiyuan Huang', 'Long Chen', 'Xudong Lin', 'Guangxing Han', 'Shih-Fu Chang']",poster,"['Representation learning', 'Global Sequence Alignment', 'Zero/Few-shot Transfer']","Video representation learning has been successful in video-text pre-training for zero-shot transfer, where each sentence is trained to be close to the paired video clips in a common feature space. For long videos, given a paragraph of description where the sentences describe different segments of the video, by matching all sentence-clip pairs,  the paragraph and the full video are aligned implicitly. However, such unit-level similarity measure may ignore the global temporal context over a long time span, which inevitably limits the generalization ability. In this paper, we propose a contrastive learning framework TempCLR to compare the full video and the paragraph explicitly. As the video/paragraph is formulated as a sequence of clips/sentences, under the constraint of their temporal order, we use dynamic time warping to compute the minimum cumulative cost over sentence-clip pairs as the sequence-level distance. To explore the temporal dynamics, we break the consistency of temporal order by shuffling the video clips or sentences according to the temporal granularity. In this way, we obtain the representations for clips/sentences, which perceive the temporal information and thus facilitate the sequence alignment. In addition to pre-training on the video and paragraph, our approach can also generalize on the matching between different video instances. We evaluate our approach on video retrieval, action step localization, and few-shot action recognition, and achieve consistent performance gain over all three tasks. Detailed ablation studies are provided to justify the approach design. ",https://api.openreview.net/pdf/f4df84ba55e6a74dd51327e33ccbe729ed2f166c.pdf,graph;optimization;zero_few-shot;representation;contrastive learning;transfer learning;multimodal;llm,https://scholar.google.com/scholar?q=TempCLR:+Temporal+Alignment+Representation+with+Contrastive+Learning
 Cycle-consistent Masked AutoEncoder for Unsupervised Domain Generalization,2023,ICLR,"['Haiyang Yang', 'Xiaotong Li', 'SHIXIANG TANG', 'Feng Zhu', 'Yizhou Wang', 'Meilin Chen', 'LEI BAI', 'Rui Zhao', 'Wanli Ouyang']",poster,[],"Self-supervised learning methods undergo undesirable performance drops when there exists a significant domain gap between training and testing scenarios. Therefore, unsupervised domain generalization (UDG) is proposed to tackle the problem, which requires the model to be trained on several different domains without supervision and generalize well on unseen test domains. Existing methods either rely on a cross-domain and semantically consistent image pair in contrastive methods or the reconstruction pair in generative methods, while the precious image pairs are not available without semantic labels. In this paper, we propose a cycle cross-domain reconstruction task for unsupervised domain generalization in the absence of paired images. The cycle cross-domain reconstruction task converts a masked image from one domain to another domain and then reconstructs the original image from the converted images. To preserve the divergent domain knowledge of decoders in the cycle reconstruction task, we propose a novel domain-contrastive loss to regularize the domain information in reconstructed images encoded with the desirable domain style. Qualitative results on extensive datasets illustrate our method improves the state-of-the-art unsupervised domain generalization methods by average $\textbf{+5.59\%}, \textbf{+4.52\%}, \textbf{+4.22\%}, \textbf{+7.02\%}$ on $1\%, 5\%, 10\%, 100\%$ PACS, and $\textbf{+5.08\%}, \textbf{+6.49\%}, \textbf{+1.79\%}, \textbf{+0.53\%}$ on $1\%, 5\%, 10\%, 100\%$ DomainNet, respectively.",https://api.openreview.net/pdf/80923f306be26ea8c160f03fb85027b691b34dbb.pdf,graph;zero_few-shot;generative model;contrastive learning;multimodal,https://scholar.google.com/scholar?q=+Cycle-consistent+Masked+AutoEncoder+for+Unsupervised+Domain+Generalization
Multimodal Analogical Reasoning over Knowledge Graphs,2023,ICLR,"['Ningyu Zhang', 'Lei Li', 'Xiang Chen', 'Xiaozhuan Liang', 'Shumin Deng', 'Huajun Chen']",poster,"['knowledge graph', 'multimodal', 'analogical reasoning', 'prompt learning', 'pre-trained language model']","Analogical reasoning is fundamental to human cognition and holds an important place in various fields. However, previous studies mainly focus on single-modal analogical reasoning and ignore taking advantage of structure knowledge. Notably, the research in cognitive psychology has demonstrated that information from multimodal sources always brings more powerful cognitive transfer than single modality sources. To this end, we introduce the new task of multimodal analogical reasoning over knowledge graphs, which requires multimodal reasoning ability with the help of background knowledge. Specifically, we construct a Multimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph MarKG. We evaluate with multimodal knowledge graph embedding and pre-trained Transformer baselines, illustrating the potential challenges of the proposed task. We further propose a novel model-agnostic Multimodal analogical reasoning framework with Transformer (MarT) motivated by the structure mapping theory, which can obtain better performance. We hope our work can deliver benefits and inspire future research. Code and datasets are available in https://github.com/zjunlp/MKG_Analogy.",https://api.openreview.net/pdf/0932fa51d71959373e6ffd7a76954ac870fb458c.pdf,graph;transformer;transfer learning;multimodal,https://scholar.google.com/scholar?q=Multimodal+Analogical+Reasoning+over+Knowledge+Graphs
Deja Vu: Continual Model Generalization for Unseen Domains,2023,ICLR,"['Chenxi Liu', 'Lixu Wang', 'Lingjuan Lyu', 'Chen Sun', 'Xiao Wang', 'Qi Zhu']",poster,"['Domain Generalization', 'Domain Adaptation']","In real-world applications, deep learning models often run in non-stationary environments where the target data distribution continually shifts over time. There have been numerous domain adaptation (DA) methods in both online and offline modes to improve cross-domain adaptation ability. However, these DA methods typically only provide good performance after a long period of adaptation, and perform poorly on new domains before and during adaptation – in what we call the “Unfamiliar Period”, especially when domain shifts happen suddenly and significantly. On the other hand, domain generalization (DG) methods have been proposed to improve the model generalization ability on unadapted domains. However, existing DG works are ineffective for continually changing domains due to severe catastrophic forgetting of learned knowledge. To overcome these limitations of DA and DG in handling the Unfamiliar Period during continual domain shift, we propose RaTP, a framework that focuses on improving models’ target domain generalization (TDG) capability, while also achieving effective target domain adaptation (TDA) capability right after training on certain domains and forgetting alleviation (FA) capability on past domains. RaTP includes a training-free data augmentation module to prepare data for TDG, a novel pseudo-labeling mechanism to provide reliable supervision for TDA, and a prototype contrastive alignment algorithm to align different domains for achieving TDG, TDA and FA. Extensive experiments on Digits, PACS, and DomainNet demonstrate that RaTP significantly outperforms state-of-the-art works from Continual DA, Source-Free DA, Test-Time/Online DA, Single DG, Multiple DG and Unified DA&DG in TDG, and achieves comparable TDA and FA capabilities.",https://api.openreview.net/pdf/308a0282513449f22438eeb7ca60ec1b03b7f924.pdf,offline reinforcement learning;graph;online learning;contrastive learning;augmentation;multimodal,https://scholar.google.com/scholar?q=Deja+Vu:+Continual+Model+Generalization+for+Unseen+Domains
SQA3D: Situated Question Answering in 3D Scenes,2023,ICLR,"['Xiaojian Ma', 'Silong Yong', 'Zilong Zheng', 'Qing Li', 'Yitao Liang', 'Song-Chun Zhu', 'Siyuan Huang']",poster,"['3D vision', 'scene understanding', 'visual question answering', 'embodied AI']","We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capability.",https://api.openreview.net/pdf/7c0e4e7d24c7883389f21b015135c6bfd4f50b62.pdf,reinforcement learning;graph;multimodal;3d,https://scholar.google.com/scholar?q=SQA3D:+Situated+Question+Answering+in+3D+Scenes
Rethinking skip connection model as a learnable Markov chain,2023,ICLR,"['Chen Dengsheng', 'Jie Hu', 'Wenwen Qiang', 'Xiaoming Wei', 'Enhua Wu']",poster,"['Language translation', 'image classification', 'transformer']","Over the past few years afterward the birth of ResNet, skip connection has become the defacto standard for the design of modern architectures due to its widespread adoption, easy optimization, and proven performance.
Prior work has explained the effectiveness of the skip connection mechanism from different perspectives.
In this work, we deep dive into the model's behaviors with skip connections which can be formulated as a learnable Markov chain.
An efficient Markov chain is preferred as it always maps the input data to the target domain in a better way.
However, while a model is explained as a Markov chain, it is not guaranteed to be optimized following an efficient Markov chain by existing SGD-based optimizers prone to getting trapped in local optimal points.
In order to move towards a more efficient Markov chain, we propose a simple routine of penal connection to make any residual-like model become a learnable Markov chain.
Aside from that, the penal connection can also be viewed as a particular model regularization and can be easily implemented with one line of code in the most popular deep learning frameworks. 
The encouraging experimental results in multi-modal translation and image recognition empirically confirm our conjecture of the learnable Markov chain view and demonstrate the superiority of the proposed penal connection.",https://api.openreview.net/pdf/2ffda559a1197d97213255e139462d4270bfe2cf.pdf,graph;optimization;zero_few-shot;multimodal,https://scholar.google.com/scholar?q=Rethinking+skip+connection+model+as+a+learnable+Markov+chain
Neural Groundplans: Persistent Neural Scene Representations from a Single Image,2023,ICLR,"['Prafull Sharma', 'Ayush Tewari', 'Yilun Du', 'Sergey Zakharov', 'Rares Andrei Ambrus', 'Adrien Gaidon', 'William T. Freeman', 'Fredo Durand', 'Joshua B. Tenenbaum', 'Vincent Sitzmann']",poster,"['Neural scene representations', '3D', 'nerf', 'scene understanding', 'neural rendering', 'object-centric representations']","We present a method to map 2D image observations of a scene to a persistent 3D scene representation, enabling novel view synthesis and disentangled representation of the movable and immovable components of the scene. Motivated by the bird’s-eye-view (BEV) representation commonly used in vision and robotics, we propose conditional neural groundplans, ground-aligned 2D feature grids, as persistent and memory-efficient scene representations. Our method is trained self-supervised from unlabeled multi-view observations using differentiable rendering, and learns to complete geometry and appearance of occluded regions. In addition, we show that we can leverage multi-view videos at training time to learn to separately reconstruct static and movable components of the scene from a single image at test time. The ability to separately reconstruct movable objects enables a variety of downstream tasks using simple heuristics, such as extraction of object-centric 3D representations, novel view synthesis, instance-level segmentation, 3D bounding box prediction, and scene editing. This highlights the value of neural groundplans as a backbone for efficient 3D scene understanding models.",https://api.openreview.net/pdf/2e877547b8a62b0459ece4c3c0c9fff256045e77.pdf,zero_few-shot;representation;segmentation;multimodal;multi-view;3d,https://scholar.google.com/scholar?q=Neural+Groundplans:+Persistent+Neural+Scene+Representations+from+a+Single+Image
GFlowNets and variational inference,2023,ICLR,"['Nikolay Malkin', 'Salem Lahlou', 'Tristan Deleu', 'Xu Ji', 'Edward J Hu', 'Katie E Everett', 'Dinghuai Zhang', 'Yoshua Bengio']",poster,"['variational inference', 'GFlowNets', 'probabilistic modeling', 'weighted importance sampling']","This paper builds bridges between two families of probabilistic algorithms: (hierarchical) variational inference (VI), which is typically used to model distributions over continuous spaces, and generative flow networks (GFlowNets), which have been used for distributions over discrete structures such as graphs. We demonstrate that, in certain cases, VI algorithms are equivalent to special cases of GFlowNets in the sense of equality of expected gradients of their learning objectives. We then point out the differences between the two families and show how these differences emerge experimentally. Notably, GFlowNets, which borrow ideas from reinforcement learning, are more amenable than VI to off-policy training without the cost of high gradient variance induced by importance sampling. We argue that this property of GFlowNets can provide advantages for capturing diversity in multimodal target distributions. Code: https://github.com/GFNOrg/GFN_vs_HVI.",https://api.openreview.net/pdf/7fcf83ff3797f23ba25909993d4d7281d23ca332.pdf,reinforcement learning;graph;zero_few-shot;generative model;inference;flow;multimodal,https://scholar.google.com/scholar?q=GFlowNets+and+variational+inference
Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot Classification,2023,ICLR,"['Hao ZHENG', 'Runqi Wang', 'Jianzhuang Liu', 'Asako Kanezaki']",poster,"['cross-domain few-shot classification', 'cross-level distillation', 'feature denoising']","The conventional few-shot classification aims at learning a model on a large labeled base dataset and rapidly adapting to a target dataset that is from the same distribution as the base dataset. However, in practice, the base and the target datasets of few-shot classification are usually from different domains, which is the problem of cross-domain few-shot classification. We tackle this problem by making a small proportion of unlabeled images in the target domain accessible in the training stage. In this setup, even though the base data are sufficient and labeled, the large domain shift still makes transferring the knowledge from the base dataset difficult. We meticulously design a cross-level knowledge distillation method, which can strengthen the ability of the model to extract more discriminative features in the target dataset by guiding the network's shallow layers to learn higher-level information. Furthermore, in order to alleviate the overfitting in the evaluation stage, we propose a feature denoising operation which can reduce the feature redundancy and mitigate overfitting. Our approach can surpass the previous state-of-the-art method, Dynamic-Distillation, by 5.44% on 1-shot and 1.37% on 5-shot classification tasks on average in the BSCD-FSL benchmark. The implementation code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/CLDFD.",https://api.openreview.net/pdf/cc48f6aebf1eb9429f8570a25b548a0e9c3e85a8.pdf,zero_few-shot;transfer learning;distillation;multimodal,https://scholar.google.com/scholar?q=Cross-Level+Distillation+and+Feature+Denoising+for+Cross-Domain+Few-Shot+Classification
kNN-Diffusion: Image Generation via Large-Scale Retrieval,2023,ICLR,"['Shelly Sheynin', 'Oron Ashual', 'Adam Polyak', 'Uriel Singer', 'Oran Gafni', 'Eliya Nachmani', 'Yaniv Taigman']",poster,[],"Recent text-to-image models have achieved impressive results. However, since they require large-scale datasets of text-image pairs, it is impractical to train them on new domains where data is scarce or not labeled.
In this work, we propose using large-scale retrieval methods, in particular, efficient k-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a substantially small and efficient text-to-image diffusion model using only pre-trained multi-modal embeddings, but without an explicit text-image dataset, (2) generating out-of-distribution images by simply swapping the retrieval database at inference time, and (3) performing text-driven local semantic manipulations while preserving object identity. To demonstrate the robustness of our method, we apply our kNN approach on two state-of-the-art diffusion backbones, and show results on several different datasets. As evaluated by human studies and automatic metrics, our method achieves state-of-the-art results compared to existing approaches that train text-to-image generation models using images-only dataset.",https://api.openreview.net/pdf/b8975b72f550b7061f9e42248a7f29257a2c71fc.pdf,generative model;inference;metric;multimodal;diffusion models,https://scholar.google.com/scholar?q=kNN-Diffusion:+Image+Generation+via+Large-Scale+Retrieval
Diagnosing and Rectifying Vision Models using Language,2023,ICLR,"['Yuhui Zhang', 'Jeff Z. HaoChen', 'Shih-Cheng Huang', 'Kuan-Chieh Wang', 'James Zou', 'Serena Yeung']",poster,"['model diagnosis', 'multi-modal contrastive learning', 'vision and language']","Recent multi-modal contrastive learning models have demonstrated the ability to learn an embedding space suitable for building strong vision classifiers, by leveraging the rich information in large-scale image-caption datasets. Our work highlights a distinct advantage of this multi-modal embedding space: the ability to diagnose vision classifiers through natural language. The traditional process of diagnosing model behaviors in deployment settings involves labor-intensive data acquisition and annotation. Our proposed method can discover high-error data slices, identify influential attributes and further rectify undesirable model behaviors, without requiring any visual data. Through a combination of theoretical explanation and empirical verification, we present conditions under which classifiers trained on embeddings from one modality can be equivalently applied to embeddings from another modality. On a range of image datasets with known error slices, we demonstrate that our method can effectively identify the error slices and influential attributes, and can further use language to rectify failure modes of the classifier.",https://api.openreview.net/pdf/abc659a15fb9f9adb64485d7e5837a76f8f5f216.pdf,graph;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Diagnosing+and+Rectifying+Vision+Models+using+Language
BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection,2023,ICLR,"['Zehui Chen', 'Zhenyu Li', 'Shiquan Zhang', 'Liangji Fang', 'Qinhong Jiang', 'Feng Zhao']",poster,"['object detection', '3d detection', 'BEV perception']","3D object detection from multiple image views is a fundamental and challenging task for visual scene understanding. Owing to its low cost and high efficiency, multi-view 3D object detection has demonstrated promising application prospects. However, accurately detecting objects through perspective views is extremely difficult due to the lack of depth information. Current approaches tend to adopt heavy backbones for image encoders, making them inapplicable for real-world deployment. Different from the images, LiDAR points are superior in providing spatial cues, resulting in highly precise localization. In this paper, we explore the incorporation of LiDAR-based detectors for multi-view 3D object detection. Instead of directly training a depth prediction network, we unify the image and LiDAR features in the Bird-Eye-View (BEV) space and adaptively transfer knowledge across non-homogenous representations in a teacher-student paradigm. To this end, we propose BEVDistill, a cross-modal BEV knowledge distillation (KD) framework for multi-view 3D object detection. 
Extensive experiments demonstrate that the proposed method outperforms current KD approaches on a highly-competitive baseline, BEVFormer, without introducing any extra cost in the inference phase. Notably, our best model achieves 59.4 NDS on the nuScenes test leaderboard, achieving new state-of-the-arts in comparison with various image-based detectors. Code will be available at https://github.com/zehuichen123/BEVDistill.",https://api.openreview.net/pdf/675b1bf3a47333e985a80f97d69ed4203c3f3ba1.pdf,graph;zero_few-shot;representation;adaptive;inference;transfer learning;distillation;multimodal;multi-view;3d,https://scholar.google.com/scholar?q=BEVDistill:+Cross-Modal+BEV+Distillation+for+Multi-View+3D+Object+Detection
Unsupervised Meta-learning via Few-shot Pseudo-supervised Contrastive Learning,2023,ICLR,"['Huiwon Jang', 'Hankook Lee', 'Jinwoo Shin']",spotlight,"['unsupervised meta-learning', 'supervised contrastive learning', 'self-supervised learning']","Unsupervised meta-learning aims to learn generalizable knowledge across a distribution of tasks constructed from unlabeled data. Here, the main challenge is how to construct diverse tasks for meta-learning without label information; recent works have proposed to create, e.g., pseudo-labeling via pretrained representations or creating synthetic samples via generative models. However, such a task construction strategy is fundamentally limited due to heavy reliance on the immutable pseudo-labels during meta-learning and the quality of the representations or the generated samples. To overcome the limitations, we propose a simple yet effective unsupervised meta-learning framework, coined Pseudo-supervised Contrast (PsCo), for few-shot classification. We are inspired by the recent self-supervised learning literature; PsCo utilizes a momentum network and a queue of previous batches to improve pseudo-labeling and construct diverse tasks in a progressive manner. Our extensive experiments demonstrate that PsCo outperforms existing unsupervised meta-learning methods under various in-domain and cross-domain few-shot classification benchmarks. We also validate that PsCo is easily scalable to a large-scale benchmark, while recent prior-art meta-schemes are not.",https://api.openreview.net/pdf/65c63b72201856c7d08ce81fba8f12b50947aa77.pdf,zero_few-shot;representation;generative model;contrastive learning;meta-learning;multimodal,https://scholar.google.com/scholar?q=Unsupervised+Meta-learning+via+Few-shot+Pseudo-supervised+Contrastive+Learning
Contrastive Audio-Visual Masked Autoencoder,2023,ICLR,"['Yuan Gong', 'Andrew Rouditchenko', 'Alexander H. Liu', 'David Harwath', 'Leonid Karlinsky', 'Hilde Kuehne', 'James R. Glass']",spotlight,"['multi-modal learning', 'audio-visual learning', 'self-supervised learning', 'masked autoencoder', 'contrastive learning']","In this paper, we first extend the recent Masked Auto-Encoder (MAE) model from a single modality to audio-visual multi-modalities. Subsequently, we propose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining contrastive learning and masked data modeling, two major self-supervised learning frameworks, to learn a joint and coordinated audio-visual representation.
Our experiments show that the contrastive audio-visual correspondence learning objective not only enables the model to perform audio-visual retrieval tasks, but also helps the model learn a better joint representation. As a result, our fully self-supervised pretrained CAV-MAE achieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with the previous best supervised pretrained model on AudioSet in the audio-visual event classification task. Code and pretrained models are at https://github.com/yuangongnd/cav-mae.",https://api.openreview.net/pdf/6de0262994e10ffd87b06b9ad0e8b4f86c84f044.pdf,zero_few-shot;representation;contrastive learning;multimodal,https://scholar.google.com/scholar?q=Contrastive+Audio-Visual+Masked+Autoencoder
Using Language to Extend to Unseen Domains,2023,ICLR,"['Lisa Dunlap', 'Clara Mohri', 'Devin Guillory', 'Han Zhang', 'Trevor Darrell', 'Joseph E. Gonzalez', 'Aditi Raghunathan', 'Anna Rohrbach']",spotlight,"['vision and language', 'robust training', 'domain adaptation']","It is expensive to collect training data for every possible domain that a vision model may encounter when deployed. We instead consider how simply $\textit{verbalizing}$ the training domain (e.g.``photos of birds'') as well as domains we want to extend to but do not have data for (e.g.``paintings of birds'') can improve robustness. Using a multimodal model with a joint image and language embedding space, our method $\textit{LADS}$ learns a transformation of the image embeddings from the source domain to each target domain, while preserving task relevant information. Without using any images from the target domain, we show that over the $\textit{extended}$ domain containing both source and target, $\textit{LADS}$ outperforms standard fine-tuning and ensemble approaches over a suite of 4 benchmarks targeting domain adaptation and dataset bias.",https://api.openreview.net/pdf/bb5314efba6d37a2ea4f8cdbdeccd9351dde3016.pdf,multimodal,https://scholar.google.com/scholar?q=Using+Language+to+Extend+to+Unseen+Domains
A probabilistic framework for task-aligned intra- and inter-area neural manifold estimation,2023,ICLR,"['Edoardo Balzani', 'Jean-Paul G Noel', 'Pedro Herrero-Vidal', 'Dora E Angelaki', 'Cristina Savin']",spotlight,"['neuroscience', 'dimensionality reduction', 'probabilistic methods', 'inter-area interactions']","Latent manifolds provide a compact characterization of neural population activity and of shared co-variability across brain areas. Nonetheless, existing statistical tools for extracting neural manifolds face limitations in terms of interpretability of latents with respect to task variables, and can be hard to apply to datasets with no trial repeats. Here we propose a novel probabilistic framework that allows for interpretable partitioning of population variability within and across areas in the context of naturalistic behavior. Our approach for task aligned manifold estimation (TAME-GP) explicitly partitions variability into private and shared sources which can themselves be subdivided in task-relevant and task irrelevant components, uses a realistic Poisson noise model, and introduces temporal smoothing of latent trajectories in the form of a Gaussian Process prior. This TAME-GP graphical model allows for robust estimation of task-relevant variability in local population responses, and of shared co-variability between brain areas. We demonstrate the efficiency of our estimator on within model and biologically motivated simulated data. We also apply it to several datasets of neural population recordings during behavior. Overall, our results demonstrate the capacity of TAME-GP to capture meaningful intra- and inter-area neural variability with single trial resolution.",https://api.openreview.net/pdf/9e46e84807837851d4357f969ea06aa885cf4f5a.pdf,graph;zero_few-shot;multimodal;llm,https://scholar.google.com/scholar?q=A+probabilistic+framework+for+task-aligned+intra-+and+inter-area+neural+manifold+estimation
CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks,2023,ICLR,"['Tuomas Oikarinen', 'Tsui-Wei Weng']",spotlight,"['Interpretability', 'Explainability', 'Network Dissection']","In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10$\times$ faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect.",https://api.openreview.net/pdf/a302e0072a6e15c8c0361c022bb9d3518f1a7127.pdf,representation;multimodal,https://scholar.google.com/scholar?q=CLIP-Dissect:+Automatic+Description+of+Neuron+Representations+in+Deep+Vision+Networks
Post-hoc Concept Bottleneck Models,2023,ICLR,"['Mert Yuksekgonul', 'Maggie Wang', 'James Zou']",spotlight,"['concepts', 'interpretability', 'concept bottleneck models', 'model editing']","Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (``the bottleneck'') and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model ""sees"" in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via concept-level feedback can provide significant performance gains without using data from the target domain or model retraining.",https://api.openreview.net/pdf/bd9522b16fb6b3a1e89ec20c6aa411c7a84f0fb3.pdf,zero_few-shot;transfer learning;multimodal,https://scholar.google.com/scholar?q=Post-hoc+Concept+Bottleneck+Models
Few-shot Cross-domain Image Generation via Inference-time Latent-code Learning,2023,ICLR,"['Arnab Kumar Mondal', 'Piyush Tiwary', 'Parag Singla', 'Prathosh AP']",spotlight,"['generative domain adaptation', 'generative adversarial network']","In this work, our objective is to adapt a Deep generative model trained on a large-scale source dataset to multiple target domains with scarce data. Specifically, we focus on adapting a pre-trained Generative Adversarial Network (GAN) to a target domain without re-training the generator. Our method draws the motivation from the fact that out-of-distribution samples can be `embedded' onto the latent space of a pre-trained source-GAN. We propose to train a small latent-generation network during the inference stage, each time a  batch of target samples is to be generated. These target latent codes are fed to the source-generator to obtain  novel target samples. Despite using the same small set of target samples and the source generator, multiple independent training episodes of the latent-generation network results in the diversity of the generated target samples. Our method, albeit simple, can be used to generate data from multiple target distributions using a generator trained on a single source distribution. We demonstrate the efficacy of our surprisingly simple method in generating multiple target datasets with only a single source generator and a few target samples.",https://api.openreview.net/pdf/acc5eab2f3488d4a16e1e9bdc1b8836b5ebccdfe.pdf,generative model;inference;multimodal,https://scholar.google.com/scholar?q=Few-shot+Cross-domain+Image+Generation+via+Inference-time+Latent-code+Learning
"UNIFIED-IO: A Unified Model for Vision, Language, and Multi-modal Tasks",2023,ICLR,"['Jiasen Lu', 'Christopher Clark', 'Rowan Zellers', 'Roozbeh Mottaghi', 'Aniruddha Kembhavi']",spotlight,[],"We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and pre-trained models will be made publicly available.",https://api.openreview.net/pdf/4f576a5041215d0298e9540a8c23041533da1724.pdf,zero_few-shot;transformer;representation;generative model;multimodal,"https://scholar.google.com/scholar?q=UNIFIED-IO:+A+Unified+Model+for+Vision,+Language,+and+Multi-modal+Tasks"
Flow Annealed Importance Sampling Bootstrap,2023,ICLR,"['Laurence Illing Midgley', 'Vincent Stimper', 'Gregor N. C. Simm', 'Bernhard Schölkopf', 'José Miguel Hernández-Lobato']",spotlight,"['Normalizing flow', 'Boltzmann distribution', 'Boltzmann generator', 'Annealed Importance Sampling', 'Approximate Inference']","Normalizing flows are tractable density models that can approximate complicated target distributions, e.g. Boltzmann distributions of physical systems. However, current methods for training flows either suffer from mode-seeking behavior, use samples from the target generated beforehand by expensive MCMC methods, or use stochastic losses that have high variance. To avoid these problems, we augment flows with annealed importance sampling (AIS) and minimize the mass-covering $\alpha$-divergence with $\alpha=2$, which minimizes importance weight variance. Our method, Flow AIS Bootstrap (FAB), uses AIS to generate samples in regions where the flow is a poor approximation of the target, facilitating the discovery of new modes. We apply FAB to multimodal targets and show that we can approximate them very accurately where previous methods fail. To the best of our knowledge, we are the first to learn the Boltzmann distribution of the alanine dipeptide molecule using only the unnormalized target density, without access to samples generated via Molecular Dynamics (MD) simulations: FAB produces better results than training via maximum likelihood on MD samples while using 100 times fewer target evaluations. After reweighting the samples, we obtain unbiased histograms of dihedral angles that are almost identical to the ground truth.",https://api.openreview.net/pdf/b982ed337b6c3ff43fb3fa4e63f9492b31f03e06.pdf,zero_few-shot;flow;multimodal,https://scholar.google.com/scholar?q=Flow+Annealed+Importance+Sampling+Bootstrap
A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning,2023,ICLR,"['Da-Wei Zhou', 'Qi-Wei Wang', 'Han-Jia Ye', 'De-Chuan Zhan']",spotlight,['class-incremental learning'],"Real-world applications require the classification model to adapt to new classes without forgetting old ones. Correspondingly, Class-Incremental Learning (CIL) aims to train a model with limited memory size to meet this requirement. Typical CIL methods tend to save representative exemplars from former classes to resist forgetting, while recent works find that storing models from history can substantially boost the performance. However, the stored models are not counted into the memory budget, which implicitly results in unfair comparisons. We find that when counting the model size into the total budget and comparing methods with aligned memory size, saving models do not consistently work, especially for the case with limited memory budgets. As a result, we need to holistically evaluate different CIL methods at different memory scales and simultaneously consider accuracy and memory size for measurement. On the other hand, we dive deeply into the construction of the memory buffer for memory efficiency. By analyzing the effect of different layers in the network, we find that shallow and deep layers have different characteristics in CIL. Motivated by this, we propose a simple yet effective baseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends specialized layers based on the shared generalized representations, efficiently extracting diverse representations with modest cost and maintaining representative exemplars. Extensive experiments on benchmark datasets validate MEMO's competitive performance. Code is available at: https://github.com/wangkiw/ICLR23-MEMO",https://api.openreview.net/pdf/1b652f4ee2aba1681f8d0e268557ff8ce743d37d.pdf,zero_few-shot;representation;multimodal,https://scholar.google.com/scholar?q=A+Model+or+603+Exemplars:+Towards+Memory-Efficient+Class-Incremental+Learning
Dirichlet-based Uncertainty Calibration for Active Domain Adaptation,2023,ICLR,"['Mixue Xie', 'Shuang Li', 'Rui Zhang', 'Chi Harold Liu']",spotlight,"['domain adaptation', 'active learning', 'uncertainty', 'Dirichlet']","Active domain adaptation (DA) aims to maximally boost the model adaptation on a new target domain by actively selecting limited target data to annotate, whereas traditional active learning methods may be less effective since they do not consider the domain shift issue. Despite active DA methods address this by further proposing targetness to measure the representativeness of target domain characteristics, their predictive uncertainty is usually based on the prediction of deterministic models, which can easily be miscalibrated on data with distribution shift. Considering this, we propose a Dirichlet-based Uncertainty Calibration (DUC) approach for active DA, which simultaneously achieves the mitigation of miscalibration and the selection of informative target samples. Specifically, we place a Dirichlet prior on the prediction and interpret the prediction as a distribution on the probability simplex, rather than a point estimate like deterministic models. This manner enables us to consider all possible predictions, mitigating the miscalibration of unilateral prediction. Then a two-round selection strategy based on different uncertainty origins is designed to select target samples that are both representative of target domain and conducive to discriminability. Extensive experiments on cross-domain image classification and semantic segmentation validate the superiority of DUC.",https://api.openreview.net/pdf/d263b9c4283973a09247e2e1effd05f8d9bd7652.pdf,graph;active learning;segmentation;multimodal,https://scholar.google.com/scholar?q=Dirichlet-based+Uncertainty+Calibration+for+Active+Domain+Adaptation
Vision Transformer Adapter for Dense Predictions,2023,ICLR,"['Zhe Chen', 'Yuchen Duan', 'Wenhai Wang', 'Junjun He', 'Tong Lu', 'Jifeng Dai', 'Yu Qiao']",spotlight,"['Plain Vision Transformer', 'Adapter', 'Dense Prediction']","This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer (ViT). Unlike recently advanced variants that incorporate vision-specific inductive biases into their architectures, the plain ViT suffers inferior performance on dense predictions due to weak prior assumptions. To address this issue, we propose the ViT-Adapter, which allows plain ViT to achieve comparable performance to vision-specific transformers. Specifically, the backbone in our framework is a plain ViT that can learn powerful representations from large-scale multi-modal data. When transferring to downstream tasks, a pre-training-free adapter is used to introduce the image-related inductive biases into the model, making it suitable for these tasks. We verify ViT-Adapter on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Notably, without using extra detection data, our ViT-Adapter-L yields state-of-the-art 60.9 box AP and 53.0 mask AP on COCO test-dev. We hope that the ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. Code and models will be released at https://github.com/czczup/ViT-Adapter.",https://api.openreview.net/pdf/a1a7cac48a3e0fa0d2a12b5a46c5b2463fe22a38.pdf,zero_few-shot;transformer;representation;transfer learning;segmentation;multimodal;llm,https://scholar.google.com/scholar?q=Vision+Transformer+Adapter+for+Dense+Predictions
Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors,2023,ICLR,"['Jianfei Yang', 'Xiangyu Peng', 'Kai Wang', 'Zheng Zhu', 'Jiashi Feng', 'Lihua Xie', 'Yang You']",spotlight,"['model adaptation', 'black-box predictors', 'transfer learning']","Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an unlabeled target domain supervised by a black-box predictor trained on a source domain. It does not require access to both the source-domain data and the predictor parameters, thus addressing the data privacy and portability issues of standard domain adaptation methods. Existing DABP approaches mostly rely on knowledge distillation (KD) from the black-box predictor, i.e., training the model with its noisy target-domain predictions, which however inevitably introduces the confirmation bias accumulated from the prediction noises and leads to degrading performance. To mitigate such bias, we propose a new strategy, \textit{divide-to-adapt}, that purifies cross-domain knowledge distillation by proper domain division. This is inspired by an observation we make for the first time in domain adaptation: the target domain usually contains easy-to-adapt and hard-to-adapt samples that have different levels of domain discrepancy w.r.t. the source domain, and deep models tend to fit easy-to-adapt samples first. Leveraging easy-to-adapt samples with less noise can help KD alleviate the negative effect of prediction noises from black-box predictors. In this sense, the target domain can be divided into an easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain at the early stage of training. Then the adaptation is achieved by semi-supervised learning. We further reduce distribution discrepancy between subdomains and develop weak-strong augmentation strategy to filter the predictor errors progressively. As such, our method is a simple yet effective solution to reduce error accumulation in cross-domain knowledge distillation for DABP. Moreover, we prove that the target error of DABP is bounded by the noise ratio of two subdomains, i.e., the confirmation bias, which provides the theoretical justifications for our method. Extensive experiments demonstrate our method achieves state of the art on all DABP benchmarks, outperforming the existing best approach by 7.0\% on VisDA-17, and is even comparable with the standard domain adaptation methods that use the source-domain data.",https://api.openreview.net/pdf/f6acdba3a448c8c49b38089c9fcca2175f862634.pdf,graph;zero_few-shot;augmentation;distillation;multimodal;llm,https://scholar.google.com/scholar?q=Divide+to+Adapt:+Mitigating+Confirmation+Bias+for+Domain+Adaptation+of+Black-Box+Predictors
Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language,2023,ICLR,"['Andy Zeng', 'Maria Attarian', 'brian ichter', 'Krzysztof Marcin Choromanski', 'Adrian Wong', 'Stefan Welker', 'Federico Tombari', 'Aveek Purohit', 'Michael S Ryoo', 'Vikas Sindhwani', 'Johnny Lee', 'Vincent Vanhoucke', 'Pete Florence']",spotlight,"['prompt engineering', 'multimodal applications', 'visual language models', 'large language models', 'commonsense reasoning']","We investigate how multimodal prompt engineering can use language as the intermediate representation to combine complementary knowledge from different pretrained (potentially multimodal) language models for a variety of tasks. This approach is both distinct from and complementary to the dominant paradigm of joint multimodal training. It also recalls a traditional systems-building view as in classical NLP pipelines, but with prompting large pretrained multimodal models. We refer to these as Socratic Models (SMs): a modular class of systems in which multiple pretrained models may be composed zero-shot via multimodal-informed prompting to capture new multimodal capabilities, without additional finetuning. We show that these systems provide competitive state-of-the-art performance for zero-shot image captioning and video-to-text retrieval, and also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes), and (iii) robot perception and planning. We hope this work provides (a) results for stronger zero-shot baseline performance with analysis also highlighting their limitations, (b) new perspectives for building multimodal systems powered by large pretrained models, and (c) practical application advantages in certain regimes limited by data scarcity, training compute, or model access.",https://api.openreview.net/pdf/92b6e024f8a9e971e8041aa14e06de2802245730.pdf,graph;zero_few-shot;representation;multimodal;llm,https://scholar.google.com/scholar?q=Socratic+Models:+Composing+Zero-Shot+Multimodal+Reasoning+with+Language
Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class-Incremental Learning,2023,ICLR,"['Yibo Yang', 'Haobo Yuan', 'Xiangtai Li', 'Zhouchen Lin', 'Philip Torr', 'Dacheng Tao']",spotlight,"['few-shot class-incremental learning', 'neural collapse']","Few-shot class-incremental learning (FSCIL) has been a challenging problem as only a few training samples are accessible for each novel class in the new sessions. Finetuning the backbone or adjusting the classifier prototypes trained in the prior sessions would inevitably cause a misalignment between the feature and classifier of old classes, which explains the well-known catastrophic forgetting problem. In this paper, we deal with this misalignment dilemma in FSCIL inspired by the recently discovered phenomenon named neural collapse, which reveals that the last-layer features of the same class will collapse into a vertex, and the vertices of all classes are aligned with the classifier prototypes, which are formed as a simplex equiangular tight frame (ETF). It corresponds to an optimal geometric structure for classification due to the maximized Fisher Discriminant Ratio. We propose a neural collapse inspired framework for FSCIL. A group of classifier prototypes are pre-assigned as a simplex ETF for the whole label space, including the base session and all the incremental sessions. During training, the classifier prototypes are not learnable, and we adopt a novel loss function that drives the features into their corresponding prototypes. Theoretical analysis shows that our method holds the neural collapse optimality and does not break the feature-classifier alignment in an incremental fashion. Experiments on the miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our proposed framework outperforms the state-of-the-art performances. Code address: https://github.com/NeuralCollapseApplications/FSCIL ",https://api.openreview.net/pdf/0ffbc09764bcd3fed340d49d2404429bae5277f5.pdf,graph;metric;multimodal;llm,https://scholar.google.com/scholar?q=Neural+Collapse+Inspired+Feature-Classifier+Alignment+for+Few-Shot+Class-Incremental+Learning
MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction,2023,ICLR,"['Bencheng Liao', 'Shaoyu Chen', 'Xinggang Wang', 'Tianheng Cheng', 'Qian Zhang', 'Wenyu Liu', 'Chang Huang']",spotlight,"['Autonomous Driving', 'Online Vectorized HD Map Construction', 'End-to-End']","High-definition (HD) map provides abundant and precise environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. We present MapTR, a structured end-to-end Transformer for efficient online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. MapTR achieves the best performance and efficiency with only camera input among existing vectorized map construction approaches on nuScenes dataset. In particular, MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX 3090, $8\times$ faster than the existing state-of-the-art camera-based method while achieving $5.0$ higher mAP. Even compared with the existing state-of-the-art multi-modality method, MapTR-nano achieves $0.7$ higher mAP and $8\times$ faster inference speed, and MapTR-tiny achieves $13.5$ higher mAP and $3\times$ faster inference speed. Abundant qualitative results show that MapTR maintains stable and robust map construction quality in complex and various driving scenes. MapTR is of great application value in autonomous driving. Code and more demos are available at https://github.com/hustvl/MapTR.",https://api.openreview.net/pdf/f0aa5f3818d2d071eed47bfd84263b7b217b437a.pdf,transformer;online learning;inference;multimodal,https://scholar.google.com/scholar?q=MapTR:+Structured+Modeling+and+Learning+for+Online+Vectorized+HD+Map+Construction
