title,year,source,authors,class,keywords,abstract,pdf_link,topic,google_scholar_link
UC2: Universal Cross-Lingual Cross-Modal Vision-and-Language Pre-Training,2021,CVPR,Mingyang Zhou;Luowei Zhou;Shuohang Wang;Yu Cheng;Linjie Li;Zhou Yu;Jingjing Liu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_UC2_Universal_Cross-Lingual_Cross-Modal_Vision-and-Language_Pre-Training_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=UC2:+Universal+Cross-Lingual+Cross-Modal+Vision-and-Language+Pre-Training
Can Audio-Visual Integration Strengthen Robustness Under Multimodal Attacks?,2021,CVPR,Yapeng Tian;Chenliang Xu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Tian_Can_Audio-Visual_Integration_Strengthen_Robustness_Under_Multimodal_Attacks_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Can+Audio-Visual+Integration+Strengthen+Robustness+Under+Multimodal+Attacks?
Probabilistic Embeddings for Cross-Modal Retrieval,2021,CVPR,Sanghyuk Chun;Seong Joon Oh;Rafael Sampaio de Rezende;Yannis Kalantidis;Diane Larlus,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Chun_Probabilistic_Embeddings_for_Cross-Modal_Retrieval_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Probabilistic+Embeddings+for+Cross-Modal+Retrieval
Skeleton Merger: An Unsupervised Aligned Keypoint Detector,2021,CVPR,Ruoxi Shi;Zhengrong Xue;Yang You;Cewu Lu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Skeleton_Merger_An_Unsupervised_Aligned_Keypoint_Detector_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Skeleton+Merger:+An+Unsupervised+Aligned+Keypoint+Detector
Audio-Visual Instance Discrimination with Cross-Modal Agreement,2021,CVPR,Pedro Morgado;Nuno Vasconcelos;Ishan Misra,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Morgado_Audio-Visual_Instance_Discrimination_with_Cross-Modal_Agreement_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Audio-Visual+Instance+Discrimination+with+Cross-Modal+Agreement
Cross-Domain Similarity Learning for Face Recognition in Unseen Domains,2021,CVPR,Masoud Faraki;Xiang Yu;Yi-Hsuan Tsai;Yumin Suh;Manmohan Chandraker,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Faraki_Cross-Domain_Similarity_Learning_for_Face_Recognition_in_Unseen_Domains_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Cross-Domain+Similarity+Learning+for+Face+Recognition+in+Unseen+Domains
Self-Aligned Video Deraining With Transmission-Depth Consistency,2021,CVPR,Wending Yan;Robby T. Tan;Wenhan Yang;Dengxin Dai,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_Self-Aligned_Video_Deraining_With_Transmission-Depth_Consistency_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Self-Aligned+Video+Deraining+With+Transmission-Depth+Consistency
Single Pair Cross-Modality Super Resolution,2021,CVPR,Guy Shacht;Dov Danon;Sharon Fogel;Daniel Cohen-Or,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Shacht_Single_Pair_Cross-Modality_Super_Resolution_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Single+Pair+Cross-Modality+Super+Resolution
Learning Cross-Modal Retrieval With Noisy Labels,2021,CVPR,Peng Hu;Xi Peng;Hongyuan Zhu;Liangli Zhen;Jie Lin,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Learning_Cross-Modal_Retrieval_With_Noisy_Labels_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Learning+Cross-Modal+Retrieval+With+Noisy+Labels
Defending Multimodal Fusion Models Against Single-Source Adversaries,2021,CVPR,Karren Yang;Wan-Yi Lin;Manash Barman;Filipe Condessa;Zico Kolter,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Defending_Multimodal_Fusion_Models_Against_Single-Source_Adversaries_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Defending+Multimodal+Fusion+Models+Against+Single-Source+Adversaries
Discover Cross-Modality Nuances for Visible-Infrared Person Re-Identification,2021,CVPR,Qiong Wu;Pingyang Dai;Jie Chen;Chia-Wen Lin;Yongjian Wu;Feiyue Huang;Bineng Zhong;Rongrong Ji,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Discover_Cross-Modality_Nuances_for_Visible-Infrared_Person_Re-Identification_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Discover+Cross-Modality+Nuances+for+Visible-Infrared+Person+Re-Identification
Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation,2021,CVPR,Zhekai Du;Jingjing Li;Hongzu Su;Lei Zhu;Ke Lu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Du_Cross-Domain_Gradient_Discrepancy_Minimization_for_Unsupervised_Domain_Adaptation_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Cross-Domain+Gradient+Discrepancy+Minimization+for+Unsupervised+Domain+Adaptation
Robust Multimodal Vehicle Detection in Foggy Weather Using Complementary Lidar and Radar Signals,2021,CVPR,Kun Qian;Shilin Zhu;Xinyu Zhang;Li Erran Li,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Qian_Robust_Multimodal_Vehicle_Detection_in_Foggy_Weather_Using_Complementary_Lidar_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Robust+Multimodal+Vehicle+Detection+in+Foggy+Weather+Using+Complementary+Lidar+and+Radar+Signals
VisualVoice: Audio-Visual Speech Separation With Cross-Modal Consistency,2021,CVPR,Ruohan Gao;Kristen Grauman,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_VisualVoice_Audio-Visual_Speech_Separation_With_Cross-Modal_Consistency_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=VisualVoice:+Audio-Visual+Speech+Separation+With+Cross-Modal+Consistency
StEP: Style-Based Encoder Pre-Training for Multi-Modal Image Synthesis,2021,CVPR,Moustafa Meshry;Yixuan Ren;Larry S. Davis;Abhinav Shrivastava,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Meshry_StEP_Style-Based_Encoder_Pre-Training_for_Multi-Modal_Image_Synthesis_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=StEP:+Style-Based+Encoder+Pre-Training+for+Multi-Modal+Image+Synthesis
Unbiased Mean Teacher for Cross-Domain Object Detection,2021,CVPR,Jinhong Deng;Wen Li;Yuhua Chen;Lixin Duan,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_Unbiased_Mean_Teacher_for_Cross-Domain_Object_Detection_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Unbiased+Mean+Teacher+for+Cross-Domain+Object+Detection
How2Sign: A Large-Scale Multimodal Dataset for Continuous American Sign Language,2021,CVPR,Amanda Duarte;Shruti Palaskar;Lucas Ventura;Deepti Ghadiyaram;Kenneth DeHaan;Florian Metze;Jordi Torres;Xavier Giro-i-Nieto,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Duarte_How2Sign_A_Large-Scale_Multimodal_Dataset_for_Continuous_American_Sign_Language_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=How2Sign:+A+Large-Scale+Multimodal+Dataset+for+Continuous+American+Sign+Language
Shared Cross-Modal Trajectory Prediction for Autonomous Driving,2021,CVPR,Chiho Choi;Joon Hee Choi;Jiachen Li;Srikanth Malla,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Choi_Shared_Cross-Modal_Trajectory_Prediction_for_Autonomous_Driving_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Shared+Cross-Modal+Trajectory+Prediction+for+Autonomous+Driving
Geo-FARM: Geodesic Factor Regression Model for Misaligned Pre-Shape Responses in Statistical Shape Analysis,2021,CVPR,Chao Huang;Anuj Srivastava;Rongjie Liu,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Geo-FARM_Geodesic_Factor_Regression_Model_for_Misaligned_Pre-Shape_Responses_in_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Geo-FARM:+Geodesic+Factor+Regression+Model+for+Misaligned+Pre-Shape+Responses+in+Statistical+Shape+Analysis
Looking Into Your Speech: Learning Cross-Modal Affinity for Audio-Visual Speech Separation,2021,CVPR,Jiyoung Lee;Soo-Whan Chung;Sunok Kim;Hong-Goo Kang;Kwanghoon Sohn,,,,https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Looking_Into_Your_Speech_Learning_Cross-Modal_Affinity_for_Audio-Visual_Speech_CVPR_2021_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Looking+Into+Your+Speech:+Learning+Cross-Modal+Affinity+for+Audio-Visual+Speech+Separation
Non-robust Features through the Lens of Universal Perturbations,2021,ICLR,"['Sung Min Park', 'Kuo-An Wei', 'Kai Yuanqing Xiao', 'Jerry Li', 'Aleksander Madry']",poster,"['adversarial examples', 'robustness', 'non-robust features']","Recent work ties adversarial examples to existence of non-robust features: features which are susceptible to small perturbations and believed to be unintelligible to humans, but still useful for prediction. We study universal adversarial perturbations and demonstrate that the above picture is more nuanced. Specifically, even though universal perturbations---similarly to standard adversarial perturbations---do leverage non-robust features, these features tend to be fundamentally different from the ``standard'' ones and, in particular, non-trivially human-aligned. Namely, universal perturbations have more human-aligned locality and spatial invariance properties. However, we also show that these human-aligned non-robust features have much less predictive signal than general non-robust features. Our findings thus take a step towards improving our understanding of these previously unintelligible features.",/pdf/b01ee46313ef48630aa6d3cb38400866db43ec8b.pdf,multimodal,https://scholar.google.com/scholar?q=Non-robust+Features+through+the+Lens+of+Universal+Perturbations
A Multi-Modal and Multitask Benchmark in the Clinical Domain,2021,ICLR,"['Yong Huang', 'Edgar Mariano Marroquin', 'Volodymyr Kuleshov']",poster,"['multi-modal', 'multitask', 'machine learning in healthcare', 'benchmark']","Healthcare represents one of the most promising application areas for machine learning algorithms,  including modern methods based on deep learning.   Modern deep learning algorithms perform best on large datasets and on unstructured modalities such as text or image data; advances in deep learning have often been driven by the availability of such large datasets. Here, we introduce Multi-Modal Multitask MIMIC-III (M3) — a dataset and benchmark for evaluating machine learning algorithms in the healthcare domain.  This dataset contains multi-modal patient data collected from intensive care units — including physiological time series, clinical notes, ECG waveforms, and tabular inputs — and defines six clinical tasks — including predicting mortality, decompensation, readmission, and other outcomes — which serve as benchmarks for comparing algorithms. We introduce new multi-modal and multitask models for this dataset, and show that they outperform previous state-of-the-art results that only rely on a subset of all tasks and modalities. This highlights the potential of multitask and multi-modal learning to improve the performance of algorithms in the healthcare domain. More generally, we envision M3 as a general resource that will help accelerate research in applying machine learning to healthcare.",/pdf/106229493bb858b25794f5f3ac77ecab060af02d.pdf,multimodal,https://scholar.google.com/scholar?q=A+Multi-Modal+and+Multitask+Benchmark+in+the+Clinical+Domain
Batch Inverse-Variance Weighting: Deep Heteroscedastic Regression,2021,ICLR,"['Vincent Mai', 'Waleed Khamies', 'Liam Paull']",poster,"['Regression', 'Noisy labels', 'Supervised Learning', 'Uncertainty', 'Variance', 'Heteroscedastic', 'Privileged Information']","In model learning, when the training dataset on which the parameters are optimized and the testing dataset on which the model is evaluated are not sampled from identical distributions, we say that the datasets are misaligned. It is well-known that this misalignment can negatively impact model performance. A common source of misalignment is that the inputs are sampled from different distributions. Another source for this misalignment is that the label generating process used to create the training dataset is imperfect. In this work, we consider this setting and additionally assume that the label generating process is able to provide us with a quantity for the role of each label in the misalignment between the datasets, which we consider to be privileged information. Specifically, we consider the task of regression with labels corrupted by heteroscedastic noise and we assume that we have access to an estimate of the variance over each sample. We propose a general approach to include this privileged information in the loss function together with dataset statistics inferred from the mini-batch to mitigate the impact of the dataset misalignment. Subsequently, we propose a specific algorithm for the heteroscedastic regression case, called Batch Inverse-Variance weighting, which adapts inverse-variance weighting for linear regression to the case of neural network function approximation.  We demonstrate that this approach achieves a significant improvement in network training performances compared to baselines when confronted with high, input-independent noise.",/pdf/3b8feb4acb73a876bf25c4c28aea3d5d339d376d.pdf,multimodal,https://scholar.google.com/scholar?q=Batch+Inverse-Variance+Weighting:+Deep+Heteroscedastic+Regression
Neural Nonnegative CP Decomposition for Hierarchical Tensor Analysis,2021,ICLR,"['Joshua Vendrow', 'Jamie Haddock', 'Deanna Needell']",poster,"['nonnegative tensor decompositions', 'topic modeling', 'hierarchical model', 'CP decomposition', 'neural network', 'backpropagation']","There is a significant demand for topic modeling on large-scale data with complex multi-modal structure in applications such as multi-layer network analysis, temporal document classification, and video data analysis; frequently this multi-modal data has latent hierarchical structure. We propose a new hierarchical nonnegative CANDECOMP/PARAFAC (CP) decomposition (hierarchical NCPD) model and a training method, Neural NCPD, for performing hierarchical topic modeling on multi-modal tensor data. Neural NCPD utilizes a neural network architecture and backpropagation to mitigate error propagation through hierarchical NCPD.  ",/pdf/5ff44b48db470aff2d5a68a8ad106eee6fb065a6.pdf,multimodal,https://scholar.google.com/scholar?q=Neural+Nonnegative+CP+Decomposition+for+Hierarchical+Tensor+Analysis
Convergence and Alignment of Gradient Descent with Random Backpropagation Weights,2021,NIPS,"['Ganlin Song', 'Ruitu Xu', 'John Lafferty']",poster,"['feedback alignment', 'deep learning']","Stochastic gradient descent with backpropagation is the workhorse of artificial neural networks. It has long been recognized that backpropagation fails to be a biologically plausible algorithm. Fundamentally, it is a non-local procedure---updating one neuron's synaptic weights requires knowledge of synaptic weights or receptive fields of downstream neurons. This limits the use of artificial neural networks as a tool for understanding the biological principles of information processing in the brain. Lillicrap et al. (2016) propose a more biologically plausible ""feedback alignment"" algorithm that uses random and fixed backpropagation weights, and show promising simulations. In this paper we study the mathematical properties of the feedback alignment procedure by analyzing convergence and alignment for two-layer networks under squared error loss. In the overparameterized setting, we prove that the error converges to zero exponentially fast, and also that regularization is necessary in order for the  parameters to become aligned with the random backpropagation weights. Simulations are given that are consistent with this analysis and suggest further generalizations. These results contribute to our understanding of how biologically plausible algorithms might carry out weight learning in a manner different from Hebbian learning, with performance that is comparable with the full non-local backpropagation algorithm.",https://api.openreview.net/pdf/1e4da80921a23cef6b1fc28bab06070662a8377c.pdf,multimodal,https://scholar.google.com/scholar?q=Convergence+and+Alignment+of+Gradient+Descent+with+Random+Backpropagation+Weights
Predicting Event Memorability from Contextual Visual Semantics,2021,NIPS,"['Qianli Xu', 'Fen Fang', 'Ana Garcia del Molino', 'Vigneshwaran Subbaraju', 'Joo Hwee Lim']",poster,"['Event memory', 'image memorability', 'visual semantics', 'episodic memory', 'lifelog']","Episodic event memory is a key component of human cognition. Predicting event memorability,i.e., to what extent an event is recalled, is a tough challenge in memory research and has profound implications for artificial intelligence. In this study, we investigate factors that affect event memorability according to a cued recall process. Specifically, we explore whether event memorability is contingent on the event context, as well as the intrinsic visual attributes of image cues. We design a novel experiment protocol and conduct a large-scale experiment with 47 elder subjects over 3 months.  Subjects’ memory of life events is tested in a cued recall process. Using advanced visual analytics methods, we build a first-of-its-kind event memorability dataset (called R3) with rich information about event context and visual semantic features. Furthermore, we propose a contextual event memory network (CEMNet) that tackles multi-modal input to predict item-wise event memorability, which outperforms competitive benchmarks.  The findings inform deeper understanding of episodic event memory, and open up a new avenue for prediction of human episodic memory.  Source code is available at https://github.com/ffzzy840304/Predicting-Event-Memorability.",https://api.openreview.net/pdf/5b7671f40a81b5ee4256a53fa672499fef577a50.pdf,multimodal,https://scholar.google.com/scholar?q=Predicting+Event+Memorability+from+Contextual+Visual+Semantics
Neural Dubber: Dubbing for Videos According to Scripts,2021,NIPS,"['Chenxu Hu', 'Qiao Tian', 'Tingle Li', 'Wang Yuping', 'Yuxuan Wang', 'Hang Zhao']",poster,"['text to speech', 'speech', 'audio-visual', 'synchronization', 'multi-modal']","Dubbing is a post-production process of re-recording actors’ dialogues, which is extensively used in filmmaking and video production. It is usually performed manually by professional voice actors who read lines with proper prosody, and in synchronization with the pre-recorded videos. In this work, we propose Neural Dubber, the first neural network model to solve a novel automatic video dubbing (AVD) task: synthesizing human speech synchronized with the given video from the text. Neural Dubber is a multi-modal text-to-speech (TTS) model that utilizes the lip movement in the video to control the prosody of the generated speech. Furthermore, an image-based speaker embedding (ISE) module is developed for the multi-speaker setting, which enables Neural Dubber to generate speech with a reasonable timbre according to the speaker’s face. Experiments on the chemistry lecture single-speaker dataset and LRS2 multi-speaker dataset show that Neural Dubber can generate speech audios on par with state-of-the-art TTS models in terms of speech quality. Most importantly, both qualitative and quantitative evaluations show that Neural Dubber can control the prosody of synthesized speech by the video, and generate high-fidelity speech temporally synchronized with the video.",https://api.openreview.net/pdf/26620b90db87b2cde6cbaa93c5886ce819821421.pdf,multimodal,https://scholar.google.com/scholar?q=Neural+Dubber:+Dubbing+for+Videos+According+to+Scripts
Cross-Modal Map Learning for Vision and Language Navigation,2022,CVPR,Georgios Georgakis;Karl Schmeckpeper;Karan Wanchoo;Soham Dan;Eleni Miltsakaki;Dan Roth;Kostas Daniilidis,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Georgakis_Cross-Modal_Map_Learning_for_Vision_and_Language_Navigation_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Map+Learning+for+Vision+and+Language+Navigation
Accelerating DETR Convergence via Semantic-Aligned Matching,2022,CVPR,Gongjie Zhang;Zhipeng Luo;Yingchen Yu;Kaiwen Cui;Shijian Lu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Accelerating_DETR_Convergence_via_Semantic-Aligned_Matching_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Accelerating+DETR+Convergence+via+Semantic-Aligned+Matching
Versatile Multi-Modal Pre-Training for Human-Centric Perception,2022,CVPR,Fangzhou Hong;Liang Pan;Zhongang Cai;Ziwei Liu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_Versatile_Multi-Modal_Pre-Training_for_Human-Centric_Perception_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Versatile+Multi-Modal+Pre-Training+for+Human-Centric+Perception
Lite-MDETR: A Lightweight Multi-Modal Detector,2022,CVPR,Qian Lou;Yen-Chang Hsu;Burak Uzkent;Ting Hua;Yilin Shen;Hongxia Jin,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lou_Lite-MDETR_A_Lightweight_Multi-Modal_Detector_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Lite-MDETR:+A+Lightweight+Multi-Modal+Detector
XYLayoutLM: Towards Layout-Aware Multimodal Networks for Visually-Rich Document Understanding,2022,CVPR,Zhangxuan Gu;Changhua Meng;Ke Wang;Jun Lan;Weiqiang Wang;Ming Gu;Liqing Zhang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_XYLayoutLM_Towards_Layout-Aware_Multimodal_Networks_for_Visually-Rich_Document_Understanding_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=XYLayoutLM:+Towards+Layout-Aware+Multimodal+Networks+for+Visually-Rich+Document+Understanding
STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes,2022,CVPR,Peishan Cong;Xinge Zhu;Feng Qiao;Yiming Ren;Xidong Peng;Yuenan Hou;Lan Xu;Ruigang Yang;Dinesh Manocha;Yuexin Ma,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Cong_STCrowd_A_Multimodal_Dataset_for_Pedestrian_Perception_in_Crowded_Scenes_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=STCrowd:+A+Multimodal+Dataset+for+Pedestrian+Perception+in+Crowded+Scenes
Towards Multimodal Depth Estimation From Light Fields,2022,CVPR,Titus Leistner;Radek Mackowiak;Lynton Ardizzone;Ullrich Köthe;Carsten Rother,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Leistner_Towards_Multimodal_Depth_Estimation_From_Light_Fields_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Towards+Multimodal+Depth+Estimation+From+Light+Fields
Tencent-MVSE: A Large-Scale Benchmark Dataset for Multi-Modal Video Similarity Evaluation,2022,CVPR,Zhaoyang Zeng;Yongsheng Luo;Zhenhua Liu;Fengyun Rao;Dian Li;Weidong Guo;Zhen Wen,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_Tencent-MVSE_A_Large-Scale_Benchmark_Dataset_for_Multi-Modal_Video_Similarity_Evaluation_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Tencent-MVSE:+A+Large-Scale+Benchmark+Dataset+for+Multi-Modal+Video+Similarity+Evaluation
RFNet: Unsupervised Network for Mutually Reinforcing Multi-Modal Image Registration and Fusion,2022,CVPR,Han Xu;Jiayi Ma;Jiteng Yuan;Zhuliang Le;Wei Liu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_RFNet_Unsupervised_Network_for_Mutually_Reinforcing_Multi-Modal_Image_Registration_and_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=RFNet:+Unsupervised+Network+for+Mutually+Reinforcing+Multi-Modal+Image+Registration+and+Fusion
Target-Aware Dual Adversarial Learning and a Multi-Scenario Multi-Modality Benchmark To Fuse Infrared and Visible for Object Detection,2022,CVPR,Jinyuan Liu;Xin Fan;Zhanbo Huang;Guanyao Wu;Risheng Liu;Wei Zhong;Zhongxuan Luo,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Target-Aware_Dual_Adversarial_Learning_and_a_Multi-Scenario_Multi-Modality_Benchmark_To_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Target-Aware+Dual+Adversarial+Learning+and+a+Multi-Scenario+Multi-Modality+Benchmark+To+Fuse+Infrared+and+Visible+for+Object+Detection
Balanced Multimodal Learning via On-the-Fly Gradient Modulation,2022,CVPR,Xiaokang Peng;Yake Wei;Andong Deng;Dong Wang;Di Hu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Peng_Balanced_Multimodal_Learning_via_On-the-Fly_Gradient_Modulation_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Balanced+Multimodal+Learning+via+On-the-Fly+Gradient+Modulation
Exploring Endogenous Shift for Cross-Domain Detection: A Large-Scale Benchmark and Perturbation Suppression Network,2022,CVPR,Renshuai Tao;Hainan Li;Tianbo Wang;Yanlu Wei;Yifu Ding;Bowei Jin;Hongping Zhi;Xianglong Liu;Aishan Liu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_Exploring_Endogenous_Shift_for_Cross-Domain_Detection_A_Large-Scale_Benchmark_and_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Exploring+Endogenous+Shift+for+Cross-Domain+Detection:+A+Large-Scale+Benchmark+and+Perturbation+Suppression+Network
MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-Based Visual Question Answering,2022,CVPR,Yang Ding;Jing Yu;Bang Liu;Yue Hu;Mingxin Cui;Qi Wu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_MuKEA_Multimodal_Knowledge_Extraction_and_Accumulation_for_Knowledge-Based_Visual_Question_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=MuKEA:+Multimodal+Knowledge+Extraction+and+Accumulation+for+Knowledge-Based+Visual+Question+Answering
WebQA: Multihop and Multimodal QA,2022,CVPR,Yingshan Chang;Mridu Narang;Hisami Suzuki;Guihong Cao;Jianfeng Gao;Yonatan Bisk,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Chang_WebQA_Multihop_and_Multimodal_QA_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=WebQA:+Multihop+and+Multimodal+QA
RADU: Ray-Aligned Depth Update Convolutions for ToF Data Denoising,2022,CVPR,Michael Schelling;Pedro Hermosilla;Timo Ropinski,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Schelling_RADU_Ray-Aligned_Depth_Update_Convolutions_for_ToF_Data_Denoising_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=RADU:+Ray-Aligned+Depth+Update+Convolutions+for+ToF+Data+Denoising
Learning Based Multi-Modality Image and Video Compression,2022,CVPR,Guo Lu;Tianxiong Zhong;Jing Geng;Qiang Hu;Dong Xu,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Learning_Based_Multi-Modality_Image_and_Video_Compression_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Learning+Based+Multi-Modality+Image+and+Video+Compression
Cross-Modal Perceptionist: Can Face Geometry Be Gleaned From Voices?,2022,CVPR,Cho-Ying Wu;Chin-Cheng Hsu;Ulrich Neumann,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Cross-Modal_Perceptionist_Can_Face_Geometry_Be_Gleaned_From_Voices_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Perceptionist:+Can+Face+Geometry+Be+Gleaned+From+Voices?
On Generalizing Beyond Domains in Cross-Domain Continual Learning,2022,CVPR,Christian Simon;Masoud Faraki;Yi-Hsuan Tsai;Xiang Yu;Samuel Schulter;Yumin Suh;Mehrtash Harandi;Manmohan Chandraker,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Simon_On_Generalizing_Beyond_Domains_in_Cross-Domain_Continual_Learning_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=On+Generalizing+Beyond+Domains+in+Cross-Domain+Continual+Learning
Dual-Key Multimodal Backdoors for Visual Question Answering,2022,CVPR,Matthew Walmer;Karan Sikka;Indranil Sur;Abhinav Shrivastava;Susmit Jha,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Walmer_Dual-Key_Multimodal_Backdoors_for_Visual_Question_Answering_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Dual-Key+Multimodal+Backdoors+for+Visual+Question+Answering
Multimodal Colored Point Cloud to Image Alignment,2022,CVPR,Noam Rotstein;Amit Bracha;Ron Kimmel,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Rotstein_Multimodal_Colored_Point_Cloud_to_Image_Alignment_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Multimodal+Colored+Point+Cloud+to+Image+Alignment
CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data,2022,CVPR,Qi Yan;Jianhao Zheng;Simon Reding;Shanci Li;Iordan Doytchinov,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_CrossLoc_Scalable_Aerial_Localization_Assisted_by_Multimodal_Synthetic_Data_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=CrossLoc:+Scalable+Aerial+Localization+Assisted+by+Multimodal+Synthetic+Data
Text2Pos: Text-to-Point-Cloud Cross-Modal Localization,2022,CVPR,Manuel Kolmet;Qunjie Zhou;Aljoša Ošep;Laura Leal-Taixé,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Kolmet_Text2Pos_Text-to-Point-Cloud_Cross-Modal_Localization_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Text2Pos:+Text-to-Point-Cloud+Cross-Modal+Localization
Multimodal Dynamics: Dynamical Fusion for Trustworthy Multimodal Classification,2022,CVPR,Zongbo Han;Fan Yang;Junzhou Huang;Changqing Zhang;Jianhua Yao,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Multimodal_Dynamics_Dynamical_Fusion_for_Trustworthy_Multimodal_Classification_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Multimodal+Dynamics:+Dynamical+Fusion+for+Trustworthy+Multimodal+Classification
LAKe-Net: Topology-Aware Point Cloud Completion by Localizing Aligned Keypoints,2022,CVPR,Junshu Tang;Zhijun Gong;Ran Yi;Yuan Xie;Lizhuang Ma,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_LAKe-Net_Topology-Aware_Point_Cloud_Completion_by_Localizing_Aligned_Keypoints_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=LAKe-Net:+Topology-Aware+Point+Cloud+Completion+by+Localizing+Aligned+Keypoints
Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos,2022,CVPR,Saghir Alfasly;Jian Lu;Chen Xu;Yuru Zou,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Alfasly_Learnable_Irrelevant_Modality_Dropout_for_Multimodal_Action_Recognition_on_Modality-Specific_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Learnable+Irrelevant+Modality+Dropout+for+Multimodal+Action+Recognition+on+Modality-Specific+Annotated+Videos
Reading To Listen at the Cocktail Party: Multi-Modal Speech Separation,2022,CVPR,Akam Rahimi;Triantafyllos Afouras;Andrew Zisserman,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Rahimi_Reading_To_Listen_at_the_Cocktail_Party_Multi-Modal_Speech_Separation_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Reading+To+Listen+at+the+Cocktail+Party:+Multi-Modal+Speech+Separation
Cross-Modal Background Suppression for Audio-Visual Event Localization,2022,CVPR,Yan Xia;Zhou Zhao,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Xia_Cross-Modal_Background_Suppression_for_Audio-Visual_Event_Localization_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Cross-Modal+Background+Suppression+for+Audio-Visual+Event+Localization
Mutual Quantization for Cross-Modal Search With Noisy Labels,2022,CVPR,Erkun Yang;Dongren Yao;Tongliang Liu;Cheng Deng,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Mutual_Quantization_for_Cross-Modal_Search_With_Noisy_Labels_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Mutual+Quantization+for+Cross-Modal+Search+With+Noisy+Labels
"3MASSIV: Multilingual, Multimodal and Multi-Aspect Dataset of Social Media Short Videos",2022,CVPR,Vikram Gupta;Trisha Mittal;Puneet Mathur;Vaibhav Mishra;Mayank Maheshwari;Aniket Bera;Debdoot Mukherjee;Dinesh Manocha,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_3MASSIV_Multilingual_Multimodal_and_Multi-Aspect_Dataset_of_Social_Media_Short_CVPR_2022_paper.pdf,multimodal,"https://scholar.google.com/scholar?q=3MASSIV:+Multilingual,+Multimodal+and+Multi-Aspect+Dataset+of+Social+Media+Short+Videos"
Multi-Modal Extreme Classification,2022,CVPR,Anshul Mittal;Kunal Dahiya;Shreya Malani;Janani Ramaswamy;Seba Kuruvilla;Jitendra Ajmera;Keng-hao Chang;Sumeet Agarwal;Purushottam Kar;Manik Varma,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Mittal_Multi-Modal_Extreme_Classification_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Multi-Modal+Extreme+Classification
Motron: Multimodal Probabilistic Human Motion Forecasting,2022,CVPR,Tim Salzmann;Marco Pavone;Markus Ryll,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Salzmann_Motron_Multimodal_Probabilistic_Human_Motion_Forecasting_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Motron:+Multimodal+Probabilistic+Human+Motion+Forecasting
COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval,2022,CVPR,Haoyu Lu;Nanyi Fei;Yuqi Huo;Yizhao Gao;Zhiwu Lu;Ji-Rong Wen,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_COTS_Collaborative_Two-Stream_Vision-Language_Pre-Training_Model_for_Cross-Modal_Retrieval_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=COTS:+Collaborative+Two-Stream+Vision-Language+Pre-Training+Model+for+Cross-Modal+Retrieval
ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval,2022,CVPR,Mengjun Cheng;Yipeng Sun;Longchao Wang;Xiongwei Zhu;Kun Yao;Jie Chen;Guoli Song;Junyu Han;Jingtuo Liu;Errui Ding;Jingdong Wang,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_ViSTA_Vision_and_Scene_Text_Aggregation_for_Cross-Modal_Retrieval_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=ViSTA:+Vision+and+Scene+Text+Aggregation+for+Cross-Modal+Retrieval
CroMo: Cross-Modal Learning for Monocular Depth Estimation,2022,CVPR,Yannick Verdié;Jifei Song;Barnabé Mas;Benjamin Busam;Ales̆ Leonardis;Steven McDonagh,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Verdie_CroMo_Cross-Modal_Learning_for_Monocular_Depth_Estimation_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=CroMo:+Cross-Modal+Learning+for+Monocular+Depth+Estimation
Polymorphic-GAN: Generating Aligned Samples Across Multiple Domains With Learned Morph Maps,2022,CVPR,Seung Wook Kim;Karsten Kreis;Daiqing Li;Antonio Torralba;Sanja Fidler,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_Polymorphic-GAN_Generating_Aligned_Samples_Across_Multiple_Domains_With_Learned_Morph_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Polymorphic-GAN:+Generating+Aligned+Samples+Across+Multiple+Domains+With+Learned+Morph+Maps
VisualHow: Multimodal Problem Solving,2022,CVPR,Jinhui Yang;Xianyu Chen;Ming Jiang;Shi Chen;Louis Wang;Qi Zhao,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_VisualHow_Multimodal_Problem_Solving_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=VisualHow:+Multimodal+Problem+Solving
Bi-Level Alignment for Cross-Domain Crowd Counting,2022,CVPR,Shenjian Gong;Shanshan Zhang;Jian Yang;Dengxin Dai;Bernt Schiele,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Gong_Bi-Level_Alignment_for_Cross-Domain_Crowd_Counting_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Bi-Level+Alignment+for+Cross-Domain+Crowd+Counting
Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning,2022,CVPR,Ligong Han;Jian Ren;Hsin-Ying Lee;Francesco Barbieri;Kyle Olszewski;Shervin Minaee;Dimitris Metaxas;Sergey Tulyakov,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Han_Show_Me_What_and_Tell_Me_How_Video_Synthesis_via_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Show+Me+What+and+Tell+Me+How:+Video+Synthesis+via+Multimodal+Conditioning
Expanding Large Pre-Trained Unimodal Models With Multimodal Information Injection for Image-Text Multimodal Classification,2022,CVPR,Tao Liang;Guosheng Lin;Mingyang Wan;Tianrui Li;Guojun Ma;Fengmao Lv,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Liang_Expanding_Large_Pre-Trained_Unimodal_Models_With_Multimodal_Information_Injection_for_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Expanding+Large+Pre-Trained+Unimodal+Models+With+Multimodal+Information+Injection+for+Image-Text+Multimodal+Classification
JIFF: Jointly-Aligned Implicit Face Function for High Quality Single View Clothed Human Reconstruction,2022,CVPR,Yukang Cao;Guanying Chen;Kai Han;Wenqi Yang;Kwan-Yee K. Wong,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_JIFF_Jointly-Aligned_Implicit_Face_Function_for_High_Quality_Single_View_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=JIFF:+Jointly-Aligned+Implicit+Face+Function+for+High+Quality+Single+View+Clothed+Human+Reconstruction
Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning,2022,CVPR,Chia-Wen Kuo;Zsolt Kira,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Kuo_Beyond_a_Pre-Trained_Object_Detector_Cross-Modal_Textual_and_Visual_Context_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Beyond+a+Pre-Trained+Object+Detector:+Cross-Modal+Textual+and+Visual+Context+for+Image+Captioning
Egocentric Scene Understanding via Multimodal Spatial Rectifier,2022,CVPR,Tien Do;Khiem Vuong;Hyun Soo Park,,,,https://openaccess.thecvf.com/content/CVPR2022/papers/Do_Egocentric_Scene_Understanding_via_Multimodal_Spatial_Rectifier_CVPR_2022_paper.pdf,multimodal,https://scholar.google.com/scholar?q=Egocentric+Scene+Understanding+via+Multimodal+Spatial+Rectifier
MACK: Multimodal Aligned Conceptual Knowledge for Unpaired Image-text Matching,2022,NIPS,"['Yan Huang', 'Yuming Wang', 'Yunan Zeng', 'Liang Wang']",poster,"['image-text matching', 'multimodal knowledge']","Recently, the accuracy of image-text matching has been greatly improved by multimodal pretrained models, all of which are trained on millions or billions of paired images and texts. Different from them, this paper studies a new scenario as unpaired image-text matching, in which paired images and texts are assumed to be unavailable during model training. To deal with this, we propose a simple yet effective method namely Multimodal Aligned Conceptual Knowledge (MACK), which is inspired by the knowledge use in human brain. It can be directly used as general knowledge to correlate images and texts even without model training, or further fine-tuned based on unpaired images and texts to better generalize to certain datasets. In addition, we extend it as a re-ranking method, which can be easily combined with existing image-text matching models to substantially improve their performance.",https://api.openreview.net/pdf/f33661ec0a37ed32a1dd3f69638f81fb417e9aeb.pdf,multimodal,https://scholar.google.com/scholar?q=MACK:+Multimodal+Aligned+Conceptual+Knowledge+for+Unpaired+Image-text+Matching
A Closer Look at the Adversarial Robustness of Deep Equilibrium Models,2022,NIPS,"['Zonghan Yang', 'Tianyu Pang', 'Yang Liu']",poster,[],"Deep equilibrium models (DEQs) refrain from the traditional layer-stacking paradigm and turn to find the fixed point of a single layer. DEQs have achieved promising performance on different applications with featured memory efficiency. At the same time, the adversarial vulnerability of DEQs raises concerns. Several works propose to certify robustness for monotone DEQs. However, limited efforts are devoted to studying empirical robustness for general DEQs. To this end, we observe that an adversarially trained DEQ requires more forward steps to arrive at the equilibrium state, or even violates its fixed-point structure. Besides, the forward and backward tracks of DEQs are misaligned due to the black-box solvers. These facts cause gradient obfuscation when applying the ready-made attacks to evaluate or adversarially train DEQs. Given this, we develop approaches to estimate the intermediate gradients of DEQs and integrate them into the attacking pipelines. Our approaches facilitate fully white-box evaluations and lead to effective adversarial defense for DEQs. Extensive experiments on CIFAR-10 validate the adversarial robustness of DEQs competitive with deep networks of similar sizes.",https://api.openreview.net/pdf/40f9c93ce28c6d9f311c28845daa95a34bab59a6.pdf,multimodal,https://scholar.google.com/scholar?q=A+Closer+Look+at+the+Adversarial+Robustness+of+Deep+Equilibrium+Models
Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,2022,NIPS,"['Pan Lu', 'Swaroop Mishra', 'Tony Xia', 'Liang Qiu', 'Kai-Wei Chang', 'Song-Chun Zhu', 'Oyvind Tafjord', 'Peter Clark', 'Ashwin Kalyan']",poster,"['science question answering', 'multimodal reasoning', 'chain of thought']","When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https://scienceqa.github.io.",https://api.openreview.net/pdf/4ce5e8e5119e4445ef249b95def0241cc497e169.pdf,multimodal,https://scholar.google.com/scholar?q=Learn+to+Explain:+Multimodal+Reasoning+via+Thought+Chains+for+Science+Question+Answering
Using Language to Extend to Unseen Domains,2023,ICLR,"['Lisa Dunlap', 'Clara Mohri', 'Devin Guillory', 'Han Zhang', 'Trevor Darrell', 'Joseph E. Gonzalez', 'Aditi Raghunathan', 'Anna Rohrbach']",spotlight,"['vision and language', 'robust training', 'domain adaptation']","It is expensive to collect training data for every possible domain that a vision model may encounter when deployed. We instead consider how simply $\textit{verbalizing}$ the training domain (e.g.``photos of birds'') as well as domains we want to extend to but do not have data for (e.g.``paintings of birds'') can improve robustness. Using a multimodal model with a joint image and language embedding space, our method $\textit{LADS}$ learns a transformation of the image embeddings from the source domain to each target domain, while preserving task relevant information. Without using any images from the target domain, we show that over the $\textit{extended}$ domain containing both source and target, $\textit{LADS}$ outperforms standard fine-tuning and ensemble approaches over a suite of 4 benchmarks targeting domain adaptation and dataset bias.",https://api.openreview.net/pdf/bb5314efba6d37a2ea4f8cdbdeccd9351dde3016.pdf,multimodal,https://scholar.google.com/scholar?q=Using+Language+to+Extend+to+Unseen+Domains
